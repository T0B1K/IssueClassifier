<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>classifier.vectorizer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>classifier.vectorizer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import WordNetLemmatizer, PorterStemmer
# nltk.download(&#39;wordnet&#39;)
import logging

import joblib
import numpy
import file_manipulation
import load_classifier
import configuration

config = configuration.Configuration()

class Vectorizer(file_manipulation.FileManipulation):
    &#34;&#34;&#34;This class is used for creating / loading an Vectorizer, which is used to create feature vectors out of the documents

    Args:
        file_manipulation (FileManipulation): This class is used to access data from documents as well as to save them later on
    &#34;&#34;&#34;

    def __init__(self, ngram: tuple = (1, 2), stripAccents=None, stopWords=None):
        &#34;&#34;&#34;This is the constructor for the Vectorrizer class

        Args:
            ngram (tuple, optional): how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).
            stripAccents (str, optional): Thich accents should be stripped &#39;ASCII&#39;, &#39;UNICODE&#39;. Defaults to None.
            stopWords (list, optional): Which stopwords should be removed i.e. {&#39;english&#39;}. Defaults to None.
        &#34;&#34;&#34;
        
        super().__init__()
        self.Vecotrizer = self.prepareVectorizer(
            stripAccents, ngram, stopWords)

    def stemmer(self, text: list) -&gt; list:
        &#34;&#34;&#34;This method is used for stemming tokens. I.e. cat should identify such strings as cats, catlike, and catty

        Args:
            text (list): a list of texts / documents

        Returns:
            list: the same document list, but the tokens are now stemmed
        &#34;&#34;&#34;
        
        return [PorterStemmer().stem(token) for token in text]

    def lemmatizer(self, text: list) -&gt; list:
        &#34;&#34;&#34;!Attention - currently not in use - just used for performance testing! TODO remove in final version 
        This method can be used for lemmatizing tokens. I.e.  &#34;better&#34; is mapped to &#34;good&#34; or &#34;walking&#34; to &#34;walk&#34;

         Args:
            text (list): a list of texts / documents

        Returns:
            list: the same document list, but the tokens are now stemmed
        &#34;&#34;&#34;
        
        return [WordNetLemmatizer().lemmatize(token) for token in text]

    def createFeatureVectors(self, X_train_documents: numpy.ndarray, X_test_documents: numpy.ndarray) -&gt; tuple:
        &#34;&#34;&#34;This method is used for the vectorisation of the training and testing documents (creating tf-idf, ngram vectors out of the data)
        So it is basically creating a vector (number vector) out of the trainingsdata (string vector) and removes the as the stopwords, emojis, ... if selected

        Args:
            X_train_documents (numpy.ndarray): List of unvectorized text documents
            X_test_documents (numpy.ndarray): List of unvectorized text documents

        Returns:
            tuple:  X_train_vectorized (numpy.ndarray): List of tfidf weighted / vectorized test documents
                    X_test_vectorized  (numpy.ndarray): List of tfidf weighted / vectorized train documents
        &#34;&#34;&#34;

        X_train_vectorized: scipy.sparse.csr.csr_matrix = self.Vecotrizer.transform(X_train_documents)
        X_test_vectorized: scipy.sparse.csr.csr_matrix = self.Vecotrizer.transform(X_test_documents)
        return X_train_vectorized, X_test_vectorized

    def prepareVectorizer(self, stripAccents, ngram, stopWords) -&gt; TfidfVectorizer:
        &#34;&#34;&#34;This method loads a vectorrizer from an .vz file if this file it exists. Otherwise it will create a new vectorrizer

        Args:
            stripAccents (str, optional): Thich accents should be stripped &#39;ASCII&#39;, &#39;UNICODE&#39;. Defaults to None.
            ngram (tuple, optional): how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).
            stopWords (list, optional): Which stopwords should be removed i.e. {&#39;english&#39;}. Defaults to None.

        Returns:
            TfidfVectorizer: An loaded or newly created TfidfVectorizer object
        &#34;&#34;&#34;
        loadVec:bool = config.getValueFromConfig(&#34;vectorrizer loadVectorizer&#34;)
        if loadVec == False:
            return self.createNewVectorizer(stripAccents, ngram, stopWords)
        try:
            return load_classifier.getVectorizer() 
        except:
            return self.createNewVectorizer(stripAccents, ngram, stopWords)
    
    def createNewVectorizer(self, stripAccents, ngram: tuple, stopWords) -&gt; TfidfVectorizer:
        &#34;&#34;&#34;This method creates a new tfidf vectorrizer and returns it

        Args:
            stripAccents (str, optional): Thich accents should be stripped &#39;ASCII&#39;, &#39;UNICODE&#39;. Defaults to None.
            ngram (tuple, optional): how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).
            stopWords (list, optional): Which stopwords should be removed i.e. {&#39;english&#39;}. Defaults to None.

        Returns:
            TfidfVectorizer: The newly created tfidf vectorrizer object
        &#34;&#34;&#34;
        saveVec:bool = config.getValueFromConfig(&#34;vectorrizer saveVectorrizer&#34;)

        train_Data:numpy.ndarray = self.getSplitedDocs(config.getValueFromConfig(&#34;trainingConstants sampleSize&#34;))
        Vecotrizer: TfidfVectorizer = TfidfVectorizer(tokenizer=None,
                                     strip_accents=stripAccents, lowercase=None, ngram_range=ngram,
                                     stop_words=stopWords,
                                     min_df=2)
        Vecotrizer.fit_transform(train_Data)
        if saveVec == True:
            joblib.dump(Vecotrizer, config.getValueFromConfig(&#34;vectorrizer path saveTo&#34;), compress=9)
        return Vecotrizer

    def getSplitedDocs(self, sampleSize:int) -&gt; numpy.ndarray:
        &#34;&#34;&#34;This method is used for getting an equal amount of documents from each label class
        by calculating the right amount and randomly selecting certain documents from each label class provided in the config

        Args:
            sampleSize (int): how many documents should be returned

        Returns:
            numpy.ndarray: a list of {sampleSize} documents, with an equal amount of documents in each label class provided in the config
        &#34;&#34;&#34;   
        
        labelClasses:list = config.getValueFromConfig(&#34;labelClasses&#34;)
        length:int = len(labelClasses)
        docCount:float = round(sampleSize / length)
        docs:numpy.ndarray = numpy.empty(0)
        for label in labelClasses:
            logging.debug(&#34;docs size: {} Byte&#34;.format(docs.itemsize))
            docs = numpy.append(self.getRandomDocs(label, docCount), docs)
        return docs</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="classifier.vectorizer.Vectorizer"><code class="flex name class">
<span>class <span class="ident">Vectorizer</span></span>
<span>(</span><span>ngram: tuple = (1, 2), stripAccents=None, stopWords=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is used for creating / loading an Vectorizer, which is used to create feature vectors out of the documents</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_manipulation</code></strong> :&ensp;<code>FileManipulation</code></dt>
<dd>This class is used to access data from documents as well as to save them later on</dd>
</dl>
<p>This is the constructor for the Vectorrizer class</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>ngram</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).</dd>
<dt><strong><code>stripAccents</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Thich accents should be stripped 'ASCII', 'UNICODE'. Defaults to None.</dd>
<dt><strong><code>stopWords</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Which stopwords should be removed i.e. {'english'}. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Vectorizer(file_manipulation.FileManipulation):
    &#34;&#34;&#34;This class is used for creating / loading an Vectorizer, which is used to create feature vectors out of the documents

    Args:
        file_manipulation (FileManipulation): This class is used to access data from documents as well as to save them later on
    &#34;&#34;&#34;

    def __init__(self, ngram: tuple = (1, 2), stripAccents=None, stopWords=None):
        &#34;&#34;&#34;This is the constructor for the Vectorrizer class

        Args:
            ngram (tuple, optional): how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).
            stripAccents (str, optional): Thich accents should be stripped &#39;ASCII&#39;, &#39;UNICODE&#39;. Defaults to None.
            stopWords (list, optional): Which stopwords should be removed i.e. {&#39;english&#39;}. Defaults to None.
        &#34;&#34;&#34;
        
        super().__init__()
        self.Vecotrizer = self.prepareVectorizer(
            stripAccents, ngram, stopWords)

    def stemmer(self, text: list) -&gt; list:
        &#34;&#34;&#34;This method is used for stemming tokens. I.e. cat should identify such strings as cats, catlike, and catty

        Args:
            text (list): a list of texts / documents

        Returns:
            list: the same document list, but the tokens are now stemmed
        &#34;&#34;&#34;
        
        return [PorterStemmer().stem(token) for token in text]

    def lemmatizer(self, text: list) -&gt; list:
        &#34;&#34;&#34;!Attention - currently not in use - just used for performance testing! TODO remove in final version 
        This method can be used for lemmatizing tokens. I.e.  &#34;better&#34; is mapped to &#34;good&#34; or &#34;walking&#34; to &#34;walk&#34;

         Args:
            text (list): a list of texts / documents

        Returns:
            list: the same document list, but the tokens are now stemmed
        &#34;&#34;&#34;
        
        return [WordNetLemmatizer().lemmatize(token) for token in text]

    def createFeatureVectors(self, X_train_documents: numpy.ndarray, X_test_documents: numpy.ndarray) -&gt; tuple:
        &#34;&#34;&#34;This method is used for the vectorisation of the training and testing documents (creating tf-idf, ngram vectors out of the data)
        So it is basically creating a vector (number vector) out of the trainingsdata (string vector) and removes the as the stopwords, emojis, ... if selected

        Args:
            X_train_documents (numpy.ndarray): List of unvectorized text documents
            X_test_documents (numpy.ndarray): List of unvectorized text documents

        Returns:
            tuple:  X_train_vectorized (numpy.ndarray): List of tfidf weighted / vectorized test documents
                    X_test_vectorized  (numpy.ndarray): List of tfidf weighted / vectorized train documents
        &#34;&#34;&#34;

        X_train_vectorized: scipy.sparse.csr.csr_matrix = self.Vecotrizer.transform(X_train_documents)
        X_test_vectorized: scipy.sparse.csr.csr_matrix = self.Vecotrizer.transform(X_test_documents)
        return X_train_vectorized, X_test_vectorized

    def prepareVectorizer(self, stripAccents, ngram, stopWords) -&gt; TfidfVectorizer:
        &#34;&#34;&#34;This method loads a vectorrizer from an .vz file if this file it exists. Otherwise it will create a new vectorrizer

        Args:
            stripAccents (str, optional): Thich accents should be stripped &#39;ASCII&#39;, &#39;UNICODE&#39;. Defaults to None.
            ngram (tuple, optional): how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).
            stopWords (list, optional): Which stopwords should be removed i.e. {&#39;english&#39;}. Defaults to None.

        Returns:
            TfidfVectorizer: An loaded or newly created TfidfVectorizer object
        &#34;&#34;&#34;
        loadVec:bool = config.getValueFromConfig(&#34;vectorrizer loadVectorizer&#34;)
        if loadVec == False:
            return self.createNewVectorizer(stripAccents, ngram, stopWords)
        try:
            return load_classifier.getVectorizer() 
        except:
            return self.createNewVectorizer(stripAccents, ngram, stopWords)
    
    def createNewVectorizer(self, stripAccents, ngram: tuple, stopWords) -&gt; TfidfVectorizer:
        &#34;&#34;&#34;This method creates a new tfidf vectorrizer and returns it

        Args:
            stripAccents (str, optional): Thich accents should be stripped &#39;ASCII&#39;, &#39;UNICODE&#39;. Defaults to None.
            ngram (tuple, optional): how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).
            stopWords (list, optional): Which stopwords should be removed i.e. {&#39;english&#39;}. Defaults to None.

        Returns:
            TfidfVectorizer: The newly created tfidf vectorrizer object
        &#34;&#34;&#34;
        saveVec:bool = config.getValueFromConfig(&#34;vectorrizer saveVectorrizer&#34;)

        train_Data:numpy.ndarray = self.getSplitedDocs(config.getValueFromConfig(&#34;trainingConstants sampleSize&#34;))
        Vecotrizer: TfidfVectorizer = TfidfVectorizer(tokenizer=None,
                                     strip_accents=stripAccents, lowercase=None, ngram_range=ngram,
                                     stop_words=stopWords,
                                     min_df=2)
        Vecotrizer.fit_transform(train_Data)
        if saveVec == True:
            joblib.dump(Vecotrizer, config.getValueFromConfig(&#34;vectorrizer path saveTo&#34;), compress=9)
        return Vecotrizer

    def getSplitedDocs(self, sampleSize:int) -&gt; numpy.ndarray:
        &#34;&#34;&#34;This method is used for getting an equal amount of documents from each label class
        by calculating the right amount and randomly selecting certain documents from each label class provided in the config

        Args:
            sampleSize (int): how many documents should be returned

        Returns:
            numpy.ndarray: a list of {sampleSize} documents, with an equal amount of documents in each label class provided in the config
        &#34;&#34;&#34;   
        
        labelClasses:list = config.getValueFromConfig(&#34;labelClasses&#34;)
        length:int = len(labelClasses)
        docCount:float = round(sampleSize / length)
        docs:numpy.ndarray = numpy.empty(0)
        for label in labelClasses:
            logging.debug(&#34;docs size: {} Byte&#34;.format(docs.itemsize))
            docs = numpy.append(self.getRandomDocs(label, docCount), docs)
        return docs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>file_manipulation.FileManipulation</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="classifier.vectorizer.Vectorizer.createFeatureVectors"><code class="name flex">
<span>def <span class="ident">createFeatureVectors</span></span>(<span>self, X_train_documents: numpy.ndarray, X_test_documents: numpy.ndarray) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>This method is used for the vectorisation of the training and testing documents (creating tf-idf, ngram vectors out of the data)
So it is basically creating a vector (number vector) out of the trainingsdata (string vector) and removes the as the stopwords, emojis, &hellip; if selected</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_train_documents</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>List of unvectorized text documents</dd>
<dt><strong><code>X_test_documents</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>List of unvectorized text documents</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>X_train_vectorized (numpy.ndarray): List of tfidf weighted / vectorized test documents
X_test_vectorized
(numpy.ndarray): List of tfidf weighted / vectorized train documents</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def createFeatureVectors(self, X_train_documents: numpy.ndarray, X_test_documents: numpy.ndarray) -&gt; tuple:
    &#34;&#34;&#34;This method is used for the vectorisation of the training and testing documents (creating tf-idf, ngram vectors out of the data)
    So it is basically creating a vector (number vector) out of the trainingsdata (string vector) and removes the as the stopwords, emojis, ... if selected

    Args:
        X_train_documents (numpy.ndarray): List of unvectorized text documents
        X_test_documents (numpy.ndarray): List of unvectorized text documents

    Returns:
        tuple:  X_train_vectorized (numpy.ndarray): List of tfidf weighted / vectorized test documents
                X_test_vectorized  (numpy.ndarray): List of tfidf weighted / vectorized train documents
    &#34;&#34;&#34;

    X_train_vectorized: scipy.sparse.csr.csr_matrix = self.Vecotrizer.transform(X_train_documents)
    X_test_vectorized: scipy.sparse.csr.csr_matrix = self.Vecotrizer.transform(X_test_documents)
    return X_train_vectorized, X_test_vectorized</code></pre>
</details>
</dd>
<dt id="classifier.vectorizer.Vectorizer.createNewVectorizer"><code class="name flex">
<span>def <span class="ident">createNewVectorizer</span></span>(<span>self, stripAccents, ngram: tuple, stopWords) ‑> sklearn.feature_extraction.text.TfidfVectorizer</span>
</code></dt>
<dd>
<div class="desc"><p>This method creates a new tfidf vectorrizer and returns it</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>stripAccents</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Thich accents should be stripped 'ASCII', 'UNICODE'. Defaults to None.</dd>
<dt><strong><code>ngram</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).</dd>
<dt><strong><code>stopWords</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Which stopwords should be removed i.e. {'english'}. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>TfidfVectorizer</code></dt>
<dd>The newly created tfidf vectorrizer object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def createNewVectorizer(self, stripAccents, ngram: tuple, stopWords) -&gt; TfidfVectorizer:
    &#34;&#34;&#34;This method creates a new tfidf vectorrizer and returns it

    Args:
        stripAccents (str, optional): Thich accents should be stripped &#39;ASCII&#39;, &#39;UNICODE&#39;. Defaults to None.
        ngram (tuple, optional): how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).
        stopWords (list, optional): Which stopwords should be removed i.e. {&#39;english&#39;}. Defaults to None.

    Returns:
        TfidfVectorizer: The newly created tfidf vectorrizer object
    &#34;&#34;&#34;
    saveVec:bool = config.getValueFromConfig(&#34;vectorrizer saveVectorrizer&#34;)

    train_Data:numpy.ndarray = self.getSplitedDocs(config.getValueFromConfig(&#34;trainingConstants sampleSize&#34;))
    Vecotrizer: TfidfVectorizer = TfidfVectorizer(tokenizer=None,
                                 strip_accents=stripAccents, lowercase=None, ngram_range=ngram,
                                 stop_words=stopWords,
                                 min_df=2)
    Vecotrizer.fit_transform(train_Data)
    if saveVec == True:
        joblib.dump(Vecotrizer, config.getValueFromConfig(&#34;vectorrizer path saveTo&#34;), compress=9)
    return Vecotrizer</code></pre>
</details>
</dd>
<dt id="classifier.vectorizer.Vectorizer.getSplitedDocs"><code class="name flex">
<span>def <span class="ident">getSplitedDocs</span></span>(<span>self, sampleSize: int) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>This method is used for getting an equal amount of documents from each label class
by calculating the right amount and randomly selecting certain documents from each label class provided in the config</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sampleSize</code></strong> :&ensp;<code>int</code></dt>
<dd>how many documents should be returned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>a list of {sampleSize} documents, with an equal amount of documents in each label class provided in the config</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getSplitedDocs(self, sampleSize:int) -&gt; numpy.ndarray:
    &#34;&#34;&#34;This method is used for getting an equal amount of documents from each label class
    by calculating the right amount and randomly selecting certain documents from each label class provided in the config

    Args:
        sampleSize (int): how many documents should be returned

    Returns:
        numpy.ndarray: a list of {sampleSize} documents, with an equal amount of documents in each label class provided in the config
    &#34;&#34;&#34;   
    
    labelClasses:list = config.getValueFromConfig(&#34;labelClasses&#34;)
    length:int = len(labelClasses)
    docCount:float = round(sampleSize / length)
    docs:numpy.ndarray = numpy.empty(0)
    for label in labelClasses:
        logging.debug(&#34;docs size: {} Byte&#34;.format(docs.itemsize))
        docs = numpy.append(self.getRandomDocs(label, docCount), docs)
    return docs</code></pre>
</details>
</dd>
<dt id="classifier.vectorizer.Vectorizer.lemmatizer"><code class="name flex">
<span>def <span class="ident">lemmatizer</span></span>(<span>self, text: list) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>!Attention - currently not in use - just used for performance testing! TODO remove in final version
This method can be used for lemmatizing tokens. I.e.
"better" is mapped to "good" or "walking" to "walk"</p>
<p>Args:
text (list): a list of texts / documents</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>the same document list, but the tokens are now stemmed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lemmatizer(self, text: list) -&gt; list:
    &#34;&#34;&#34;!Attention - currently not in use - just used for performance testing! TODO remove in final version 
    This method can be used for lemmatizing tokens. I.e.  &#34;better&#34; is mapped to &#34;good&#34; or &#34;walking&#34; to &#34;walk&#34;

     Args:
        text (list): a list of texts / documents

    Returns:
        list: the same document list, but the tokens are now stemmed
    &#34;&#34;&#34;
    
    return [WordNetLemmatizer().lemmatize(token) for token in text]</code></pre>
</details>
</dd>
<dt id="classifier.vectorizer.Vectorizer.prepareVectorizer"><code class="name flex">
<span>def <span class="ident">prepareVectorizer</span></span>(<span>self, stripAccents, ngram, stopWords) ‑> sklearn.feature_extraction.text.TfidfVectorizer</span>
</code></dt>
<dd>
<div class="desc"><p>This method loads a vectorrizer from an .vz file if this file it exists. Otherwise it will create a new vectorrizer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>stripAccents</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Thich accents should be stripped 'ASCII', 'UNICODE'. Defaults to None.</dd>
<dt><strong><code>ngram</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).</dd>
<dt><strong><code>stopWords</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Which stopwords should be removed i.e. {'english'}. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>TfidfVectorizer</code></dt>
<dd>An loaded or newly created TfidfVectorizer object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepareVectorizer(self, stripAccents, ngram, stopWords) -&gt; TfidfVectorizer:
    &#34;&#34;&#34;This method loads a vectorrizer from an .vz file if this file it exists. Otherwise it will create a new vectorrizer

    Args:
        stripAccents (str, optional): Thich accents should be stripped &#39;ASCII&#39;, &#39;UNICODE&#39;. Defaults to None.
        ngram (tuple, optional): how many neighbor words should be taken into consideration when creating an featurevector. Defaults to (1, 2).
        stopWords (list, optional): Which stopwords should be removed i.e. {&#39;english&#39;}. Defaults to None.

    Returns:
        TfidfVectorizer: An loaded or newly created TfidfVectorizer object
    &#34;&#34;&#34;
    loadVec:bool = config.getValueFromConfig(&#34;vectorrizer loadVectorizer&#34;)
    if loadVec == False:
        return self.createNewVectorizer(stripAccents, ngram, stopWords)
    try:
        return load_classifier.getVectorizer() 
    except:
        return self.createNewVectorizer(stripAccents, ngram, stopWords)</code></pre>
</details>
</dd>
<dt id="classifier.vectorizer.Vectorizer.stemmer"><code class="name flex">
<span>def <span class="ident">stemmer</span></span>(<span>self, text: list) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>This method is used for stemming tokens. I.e. cat should identify such strings as cats, catlike, and catty</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>list</code></dt>
<dd>a list of texts / documents</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>the same document list, but the tokens are now stemmed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stemmer(self, text: list) -&gt; list:
    &#34;&#34;&#34;This method is used for stemming tokens. I.e. cat should identify such strings as cats, catlike, and catty

    Args:
        text (list): a list of texts / documents

    Returns:
        list: the same document list, but the tokens are now stemmed
    &#34;&#34;&#34;
    
    return [PorterStemmer().stem(token) for token in text]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="classifier" href="index.html">classifier</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="classifier.vectorizer.Vectorizer" href="#classifier.vectorizer.Vectorizer">Vectorizer</a></code></h4>
<ul class="">
<li><code><a title="classifier.vectorizer.Vectorizer.createFeatureVectors" href="#classifier.vectorizer.Vectorizer.createFeatureVectors">createFeatureVectors</a></code></li>
<li><code><a title="classifier.vectorizer.Vectorizer.createNewVectorizer" href="#classifier.vectorizer.Vectorizer.createNewVectorizer">createNewVectorizer</a></code></li>
<li><code><a title="classifier.vectorizer.Vectorizer.getSplitedDocs" href="#classifier.vectorizer.Vectorizer.getSplitedDocs">getSplitedDocs</a></code></li>
<li><code><a title="classifier.vectorizer.Vectorizer.lemmatizer" href="#classifier.vectorizer.Vectorizer.lemmatizer">lemmatizer</a></code></li>
<li><code><a title="classifier.vectorizer.Vectorizer.prepareVectorizer" href="#classifier.vectorizer.Vectorizer.prepareVectorizer">prepareVectorizer</a></code></li>
<li><code><a title="classifier.vectorizer.Vectorizer.stemmer" href="#classifier.vectorizer.Vectorizer.stemmer">stemmer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>