[{"labels":["api",null,null],"text":"**tl;dr;**: [Hyperone](https://www.hyperone.com/) and good citizen @ad-m will be happy to help build a new Airflliw CLI.\r\nHere is docs for POC: \r\nhttps://gist.github.com/ad-m/19cc06d91a7ee756f461c95f5d656eb6\r\n\r\nIf we have a [Stable API](https://airflow.readthedocs.io/en/latest/stable-rest-api-ref.html) ready, we can start working on tools that use it. A highly anticipated feature is the ability to manage Airflow from the CLI. Currently, there is [similar feature](https://airflow.readthedocs.io/en/latest/usage-cli.html#set-up-connection-to-a-remote-airflow-instance) in Airflow, but with many limitations:\r\n\r\n- A very small number of commands are available (7 pool commands and 2 dag commands only).\r\n- Use deprecated experimental REST API.\r\n- Low test coverage.\r\n- No access control. (Big security risks)\r\n- Requires full Airflow to be installed along with a large number of unnecessary dependencies. \r\n- [compatibility issues on Windows](https://github.com/apache/airflow/issues/10388).\r\n- Installation is via pip which is great. However, users expect such a tool to be available as a single package with all dependencies included.\r\n\r\nI also suggested that this be deleted in Airflow 2.0 as it is rarely used.\r\nhttps://lists.apache.org/thread.html/rfada8ac2fce87c0516d62923e35e3bebfc44ee5a379103b890f8c61c%40%3Cdev.airflow.apache.org%3E\r\n\r\nSome users use the normal CLI, but this also has many of the disadvantages mentioned above, but for this it poses an even greater security risk as it requires a direct connection to the database. On the other hand, it has many more features, but not all that are available in Web UI, eg there is no access to remote logs but they are available in Web UI and stable API.\r\n\r\nI spoke with good citizen @ad-m how [HyperOne](https://www.hyperone.com/) will develop CLI for its services. HyperOne is a cloud provider that develops services for polish good citizens in Poland. They use OpenAPI very intensively for build thier platform. They also strive for automation and self-maintenance. I think we can rely on their experience, knowledge and talent.\r\n\r\nFrom a conversation with this expert, I learned about the existence of the [h1-cli](https://github.com/hyperonecom/h1-cli/) project. This is the CLI that makes it easy to manage HyperOne services. Have the key-features from our perspective:\r\n - Use NodeJS (-/+)\r\n - Uses OpenAPI (+)\r\n - Available as a single binary(+). To achieve it, they used the [pkg](https://github.com/vercel/pkg) utility. \r\n - Works on many multiple operating systems (+)\r\n\r\nThis is not the end of the story! They are working on [a new CLI v2](https://github.com/hyperonecom/h1-cli/tree/v2) that will use the API not only to verify the message format, but also **the new commands** will be built based on the OpenAPI specification. Yes! Adding a new service to the platform does not require any changes to the CLI. \r\n\r\nThe fantastic features don't stop there. CLI is written in NodeJS and thanks to the additional [cli-device-browser](https://github.com/hyperonecom/h1-cli/tree/v2/packages/cli-device-browser) module it is possible to use this CLI also **from the browser**. Yes! We can have an airflow console in the browser.\r\n\r\nWhen a resource is created, a CLI command equivalent is also generated. The use of **automatic command generation** makes the maintenance of the CLI a minimum effort. The CLI developed in this way is able to automatically adapt to new API parameters, which reduces the effort required for its maintenance. The reference documentation of CLI is also updated.\r\n\r\nCLI is integrated with the HyperOne Management Panel. When an operation is performed via the Panel, an **example of how to perform an analogous operation with CLI** is displayed. Besides the example developed in this way can be directly run in the CLI in the browser. An excellent way to educate users and popularize CLI. This mechanism is part of the CLI framework and we can integrate our web-UI similarly.\r\n\r\nI spoke to @ad-m and HyperOne are eager to collaborate and share their experience and technology so we can similarly build our CLI. @ad-m even prepared a POC based on our specification. After writing ~300 lines of code, he had Airfllow CLI ready, which after some improvements, could be used more widely.\r\n\r\nThe CLI framework is under active development. New functionalities are planned that we will be able to use. I found out that autocomplete support is planned (including remote data). They also work as hard as possible to improve the documentation format, e.g. generate CLI context help based on examples from OpenAPI, provide documentation in new formats.\r\n\r\nHere is the documentation that shows the idea of how it will be possible to use such a CLI.\r\n\r\nhttps://gist.github.com/ad-m/19cc06d91a7ee756f461c95f5d656eb6\r\n\r\nIn a further development, we may write additional commands that address common use cases. The CLI framework assumes that basic operations are generated, and specific operations can additionally be added or even dynamically loaded (plugin-way). At this stage, these POC of CLIs are more advanced and more powerful than the remote mode in the current Airflow CLI.\r\n\r\nAs of now, I don't have the capacity to lead this idea, so I'm not starting a mailing list discussion. However, if I have positive opinions, I can try to book a few hours for its implementation. For now, I would like to know your expectations from the CLI client and your use cases in order to be able to better understand the expectations and develop a CLI in the future."},{"labels":["api",null],"text":"Hello,\r\n\r\nIf we have a [Stable API](https://airflow.readthedocs.io/en/latest/stable-rest-api-ref.html) ready, we can start working on tools that use it.  We can start with the Terraform provider. Terraform. is a tool for building, changing, and versioning infrastructure as a code.  A terraform provider is a way to integrate other services. Integrations already exist for a wide variety of services and software, including [Kubernetes](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs), [Google Cloud](https://www.terraform.io/docs/providers/google/index.html), [Keycloak](https://github.com/mrparkers/terraform-provider-keycloak).\r\n\r\n@houqp started working on it and prepared [small POC](https://github.com/houqp/terraform-provider-airflow).  Currently, this allows you to manage Airflow variables.\r\n\r\nI discussed this topic with good citizen @ad-m and he said we can use [terraform-provider-openapi](https://github.com/dikhan/terraform-provider-openapi) to generate a terraform provider based on the OpenAPI specification. He pointed out that this is how the company for which he works developed the [terraform-provider-hyperone](https://github.com/hyperonecom/terraform-provider-hyperone). This is also an implementation idea that we can explore.\r\n\r\nI talked to @houqp , He pointed out that implementations may be affected by the range of resources we want available. Not all resources in the API makes sense as resources in Terraform, eg TaskInstnace.\r\n\r\nWhat do you think about the Terraform provider for Airflow? What use cases should we support? What are your ideas for implementing this concept?\r\n\r\nBest regards,\r\nKamil Breguła\r\n\r\nCC: @jaketf @potiuk"},{"labels":["api",null],"text":"## Rationale\r\nAirflow currently has 185 permissions (where 1 permission is a combo of Class + Action). The design of the permissions is inconsistent, making them difficult to modify or organize into roles. This is especially problematic for the stable API. Because the UI permissions are named after the UI view and methods, and are intended to control access to the UI view rather than the underlying resource, they don't map cleanly to API methods. \r\n\r\nWe need to define a set of permissions that control access to underlying resources, independent from the interface used to access those resources.\r\n\r\n### Aside about names\r\nFlask AppBuilder names permissions in a confusing manner. It uses the term \"permission\" to refer to both an action (for example, \"can_edit\") and an action + resource pair (\"can_edit on the TaskInstanceModelView\"). This document refers to a `permission` as a unique combination of `action` + `domain`, where `domain` is the entity to which access is granted. For old permission design, the domain is the UI view class. For new permission design, the domain refers to the resource model.\r\n\r\n**Description**\r\n\r\nThere are multiple types of permissions. We should consider each in turn.\r\n\r\n* **Variants of create/read/edit/delete for Airflow modelViews** - (ex. `domain: RoleModelView, permission: can_show`) - Consolidate into `can_create`, `can_read`, `can_edit`, and `can_delete` for the `Role` resource.\r\n\r\n* **Menu access for Airflow modelViews** - (ex. `domain: Task Instances, permission: menu_access`) - We should do away with menu-specific permissions. If a user has read access for the resource, they should have menu access in the UI.\r\n\r\n* **Muldelete for Airflow modelViews** - (ex. `domain: PoolModelView, permission: muldelete`) - I propose combining `muldelete` and `delete` into a single `delete` action. If you can delete one, you can delete multiple.\r\n\r\n* **List for Airflow modelViews** - (ex. `domain: PoolModelView, permission: can_list`) - Same as `muldelete`. I propose combining `list` and `read` into a single `read` permission.\r\n\r\n* **Default FAB permissions** - (ex. `domain: ResetPasswordView, permission: can_this_form_post`) - I propose leaving these in place. We technically could change these to match the new pattern by subclassing the default FAB views and settings custom permission mappings. If we choose to do that, it should be part of a separate issue.\r\n\r\n* **Edit/read for DAGs** - (ex. `domain: example_branch_dop_operator_v3, permission: can_dag_read`) - Change to `can_edit` and `can_read` for the same `example_branch_dop_operator_v3` domain.\r\n\r\n* **Airflow view permissions** - The following is a list of examples, mapping existing permissions to proposed new ones. Where possible, this involves mapping the view permission to `model.can_read`, or one of the other CRUD actions.\r\n\r\n> * Airflow.can_redirect_to_external_log => TaskInstance.can_read\r\n> * Airflow.can_delete => Dag.can_delete\r\n> * Airflow.can_task => Task.can_read\r\n> * Airflow.can_trigger => Dag.can_trigger\r\n> * Airflow.can_dag_details => Dag.can_read\r\n> * Airflow.can_clear => TaskInstance.can_delete\r\n> * Airflow.can_refresh_all => Dag.can_read\r\n> * Airflow.can_extra_links => Dag.can_edit\r\n> * Airflow.can_index => Seems unnecessary?\r\n> * Airflow.can_refresh => Dag.can_read\r\n> * Airflow.can_xcom => XCom.can_read\r\n> * Airflow.can_rendered => TaskInstance.can_read\r\n> * Airflow.can_blocked => Dag.can_read (does this modify anything?)\r\n\r\n**Additional considerations**\r\nThere are two motivations.\r\n1. Make view permissions fit for API endpoints.\r\n2. Simplify permissions to make them more usable.\r\n\r\nUltimately, there are two key questions for this issue:\r\n1. Are these permission updates the right changes to make.\r\n2. If they're the right ones, is now the right time to make them?\r\n\r\n**Related Issues**\r\n\r\n#8112 \r\n"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe need additional auth backends for the new API to allow access to users who aren't use an auth proxy service.\r\n\r\n**Use case / motivation**\r\n\r\nThe new API does not have support for common authentication methods, such as JWT or basic authentication. It exclusively supports use of an authentication proxy service. Small teams using Airflow are unlikely to have auth proxies, meaning the API is difficult for them to use. By adding pre-built auth backends for common auth schemes, we increase the number of users who can easily build on top of the API\r\n\r\n**Related Issues**\r\n\r\nhttps://github.com/apache/airflow/issues/8111\r\nhttps://github.com/apache/airflow/issues/8112\r\n"},{"labels":["api",null,null],"text":"Hello,\r\n\r\nWe use [Speccy](https://github.com/wework/speccy) for linting API, but we should start using [Spectral](https://meta.stoplight.io/docs/spectral/README.md).\r\n\r\n> What is the difference between Spectral and Speccy\r\n> Speccy was a great inspiration for Spectral, but was designed to work only with OpenAPI v3. Spectral can apply rules to any JSON/YAML object (including OpenAPI v2/v3 and AsyncAPI).\r\n> \r\n> Speccy has been abandoned, but Spectral is steaming ahead, adding loads of functionality like custom functions, exceptions, and AsyncAPI support.\r\n\r\nBest regards,\r\nKamil\r\nCC: @houqp "},{"labels":["api",null],"text":"Hello,\r\n\r\nWe have a reference API based on [redoc](https://github.com/Redocly/redoc).\r\nhttps://airflow.readthedocs.io/en/latest/stable-rest-api/redoc.html\r\nIt has possibility to add code samples of using the API in different languages thanks to the x-codesamples extension. This will facilitate the use of this API and will also provide ready documentation for these clients. Less duplicate documentation.\r\n![image](https://github.com/Redocly/redoc/raw/master/docs/images/code-samples-demo.gif)\r\n\r\nWe can think about generating code samples automatically using ready-made tools (If possible)\r\nhttps://github.com/ErikWittern/openapi-snippet\r\nhttps://github.com/richardkabiling/openapi-snippet-cli/blob/master/src/index.ts\r\nhttps://github.com/cdwv/oas3-api-snippet-enricher\r\n\r\nBest regards,\r\nKamil Breguła\r\n"},{"labels":["api",null],"text":"**Description**\r\n\r\nI notice that we have API specifications that require manual maintenance over time. In order to minimize the risk that it will become outdated, I propose to use its elements during integration tests ( https://github.com/apache/airflow/tree/master/tests/api_connexion/endpoints ).\r\n\r\nFrom my own experience I know that simple verification whether the scheme (selected by hard-coded name) contained in the specification contains all the fields placed in the sample answers allows to avoid many oversights. More attention may be required if the relevant schema will be searched based on the path rather than based on the schema name. Then, however, it should not be difficult to check if the status code of the answer is provided by the specifications also.\r\n\r\nIt requires attention to ensure that this type of test verifies both that the required fields are included in the response and that the response does not contain any fields not included in the specification.\r\n\r\nThe implementation of basic tests of this type of tests should not be complex, as the current tests contain a response structure in memory, so you should find the appropriate scheme and validate this response, just as HTTP response status code is now verified: \r\n\r\nhttps://github.com/apache/airflow/blob/5eb2808/tests/api_connexion/endpoints/test_task_endpoint.py#L131-L133\r\n\r\nExample implementation in JavaScript:\r\n\r\nhttps://github.com/hyperonecom/h1-cli/blob/5895f7414d3ebf8ceed912ef5443ebc01dd9eb69/lib/tests.js#L55-L65\r\nhttps://github.com/hyperonecom/h1-cli/blob/5895f7414d3ebf8ceed912ef5443ebc01dd9eb69/lib/tests.js#L188-L191\r\n\r\nOpenAPI uses the JSONSchema standard for the schema definition, for which there are numerous validators even for old-fashion Python ( https://json-schema.org/implementations.html).\r\n\r\n**Use case / motivation**\r\n\r\nReduce maintenance burden of OpenAPI specification\r\n\r\n**Related Issues**\r\n\r\n#8107"},{"labels":["api",null],"text":"I am not sure whether to put this as a bug or feature request (depends on how you see this).\r\nAs this might be completely independent from versions, os, k8s, ... I will not add this detail here.\r\n\r\nShort: When trying to trigger a DAG via the REST API interface more than once per second the Airflow returns a HTTP 500 error code. \r\n\r\nLong: When using Airflow REST API I can send any amount of POST calls, but when sending more calls per second returning an HTTP 500 might be very misleading. If it is expected to not send more requests as 1 per second, it should return an HTTP 400 and have a good error message. If it is intended to be able to send more than 1 POST call per second, I would expect this to be a bug and hopefully be fixed. AFAIK Airflow inside its implementation uses a timestamp with precision of seconds to create a DAG run. But this might not be fitting to having received multiple requests in the same second, since then the uniqueness requirement might be failing."},{"labels":["api",null],"text":"Hello,\r\n\r\nWe have a fantastic REST API.  However, something can always be improved. The current problem reported by users are not very friendly error messages for end-users  To address this issue, we should follow [RFC-7807](https://tools.ietf.org/html/rfc7807) more closely. \r\n\r\nOur error responses do not include the URI in the type field. We can fill this field with the link to the specification in Web UI:\r\nhttps://github.com/apache/airflow/pull/9504\r\nhttps://github.com/apache/airflow/pull/9144 \r\n\r\n>    This specification does this by identifying a specific type of problem (e.g., \"out of credit\") with a URI [RFC3986]; HTTP APIs can do this by nominating new URIs under their control, or by reusing existing ones.\r\n\r\nTitle and details are not standardized.  The API was developed by many contributors and there was freedom in the content of these fields. The content of these fields was not reviewed. \r\nIt is worth noting that the `title` field should do not contain any arguments. A common mistake is to put identifiers in the title e.g. `Dag run with ID = {} is not found`. It should not change from occurrence to occurrence of the problem.  The `details` field may contain more detailed information.\r\n\r\nBest regards,\r\nKamil\r\n\r\n  "},{"labels":["api",null],"text":"Helllo,\r\n\r\nWe have the following ranger filter parameters:\r\n\r\n- FilterDurationGTE\r\n- FilterDurationLTE\r\n- FilterEndDateGTE\r\n- FilterEndDateLTE\r\n- FilterExecutionDateGTE\r\n- FilterExecutionDateLTE\r\n- FilterStartDateGTE\r\n- FilterStartDateLTE\r\n\r\nOnly closed ranged(in interval notation: [start_xxx, end_xxx]) can be fetched using them.\r\n\r\nI think, filters representing ranges should use inclusive start values and exclusive end values (half-closed intervals); in interval notation: [start_xxx, end_xxx).\r\n\r\nExclusive end values are preferable for the following reasons:\r\n\r\n* It conforms to user expectations, particularly for continuous values such as timestamps, and avoids the need to express imprecise “limit values” (e.g. 2012-04-20T23:59:59).\r\n* It is consistent with most common programming languages, including C++, Java, Python, and Go.\r\n* It is easier to reason about abutting ranges: [0, x), [x, y), [y, z), where values are chainable from one range to the next.\r\n\r\nMore information: https://google.aip.dev/145\r\n\r\nAlternatively, we can also add LT and GT filters, but this could only complicate situations without much benefit. We would have 4 different parameters for one field. Changing parameter names and changing behavior seems better to me.\r\n\r\nBest regards,\r\nKamil Breguła"},{"labels":["api",null,null],"text":"Hello \r\n\r\nWe need to create several endpoints that perform basic CRUD operations on **Variable**. We need the following endpoints:\r\n\r\nPOST /variables\r\nDELETE /variables/{variable_key}\r\nPATCH /variables/{variable_key}\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,"},{"labels":["api",null,null],"text":"Hello \r\n\r\nWe need to create several endpoints that perform basic CRUD operations on **Pools**. We need the following endpoints:\r\n\r\nPOST /pools\r\nDELETE /pools/{pool_name}\r\nPATCH /pools/{pool_name}\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,"},{"labels":["api",null,null],"text":"Hello \r\n\r\nWe need to create several endpoints that perform one ono read-only operation on **Import Error**. We need the following endpoint:\r\n\r\nDELETE /importErrors/{import_error_id}\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,"},{"labels":["api",null,null],"text":"Hello \r\n\r\nWe need to create several endpoints that perform batch operations on **XCOM Entries*. We need the following endpoints:\r\n\r\n- ``POST /dags/~/dagRuns/list``\r\n- ``POST /dags/~/dagRuns/~/taskInstances/list``\r\n\r\nIt depends on https://github.com/apache/airflow/issues/8132, https://github.com/apache/airflow/issues/8129\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,"},{"labels":["api",null,null],"text":"Hello \r\n\r\nWe need to create several endpoints that perform basic CRUD operations on **XCOM Entries*. We need the following endpoints:\r\n\r\nPOST /dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}/xcomEntries\r\nDELETE /dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}/xcomEntries/{xcom_key}\r\nPATCH /dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}/xcomEntries/{xcom_key}\r\n\r\nIt depends on https://github.com/apache/airflow/issues/8134\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,"},{"labels":["api",null,null],"text":"Hello \r\n\r\nWe need to create several endpoints that perform basic CRUD operations on **DAG Run**. We need the following endpoints:\r\n\r\nDELETE /dags/{dag_id}/dagRuns/{dag_run_id}\r\nPATCH /dags/{dag_id}/dagRuns/{dag_run_id}\r\nPOST /dags/{dag_id}/dagRuns/{dag_run_id}\r\n\r\nIt depends on https://github.com/apache/airflow/issues/8129\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,"},{"labels":["api",null,null],"text":"Hello \r\n\r\nWe need to create several endpoints that perform basic CRUD operations on **DAG**. We need the following endpoints:\r\n\r\n- PATCH /dags/{dag_id}\r\n\r\nIt depends on https://github.com/apache/airflow/issues/8128\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,"},{"labels":["api",null,null],"text":"Hello \r\n\r\nWe need to create several endpoints that perform basic CRUD operations on **Connection** . We need the following endpoints:\r\n\r\n- POST /connections\r\n- DELETE /connections/{connection_id}\r\n- PATCH /connections/{connection_id}\r\n\r\nIT depends on https://github.com/apache/airflow/issues/8127\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nThe OpenAPI allows to generate automatically clients for various languages. There are however some unresolved problems with some of the constructs in the Open API - for example oneOf with single schema produces uncompilable code  despite being correct specification. Another example is \"key\" generic name of parameter, which in Java code might cause compilation problems in some cases as \"key\" variable name is used internally in generated methods.\r\n\r\nWe would like to make sure that our OpenAPI specification produces compilable and usable clients when auto-client generation is used.  This can be easily setup as a CI step.\r\n\r\n**Use case / motivation**\r\n\r\nWe want our customers to have very easy path in using the API. Being able to use pre-generated client API in their favorite language can save many days of work for integration.\r\n\r\nCI building the API clients + API clients published as artifacts are great way to achieve that consistently.\r\n\r\n**Related Issues**\r\n#7549 \r\n\r\n"},{"labels":["api",null],"text":"**Description**\r\n\r\nThere is no endpoint to check if the instance is in good condition\r\n\r\nWe need to prepare the change to the API specification and then implement this change.\r\n\r\nMore information about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/a"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe should prepare documentation for the REST API:\r\n\r\n- [Guide about API authentication (including custom)](https://github.com/apache/airflow/issues/8123)\r\n- [Guide about authorization and permission](https://github.com/apache/airflow/issues/8122)\r\n- [Migration guide from the experimental API to the REST API](https://github.com/apache/airflow/issues/8121)\r\n- [Guide \"How to use REST API\"](https://github.com/apache/airflow/issues/8120)\r\n- [REST API Reference](https://github.com/apache/airflow/issues/8119)\r\n\r\nMore information about the REST API is available:\r\n[AIP-32 - Airflow REST API - High-level info](https://github.com/apache/airflow/issues/8107)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A\r\n"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nWe need endpoints that allow you to retrieve an element based on the object key - `lookup`.\r\n\r\n- GET /variables/lookup\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nWe need endpoints that allow you to read [extra links](https://airflow.readthedocs.io/en/latest/howto/define_extra_link.html).\r\n\r\nGET /dags/{dag_id}/taskInstances/{task_id}/{execution_date}/links\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe need to have endpoints that allow us to read information about workflow - DAG, Task. Endpoints should properly support serialization.\r\n\r\n- GET /dags/{dag_id}/details\r\n- GET /dags/{dag_id}/tasks\r\n- GET /dags/{dag_id}/tasks/{task_id}\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nHello, \r\n\r\nWe need an endpoint that allows you to read the DAG File source code. \r\nWhen ``store_dag_code`` = True, then the code should be read from the database.\r\nWhen ``store_dag_code`` = False, then the code should be read from the file.\r\n\r\n- `GET /dagSources/{file_token}`\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nHello,\r\n\r\nWe need an endpoint that allows us to **read** the current Airflow configuration.\r\n\r\n- GET /config\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\nHello,\r\n\r\nWe need to prepare an endpoint that will read the logs for the task. Logs are already available in Web UI, so abstractions should be created and used in both places. The code responsible for reading the log in the Web UI is in the file [`airflow/www/views.py`](https://github.com/apache/airflow/blob/master/airflow/www/views.py) (method `get_logs_with_metadata`)\r\nThe token is metadata data serialized to JSON and encoded with base64. This prevents user from building clients based on a specific metadata structure. We cannot guarantee their permanent structure because it use external plugins.\r\n\r\n- GET /dags/{dag_id}/taskInstances/{task_id}/{execution_date}/logs/{task_try_number}\r\n\r\nMore details about API Endpoints: \r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **XCOM** . We need the following endpoints:\r\n\r\n- GET /dags/{dag_id}/taskInstances/{task_id}/{execution_date}/xcomValues\r\n- GET /dags/{dag_id}/taskInstances/{task_id}/{execution_date}/xcomValues/{key}\r\n\r\nFor now, we focus only on read-only operations, but the others will also be implemented as the next step.\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **Variable** . We need the following endpoints:\r\n\r\n- GET /variables\r\n- GET /variables/{variable_key}\r\n\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **Task instance** . We need the following endpoints:\r\n\r\n- GET /dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances\r\n- GET /dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}\r\n\r\nFor now, we focus only on read-only operations, but others will also be implemented as the next step.\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **Pools** . We need the following endpoints:\r\n\r\n- GET /pools\r\n- GET /pools/{pool_name}\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **Import error** . We need the following endpoints:\r\n\r\n- GET /importErrors\r\n- GET /importErrors/{import_error_id}\r\n\r\nFor now, we focus only on read-only operations, but others will also be implemented as the next step.\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},{"labels":["api",null,null],"text":"Hello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **DAG Runs** . We need the following endpoints:\r\n\r\n- GET /dags/{dag_id}/dagRuns\r\n- GET /dags/{dag_id}/dagRuns/{dag_run_id}\r\n\r\nFor now, we focus only on read-only operations, but others will also be implemented as the next step.\r\n\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **DAG Model** . We need the following endpoints:\r\n\r\n- GET /dags\r\n- GET /dags/{dag_id}\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLove,\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nHello \r\n\r\nWe need to create several endpoints that perform read-only operations on **Connection** . We need the following endpoints:\r\n\r\n- GET /connections\r\n- GET /connections/{connection_id}\r\n\r\nFor now, we focus only on read-only operations, but others will also be implemented as the next step.\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLots of love,\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe need a view that makes it easier to set identical permissions for Web UI and API.\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nWe should write a guide that describes how to add new ones or use the current authentication method.\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nWe should prepare a guide that will describe how to manage permissions for the API. If there is a guide for Web UI, we can extend it. If it does not exist, then we must write from scratch.\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null,null,null],"text":"**Description**\r\n\r\nWe should prepare a guide that will facilitate the migration from the experimental API to the API for Airflow 2.0.\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nWe need one that will facilitate the use of the REST API\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null,null],"text":"**Description**\r\n\r\nWe need documentation to be available in a user-friendly form.  The YAML file does not belong to this format. We can use swagger UI or something similar\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nUsers expect the specification to be in an accessible form.\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe should create endpoints that allow us to perform operations in Airflow\r\n\r\nWe need the following read-only endpoints - Database:\r\n\r\n- [API Endpoints - Read-only - Connection ](https://github.com/apache/airflow/issues/8127)\r\n- [API Endpoints - Read-only - DAG Model ](https://github.com/apache/airflow/issues/8128)\r\n- [API Endpoints - Read-only - DAG Runs ](https://github.com/apache/airflow/issues/8129)\r\n- [API Endpoints - Read-only - Import errors ](https://github.com/apache/airflow/issues/8130)\r\n- [API Endpoints - Read-only - Pools ](https://github.com/apache/airflow/issues/8131)\r\n- [API Endpoints - Read-only - Task Instance ](https://github.com/apache/airflow/issues/8132)\r\n- [API Endpoints - Read-only - Variable ](https://github.com/apache/airflow/issues/8133)\r\n- [API Endpoints - Read-only - XCOM ](https://github.com/apache/airflow/issues/8134)\r\n\r\nWe need to implement other endpoints: \r\n\r\n- [API Endpoint - Logs ](https://github.com/apache/airflow/issues/8135)\r\n- [API Endpoint - Config ](https://github.com/apache/airflow/issues/8136)\r\n- [API Endpoint - Dag source ](https://github.com/apache/airflow/issues/8137)\r\n- [API Endpoint - Dags structure/Task ](https://github.com/apache/airflow/issues/8138)\r\n- [API Endpoint - Links ](https://github.com/apache/airflow/issues/8140)\r\n- [API Endpoint - Health - Spec and impelementataion](https://github.com/apache/airflow/issues/8144)\r\n\r\n\r\n\r\nMore information about the REST API is available:\r\n[AIP-32 - Airflow REST API - High-level info](https://github.com/apache/airflow/issues/8107)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nLinks should be added to the objects to make it easier to use. I recommend https://flask-marshmallow.readthedocs.io/en/latest/\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe need to build views that share common behavior. It would be nice if the code was not repeated. We need views that will only perform CRUD operations (Create, Update, List, Update, [GET])\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe need a common mechanism for deserialization and serialization of Python objects.\r\n\r\nPR with this change should define all schemas with Python objects (DAG. BaseOperator)  in accordance with OpenAPI specification\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThis will make writing new endpoints much easier. We will also have one place defining the structure, which will facilitate backward compatibility.\r\n\r\n**Related Issues**\r\n\r\nThis task is related to\r\nhttps://github.com/apache/airflow/issues/8110\r\nhttps://github.com/apache/airflow/issues/8114\r\nhttps://github.com/apache/airflow/issues/8115\r\nIt was divided so that we could do the work with smaller pieces."},{"labels":["api",null],"text":"**Description**\r\n\r\nWe need a common mechanism for deserialization and serialization of common objects. I recommend marshmallow\r\nhttps://marshmallow.readthedocs.io/en/latest/\r\nPR with this change should define all schemas without database objects and Python objects (DAG. BaseOperator)  in accordance with OpenAPI specification\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThis will make writing new endpoints much easier. We will also have one place defining the structure, which will facilitate backward compatibility.\r\n\r\n**Related Issues**\r\n\r\nThis task is related to\r\nhttps://github.com/apache/airflow/issues/8110\r\nhttps://github.com/apache/airflow/issues/8114\r\nhttps://github.com/apache/airflow/issues/8115\r\nIt was divided so that we could do the work with smaller pieces."},{"labels":["api",null],"text":"**Description**\r\n\r\nBefore releasing Airflow 2.0, we should review the code and consider whether it is safe.\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nI want to make the internet a safer and better place in the digital world.\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe need a simple mechanism to authorize operations performed by the API.  It should be compatible with the Flask App Builder used by Airflow.\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThe purpose of the authorization is access control, which confirms whether a given entity is authorized to use the requested resource.\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe should prepare an authentication mechanism that allows easy extension and adding a new authentication method.\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThe purpose of authentication is to achieve a certain level of confidence that the entity is in fact the one it claims to be. Extending authentication methods allows the security requirements of various organizations to be met.\r\n\r\nPR does not need to implement any common authentication mechanism. All they have to do is accept the user ID in the request and log in.\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe need a common mechanism for deserialization and serialization of SQLAlchemy objects. I recommend marshmallow-sqlalchemy\r\nhttps://marshmallow-sqlalchemy.readthedocs.io/en/latest/\r\nPR with this change should define all schemas for database objects in accordance with OpenAPI specification\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThis will make writing new endpoints much easier. We will also have one place defining the structure, which will facilitate backward compatibility.\r\n\r\n**Related Issues**\r\n\r\nThis task is related to\r\nhttps://github.com/apache/airflow/issues/8110\r\nhttps://github.com/apache/airflow/issues/8114\r\nhttps://github.com/apache/airflow/issues/8115\r\nIt was divided so that we could do the work with smaller pieces."},{"labels":["api",null],"text":"**Description**\r\n\r\nWe must provide basic integration of Airflow with connexion. This should contain a foundation for other works. For now, it can return empty objects on any request. Each endpoint should be publicly available.\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nCreating a foundation for other work.\r\n\r\n**Related Issues**\r\n\r\nN/A"},{"labels":["api",null],"text":"**Description**\r\n\r\nWe need to develop an OpenAPI spec that contains at least the following elements:\r\n\r\n- endpoints\r\n- scheme\r\n- response bodies\r\n\r\nSubsequent PR may extend the Open API spec of subsequent elements, e.g. HATEOAS, but HATEOAS must be defined after the endpoints, so it seems logical to split this task.\r\n\r\nPR is available: https://github.com/apache/airflow/issues/8107\r\n\r\nMore information about the REST API is available:\r\n[AIP-32 - Airflow REST API - High-level info](https://github.com/apache/airflow/issues/8107)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nhttps://github.com/apache/airflow/issues/8107"},{"labels":["api",null],"text":"We currently have one [experimental API|https://airflow.readthedocs.io/en/latest/rest-api-ref.html], but despite its existence for 2 years, it has not reached a stable level. \r\n\r\nThe Polidea and Google teams together with the community want to make another attempt based on our and community experience. Airflow deserves new stable solutions.\r\n\r\nDetails are available in the AIP:\r\n\r\nThis is the first ticket regarding AIP-32 on Github.  I will try to update it regularly to inform you about the high-level status of this task.\r\n\r\nWe have merged spec. You could preview it using the following links:\r\nSwaagger UI:\r\nhttps://editor.swagger.io/?url=https://raw.githubusercontent.com/apache/airflow/master/airflow/api_connexion/openapi/v1.yaml\r\nRedoc:\r\nhttps://redocly.github.io/redoc/?url=https://raw.githubusercontent.com/apache/airflow/master/airflow/api_connexion/openapi/v1.yaml\r\n\r\nThis task consists of the following areas:\r\n\r\n- [API Endpoints](https://github.com/apache/airflow/issues/8118)\r\n- [Docs for REST API](https://github.com/apache/airflow/issues/8118)\r\n\r\nIn addition, we have the following tasks:\r\n- [Basic OpenAPI spec](https://github.com/apache/airflow/issues/8108)\r\n- [Basic integration Airflow and connexion](https://github.com/apache/airflow/issues/8109)\r\n- [(de)serialization for Python objects](https://github.com/apache/airflow/issues/8115)\r\n- [(de)serialization common objects](https://github.com/apache/airflow/issues/8114)\r\n- [(de)serialziation for SQLAlchemy objects](https://github.com/apache/airflow/issues/8110)\r\n- [HATEOS for API](https://github.com/apache/airflow/issues/8117)\r\n- [CRUD Framework for API](https://github.com/apache/airflow/issues/8116)\r\n- [Authorization and Permissions](https://github.com/apache/airflow/issues/8112)\r\n- [Authentication in API](https://github.com/apache/airflow/issues/8111)\r\n- [Custom WEB UI screen to control permissions](https://github.com/apache/airflow/issues/8124)\r\n- [API security tests](https://github.com/apache/airflow/issues/8113)\r\n\r\n# Resources:\r\n\r\nAIP-32 on Wiki:\r\nhttps://cwiki.apache.org/confluence/display/AIRFLOW/AIP-32%3A+Airflow+REST+API\r\nDiscussion about AIP on the mailing list:\r\nhttps://lists.apache.org/thread.html/rbcee49452f1e2714fbdb91f12cfcb115d24681d01ccb6a3845a7e699%40%3Cdev.airflow.apache.org%3E\r\nVoting: on the mailing list \r\nhttps://lists.apache.org/thread.html/rcc379dc7067397e1a631c19b9f194f2f572a2ea7a338648788911c91%40%3Cdev.airflow.apache.org%3E\r\n\r\n# Contribution\r\n\r\nWe invite everyone to contribute.  We have #sig-api to decisions and to coordinate our work.\r\nRegistration link: https://apache-airflow-slack.herokuapp.com/\r\nAll changes are labelled [\"area:API\"](https://github.com/apache/airflow/labels/area%3AAPI) on Github.\r\nInformation about current tasks are available on Github Project: https://github.com/apache/airflow/projects/1\r\n\r\n\r\n"},{"labels":["api",null],"text":"<!--\r\n\r\nWelcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.\r\nDon't worry if they're not all applicable; just try to include what you can :-)\r\n\r\nIf you need to include code snippets or logs, please put them in fenced code\r\nblocks.  If they're super-long, please use the details tag like\r\n<details><summary>super-long log</summary> lots of stuff </details>\r\n\r\nPlease delete these comment blocks before submitting the issue.\r\n\r\n-->\r\n\r\n**Description**\r\n\r\nFrom: https://issues.apache.org/jira/projects/AIRFLOW/issues/AIRFLOW-621?filter=allopenissues&orderby=affectedVersion+ASC%2C+priority+DESC%2C+updated+DESC\r\n\r\n\"I'm looking for a mechanism to fetch a scheduled DAG instance's expected start time, in Oozie world it is populated as nominal time.\r\n\r\nFor my specific use case, I have a DAG that is scheduled to run every 15 mins and one of the task needs to use the DAG instance start time to query database and fetch all the rows that have been inserted/updated in last 15 mins. I need a mechanism to fetch the expected start time of the DAG instance and pass it to the task since the task actual start time might be after the expected start time.\"\r\n\r\n\r\n**Use case / motivation**\r\n\r\na mechanism to fetch a scheduled DAG instance's expected start time\r\n\r\n**Related Issues**\r\n\r\n<!-- Is there currently another issue associated with this? -->\r\n"}]