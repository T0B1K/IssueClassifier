[{"labels":["api",null,null,null],"text":"What is the expected output of passing a dictionary to `Series.transform`? For example:\r\n\r\n    s = pd.Series([1, 2, 3])\r\n    result1 = s.transform({'a': lambda x: x + 1})\r\n    result2 = s.transform({'a': lambda x: x + 1, 'b': lambda x: x + 2})\r\n\r\nThe docs say that `dict of axis labels -> functions` is acceptable, but I can't find any example in the docs where the output is described/shown. Under the hood, `Series.transform` is just calling `Series.aggregate` which produces the following outputs for `result1` and `result2`.\r\n\r\n````\r\n# result1\r\na  0    2\r\n   1    3\r\n   2    4\r\ndtype: int64\r\n\r\n# result2\r\na  0    2\r\n   1    3\r\n   2    4\r\nb  0    3\r\n   1    4\r\n   2    5\r\ndtype: int64\r\n````\r\n\r\n`result1` is deemed acceptable (the length of the result equals the length of the input) and is returned, but `result2` raises; it is not a transformation.\r\n\r\nI am wondering if a better return would be a DataFrame where the keys are the column names ('a' and 'b' in this example)."},{"labels":["api",null,null,null,null],"text":"Issues where `agg` is used with non-reducing functions:\r\n\r\n* #18103\r\n* #14741\r\n* #35490\r\n\r\nI think `agg` should raise if the function(s) provided are not reducers. This can be tested by if the resulting index is equal to `self.grouper.result_index`."},{"labels":["api",null],"text":"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.\r\n\r\n- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).\r\n\r\n---\r\n\r\n#### Question about pandas\r\n\r\nAlthough extremely common in the industry, \"dummy\" has some unfortunate history. One current use is for substitutes - mannequins, stand-ins, etc. This use grew from its original definition, \"mute person\". Mute people are not substitutes or stand-ins and I would prefer Pandas to not contribute to this view. There are other words, like \"indicator\", for statistics.\r\n\r\n[Pandas currently uses \"get_dummies\" as a function name, with the documentation referencing \"indicator\" as a synonym.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)\r\n\r\nCitations:\r\n\r\n1. https://www.etymonline.com/word/dummy\r\n2. https://www.etymonline.com/word/dumb\r\n3. https://www.etymonline.com/word/indicator"},{"labels":["api",null],"text":"Is there a way to convert DataFrames containing nullable types to DataFrames containing the equivalent non-nullable types?\r\n\r\nFor example:\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> a = pd.DataFrame()\r\n>>> a['a'] = pd.Series([1, None, 3], dtype=\"Int32\")\r\n>>> a['b'] = pd.Series([True, False, None], dtype=\"boolean\")\r\n>>> a\r\n      a      b\r\n0     1   True\r\n1  <NA>  False\r\n2     3   <NA>\r\n>>> a.dtypes\r\na      Int32\r\nb    boolean\r\ndtype: object\r\n```\r\n\r\nIs there a way to convert this to a DataFrame with `int32` and `bool` dtypes, assuming the user can specify `na_values` for those? Ideally, if there are no nulls in the input, then the user wouldn't even have to specify `na_value`.\r\n\r\n### Context:\r\n\r\ncuDF is [moving in the direction](https://github.com/rapidsai/cudf/issues/5754) of returning nullable Pandas objects when the `.to_pandas()` method is called on cuDF objects. Our hope is that this is a step in the right direction of the community adopting arrow-like nullable types. In the near term, however, this will break user workflows (e.g., https://github.com/rapidsai/cudf/issues/5928) and it would be fantastic if we could point users to a convenient way to recover \"classical\" Pandas objects from what we return.\r\n\r\ncc: @brandon-b-miller @kkraus14 "},{"labels":["api",null,null],"text":"- [x] I have checked that this issue has not already been reported.\r\n\r\n- [x] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n#### Code Sample:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n\r\ndf = pd.DataFrame({\"date\": [\"2019-02-10\", \"2019-02-10\", \"2019-02-11\"]})\r\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\r\n\r\nprint(\"Type before the for cycle:\")\r\nprint(type(df[\"date\"][0]))  # pandas._libs.tslibs.timestamps.Timestamp\r\n\r\nfor day in df[\"date\"].unique(): \r\n    print(\"Type in the loop:\")\r\n    print(type(day))  # here is a numpy.datetime64\r\n```\r\n\r\nwhich returns:\r\n\r\n```\r\nType before the for cycle:\r\n<class 'pandas._libs.tslibs.timestamps.Timestamp'>\r\nType in the loop:\r\n<class 'numpy.datetime64'>\r\nType in the loop:\r\n<class 'numpy.datetime64'>\r\n```\r\n\r\n#### Problem description\r\n\r\nThe function `unique()` should not cast the data type.\r\n\r\n#### Expected Output\r\n\r\ntypes of `df_target[\"date\"].unique()` should be the same as in ` set(df_target[\"date\"].to_list())`. E.g.\r\n\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n\r\ndf = pd.DataFrame({\"date\": [\"2019-02-10\", \"2019-02-10\", \"2019-02-11\"]})\r\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\r\n\r\nprint(\"Type before the for cycle:\")\r\nprint(type(df[\"date\"][0])) \r\n\r\nfor day in set(df[\"date\"].to_list()): \r\n    print(\"Type in the loop:\")\r\n    print(type(day))\r\n```\r\n\r\nReturning:\r\n```\r\nType before the for cycle:\r\n<class 'pandas._libs.tslibs.timestamps.Timestamp'>\r\nType in the loop:\r\n<class 'pandas._libs.tslibs.timestamps.Timestamp'>\r\nType in the loop:\r\n<class 'pandas._libs.tslibs.timestamps.Timestamp'>\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.7.final.0\r\npython-bits      : 64\r\nOS               : Darwin\r\nOS-release       : 19.5.0\r\nmachine          : x86_64\r\nprocessor        : i386\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_GB.UTF-8\r\nLOCALE           : en_GB.UTF-8\r\n\r\npandas           : 1.0.5\r\nnumpy            : 1.19.0\r\npytz             : 2020.1\r\ndateutil         : 2.8.1\r\npip              : 20.1.1\r\nsetuptools       : 47.3.1\r\nCython           : None\r\npytest           : 5.4.1\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.11.2\r\nIPython          : 7.16.1\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.2.2\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\npytest           : 5.4.1\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.5.2\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\nnumba            : None\r\n```\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"xref  #6249, #18262, #35301 \r\n\r\nFollowing up on #35301. This is an outstanding piece of `DataFrame.xs` functionality that's currently not available in `.loc`. Since `xs` is slated for deprecation I'd like implement this in `.loc`\r\n\r\n#### Example\r\nLet's define a multiindexed dataframe (example from `xs` docs)\r\n```\r\n   ...:  import pandas as pd\r\n   ...:  \r\n   ...: d = {  \r\n   ...:     'num_legs': [4, 4, 2, 2],  \r\n   ...:     'num_wings': [0, 0, 2, 2],  \r\n   ...:     'class': ['mammal', 'mammal', 'mammal', 'bird'],  \r\n   ...:     'animal': ['cat', 'dog', 'bat', 'penguin'],  \r\n   ...:     'locomotion': ['walks', 'walks', 'flies', 'walks'] \r\n   ...:     }  \r\n   ...:  \r\n   ...: df = pd.DataFrame(data=d) \r\n   ...: df.set_index(['class', 'animal', 'locomotion'], inplace=True)                                                 \r\n\r\nIn [2]: df                                                                                                            \r\nOut[2]: \r\n                           num_legs  num_wings\r\nclass  animal  locomotion                     \r\nmammal cat     walks              4          0\r\n       dog     walks              4          0\r\n       bat     flies              2          2\r\nbird   penguin walks              2          2\r\n```\r\n\r\n`xs` and `.loc` both let us take a cross-section through a `MultiIndex`\r\n\r\n```\r\nIn [3]: import pandas._testing as tm \r\n    ...: \r\n    ...: res_xs = df.xs(('mammal', slice(None))) \r\n    ...: res_loc  = df.loc[('mammal', slice(None))] \r\n    ...: tm.assert_frame_equal(res_xs, res_loc)                                                                       \r\n```\r\nbut with `xs` we can choose to keep the index columns through which we're slicing using the `drop_level` argument:\r\n```\r\nIn [4]: df.xs(('mammal', slice(None), 'flies'), drop_level=False)                                                    \r\nOut[4]: \r\n                          num_legs  num_wings\r\nclass  animal locomotion                     \r\nmammal bat    flies              2          2\r\n```\r\n\r\n#### new API\r\nLooking for ideas on what's appropriate here. Based off of suggestions in #6249 would something like this work?\r\n```\r\nIn [ ]: df.loc(drop_levels=False)[('mammal', slice(None), 'flies')]      \r\nOut[4]: \r\n                          num_legs  num_wings\r\nclass  animal locomotion                     \r\nmammal bat    flies              2          2\r\n                 \r\n```"},{"labels":["api"],"text":"- [x] I have checked that this issue has not already been reported.\r\n\r\n- [x] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n---\r\n#### Code snippet\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame(data={'a': np.arange(100), 'b': np.arange(100),})\r\n\r\ndf = df.quantile([0.2, 0.5, 0.8], axis=1)\r\n\r\n```\r\n#### Problem description\r\n\r\nUsing  `DataFrame.quantile` over axis=1 with multiple values for q transposes the dataframe. I would expect the quantiles to appear over the specified axis, so along the columns axis when axis = 1, and along rows when axis = 0. Ofcourse, an easy fix is a call to `DataFrame.transpose`. Ideally this should not be necessary in my opinion.\r\n\r\n#### Expected Output\r\nQuantile values over the specified axis without transposing the dataframe.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nC:\\Anaconda64\\lib\\site-packages\\setuptools\\distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\r\n  \"Distutils was imported before Setuptools. This usage is discouraged \"\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.6.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : nl_NL.UTF-8\r\nLOCALE           : None.None\r\n\r\npandas           : 1.0.5\r\nnumpy            : 1.18.5\r\npytz             : 2020.1\r\ndateutil         : 2.8.1\r\npip              : 20.1.1\r\nsetuptools       : 49.2.0.post20200712\r\nCython           : 0.29.21\r\npytest           : 5.4.3\r\nhypothesis       : 5.19.3\r\nsphinx           : 3.1.2\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 1.2.9\r\nlxml.etree       : 4.5.2\r\nhtml5lib         : 1.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.11.2\r\nIPython          : 7.16.1\r\npandas_datareader: None\r\nbs4              : 4.9.1\r\nbottleneck       : 1.3.2\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.5.2\r\nmatplotlib       : 3.2.2\r\nnumexpr          : 2.7.1\r\nodfpy            : None\r\nopenpyxl         : 3.0.4\r\npyarrow          : None\r\npytables         : None\r\npytest           : 5.4.3\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.3.1\r\nsqlalchemy       : 1.3.18\r\ntables           : 3.6.1\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : 1.2.9\r\nnumba            : 0.50.1\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"**Is your feature request related to a problem?**\r\nWhen doing feature selection for models, or with handling missing data, in most cases you want to dynamically set a threshold to drop column which exceed this. A percentage is a more general metric to use to see how many data is missing data per row or per column. Right now we have the `thresh` argument, which accepts an int, but in most cases this is not desirable. For example when running the code in production, the shape of the data changes over time and thus a percentage threshold would make more sense to me. Or when you don't know the data well enough, what does \"a number\" mean in this case. Percentage is standarized.\r\n\r\nBesides that, the thresh right now is not flexible to go over column or index axis. For data cleaning you might want to drop rows which exceed the threshold, but for feature selection, you want to treat each column separately.\r\n\r\n**Describe the solution you'd like**\r\nHave a percentage threshold for both column and index axis.\r\n\r\n**API breaking implications**\r\nNot that I can think of, but not 100% sure.\r\n\r\n**Describe alternatives you've considered**\r\nWriting own custom functions.\r\n\r\n**Additional context**\r\n\r\nLocally I created a version where the `perc` argument is created (see linked draft PR):\r\n\r\n**example 1**:\r\n```python\r\n>>> df = pd.DataFrame({'col': [\"A\", \"A\", \"B\", \"B\"],\r\n...                    'A': [80, np.nan, np.nan, np.nan],\r\n...                    'B': [80, np.nan, 76, 67]})\r\n>>> df\r\n  col     A     B\r\n0   A  80.0  80.0\r\n1   A   NaN   NaN\r\n2   B   NaN  76.0\r\n3   B   NaN  67.0\r\n\r\n>>> df.dropna(perc=0.5)\r\n  col     A     B\r\n0   A  80.0  80.0\r\n2   B   NaN  76.0\r\n3   B   NaN  67.0\r\n\r\n>>> df.dropna(perc=0.5, axis=1)\r\n  col     B\r\n0   A  80.0\r\n1   A   NaN\r\n2   B  76.0\r\n3   B  67.0\r\n```\r\n---\r\n**example 2**:\r\n```python\r\n>>> df = pd.DataFrame(np.random.randint(1, 10, (10,4)), columns=list('ABCD'))\r\n>>> df.loc[3, 'B':] = np.nan\r\n>>> df.loc[8, :'B'] = np.nan\r\n>>> df.loc[1:6, 'C'] = np.nan\r\n>>> df\r\n     A    B    C    D\r\n0  7.0  2.0  3.0  7.0\r\n1  3.0  2.0  NaN  9.0\r\n2  2.0  2.0  NaN  2.0\r\n3  7.0  NaN  NaN  NaN \r\n4  1.0  9.0  NaN  4.0\r\n5  9.0  9.0  NaN  1.0\r\n6  2.0  2.0  NaN  6.0\r\n7  9.0  3.0  5.0  6.0\r\n8  NaN  NaN  9.0  5.0\r\n9  7.0  7.0  3.0  1.0\r\n```\r\nAs we can see, index 3 (`axis=0`) has 75% missing values and column C (`axis=1`) has 60% missing values. With the `percentage` argument we can specify what the threshold is, but also consider row wise or column wise:\r\n\r\nExample consider **per row**:\r\n```python\r\n>>> df.dropna(perc=.4, axis=0)\r\n     A    B    C    D\r\n0  7.0  2.0  3.0  7.0\r\n1  3.0  2.0  NaN  9.0\r\n2  2.0  2.0  NaN  2.0\r\n4  1.0  9.0  NaN  4.0\r\n5  9.0  9.0  NaN  1.0\r\n6  2.0  2.0  NaN  6.0\r\n7  9.0  3.0  5.0  6.0\r\n9  7.0  7.0  3.0  1.0\r\n```\r\nAbove we can see row 3 and 8 got dropped because these had ` > 40%` missing values.\r\n```python\r\n>>> df.dropna(perc=.4, axis=1)\r\n     A    B    D\r\n0  7.0  2.0  7.0\r\n1  3.0  2.0  9.0\r\n2  2.0  2.0  2.0\r\n3  7.0  NaN  NaN\r\n4  1.0  9.0  4.0\r\n5  9.0  9.0  1.0\r\n6  2.0  2.0  6.0\r\n7  9.0  3.0  6.0\r\n8  NaN  NaN  5.0\r\n9  7.0  7.0  1.0\r\n```\r\nSame command but with `axis=1`, so we can consider percentage threshold **per column**, and we see that column C got dropped because it had 60% missing values\r\n\r\n\r\n\r\n"},{"labels":["api",null],"text":"xref https://github.com/pandas-dev/pandas/pull/34998#issuecomment-658318898.\r\n\r\nCurrently (on master and in my PR at https://github.com/pandas-dev/pandas/pull/34998), `.groupby(...).apply` has some value-dependent behavior. Specifically\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df1 = pd.DataFrame({\"A\": [2, 1, 2], \"B\": [1, 2, 3]})\r\n   ...: df2 = pd.DataFrame({\"A\": [2, 1, 2], \"B\": [1, 2, 1]})  # duplicates in group \"2\"\r\n\r\nIn [3]: df1.groupby(\"A\", group_keys=False).apply(lambda x: x.drop_duplicates())\r\nOut[3]:\r\n   A  B\r\n0  2  1\r\n1  1  2\r\n2  2  3\r\n\r\nIn [4]: df2.groupby(\"A\", group_keys=False).apply(lambda x: x.drop_duplicates())\r\nOut[4]:\r\n   A  B\r\n1  1  2\r\n0  2  1\r\n```\r\n\r\nInternally, groupby constructs a list of DataFrames, one per group, that are the results of the UDF applied to each group. Those are concatenated together, and are at this point in \"group\" order. If we detect that the `.apply` was actually a transform, we reindex the concatenated result back to the original index.\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/b6222ec976a71b4ba0c643411606e2376e744c4d/pandas/core/groupby/groupby.py#L1114-L1128\r\n\r\nOut[3] has been viewed as a transform and so was reindexed. Whether or not the UDF was a transform depends on the values, and we generally discourage this type of values-dependent behavior.\r\n\r\nTo solve this, we have a few options\r\n\r\n1. Implement a \"table-wise\" transform. This solves the usecase where people are using `.apply` rather than transform just because it operates on dataframes rather than columns. We could do this through `.groupby(..., axis=None).transform()` or through `.groupby(...).transform_table()` / `transform_frame()`. This doesn't help with the `drop_duplicates` example, which is more of a filter (that sometimes doesn't filter anything).\r\n2. Implement a \"table-wise\" filter. Currently `.groupby().filter()` expects the UDF to return a scalar, and filters *groups* based on that. It could be expanded to also allow the UDF to return an array. In this case it would filter *rows* where the returned value evaluates to True. This would solve the `drop_duplicates` use case, but not all use cases.\r\n3. Regardless of whether 1 or 2 are implemented, add a `reindex_output` keyword to groupby to control this very narrow case. This would only be relevant when `group_keys=False` and we've detected an apply. It gives users control over whether or not the result is reindexed.\r\n\r\n```python\r\n>>> df1.groupby(\"A\", group_keys=False, reindex_output=True).apply(lambda x: x.drop_duplicates())\r\n   A  B\r\n0  2  1\r\n1  1  2\r\n2  2  3\r\n\r\n>>> df1.groupby(\"A\", group_keys=False, reindex_output=False).apply(lambda x: x.drop_duplicates())\r\n   A  B\r\n1  1  2\r\n0  2  1\r\n2  2  3\r\n```\r\n\r\nIt has no effect in any other case, including `group_keys=False`. By default, it can be `None` to preserve the values-dependent behavior on master. Though we can explore deprecating it if there's any desire."},{"labels":["api",null,null],"text":"In most cases, there is a relationship between `index.get_loc` and `index.__eq__`.  `index.get_loc(key)` roughly matches `(index == key).nonzero()`\r\n\r\n`DatetimeIndex.get_loc` has special treatment for strings that we could extend to DatetimeIndex (and DatetimeArray) `__eq__`.\r\n\r\n```\r\n>>> dti = pd.date_range(\"2016-01-01 20:00\", periods=10, freq=\"H\")\r\n>>> s = \"2016-01-01\r\n>>> dti.get_loc(s)\r\nslice(0, 4, None)\r\n>>> dti == s\r\narray([False, False, False, False, False, False, False, False, False, False])\r\n```\r\n\r\nUnder this proposal, the last comparison here would give `array([True, True, True, True, False, False, False, False, False, False])`\r\n\r\nThe main benefit of this would be internal consistency.  Possibly some code sharing, but I'm not ready to make any promises on that front."},{"labels":["api",null],"text":"#### Location of the documentation\r\n\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\r\n\r\n#### Documentation problem\r\n\r\nHow should I get the correlation among the rows? I can obviously do `df.T.corr()` but this strikes me as a workaround rather than a nice way to do things. I wrote a `df.corr(axis=1)` assuming that would work. Curious what people feel is idiomatic, and would recommend some mention in the docs. "},{"labels":["api",null],"text":"i.e. when we get `pd.Timestamp.now(\"UTC\")` we should return a Timestamp with `datetime.timezone.utc` rather than `pytz.UTC`.  Similarly when we parse an ISO8601 datetime we should use tzinfo of `timezone(timedelta(seconds=val*60))` instead of `pytz.FixedOffset(val)`\r\n\r\nThis isn't _that_ hard to implement, but doing it breaks a couple dozen tests where we are currently checking for pytz objects.  This would technically be an API change, so putting it up for discussion before implementing it."},{"labels":["api",null],"text":"On master PeriodDtype has a `.dtype_code` attribute.\r\n\r\n```python\r\nIn [5]: pd.PeriodDtype('D').dtype_code\r\nOut[5]: 6000\r\n```\r\n\r\nThat wasn't there in 1.0.x. I think we want it to be private."},{"labels":["api",null,null],"text":"This was inspired by #33888 and #34584\r\n\r\n#### Problem description\r\nThe behavior of `copy` argument in `df.reindex` is confusing. Current docstring does it explain it sufficiently clear. It also seems to me `copy` is unnecessary.\r\n\r\nCurrently the docstring says\r\n```\r\n...\r\n\r\nA new object is produced unless the new index is equivalent to the current one and ``copy=False``.\r\n\r\n...\r\n\r\ncopy : bool, default True\r\n       Return a new object, even if the passed indexes are the same.\r\n```\r\n\r\nIt is hard to clarify what is considered an \"equivalent\" index. See below for more details.\r\n\r\nFurther, I believe users rarely purposefully tries to `reindex` with an \"equivalent\" index. It happens only if the user does not yet know the current index or the index to conform to, in which case a consistent behavior (e.g. always return new object) is probably preferred.\r\n\r\n```python\r\n# On current master\r\n>>> pd.__version__\r\n'1.1.0.dev0+1802.g942beba1e'\r\n\r\n>>> df = pd.DataFrame(range(3))\r\n>>> df\r\n   0\r\n0  0\r\n1  1\r\n2  2\r\n>>> df.index\r\nRangeIndex(start=0, stop=3, step=1)\r\n\r\n# not equivalent\r\n>>> df is df.reindex(range(3), copy=False)\r\nFalse\r\n\r\n# not equivalent\r\n>>> df is df.reindex(list(range(3)), copy=False)\r\nFalse\r\n\r\n# equivalent\r\n>>> df is df.reindex(pd.RangeIndex(start=0, stop=3, step=1), copy=False)\r\nTrue\r\n\r\n>>> df = pd.DataFrame(range(3), index=list(range(3)))\r\n>>> df\r\n   0\r\n0  0\r\n1  1\r\n2  2\r\n>>> df.index\r\nInt64Index([0, 1, 2], dtype='int64')\r\n\r\n# not equivalent\r\n>>> df is df.reindex(range(3), copy=False)\r\nFalse\r\n\r\n# even this is considered not equivalent\r\n>>> df is df.reindex(list(range(3)), copy=False)\r\nFalse\r\n\r\n>>> df is df.reindex(pd.Int64Index([0, 1, 2]), copy=False)\r\nTrue\r\n\r\n```\r\n\r\nYou can see it is actually pretty strict to be \"equivalent\".  I feel it does really make sense to have this `copy` parameter because `reindex` will return a new object in most cases anyway even when `copy=False`.\r\n\r\nSo the question is, can we deprecate `copy`? "},{"labels":["api",null],"text":"xref mailing list [discussion](https://mail.python.org/pipermail/pandas-dev/2020-June/001246.html)\r\n\r\nATM most calls to `DataFrame(foo)` will end up going through either `core.internals.managers.create_manager_from_blocks` or `core.internals.managers.create_block_manager_from_arrays`, both of which call `mgr._consolidate_inplace()` before returning a `BlockManager` object.  I think we should consider disabling this consolidation.  (There is also consolidation-by-another-name within form_blocks that im still tracking down)\r\n\r\nBehavior-wise, consider:\r\n\r\n```\r\narr = np.random.randn(10)\r\nser = pd.Series(arr)\r\n\r\ndf = pd.DataFrame({\"A\": arr, \"B\": ser})\r\n```\r\n\r\nATM we get a new 2x10 array backing `df`.  The proposed change would mean that we keep 2 separate arrays, and `df[\"B\"]` shares data with `ser`.  I find the proposed behavior more intuitive.\r\n\r\nPerformance-wise, we expect faster construction but slower subsequent operations. [note to self: get some estimates/measurements here].  For long-lived DataFrames where the subsequent slowdowns add up, we can suggest users call `_consolidate()` (which i guess we would have to de-privatize)\r\n"},{"labels":["api",null,null],"text":"In the process of untangling Resolution/FreqGroup/PeriodDtype/Offsets im finding it would be helpful if `to_offset(\"year\")` behaved like `to_offset(\"Y\")` etc.  But apparently these are specifically [deprecated](https://github.com/pandas-dev/pandas/blob/master/pandas/tests/scalar/period/test_period.py#L724).  \r\n\r\nThis test was introduced [here](https://github.com/pandas-dev/pandas/commit/4da9b15f58cfcde998d6279a90d55a170145f8c2) (commit but no PR) and is listed as closing #11854 and #12540, but neither of those really make it clear to me why we wouldn't allow these.\r\n\r\nAny objection to having to_offset recognize these strings?"},{"labels":["api",null,null],"text":"It would be really handy to have the number of missing values as well as the cardinality, i.e. the lenght of df.x.value_counts() for each column x in the describe table. \r\n\r\nCount is indirectly reflecting this value, but ultimatily you have to think about the corner and first calculate the length of the dataframe before it leads to the numbe of missing values. \r\n\r\nThe cardinality is important for categorical features. "},{"labels":["api"],"text":"#### Is your feature request related to a problem?\r\n\r\nEvery single time I have exported a dataframe (usually with .to_csv), I have not needed the index. \r\n\r\n#### Describe the solution you'd like\r\n\r\nChange the default from True to False.\r\n\r\n#### API breaking implications\r\n\r\nAll code that has not explicitely set index=True should make it.\r\n\r\n#### Describe alternatives you've considered\r\n\r\nLeave things as they are now.\r\n\r\n\r\nNot using the index is something usual or am I biased by my own experience? \r\n\r\n\r\n"},{"labels":["api",null,null,null],"text":"#### Location of the documentation\r\n\r\nhttps://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_markdown.html\r\n\r\n\r\n#### Documentation problem\r\n\r\nThe document of `pandas.DataFrame.to_markdown` is unfriendly because many options are shown as `**kwargs`.\r\nI think these arguments should be documented explicitly like a `to_csv`.\r\n\r\nhttps://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_csv.html\r\n\r\n\r\n#### Suggested fix for documentation\r\n\r\nAll arguments supported by `to_markdown` should be shown at document.\r\nIf don't mind, I would like to tackle this improvement."},{"labels":["api",null],"text":"Currently, the `SparseArray.astype` function will always convert the specified target dtype to a sparse dtype, if it is not one. For example, this gives:\r\n\r\n```\r\nIn [64]: arr = pd.arrays.SparseArray([1, 0, 0, 2])  \r\n\r\nIn [65]: arr   \r\nOut[65]: \r\n[1, 0, 0, 2]\r\nFill: 0\r\nIntIndex\r\nIndices: array([0, 3], dtype=int32)\r\n\r\nIn [66]: arr.astype(float)  \r\nOut[66]: \r\n[1.0, 0.0, 0.0, 2.0]\r\nFill: 0.0\r\nIntIndex\r\nIndices: array([0, 3], dtype=int32)\r\n```\r\n\r\nThis ensures that a simple `astype` doesn't densify the sparse array (and you don't need to do `astype(pd.SparseDtype(float, fill_value))`). \r\nAnd note this also gives this behaviour to `Series.astype(..)`\r\n\r\nBut, this also gives the inconsistency that `arr.astype(target_dtype).dtype != target_dtype`, so you can rely on the fact that you get back an array of the actual dtype that you specified. \r\nSee eg the workaround I need to add for this in https://github.com/pandas-dev/pandas/pull/34338\r\n"},{"labels":["api",null,null],"text":"I discovered this while trying to tackle issue #32344, where @ryankarlos mentioned `groupby.transform('tshift', ...)` seems to behave incorrectly.\r\n\r\nHowever, before we can address #32344, we probably need to address this.\r\n```python\r\n# on current master\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n\r\n>>> pd.__version__\r\n'1.1.0.dev0+1708.g043b60920'\r\n\r\n>>> df = pd.DataFrame(\r\n...     {\r\n...     \"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"baz\"],\r\n...     \"B\": [1, 2, np.nan, 3, 3, np.nan, 4],\r\n...     },\r\n...     index=pd.date_range('2020-01-01', '2020-01-07')\r\n... )\r\n>>> df\r\n              A    B\r\n2020-01-01  foo  1.0\r\n2020-01-02  foo  2.0\r\n2020-01-03  foo  NaN\r\n2020-01-04  foo  3.0\r\n2020-01-05  bar  3.0\r\n2020-01-06  bar  NaN\r\n2020-01-07  baz  4.0\r\n\r\n>>> df.groupby(\"A\").tshift(1, \"D\")\r\n                  B\r\nA\r\nbar 2020-01-06  3.0\r\n    2020-01-07  NaN\r\nbaz 2020-01-08  4.0\r\nfoo 2020-01-02  1.0\r\n    2020-01-03  2.0\r\n    2020-01-04  NaN\r\n    2020-01-05  3.0\r\n\r\n>>> df.groupby(\"A\").ffill()\r\n              B\r\n2020-01-01  1.0\r\n2020-01-02  2.0\r\n2020-01-03  2.0\r\n2020-01-04  3.0\r\n2020-01-05  3.0\r\n2020-01-06  3.0\r\n2020-01-07  4.0\r\n\r\n>>> df.groupby(\"A\").cumsum()\r\n              B\r\n2020-01-01  1.0\r\n2020-01-02  3.0\r\n2020-01-03  NaN\r\n2020-01-04  6.0\r\n2020-01-05  3.0\r\n2020-01-06  NaN\r\n2020-01-07  4.0\r\n```\r\n\r\nWe can see that `groupby.tshift` is inconsistent with other groupby transformations. It retains the groupby column, and more importantly reordered the data.\r\n\r\nSince 0.25 we have had deliberate effort to make all groupby transformations consistent, see https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.25.0.html#dataframe-groupby-ffill-bfill-no-longer-return-group-labels\r\n\r\nFollowing this thinking I would expect the returned data to behave more like \r\n```python\r\n>>> df.groupby(\"A\").tshift(1, \"D\")  # this is actually the result of df.tshift(1, \"D\").drop(columns='A')\r\n              B\r\n2020-01-02  1.0\r\n2020-01-03  2.0\r\n2020-01-04  NaN\r\n2020-01-05  3.0\r\n2020-01-06  3.0\r\n2020-01-07  NaN\r\n2020-01-08  4.0\r\n```\r\n\r\nHowever, if we are to make `groupby.tshift` consistent with other groupby transformation like the above, this makes it no different from `df.tshift(1, \"D\").drop(columns='A')', and `groupby` has lost its meaning here.\r\n\r\nPerhaps we should just deprecate `groupby.tshift` entirely? I know #11631 discussed about deprecating `tshift`, but that has been stalled for a long time."},{"labels":["api",null,null],"text":"- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.\r\n\r\n#### Code Sample, a copy-pastable example\r\n\r\n```python\r\nimport pandas as pd\r\nfrom datetime import datetime\r\ndata=pd.DataFrame([[430., 630.],\r\n                         [540., 660.],\r\n                         [610., 720.]],\r\n                        columns=[1, 2],\r\n                        index=pd.date_range(datetime(2019, 1, 1, 12),\r\n                                            datetime(2019, 1, 1, 12, 30),\r\n                                            freq='15min'))\r\npd.tseries.frequencies.to_offset(pd.infer_freq(data.index))\r\n>> <15 * Minutes>\r\npd.tseries.frequencies.to_offset(pd.infer_freq(data.index)) / 2\r\n>> <450 * Secondes>\r\npd.tseries.offsets.DateOffset(minutes=15)\r\n>> <DateOffset: minutes=15>\r\npd.tseries.offsets.DateOffset(minutes=15) / 2\r\n>> TypeError: unsupported operand type(s) for /: 'DateOffset' and 'int'\r\n\r\n```\r\n#### Problem description\r\nThe function `pd.tseries.frequencies.to_offset` does not really provide a `DateOffset`object. As I understand the code right there are several types of DateOffsets e.g. Minutes, Seconds that are not compareable with the DateOffset object because they are having different properties.\r\n\r\n\r\n\r\n#### Expected Output\r\nOne Unified pd.DateOffset Object with same properties for all inherit Objects like Minutes and Seconds\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[INSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.6.9.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 4.15.0-99-generic\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : de_DE.UTF-8\r\nLOCALE           : de_DE.UTF-8\r\n\r\npandas           : 1.0.3\r\nnumpy            : 1.16.0\r\npytz             : 2018.9\r\ndateutil         : 2.8.1\r\npip              : 20.0.2\r\nsetuptools       : 44.0.0\r\nCython           : 0.28.3\r\npytest           : 4.3.0\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : 4.2.1\r\nhtml5lib         : 0.999999999\r\npymysql          : 0.9.3\r\npsycopg2         : 2.8.4 (dt dec pq3 ext lo64)\r\njinja2           : 2.10\r\nIPython          : 7.11.1\r\npandas_datareader: None\r\nbs4              : 4.6.0\r\nbottleneck       : 1.3.0\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.2.1\r\nmatplotlib       : 2.2.2\r\nnumexpr          : 2.6.4\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 0.13.0\r\npytables         : None\r\npytest           : 4.3.0\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.4.1\r\nsqlalchemy       : 1.3.16\r\ntables           : 3.4.2\r\ntabulate         : 0.8.5\r\nxarray           : 0.10.9\r\nxlrd             : 1.1.0\r\nxlwt             : None\r\nxlsxwriter       : None\r\nnumba            : 0.42.0\r\n]\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"From [SO](https://stackoverflow.com/a/61891863/2901002):\r\n\r\n    df1 = (df.set_index(['Animal', df.groupby('Animal').cumcount().add(1)])\r\n             .unstack()\r\n             .sort_index(axis=1, level=1))\r\n    df1.columns = [f'{a}_{b}' for a, b in df1.columns]\r\n    df1 = df1.reset_index()\r\n    print (df1)\r\n      Animal  Age_1 Color_1 Length_1  Age_2 Color_2 Length_2  Age_3 Color_3  \\\r\n    0    Cat      1   Brown     50cm      2   White     60cm      3   Brown   \r\n    1    Dog      1   White     99cm      2   White    129cm      3   White   \r\n    \r\n      Length_3  \r\n    0     55cm  \r\n    1    105cm  \r\n\r\nIf I want chain flatten `MultIindex` for one line solution, is it possible? Now `f-string`s cannot chain with `sort_index` and `reset_index` (or not idea how).\r\n\r\nIs possible implemented [`MultiIndex.to_flat_index`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.to_flat_index.html) for `DataFrame` ?\r\n\r\n\r\n    df1 = (df.set_index(['Animal', df.groupby('Animal').cumcount().add(1)])\r\n             .unstack()\r\n             .sort_index(axis=1, level=1)\r\n             .to_flat_index(axis=1)\r\n             .reset_index())\r\n"},{"labels":["api"],"text":"Then we would never actually return an Index object.\r\n\r\nWe could then make ABCIndex behave like ABCIndexClass, and get rid of the latter.\r\n\r\nBriefly discussed in #34159"},{"labels":[null,"api",null,null,null],"text":"What do you think of adding an option to « idmax » in order to return not just the first idmax but all of them in case of ties.\r\n\r\nI think this would be really useful, especially  when used with groupby. For exemple when you want the most recent yearly published data for an entity - say a company.\r\n\r\nWe could simply add a Boolean to the function like returnTies that would be false by default."},{"labels":["api",null],"text":"@jorisvandenbossche I expected this to return pd.NA.  Is this intentional?"},{"labels":["api",null,null],"text":"- [x] I have checked that this issue has not already been reported.\r\n\r\n- [x] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.\r\n\r\n#### Code Sample, a copy-pastable example\r\n\r\n<img width=\"1143\" alt=\"image\" src=\"https://user-images.githubusercontent.com/33289453/81488715-0c2db980-929f-11ea-9960-61d24d1ab44f.png\">\r\n\r\n```python\r\n\r\n#%%\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n#%%\r\ndf = pd.DataFrame(np.arange(16).reshape(4,4), index=pd.MultiIndex.from_arrays([list(\"AABB\"), list(\"XYXY\")]), columns=list(\"abcd\"))\r\ndf\r\n\r\n#%%\r\ndf.index, df.index.levels\r\n\r\n#%%\r\ndf_new = df.drop(index=\"A\")\r\ndf_new\r\n\r\n#%%\r\ndf_new.index, df_new.index.levels\r\n\r\n```\r\n\r\n#### Problem description\r\n\r\n[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]\r\nThe levels of MultiIndex of DataFrame never change when creating a new DataFrame by dropping a row. When creating a new DataFrame, the levels should match the new MultiIndex.\r\n\r\n#### Expected Output\r\ndf_new.index.levels \r\n    should be \r\nFrozenList([['B'], ['X', 'Y']]))\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.7.final.0\r\npython-bits      : 64\r\nOS               : Darwin\r\nOS-release       : 19.4.0\r\nmachine          : x86_64\r\nprocessor        : i386\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : None.UTF-8\r\n\r\npandas           : 1.0.3\r\nnumpy            : 1.18.1\r\npytz             : 2019.3\r\ndateutil         : 2.8.1\r\npip              : 20.0.2\r\nsetuptools       : 46.1.3.post20200330\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : 2.4.4\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.11.1\r\nIPython          : 7.13.0\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.1.3\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : 3.0.3\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\npytest           : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : None\r\nnumba            : None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Location of the documentation\r\n\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.skew.html\r\n\r\n#### Documentation problem\r\n\r\nIn the function signature it says `skipna=None` even though the parameter documentation underneath says `skipna bool, default True`.\r\n\r\n#### Suggested fix for documentation\r\n\r\nThe `skipna=None` in function signature should be changed to `skipna=True`.\r\n"},{"labels":["api",null,null],"text":"Would it be possible to create a default pandas global setting to enable numba engine whenever possible?\r\n\r\nIn pandas.DataFrame.apply, .transform, etc.\r\n\r\nThanks!"},{"labels":["api",null],"text":"Now we are adding ExtensionArray.argmin/argmax (#24382 / #27801), we also need to decide on its behaviour for nullable / masked arrays. \r\nFor the default of `skipna=True`, I think the behaviour is clear (we skip those values, and calculate the argmin/argmax of the remaining values), but we need to decide on the behaviour in case of `skipna=False` in presence of missing values.\r\n\r\nI don't think it makes much sense to follow the current behaviour for Series with NaNs:\r\n\r\n```\r\nIn [25]: pd.Series([1, 2, 3, np.nan]).argmin() \r\nOut[25]: 0\r\n\r\nIn [26]: pd.Series([1, 2, 3, np.nan]).argmin(skipna=False)  \r\nOut[26]: -1\r\n```\r\n\r\n(the -1 is discussed separately in https://github.com/pandas-dev/pandas/issues/33941 as well)\r\n\r\nWe also can't really compare with numpy, as there NaNs are regarded as the largest values (which follows somewhat from the sorting behaviour with NaNs).\r\n\r\nFor other reductions, the logic is: once there is an NA present, the result is also an NA with `skipna=False` (with the logic here that the with NAs, the min/max is not known, and thus also not the location). \r\n\r\nSo something like:\r\n\r\n```python\r\n>>> pd.Series([1, 2, 3, pd.NA], dtype=\"Int64\").argmin(skipna=False)  \r\n<NA>\r\n```\r\n\r\nHowever, this also means that `argmin`/`argmax` return something that is not an integer, and since those methods are typically used to get a value that can be used to index, that might also be annoying. \r\nAn alternative could also be to raise an error (similar as for empty arrays).\r\n\r\n"},{"labels":["api",null],"text":"Ran into this while looking at https://github.com/pandas-dev/pandas/pull/27801. \r\n\r\nBoth numpy and pandas raise an error for empty arrays/Series in `argmin` and `argmax`:\r\n\r\n```python\r\n>>> pd.Series([], dtype=float).argmin()   \r\n...\r\nValueError: attempt to get argmin of an empty sequence\r\n\r\n>>> np.array([], dtype=float).argmin()  \r\n...\r\nValueError: attempt to get argmin of an empty sequence\r\n```\r\n\r\nHowever, when having all NaN data, we see a different behaviour:\r\n\r\n```python\r\n>>> pd.Series([np.nan, np.nan], dtype=float).argmin() \r\n-1\r\n\r\n>>> np.array([np.nan, np.nan], dtype=float).argmin() \r\n0\r\n```\r\n\r\nDoes somebody have an explanation of *why* this would return -1 ?\r\n\r\nIn principle, in pandas, `argmin` has a `skipna=True` keyword, so for pandas I would expect that an all-NaN Series behaves the same as an empty Series."},{"labels":["api",null],"text":"It would be great if we could construct a `pd.DataFrame` from a function.\r\n\r\n#### Describe the solution you'd like\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndef my_random():\r\n    return np.random.rand() + 1\r\n\r\ndf = pd.DataFrame(my_random, index=range(10), columns=range(15))\r\n```\r\n\r\nThe output would be equivalent to\r\n\r\n```python\r\ncol_len = 10\r\nindex_len = 15\r\ndata = np.array([[my_random() for i in range(col_len)] for j in range(index_len)])\r\ndf = pd.DataFrame(data, index=range(index_len), columns=range(columns_len))\r\n```\r\n\r\nAlternatively, we could do the same with something like\r\n\r\n```python\r\ndf = pd.DataFrame.from_function(my_random, *args, **kwargs)\r\n```\r\n\r\n`args` and `kwargs` are passed to the df constructor.\r\n\r\n## EDIT\r\nI just realized I can easily obtain what I want with\r\n\r\n```python\r\ndf = pd.DataFrame(index=range(10), columns=range(15)).applymap(\r\n    lambda x: my_random()\r\n)\r\n```\r\n\r\nIt's quite compact and easy to read. It's slowish but I don't expect it to be used in high performance tasks."},{"labels":["api"],"text":"This will allow pd.factorize to dispatch to the EA/Index/Series implementations instead of the other way around."},{"labels":["api"],"text":"These should behave the same, shouldn't they?  If so, we should use the more standard usage (view).\r\n\r\nDitto for `_shallow_copy()` with no args."},{"labels":[null,"api",null],"text":"#### Is your feature request related to a problem?\r\n\r\nControl the NA value used when converting a DataFrame to an ndarray.\r\n\r\n#### Describe the solution you'd like\r\n\r\nAdd an `na_value` argument to `DataFrame.to_numpy()`. Has the same meaning as `na_value` in `Series.to_numpy`, except it applies to all the columns.\r\n\r\n#### Additional context\r\n\r\n```python\r\nIn [8]: df = pd.DataFrame({\"A\": pd.array([1, None])})\r\n\r\nIn [9]: df.to_numpy(na_value=np.nan)\r\n```\r\n\r\nThis has come up in a few places (cc @jorisvandenbossche @dsaxton)"},{"labels":[null,"api",null],"text":"For arithmetic-like ops when we see a list we cast it to ndarray, while with tuple we _sometimes_ cast to ndarray, sometimes treat it as scalar-like.  We should be consistent about this.\r\n\r\nI lean towards always-scalar-like, but would be fine either way."},{"labels":["api",null,null],"text":"https://github.com/pandas-dev/pandas/pull/33538 implemented sum as a standalone method for IntegerArray which somewhat duplicates the already-existing _reduce path taken by DataFrame / Series for certain reductions (see https://github.com/pandas-dev/pandas/pull/33538#pullrequestreview-393402534). Likely it makes sense to implement these at the IntegerArray / BooleanArray / etc. level using that same approach, e.g., something like defining each reduction to simply call _reduce with the appropriate operation name instead of copy / pasting the logic into each method."},{"labels":["api",null,null,null],"text":"Migrating the discussion from #33597:\r\n\r\nWe can improve performance (and memory footprint) by returning views in more cases.  The downsides are 1) the returned DataFrames are less-consolidated and 2) users may be relying on getting copies in some situations."},{"labels":["api"],"text":"When doing a setitem into a dt64/td64 Series, we cast `np.nan` to `NaT`.  The alternative would be to cast to object dtype and set np.nan unchanged.\r\n\r\nSimilarly anything that ends up calling `is_valid_nat_for_dtype` will allow through np.nan which ends up getting cast to NaT.\r\n\r\nWe should consider deprecating this behavior and requiring users to pass NaT directly."},{"labels":["api",null,null,null,null],"text":"- [x] I have checked that this issue has not already been reported.\r\n\r\n- [x] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [x] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.\r\n\r\n#### Code Sample, a copy-pastable example\r\n\r\n```python\r\n>>> pd.Categorical([1, \"2\", 3, 4])\r\n[1, 2, 3, 4]\r\nCategories (4, object): [1, 3, 4, 2]\r\n```\r\n\r\n#### Problem description\r\n\r\ndoes not show the string elements quoted. This is inconsistent with numpy.\r\n\r\n```python\r\n>>> np.array([1, \"2\", 3, 4], dtype=\"object\")\r\narray([1, '2', 3, 4], dtype=object)\r\n```\r\n#### Expected Output\r\n\r\n```python\r\n>>> pd.Categorical([1, \"2\", 3, 4])\r\n[1, '2', 3, 4]\r\nCategories (4, object): [1, 3, 4, '2']\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null,null],"text":"- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.\r\n\r\n#### Code Sample, a copy-pastable example\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfs = 50000\r\ni = pd.to_timedelta(np.arange(900 * fs, dtype=np.int) / fs * 1e9, unit='ns')\r\ndf = pd.DataFrame({'dummy': np.arange(len(i))}, index=i)\r\nassert np.isclose(len(df['710s':'711s']) / fs, 2.0)\r\nassert np.isclose(len(df['710s':'719s']) / fs, 10.0)\r\nassert np.isclose(len(df['610s':'620s']) / fs, 11.0)\r\nassert np.isclose(len(df['710s':'720.00000001s']) / fs, 10.00002)\r\nassert np.isclose(len(df['710s':'720s']) / fs, 11.0)  # fails! Slices 70 seconds of data??\r\n```\r\n\r\n#### Problem description\r\nSlicing a dataframe with a TimeDeltaIndex with the particular right bound '720s' seems to be incorrectly parsed, not returning the time slice as expected. As can be seen in the above example, other bounds work as expected, but using '720s' as the right bound returns 60 more seconds of data than it should have.\r\n\r\n#### Expected Output\r\nSlicing between '710s' and '720s' should return 11 seconds of data, as slicing '610s' and '620s' does.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.3.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.0.0-29-generic\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.0.3\r\nnumpy            : 1.18.2\r\npytz             : 2019.3\r\ndateutil         : 2.8.1\r\npip              : 9.0.1\r\nsetuptools       : 46.1.3\r\nCython           : None\r\npytest           : 4.3.1\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : 0.999999999\r\npymysql          : 0.9.3\r\npsycopg2         : None\r\njinja2           : 2.10.3\r\nIPython          : None\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.2.1\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : 2.4.11\r\npandas_gbq       : None\r\npyarrow          : 0.13.0\r\npytables         : None\r\npytest           : 4.3.1\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.4.1\r\nsqlalchemy       : 1.3.12\r\ntables           : None\r\ntabulate         : 0.8.6\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : None\r\nnumba            : 0.48.0\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"- [x] I have checked that this issue has not already been reported.\r\n\r\n- [x] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n#### Code Sample, a copy-pastable example\r\n\r\n```\r\ndf = pd.DataFrame({'x': [np.nan, 1., 2.]}).astype(pd.SparseDtype(\"float\", np.nan))\r\ndf = df.reindex(['x', 'y'], axis='columns')\r\ndf.info()\r\n```\r\n```\r\n<class 'pandas.core.frame.DataFrame'>\r\nRangeIndex: 3 entries, 0 to 2\r\nData columns (total 2 columns):\r\nx    -4 non-null Sparse[float64, nan]\r\ny    -6 non-null float64\r\ndtypes: Sparse[float64, nan](1), float64(1)\r\nmemory usage: 176.0 bytes\r\n```\r\n\r\n#### Problem description\r\n\r\nWhen re-indexing the columns of a sparse dataframe, new columns are not sparse. This is problematic especially since the new columns would be completely sparse.\r\n\r\n#### Expected Output\r\n\r\nI'd expect that the new column was also of type `Sparse[float64, 0.0]`.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.5.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.3.0-45-generic\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 0.25.0\r\nnumpy            : 1.18.1\r\npytz             : 2019.3\r\ndateutil         : 2.8.1\r\npip              : 19.3.1\r\nsetuptools       : 42.0.2\r\nCython           : 0.29.15\r\npytest           : 5.3.5\r\nhypothesis       : None\r\nsphinx           : 2.4.1\r\nblosc            : None\r\nfeather          : 0.4.0\r\nxlsxwriter       : None\r\nlxml.etree       : 4.4.2\r\nhtml5lib         : None\r\npymysql          : 0.9.3\r\npsycopg2         : 2.8.4 (dt dec pq3 ext lo64)\r\njinja2           : 2.11.1\r\nIPython          : 7.12.0\r\npandas_datareader: None\r\nbs4              : 4.8.2\r\nbottleneck       : 1.3.1\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.4.2\r\nmatplotlib       : 3.1.3\r\nnumexpr          : 2.7.1\r\nodfpy            : None\r\nopenpyxl         : 3.0.3\r\npandas_gbq       : None\r\npyarrow          : 0.16.0\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.3.2\r\nsqlalchemy       : 1.3.13\r\ntables           : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Is your feature request related to a problem?\r\n\r\nI wish I could use pandas to do\r\n\r\n```\r\ns = pd.Series([1,2,1,3,1,2])\r\ns.groupby()\r\n```\r\nrather than\r\n\r\n```\r\ns.groupby(s)\r\n```\r\n\r\n#### Describe the solution you'd like\r\n\r\nI would like pandas to interpret the series values as `grouper` when `by` or `level` is not provided.\r\n\r\n#### API breaking implications\r\n\r\nI can't think of any reason this will affect the API.\r\n\r\n#### Describe alternatives you've considered\r\n\r\nGrouping using series itself as `grouper`.\r\n\r\n`s.groupby(s)`\r\n"},{"labels":[null,"api",null,null],"text":"```\r\nser = pd.Series([1, 2, 3, 4], index=[1.1, 2.1, 3.0, 4.1])\r\n\r\n>>> ser[5]                    # <--we are treating 5 as a label\r\nKeyError: 5\r\n\r\n>>> ser[5] = 5                # < --we are treating 5 as position\r\nIndexError: index 5 is out of bounds for axis 0 with size 4\r\n\r\n>>> ser[3] = 5                # <-- we are treating 3 as a label\r\n>>> ser\r\n1.1    1\r\n2.1    2\r\n3.0    5\r\n4.1    4\r\ndtype: int64\r\n```\r\n\r\nThe `ser[5] = 5` case is an outlier because instead of having its label-vs-positional behavior determined by `ser.index._should_fallback_to_positional`, it is defermined by `if is_integer(key) and not self.index.inferred_type == \"integer\":`"},{"labels":["api",null],"text":"Currently, converting directly to StringDtype is not really safe unless you know you've got a ``str`` type:\r\n\r\n```python\r\n>>> pd.Series([1,2, np.nan], dtype=str)[0]\r\n'1'  # string, works\r\n>>> pd.Series([1,2, np.nan], dtype=\"string\")\r\nValueError  # does not work\r\n>>> pd.Series([1,2, np.nan], dtype=str).astype(\"string\")\r\n# works\r\n>>> pd.Series([1,2, np.nan]).astype(\"string\")\r\nValueError  # does not work\r\n>>> pd.Series([1,2, np.nan]).astype(str).astype(\"string\")\r\n# works\r\n```\r\nThe net result is that we have to wrap conversions, so in general will have to do  ``x.astype(str).astype(\"string\")``, which seems unnecessary.\r\n\r\nThis extends to other conversions to StringDtype, e.g. for ``read_excel`` we have to do:\r\n\r\n```python\r\n>>> df = pd.read_excel(..., dtype={\"col_x\": str}).astype({\"col_x\": \"string\"})\r\n```\r\nwhere directly setting ``dtype={\"col_x\": \"string\"}`` would seem more natural.\r\n\r\n I think we should be able to convert directly to StringDtype by internally converting first to ``str``, if needed. I might be missing some nuance why this is not allowed, though.\r\n\r\nAny comment on loosening up here?"},{"labels":["api",null,null],"text":"in https://github.com/pandas-dev/pandas/issues/17673#issue-260396559 @jorisvandenbossche wrote\r\n\r\n>Underlying reason is that this function of course can do way too many things at the same time (or the same things in too many different ways) ... (orthogonal to this, we could maybe also think if certain functionality could be moved into its own function).\r\n\r\nopened this issue to discuss."},{"labels":["api",null,null],"text":"Across a set of issues/PRs (eg https://github.com/pandas-dev/pandas/issues/32586, https://github.com/pandas-dev/pandas/issues/32673, https://github.com/pandas-dev/pandas/pull/33064), there has lately been quite some discussion regarding `_values_for_factorzie` / `_values_for_argsort`, the need for roundtrippability, the need for order-preserving, the need for `_ndarray_values`, ... \r\n\r\nThose different questions and topics that came up (as far as I kept track, probably not complete! but already too long .. ;)):\r\n\r\n* In https://github.com/pandas-dev/pandas/issues/32586, the question was raised what the difference is between `_values_for_argsort` and `_values_for_factorize`, and whether we need both.\r\n\r\n  Some difference that came up:\r\n\r\n  * The main difference might be that the values in  `_values_for_factorize` need to be hashable, while the ones in `_values_for_argsort` don't need to be (although this difference is not properly documented). Looking back at https://github.com/pandas-dev/pandas/pull/19957 from @TomAugspurger, it was mentioned the that \"sortable\" is an easier requirement than what other algos like factorize might need.  \r\n  Related to this difference is that `_values_for_factorize` needs to return a dtype supported by the hashtables (int64, uint64, float64, object), while `_values_for_argsort` can return any sortable dtype (so also int8, int32, etc). \r\n  * The return type is different: `_values_for_factorize` also returns a `na_value` sentinel, which means you can encode missing values in a different way than a \"missing value\" (eg nan in float dtype). While for `_values_for_argsort`, it simply returns one array (I would need to look into the details how missing values are handled here, it seems they are filtered out in `nargsort`, so it might not matter how they are encoded in the returned array).\r\n\r\n  Is this sufficiently different to warrant two methods? Probably, with a bit work, they could be combined in a single method. However, their original purpose was *only* to help implement `EA.factorize()` and `EA.argsort()`. So for that purpose only, it might not necessarily be worth trying to combine them. And see the last bullet point for a more general \"hashable, orderable array\"\r\n\r\n* In addition, we actually also have `_values_for_rank` for Categorical, which we probably should try to get rid off as well -> https://github.com/pandas-dev/pandas/issues/33245\r\n\r\n* I have argued that in general, we should also look at a \"masked\" version of eg `_values_for_factorize`. Having the option to return a (values, mask) tuple in addition to (values, na_sentinel) in case this is easier/cheaper to provide (which is especially the case for the masked arrays; this will need to support of the factorize algos for masks though -> eg https://github.com/pandas-dev/pandas/pull/33064)\r\n\r\n* We also had a vaguely defined `_ndarray_values` (https://github.com/pandas-dev/pandas/issues/23565), that was recently removed (https://github.com/pandas-dev/pandas/pull/32768). It was eg used in indexing code (index engine, joining indexes), where it was replaced with `_values_for_argsort` (https://github.com/pandas-dev/pandas/pull/32467, https://github.com/pandas-dev/pandas/pull/32611). \r\n\r\n* *What else can they be used for internally?* (`_values_for_factorize` / `_values_for_argsort`)  \r\n  As mentioned above, `_values_for_argsort` is since recently used for ExtensionIndex joining and engine values. Further, `_values_for_factorize` is used in the general merging code. \r\n\r\n  However, the initial purpose of `_values_for_factorize` / `_values_for_argsort` was *not* to be used internally in pandas, but *only* has a helper to `EA.factorize()` and `EA.argsort()`. So following our current EA interface spec, we should not use them internally (which means we should fix the few cases where we started using them). \r\n  The spec about `factorize` is clear that there are two ways to override its behaviour: implement `_values_for_factorize`/`_from_factorized`, or implement `factorize` itself:\r\n\r\n  https://github.com/pandas-dev/pandas/blob/c47e9ca8b042881d44c9e679a9bf42bacabbb732/pandas/core/arrays/base.py#L740-L747\r\n\r\n  So external EAs are not guaranteed to have an efficient implementation of `_values_for_factorize`/`_values_for_argsort` (they still have the default `astype(object)` implementation). \r\n  Fletcher is an example of external EAs that implement `factorize` and not `_values_for_factorize`.\r\n\r\n  So ideally, for anything factorize/argsort-related, we should actually always call the `EA.factorize()` or `EA.argsort()` methods. \r\n\r\n* In https://github.com/pandas-dev/pandas/issues/32673, @jbrockmendel questioned whether the `_values_for_factorize` and `_from_factorized` combo should faithfully roundtrip? Currently, they do, but not necessarily when missing values are included. \r\n  However, when only considering them as \"internal\" to the `EA.factorize()` implementation, this question doesn't actually matter. But it *does* matter when we want to use those values more generally.\r\n\r\n* I mentioned above that ideally we should use `factorize()` or `argsort()` directly as much as possible and avoid `_values_for_factorize/argsort` (since this is the official EA interface). However, there are still cases where such direct usage is not sufficient, and where we actually need some \"values\".\r\n  \r\n  For example, in the merging/joining code, you can't \"just\" `factorize()` the left and right array, because then the integer codes of left and right both don't necessarily can be matched (it depends on the uniques being present what those integers mean). \r\n\r\n  I think it is clear we have *some* use case for \"ndarray values\", but so we should think about for which use cases we need that and what requirements we have for those. \r\n  @jbrockmendel started to list some requirements here: https://github.com/pandas-dev/pandas/issues/32586#issuecomment-605082900\r\n\r\n---\r\n\r\nHaving read again through all the recent issues and having written up the above, my *current* take-away point are:\r\n\r\n- To start, we should maybe put the questions around `_values_for_factorize` / `_values_for_argsort` aside for a moment. In principle they are internal to `EA.factorize()` / `EA.argsort()`, and so we *could* also remove those methods (if we wanted that) if we \"just\" require EA authors to implement factorize/argsort directly instead of through the helper _values_for.. .  \r\n  And if we figure out the general \"ndarray values\" requirements (see below), we can still come back to this to see if we can actually replace both `_values_for_factorize` / `_values_for_argsort` with this single \"ndarray values\" interface.\r\n\r\n- I now think that replacing `_ndarray_values` with `_values_for_argsort` to be able to remove `_ndarray_values` actually didn't solve much. We replaced one vaguely specified property (`_ndarray_values`) with another (`_values_for_argsort` for *other* purposes than just sorting, as there are currnetly also no guarantees / requirements outside of sorting specified for `_values_for_argsort`). \r\n\r\n- Instead, I would focus on figuring out what the requirements are for the \"hashable / value preserving ndarray values\":\r\n  1) What are the exact use cases we need this for?\r\n  2) What are the exact semantics needed for those use cases? (hashable, orderable, deterministic across arrays, ..)\r\n  3) Do those use cases need roundtripping of those values? \r\n  4) How would we implement those values for the internal EAs?*\r\n\r\n  It might be that this ends up to be something close to what `_values_for_argsort` or `_values_for_factorize` now are. But I also think that my comment above about the possibility to include a mask in this interface is important for the nullable dtypes.\r\n\r\n- An alternative that we didn't really mention yet, is adding more to the EA interface instead of requiring this \"ndarray values\" concept. For example, if we want that external EAs can have control over joining, we could have a `EA.__join__(other_EA, how) -> Tuple[ndarray[int], ndarray[int]]` that returns indices into left and right EAs that determine how to join them. \r\n  For joining that might be a relatively straightforward interface, for the indexing engine that looks more complex though (but so let's first define the use cases).\r\n\r\n"},{"labels":["api",null],"text":"Copying part of the discussion out of https://github.com/pandas-dev/pandas/issues/32586 into a more specific issue.\r\n\r\nProblem statement: currently, `_from_sequence` is not very explicit in what it should accept as scalars. In practice, this means that it is mostly very liberal, as it is also used under the hood when creating an array of any list-like of objects in `pd.array(.., dtype)`. \r\nHowever, in some cases we need a more strict version that *only* accepts actual scalars of the array (meaning, the type of values you get from `array[0]` or `array.max()` in case it supports that kind of reductions). This causes some issues like https://github.com/pandas-dev/pandas/issues/31108.\r\n\r\nSo, what should `_from_sequence` accept? Should it only be sequences that are unambiguously this dtype?\r\n\r\nI think it will be useful to have a \"strict\" version that basically only accepts instances of ExtensionDtype.type or NA values. But we also still need a \"liberal\" method for the other use cases like `pd.array(.., dtype)`.\r\n\r\nThe strict version would be used when, for some reason, we go through object dtype (or a list of scalars, or something equivalent). For example in groupby, where we assemble a list of scalars from the reductions into a new column. \r\nFrom a testing point of view, that would mean we can test that `EA._the_strict_method(np.asarray(EA, dtype=object), dtype=EA.dtype)` and   `EA._the_strict_method(list(EA), dtype=EA.dtype)` can roundtrip faithfully.\r\n\r\n---\r\n\r\nAssuming we agree that we need a strict version for certain use cases, I think there are two main options:\r\n\r\n1) Keep `_from_sequence` as is, and add a new `_from_scalars` method that is more strict (that in the base class can call `_from_sequence` initially for backwards compatibility). We can use `_from_scalars` in those cases where we need the strict version, and keep using `_from_sequence` elsewhere (eg in `pd.array(.., dtype=)`)\r\n\r\n2) Update the expectation in our spec that `_from_sequence` should only accept a sequence of scalars of the array's type (so make `_from_sequence` the strict method), and use the `astype` machinery for construction. Basically, the current flexible `_from_sequence` would then be equivalent to casting an object ndarray (or generally any type) to your EA dtype.\r\n\r\nAre there preferences? (or other options?)\r\n\r\nFrom a backwards compatibility point of view, I think both are similar (in both cases you need to update a method (`_from_scalars` or `_from_sequence`), and in both cases initially the flexible version will still be used as fallback until the update is done).\r\n\r\nThe second option of course requires an update to the astype machinery (#22384), which doesn't exist today, but on the other hand is also something we need to do at some point eventually (but a much bigger topic to solve).\r\n"},{"labels":["api",null],"text":"The `.at` indexer is documented as the fast, restricted version of `loc` to \"access a scalar value\"  (https://pandas.pydata.org/docs/user_guide/indexing.html#fast-scalar-value-getting-and-setting).\r\n\r\nHowever, currently, there is not always a guarantee that the result will actually be a scalar. For example, with a Series with duplicate index values:\r\n\r\n```\r\nIn [3]: s = pd.Series([1, 2, 3], index=['a', 'b', 'a'])  \r\n\r\nIn [4]: s.at['a'] \r\nOut[4]: \r\na    1\r\na    3\r\ndtype: int64\r\n```\r\n\r\nThere are some other possible cases that could give a non-scalar value as well:\r\n\r\n- MultiIndex if not all levels are indexed (#26989)\r\n- DataFrame with duplicate index labels in index and/or columns (#33041)\r\n\r\nHowever, those cases currently all fail (error or produce wrong result, see below for examples). \r\nSo a question that can be posed: **should we consider those cases as bugs, or should we rather actually require a scalar result** and thus see the series behaviour above as too liberal?\r\n\r\nSince those linked issues have open PRs to \"fix\" them, I think we should first decide on what guarantees on the `.at` API we want to provide. And personally, I think taking the documented intention of accessing a scalar would be a plus (it gives a certainty about the type of the result, i.e. always a scalar, and not sometimes a series or dataframe depening on the value of the label passed to `at`).\r\n\r\nIf we prefer the more strict scalar result requirement, we can deprecate the Series case (but only if accessing a duplicated label) and provide better error messages for the error cases.\r\n\r\n---\r\n\r\nExamples of the failing behaviours (on pandas 1.0.1):\r\n\r\n```python\r\n# duplicated row label -> you get a numpy array (which I would say is always wrong)\r\n>>> df = pd.DataFrame(np.random.randn(3, 2), index=['a', 'b', 'a'], columns=['A', 'B'])  \r\n>>> df.at['a', 'A'] \r\narray([-0.02914828,  0.2856617 ])\r\n\r\n# duplicated column label -> error\r\n>>> df = pd.DataFrame(np.random.randn(3, 2), index=['a', 'b', 'c'], columns=['A', 'A'])  \r\n>>> df.at['a', 'A']  \r\n...\r\nAttributeError: 'BlockManager' object has no attribute 'T'\r\n\r\n# Selecting one level of a MultiIndex -> error\r\n>>> s = pd.Series(np.random.randn(4), index=pd.MultiIndex.from_product([['A', 'B'], [1, 2]]))  \r\n>>> s.at['A']  \r\n...\r\nKeyError: 'A'\r\nDuring handling of the above exception, another exception occurred:\r\n...\r\nAttributeError: 'numpy.ndarray' object has no attribute '_values'\r\n\r\n# selecting all MultiIndex levels -> this should actually work (#26989)\r\n>>> s.at['A', 1] \r\n...\r\nTypeError: _get_value() got multiple values for argument 'takeable'\r\n\r\n# DataFrame with all MultiIndex levels specified works OK\r\n>>>s.to_frame('col').at[('A', 1), 'col']\r\n1.341421652269149\r\n```\r\n\r\n"},{"labels":["api",null],"text":"#### Short description\r\n\r\nUsing the following invalid CSV:\r\n```csv\r\ncol1_name,col2_name,col3_name\r\n0,1,2,X\r\n4,5,6,\r\n6,7,8\r\n```\r\nIt has 3 columns but the *first row* has 4 values and loading it should raise a `ParsingError`.\r\n\r\nHowever, loading using:\r\n```python\r\npd.read_csv(\"test.csv\", index_col=False)\r\n```\r\ndoes not raise any exception and returns the following DataFrame:\r\n```\r\n   col1_name  col2_name  col3_name\r\n0          0          1          2\r\n1          4          5          6\r\n2          6          7          8\r\n```\r\nThus, it silently drops the additional value `X`.\r\n\r\n#### Problem description\r\n\r\nThis will happening if the first row and the following ones are invalid.\r\nIf the first invalid row is not the first row, it will throw a `ParsingError` exception.\r\nI.e. the following CSV produces the same results (silently dropping additional value):\r\n```csv\r\ncol1_name,col2_name,col3_name\r\n0,1,2,X\r\n4,5,6,X\r\n6,7,8\r\n```\r\n```csv\r\ncol1_name,col2_name,col3_name\r\n0,1,2,X\r\n4,5,6,X\r\n6,7,8,X\r\n```\r\n```csv\r\ncol1_name,col2_name,col3_name\r\n0,1,2,X\r\n4,5,6\r\n6,7,8,X\r\n```\r\nBut this, as expected, throws an exception:\r\n```csv\r\ncol1_name,col2_name,col3_name\r\n0,1,2\r\n4,5,6,X\r\n6,7,8,X\r\n```\r\nFinally, if there are two additional values instead of a single one, it throws the following exception:\r\n```\r\nIndexError: list index out of range\r\n```\r\n\r\nHaving a consistent behavior by throwing an exception in every of the previous cases would be enjoyable.\r\nThe fact that it is silent make it harder to validate CSV files.\r\n\r\n#### Expected Output\r\n\r\nThrow a `ParsingError` in the previous cases.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.7.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 4.15.0-91-generic\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.0.3\r\nnumpy            : 1.18.1\r\npytz             : 2019.3\r\ndateutil         : 2.8.1\r\npip              : 20.0.2\r\nsetuptools       : 46.1.1.post20200323\r\nCython           : 0.29.15\r\npytest           : 5.4.1\r\nhypothesis       : None\r\nsphinx           : 2.4.4\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.11.1\r\nIPython          : 7.13.0\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.1.3\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\npytest           : 5.4.1\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.4.1\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\nnumba            : None\r\n</details>\r\n\r\n#### Edits\r\n- corrected copy-paste error in the returned DataFrame example (cf. [reply by @gfyoung](https://github.com/pandas-dev/pandas/issues/33037#issuecomment-604811382))"},{"labels":["api"],"text":"https://github.com/pandas-dev/pandas/blob/f78b1cc46268e8cee8f5ccdacc41e667a6b92ef7/pandas/core/generic.py#L1371\r\n\r\nThis method conflicts with python's native `bool`. Wondering if it would be okay to rename this to `to_bool`. \r\n\r\nI can get the needed changes done, but wanted to make sure it's okay before sending out a PR. "},{"labels":["api",null],"text":"Followup to https://github.com/pandas-dev/pandas/issues/31925.\r\n\r\nOver there, we removed `_ensure_type` because it broke things like\r\n\r\n```python\r\nimport pandas as pd\r\nimport pandas.testing as pdt\r\n\r\nclass MyDataFrame(pd.DataFrame):\r\n    pass\r\n\r\nmdf = MyDataFrame()\r\ndf = pd.DataFrame()\r\n# In DataFrame.reindex_like, the derived class is compared to\r\n# the base class. The following line will throw\r\n# 'AssertionError: <class 'pandas.core.frame.DataFrame'>'\r\nmdf.reindex_like(df)\r\n```\r\n\r\nHowever, I don't think that example is very compelling. It's strange for MyDataFrame.reindex to return anything other than MyDataFrame. The root cause is MyDataFrame._constructor returning `DataFrame`, rather than `MyDataFrame`.\r\n\r\nI think we should somehow get subclasses to have their _constructor return something that returns their subclass. We can\r\n\r\n1. Require this, by doing a runtime check on the  `DataFrame._constructor`\r\n2. Change `DataFrame._constructor` to return `type(self)`\r\n\r\n2 is nicer, but it requires that the subclasses `__init__` method be compatible enough with DataFrame's. It's *probably* safe to assume that, but it might be an API breaking change."},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nfrom datetime import datetime, timezone\r\nimport pandas as pd\r\n\r\ndt1 = pd.to_datetime(datetime(2020, 3, 11), utc=True)\r\nprint(repr(dt1))\r\nprint(type(dt1.tz))\r\n\r\ndt2 = pd.to_datetime(datetime(2020, 3, 11, tzinfo=timezone.utc))\r\nprint(repr(dt2))\r\nprint(type(dt2.tz))\r\n\r\n# dragons here\r\nprint(dt1 - dt2)\r\n```\r\n\r\noutputs\r\n\r\n```\r\nTimestamp('2020-03-11 00:00:00+0000', tz='UTC')\r\n<class 'pytz.UTC'>\r\nTimestamp('2020-03-11 00:00:00+0000', tz='UTC')\r\n<class 'datetime.timezone'>\r\n\r\nTypeError: Timestamp subtraction must have the same timezones or no timezones\r\n```\r\n\r\n#### Problem description\r\n\r\nThere is no ability to specify which \"UTC\" the Timestamp should be. I suggest extending the interface of `pd.to_datetime()` to specify `utc_cls=pytz.UTC`.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.5.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.3.0-40-generic\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.0.1\r\nnumpy            : 1.17.4\r\npytz             : 2019.2\r\ndateutil         : 2.7.3\r\npip              : 19.3.1\r\nsetuptools       : 42.0.1\r\nCython           : 0.29.14\r\npytest           : 5.3.1\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : 4.5.0\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : 2.8.4 (dt dec pq3 ext lo64)\r\njinja2           : 2.10.3\r\nIPython          : 7.10.0\r\npandas_datareader: None\r\nbs4              : 4.8.1\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.5.0\r\nmatplotlib       : 3.1.2\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 0.16.0\r\npytables         : None\r\npytest           : 5.3.1\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.2.1\r\nsqlalchemy       : 1.3.12\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\nnumba            : None\r\n```\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"This is as good a time as any to revisit the \"experimental\" EA interface.\r\n\r\nMy read of the Issues and recollection of threads suggests there are three main groups of topics:\r\n\r\nClarification of the Interface\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n1) _values_for_argsort and values_for_factorize\r\n    - Do we need both?  The docs both say they should be order-preserving.\r\n    - Is it safe to return a view?  (Categorical.values_for_argsort makes a copy for no obvious reason)\r\n    - What else can they be used for internally?  e.g. in #32467 _values_for_argsort is used for ExtensionIndex join_non_unique and join_monotonic\r\n2) <s>What characteristics should _ndarray_values have?  Is it needed?  (#32412)</s> _ndarray_values has been removed\r\n3) What should _from_sequence accept?\r\n    - Should it only be sequences that are unambiguously this dtype?\r\n    - In particular, should DTA/TDA/PA _not_ accept i8 values?\r\n4) What should fillna accept? (#22954, #32414)\r\n4.5) Require that `__iter__` return native types?  #29738\r\n\r\nNdarray Compat\r\n^^^^^^^^^^^^^^^^^\r\n5) Headaches have been caused by trivial ndarray methods not being on EA\r\n    - #31199 size\r\n    - #32342 \"T\" (just the most recent; this has come up a lot)\r\n    - #24583 ravel\r\n6) For arithmetic we're going to need something like either `tile` or `broadcast_to`\r\n\r\nMethods Needed/Wanted For Index/Series/DataFrame/Block\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n7) Suggested Methods (partial list)\r\n    - #27264 duplicated\r\n    - #23437 empty\r\n    - #28955 apply\r\n    - #23179 map\r\n    - #22680 hasnas\r\n    - #27081 equals\r\n    - #24144 where\r\n    - putmask would be helpful for ExtensionIndex\r\n\r\nI suggest we discuss these in order.  Before jumping in, is there anything vital missing from this list?  (this is only a small subset of the issues on the tracker)\r\n\r\ncc @pandas-dev/pandas-core @xhochy "},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [1]: import pandas as pd                                                                                                                                                                            \r\n\r\nIn [2]: df = pd.DataFrame({\"A\": [\"foo\"] * 3 + [\"bar\"] * 3, \"B\": [1] * 6})                                                                                                                              \r\n\r\nIn [3]: g = df.groupby(\"A\")                                                                                                                                                                            \r\n\r\nIn [4]: g._selected_obj                                                                                                                                                                                \r\nOut[4]: \r\n     A  B\r\n0  foo  1\r\n1  foo  1\r\n2  foo  1\r\n3  bar  1\r\n4  bar  1\r\n5  bar  1\r\n\r\nIn [5]: g.sum()                                                                                                                                                                                        \r\nOut[5]: \r\n     B\r\nA     \r\nbar  3\r\nfoo  3\r\n\r\nIn [6]: g._selected_obj                                                                                                                                                                                \r\nOut[6]: \r\n   B\r\n0  1\r\n1  1\r\n2  1\r\n3  1\r\n4  1\r\n5  1\r\n```\r\n#### Problem description\r\n\r\nNoticed this while working on #32332\r\n\r\n#### Expected Output\r\n\r\nI wasn't expecting this to have side-effects\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nIn [7]: pd.show_versions()                                                                                                                                                                             \r\n/home/SERILOCAL/m.gorelli/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/fastparquet/dataframe.py:5: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\r\n  from pandas.core.index import CategoricalIndex, RangeIndex, Index, MultiIndex\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : f7bed0583556f66f18f6391eddf3e0802c4933f3\r\npython           : 3.7.6.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 4.15.0-88-generic\r\nVersion          : #88-Ubuntu SMP Tue Feb 11 20:11:34 UTC 2020\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_GB.UTF-8\r\nLOCALE           : en_GB.UTF-8\r\n\r\npandas           : 1.1.0.dev0+694.gf7bed0583\r\nnumpy            : 1.18.1\r\npytz             : 2019.3\r\ndateutil         : 2.8.1\r\npip              : 19.3.1\r\nsetuptools       : 45.1.0.post20200119\r\nCython           : 0.29.14\r\npytest           : 5.3.4\r\nhypothesis       : 5.3.0\r\nsphinx           : 2.3.1\r\nblosc            : 1.8.3\r\nfeather          : None\r\nxlsxwriter       : 1.2.7\r\nlxml.etree       : 4.4.2\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.3\r\nIPython          : 7.11.1\r\npandas_datareader: None\r\nbs4              : 4.8.2\r\nbottleneck       : 1.3.1\r\nfastparquet      : 0.3.2\r\ngcsfs            : None\r\nmatplotlib       : 3.1.2\r\nnumexpr          : 2.7.1\r\nodfpy            : None\r\nopenpyxl         : 3.0.1\r\npandas_gbq       : None\r\npyarrow          : 0.15.1\r\npytables         : None\r\npyxlsb           : None\r\ns3fs             : 0.4.0\r\nscipy            : 1.4.1\r\nsqlalchemy       : 1.3.12\r\ntables           : 3.6.1\r\ntabulate         : 0.8.6\r\nxarray           : 0.14.1\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nnumba            : 0.47.0\r\n\r\n</details>\r\n"},{"labels":["api"],"text":"I have a simple question:\r\n\r\nHow do you know if your accessor namespace has already been registered with the extension framework?\r\n\r\n\r\nLet's say I do:\r\n\r\n```python\r\nimport pandas as pd\r\nimport coolaccessor # registers dataframe accessor/namespace\r\n```\r\n\r\nNow later on in the code, a person does:\r\n\r\n```python\r\ndef cool_function(a,b):\r\n   import coolaccessor\r\n   return pd.cool.shibby(a,b)\r\n```\r\n\r\nIt will try to re-register the accessor.  This produces a warning during testing if that case occurs.  So how can I check that if on re-import the registration doesn't need to occur again?"},{"labels":["api",null,null],"text":"**Context**: in the original `pd.NA` proposal (https://github.com/pandas-dev/pandas/issues/28095) the topic about `pd.NA` vs `np.nan` was raised several times. And also in the recent pandas-dev mailing list discussion on pandas 2.0 it came up (both in context of `np.nan` for float as `pd.NaT` for datetime-like).\r\n\r\nWith the introduction of `pd.NA`, and if we want consistent \"NA behaviour\" across dtypes at some point in the future, I think there are two options for float dtypes:\r\n\r\n- Keep using `np.nan` as we do now, but change its behaviour (e.g. in comparison ops) to match `pd.NA`\r\n- Start using `pd.NA` in float dtypes\r\n\r\nPersonally, I think the first one is not really an option. Keeping it as np.nan, but deviating from numpy's behaviour feels like a non-starter to me. And it would also give a discrepancy between the vectorized behaviour in pandas containers vs the scalar behaviour of `np.nan`.  \r\nFor the second option, there are still multiple ways this could be implemented (a single array that still uses np.nan as the missing value sentinel but we convert this to pd.NA towards the user, versus a masked approach like we do for the nullable integers). But in this issue, I would like to focus on the user-facing behaviour we want: Do we want to have both np.nan and pd.NA, or only allow pd.NA? Should np.nan still be considered as \"missing\" or should that be optional? What to do on conversion from/to numpy? (And the answer to some of those questions will also determine which of the two possible implementations is preferrable)\r\n\r\n---\r\n\r\n**Actual discussion items:** assume we are going to add floating dtypes that use `pd.NA` as missing value indicator. Then the following question comes up:\r\n\r\n> If I have a Series[float64] could it contain both np.nan and pd.NA, and these signify different things?\r\n\r\nSo yes, it is *technically possible* to have both np.nan and pd.NA with different behaviour (`np.nan` as \"normal\", unmasked value in the actual data, `pd.NA` tracked in the mask). But we also need to decide if we *want* this.\r\n\r\nThis was touchec upon a bit in the original issue, but not really further discussed. Quoting a few things from the original thread in https://github.com/pandas-dev/pandas/issues/28095:\r\n\r\n\r\n> [@Dr-Irv in [comment](https://github.com/pandas-dev/pandas/issues/28095#issuecomment-543162838)] I think it is important to distinguish between NA meaning \"data missing\" versus NaN meaning \"not a number\" / \"bad computational result\".\r\n\r\nvs\r\n\r\n> [@datapythonista in [comment](https://github.com/pandas-dev/pandas/issues/28095#issuecomment-539021471)] I think NaN and NaT, when present, should be copied to the mask, and then we can forget about them (for what I understand values with True in the NA mask won't be ever used).\r\n\r\nSo I think those two describe nicely the two options we have on the question **do we want both `pd.NA` and `np.nan` in a float dtype and have them signify different things?** -> 1) Yes, we can have both, versus 2) No, towards the user, we only have pd.NA and \"disallow\" NaN (or interpret / convert any NaN on input to NA).\r\n\r\nA reason to have both is that they can signify different things (another reason is that most other data tools do this as well, I will put some comparisons in a separate post). \r\nThat reasoning was given by @Dr-Irv in https://github.com/pandas-dev/pandas/issues/28095#issuecomment-538786581: there are times when I get NaN as a result of a computation, which indicates that I did something numerically wrong, versus NaN meaning \"missing data\". So should there be separate markers - one to mean \"missing value\" and the other to mean \"bad computational result\" (typically `0/0`) ?\r\n\r\nA dummy example showing how both can occur:\r\n\r\n```python\r\n>>>  pd.Series([0, 1, 2]) / pd.Series([0, 1, pd.NA])\r\n0    NaN\r\n1    1.0\r\n2   <NA>\r\ndtype: float64\r\n```\r\n\r\nThe NaN is introduced by the computation, the NA is propagated from the input data (although note that in an arithmetic operation like this, NaN would also propagate).\r\n\r\nSo, yes, it is possible and potentially desirable to allow both `pd.NA` and `np.nan` in floating dtypes. But, it also brings up several questions / complexities. Foremost, **should NaN still be considered as missing**? Meaning, should it be seen as missing in functions like `isna`/`notna`/`dropna`/`fillna` ? Or should that be an option? Should NaN still be considered as missing (and thus skipped) in reducing operations (that have a `skipna` keyword, like sum, mean, etc)?\r\n\r\nPersonally, I think we will need to keep NaN as missing, or at least initially. But, that will also introduce inconsistencies: although NaN would be seen as missing in the methods mentioned above, in arithmeric / comparison / scalar ops, it would behave as NaN and not as NA (so eg comparison gives False instead of propagating). It also means that in the missing-related methods, we will need to check for both NaN in the values as the mask (which can also have performance implications).\r\n\r\n---\r\n\r\nSome other various considerations:\r\n\r\n* Having both pd.NA and NaN (np.nan) might actually be more confusing for users.\r\n\r\n* If we want a consistent indicator and behavior for missing values across dtypes, I think we need a separate concept from NaN for float dtypes (i.e. pd.NA). Changing the behavior of NaN when inside a pandas container seems like a non-starter (the behavior of NaN is well defined in IEEE 754, and it would also deviate from the underlying numpy array)\r\n\r\n* How do we handle compatibility with numpy? \r\n  The solution that we have come up (for now) for the other nullable dtypes is to convert to object dtype by default, and have a `to_numpy(.., na_value=np.nan)` explicit conversion. \r\n  But given how np.nan is in practice used in the whole pydata ecosystem as a missing value indicator, this might be annoying. \r\n\r\n  For conversion to numpy, see also some relevant discussion in https://github.com/pandas-dev/pandas/issues/30038\r\n\r\n* What with conversion / inference on input? \r\n  Eg creating a Series from a float numpy array with NaNs (`pdSeries(np.array([0.1, np.nan]))`) Do we convert NaNs to NA automatically by default?\r\n\r\n\r\ncc @pandas-dev/pandas-core @Dr-Irv @dsaxton \r\n"},{"labels":["api",null,null,null],"text":"Currently when you align columns and create a new column, align will create a new float64 column filled with NaNs.\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: a = pd.DataFrame({\"A\": [1, 2], \"B\": [pd.Timestamp('2000'), pd.NaT]})\r\n\r\nIn [3]: b = pd.DataFrame({\"A\": [1, 2]})\r\n\r\nIn [4]: a.align(b)[1].dtypes\r\nOut[4]:\r\nA      int64\r\nB    float64\r\ndtype: object\r\n```\r\n\r\nI think it'd be more useful for the dtypes of new columns to be the same as the dtype from the other.\r\n\r\n```python\r\n# proposed behavior\r\nIn [4]: a.align(b)[1].dtypes\r\nOut[4]:\r\nA             int64\r\nB    datetime64[ns]\r\ndtype: object\r\n```\r\n\r\nThe newly created `B` column has dtype `datetime64[ns]`, the same as `a.B`.\r\n\r\nThis proposal would make the `fill_value` keyword a bit more complex.\r\n\r\n1. The default of `np.nan` would change to `None`, which means \"the right NA value for the dtype\". \r\n2. We would maybe need to accept a Mapping so users could specify specific fill values per column.\r\n\r\nI think this would make the workaround in https://github.com/pandas-dev/pandas/pull/31679 unnecessary, as we'd have the correct dtype going into the operation.\r\n\r\n---\r\n\r\nIf we think this is a good idea, it's probably an API breaking change. We *might* be able to deprecate this cleanly by (ab)using `fill_value`. We would warn when creating new columns.\r\n\r\n```python\r\nif new_columns and fill_value is no_default:\r\n    warnings.warn(\"Creating new float64 columns filled with NaN. In the future... \"\r\n                             \"Specify fill_value=None to accept the future behavior now.\")\r\n    fill_value = np.nan  # \r\n```\r\n\r\nUnfortunately, that'll happen in the background during binops. Not sure how to get around that, aside from instructing users to explicitly align first."},{"labels":["api",null,null],"text":"    #IT WORKS\r\n    df['Total'] = df['Total'].str.replace(r'\\.', '')\r\n    df['Homes'] = df['Homes'].str.replace(r'\\.', '')\r\n    df['Dones'] = df['Dones'].str.replace(r'\\.', '')\r\n    \r\n    #IT DOESNT WORK\r\n    cols = [\"Total\", \"Homes\", \"Dones\"]\r\n    df[cols] = df[cols].str.replace(r'\\.', '')\r\n\r\nOn Pandas 1.0.1"},{"labels":["api",null],"text":"#### Problem description\r\n\r\nMAD is a overloaded shortcut: It stands for *M*edian *A*bsolute *D*eviation (more common), but also can be used as *M*ean *A*bsolute *D*eviation (which is used in pandas). \r\n\r\n#### Expected Output\r\n\r\nTo avoid confusion, the method should be renamed from `.mad()` to `.mean_absolute_deviation()`, and when implemented the more commonly used `.median_absolute_deviation()`.\r\n\r\nIf you are fine with it, I would kick off a PR. \r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.3.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.3.0-24-generic\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : en_US.UTF-8\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 0.25.0\r\nnumpy            : 1.17.0\r\npytz             : 2019.2\r\ndateutil         : 2.8.0\r\npip              : 19.2.3\r\nsetuptools       : 41.0.1\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : 4.4.0\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.1\r\nIPython          : 7.7.0\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.4.0\r\nmatplotlib       : 3.1.1\r\nnumexpr          : 2.6.9\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.3.1\r\nsqlalchemy       : 1.3.6\r\ntables           : 3.5.2\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : None\r\nxlsxwriter       : None\r\n```\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample\r\n\r\nCreate a dataframe with nullable integer, string, and float data types.\r\n\r\n```python\r\n>>> df = df = pd.DataFrame({'a': [1, 3, np.nan], 'b': ['rain', 'shine', None], \r\n                                                    'a_float': [1.1, 3.3, np.nan]})\r\n>>> df = df.convert_dtypes()\r\n>>> df\r\n```\r\n\r\n|    | a    | b     |   a_float |\r\n|---:|:-----|:------|----------:|\r\n|  0 | 1    | rain  |       1.1 |\r\n|  1 | 3    | shine |       3.3 |\r\n|  2 | `<NA>` | `<NA>`  |     nan   |\r\n\r\nVerify data types and attempt to use `query`\r\n\r\n```python\r\n>>> df.dtypes\r\na            Int64\r\nb            string\r\na_float      float64\r\n\r\n>>> df.query('a > 2') # same as df[df['a'] > 2]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/Documents/Code Practice/pandas-dev/pandas/pandas/core/frame.py in query(self, expr, inplace, **kwargs)\r\n   3227         try:\r\n-> 3228             new_data = self.loc[res]\r\n   3229         except ValueError:\r\n\r\n~/Documents/Code Practice/pandas-dev/pandas/pandas/core/indexing.py in __getitem__(self, key)\r\n   1683             maybe_callable = com.apply_if_callable(key, self.obj)\r\n-> 1684             return self._getitem_axis(maybe_callable, axis=axis)\r\n   1685 \r\n\r\n~/Documents/Code Practice/pandas-dev/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)\r\n   1798             return self._get_slice_axis(key, axis=axis)\r\n-> 1799         elif com.is_bool_indexer(key):\r\n   1800             return self._getbool_axis(key, axis=axis)\r\n\r\n~/Documents/Code Practice/pandas-dev/pandas/pandas/core/common.py in is_bool_indexer(key)\r\n    133                 if np.any(key.isna()):\r\n--> 134                     raise ValueError(na_msg)\r\n    135             return True\r\n\r\nValueError: cannot mask with array containing NA / NaN values\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-52-e5d239635d7b> in <module>\r\n----> 1 df.query('a > 2')\r\n\r\n~/Documents/Code Practice/pandas-dev/pandas/pandas/core/frame.py in query(self, expr, inplace, **kwargs)\r\n   3230             # when res is multi-dimensional loc raises, but this is sometimes a\r\n   3231             # valid query\r\n-> 3232             new_data = self[res]\r\n   3233 \r\n   3234         if inplace:\r\n\r\n~/Documents/Code Practice/pandas-dev/pandas/pandas/core/frame.py in __getitem__(self, key)\r\n   2784 \r\n   2785         # Do we have a (boolean) 1d indexer?\r\n-> 2786         if com.is_bool_indexer(key):\r\n   2787             return self._getitem_bool_array(key)\r\n   2788 \r\n\r\n~/Documents/Code Practice/pandas-dev/pandas/pandas/core/common.py in is_bool_indexer(key)\r\n    132             if is_extension_array_dtype(key.dtype):\r\n    133                 if np.any(key.isna()):\r\n--> 134                     raise ValueError(na_msg)\r\n    135             return True\r\n    136     elif isinstance(key, list):\r\n\r\nValueError: cannot mask with array containing NA / NaN values\r\n\r\n>>> df.query('a_float > 2')\r\n```\r\n|    |   a | b     |   a_float |\r\n|---:|----:|:------|----------:|\r\n|  1 |   3 | shine |       3.3 |\r\n\r\nUsing `query` with strings works...\r\n\r\n```python\r\n>>> df.query('b == \"rain\"')\r\n```\r\n|    |   a | b    |   a_float |\r\n|---:|----:|:-----|----------:|\r\n|  0 |   1 | rain |       1.1 |\r\n\r\n\r\n...but fails for boolean selection\r\n```python\r\n>>> df[df['b'] == 'rain']\r\nValueError: cannot mask with array containing NA / NaN values\r\n```\r\n\r\nstrings also fail for inequalities\r\n```\r\n>>> df.query('b >= \"rain\"') # also df.query('b > \"rain\"')\r\nValueError: cannot mask with array containing NA / NaN values\r\n```\r\n\r\n#### Problem description\r\n\r\nThe `query` method  behaves differently for nullable integers, strings, and floats. Here's my summary of how I think they work with the `query method` assuming there are missing values in the columns.\r\n\r\n* nullable integers - fails\r\n* strings - works with equality, fails with inequality\r\n* float - works for all\r\n\r\nI find it extremely difficult to use if the behavior for all of these types are different for query and boolean selection.\r\n\r\n#### Expected Output\r\n\r\nI think I would prefer to have both query and boolean selection working like they do with floats, where missing values evaluate as False in a condition. And even if there are missing values in the boolen mask itself, treat those as False. This would harmonize the behavior for all data types. \r\n\r\nThis would leave it up to the user to check for missing values. I believe SQL where clauses work in such a manner (missing values in conditions evaluate as False).\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.8.1.final.0\r\npython-bits      : 64\r\nOS               : Darwin\r\nOS-release       : 19.2.0\r\nmachine          : x86_64\r\nprocessor        : i386\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 0+untagged.1.gce8af21.dirty\r\nnumpy            : 1.18.1\r\npytz             : 2019.3\r\ndateutil         : 2.8.1\r\npip              : 20.0.2\r\nsetuptools       : 45.1.0.post20200127\r\nCython           : 0.29.14\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : 4.5.0\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.3\r\nIPython          : 7.11.1\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.5.0\r\nmatplotlib       : 3.1.1\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\npytest           : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.3.1\r\nsqlalchemy       : 1.3.13\r\ntables           : None\r\ntabulate         : 0.8.3\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\nnumba            : None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> pd.__version__\r\n'1.0.0rc0+233.gec0996c67'\r\n>>> df = pd.DataFrame({'A':[1,2]})\r\n>>> df.A.append(df)\r\n     0    A\r\n0  1.0  NaN\r\n1  2.0  NaN\r\n0  NaN  1.0\r\n1  NaN  2.0\r\n\r\n```\r\n#### Problem description\r\n\r\nThe behavior of append is the same as it is in 0.25.3 when a DataFrame is appended to Series. But it should actually throw a TypeError when passed a DataFrame.\r\nWe clearly document that the elements passed to append should be Series or list/tuple of Series.\r\nAlso according to [this](https://github.com/pandas-dev/pandas/issues/30975), a TypeError seems appropriate. The change can be implemented for 1.0.1 or 1.1 after [this](https://github.com/pandas-dev/pandas/pull/31036#issuecomment-577248825) discussion.\r\n\r\n#### Expected Output\r\n```python\r\n>>> df = pd.DataFrame({'A':[1, 2]})\r\n>>> df.A.append([df])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Work\\Git projects\\pandas\\pandas\\core\\series.py\", line 2572, in append\r\n    raise TypeError(msg)\r\nTypeError: to_append should be a Series or list/tuple of Series, got DataFrame\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : ec0996c6751326eed17a0bb456fe1c550689a618\r\npython           : 3.7.6.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : None.None\r\n\r\npandas           : 1.0.0rc0+233.gec0996c67.dirty\r\nnumpy            : 1.17.0\r\npytz             : 2018.9\r\ndateutil         : 2.8.0\r\npip              : 19.3.1\r\nsetuptools       : 44.0.0.post20200106\r\nCython           : 0.29.14\r\npytest           : 4.3.1\r\nhypothesis       : 5.1.5\r\nsphinx           : 1.8.5\r\nblosc            : None\r\nfeather          : 0.4.0\r\nxlsxwriter       : 1.1.5\r\nlxml.etree       : 4.3.2\r\nhtml5lib         : 1.0.1\r\npymysql          : 0.9.3\r\npsycopg2         : None\r\njinja2           : 2.10\r\nIPython          : 7.4.0\r\npandas_datareader: None\r\nbs4              : 4.6.3\r\nbottleneck       : 1.2.1\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.3.2\r\nmatplotlib       : 3.0.3\r\nnumexpr          : 2.6.9\r\nodfpy            : None\r\nopenpyxl         : 2.6.1\r\npandas_gbq       : None\r\npyarrow          : 0.15.1\r\npytables         : None\r\npytest           : 4.3.1\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.4.1\r\nsqlalchemy       : 1.3.1\r\ntables           : 3.5.1\r\ntabulate         : 0.8.6\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : 1.1.5\r\nnumba            : 0.43.1\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"Hi,\r\n\r\nI think it could be really useful to add a **Verbose** parameter in the **pandas.Series.apply** function. It can take a really long time to get an output so having some kind of verbosity with something like tqdm could be really useful. \r\n\r\nCheers,"},{"labels":["api",null],"text":"As part of https://github.com/pandas-dev/pandas/pull/30588, we now raise when trying to create a 2D index. This introduces a behavior change when you call DataFrame.set_index with duplicate data.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df = pd.DataFrame([[1, 2, 3]], columns=['a', 'a', 'b'])\r\n\r\nIn [3]: result = df.set_index('a')\r\n```\r\n\r\nOn pandas 0.25.3, that gives back a DataFrame with a broken Index. Some DataFrame operations will work, but even things like printing the repr will fail\r\n\r\n```python\r\n# 0.25.3\r\nIn [17]: type(result)\r\nOut[17]: pandas.core.frame.DataFrame\r\n\r\nIn [18]: result.shape\r\nOut[18]: (1, 1)\r\n```\r\n\r\nWith 1.0.0rc0, that raises\r\n\r\n```pytb\r\n~/sandbox/pandas/pandas/core/indexes/numeric.py in __new__(cls, data, dtype, copy, name)\r\n     76         if subarr.ndim > 1:\r\n     77             # GH#13601, GH#20285, GH#27125\r\n---> 78             raise ValueError(\"Index data must be 1-dimensional\")\r\n     79\r\n     80         name = maybe_extract_name(name, data, cls)\r\n\r\nValueError: Index data must be 1-dimensional\r\n```\r\n\r\n#### Problem description\r\n\r\nThe old output is clearly broken, so I wouldn't consider this a (major) regression. And I don't think people should be doing this in the first place. But I wanted to ask, should `DataFrame.set_index(scalar)` return a MultiIndex when `scalar` is a duplicate label?"},{"labels":["api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n# Your code here\r\n>>> import pandas as pd\r\n>>> pd.__version__\r\n'0.26.0.dev0+1717.g6437f5eb3'\r\n>>> s = (\r\n...     pd.DataFrame({0: [1, 2, 3], 1: [4, 5, 6]})\r\n...     .unstack()\r\n...     .rename_axis([(1,), None])\r\n... )\r\n>>> s\r\n(1,)\r\n0     0    1\r\n      1    2\r\n      2    3\r\n1     0    4\r\n      1    5\r\n      2    6\r\ndtype: int64\r\n>>>\r\n>>> s.rename({0: \"foo\"}, level=None)\r\n(1,)\r\nfoo   foo    1\r\n      1      2\r\n      2      3\r\n1     foo    4\r\n      1      5\r\n      2      6\r\ndtype: int64\r\n```\r\n#### Problem description\r\n\r\nIs this an edge case or not supported.\r\n \r\n#### Expected Output\r\n\r\n```python\r\n>>> s.rename({0: \"foo\"}, level=1)\r\n(1,)\r\n0     foo    1\r\n      1      2\r\n      2      3\r\n1     foo    4\r\n      1      5\r\n      2      6\r\ndtype: int64\r\n\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"Currently, the following does not work (but probably should):\r\n\r\n```\r\nIn [12]: arr1 = pd.array([1, 2, 3]) \r\n\r\nIn [13]: arr2 = pd.array([0, 2])\r\n\r\nIn [14]: arr1[arr2]  \r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-14-1646c66d4d26> in <module>\r\n----> 1 arr1[arr2]\r\n\r\n~/scipy/pandas/pandas/core/arrays/integer.py in __getitem__(self, item)\r\n    375             item = check_bool_array_indexer(self, item)\r\n    376 \r\n--> 377         return type(self)(self._data[item], self._mask[item])\r\n    378 \r\n    379     def _coerce_to_ndarray(self, dtype=None, na_value=lib._no_default):\r\n\r\nIndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\r\n```\r\n\r\nSo that raises the following questions:\r\n\r\n- This should probably simply work for the above case? (converting the IntegerArray to a numpy integer array, instead of object array, so numpy's indexing works) We might want to combine this with the boolean array checking?\r\n- What if there are missing values? This should probably simply raise an error for now (which is what pandas 0.25 also does), although we could consider propagating an NA value as well, I think.\r\n\r\nFor Series, this seems to work partly. For `iloc` it works for the case without missing values. For `__getitem__` you get the same error as above. \r\n"},{"labels":["api",null,null,null],"text":"I would like to add functionality to pandas so that data frames can be queried like database tables, similar to the way that they can be in spark-sql. \r\n\r\nI think it should work in a similar fashion.\r\n\r\nA table can be registered using register_temp_table(dataframe, table_name).\r\n\r\nThen using ```pandas.query(\"select * from table_name\")``` you can query the data frame or any other ones registered using standard sql syntax.\r\n\r\nI've already implemented the entire thing, but I was told to open an issue for it. \r\n\r\nAlso I'm aware that there is a package called pandassql but this package actually just puts a data frame into a sql lite database, as opposed to querying a data frame directly, and transforming the sql into pandas methods that are then applied to the data frame.\r\n\r\nMotivation:\r\nThe motivation for this enhancement is to make pandas more accessible to a crowd of users that may not be as technical and also to provide ease of transition for legacy code in systems like sas that have SQL already embedded in their programs. I'll supply a context free grammar in my documentation to show exactly what this system can handle, but it can basically handle any traditional SQL select statement, including subqueries, joins, where clauses, group by clauses, any aggregate function already supported by pandas, limit, and order by clauses. It also has support for rank and dense_rank window functions. It can't do things that sql wouldn't normally do like cross tab and you can't use a user defined function in it although I think that could be a good add-on. \r\n\r\nDatatypes:\r\nThe interface supports all pandas datatypes, so to cast something as an integer the syntax would currently be cast(some_number as int64) or cast(some_int as object). I've played around with the idea of varchar, char, bigint and smallint, but I think those would be misleading as those aren't datatypes that are supported by pandas currently.\r\n\r\nErrors:\r\nCurrently the exceptions that it will throw that come this api are based solely around trying to select from an unregistered table, or from submitting an improperly written sql query, both of which you wouldn't want to silence so there's only one error mode. \r\n\r\nApi Choices:\r\nThe reason I made the register_temp_table section of the api top level was to avoid attaching a method to DataFrame although if others think it might be better as a method, I would change it in that manner (DataFrame.register_temp_table(table_name)). The reason pandas.query is a top level method is that it's relational in structure. You can select from multiple tables and join them and such and so it wouldn't make sense for it to be on a DataFrame level. The only similarity to the .query DataFrame method though is the name. DataFrame.query is just an alternate way of expressing things like DataFrame[some_condition] whereas my .query encompasses a large amount of the pandas api.\r\n\r\nBuilt In:\r\nI have two reasons that I think this would be better built in. The first is that the target audience for this is less technical pandas users. Part of making this api easier to use is lessening the burden of researching code and learning how python works, so I think that for them to go looking for an external package may be hard to begin with and they would also need to know to look for one. \r\nMy second reason is that, from using what I've built, I've found pandas a lot easier to use just as a developer.\r\nSuppose we have a DataFrame with one column called A, it goes from \r\nThis code:\r\n```\r\ndataframe[name_1] = dataframe[a] - 1\r\ndataframe[name_2] = dataframe[a] + 1\r\ndataframe = dataframe[dataframe[name_1] == dataframe[name_2]]\r\ndataframe.drop(columns=['a'], inplace=True)\r\n```\r\n\r\nTo this code:\r\n```pd.query(\"select a - 1 as name_1, a + 1 as name_2 from some_table where name_1 = name_2\")```\r\n\r\nAlso although I did implement register_temp_table as an api level function, it would serve best as a method on a DataFrame so that's another thing to consider. \r\n\r\nI can't really provide any support for the lark part, other than that it seemed like the best tool for what I was making.\r\n\r\nI apologize for the style and such, I'll be fixing all that before I'm done. I implemented this outside of pandas first, so that's why there are so many style and documentation discrepancies.  \r\n\r\n"},{"labels":["api",null,null],"text":"Observed in the PR #30402  and PR #27237 \r\n\r\nAnnotation seems not to be able to added for `sort_values`. If added, error message will pop up in CI:\r\n```\r\npandas/core/series.py:2691: error: Signature of \"sort_values\" incompatible with supertype \"NDFrame\"\r\npandas/core/frame.py:4701: error: Signature of \"sort_values\" incompatible with supertype \"NDFrame\"\r\n```\r\n"},{"labels":["api",null,null],"text":"Not sure if it was added intentionally, but it's possible to call numpy with the `np` attribute of the pandas module:\r\n```python\r\nimport pandas\r\nx = pandas.np.array([1, 2, 3])\r\n```\r\nWhile this is not documented, I've seen couple of places suggesting this as a \"trick\" to avoid importing numpy directly.\r\n\r\nI personally find this hacky, and I think should be removed. "},{"labels":["api",null,null],"text":"Could a solution be implemented to simplify loading a dataset from a multi-file zip archive? I know it is a bit insignificant compared to most other issues (especially since there are workarounds), but I'd be happy to implement it myself if I'm given the go-ahead so as to not waste your time.\r\nThanks for this amazing library!"},{"labels":["api",null],"text":"Usually when renaming a column you have to write:\r\n\r\n```\r\ndf.rename(columns={\"old\": \"new\"})\r\n```\r\n\r\nIt would be nice to have a convenience method to rename one column like this:\r\n\r\n```\r\ndf.renamecol(\"old\", \"new\")\r\n```"},{"labels":["api",null,null],"text":"This is inspired by the discussion in https://github.com/scikit-learn/enhancement_proposals/pull/25.\r\n\r\nNumPy defines an ``__array__`` protocol that allows developers to implement classes that can be converted to an array by calling ``np.asarray()`` . That makes it easy to have a common interface between libraries and it's heavily used by pandas and sklearn.\r\n\r\nIt would be great to have a similar protocol for converting something to a pandas ``DataFrame``. The goal would be to allow users to pass other data structures to libraries that expect a dataframe, say seaborn, as long as the data structures allow conversion to ``pd.DataFrame``.\r\n\r\nA workaround is for the developer of the new datastructure to provide an ``.asframe`` method, but that creates friction and requires the users to know what data type a particular library or function expects. If instead the developer of the datastructure can declare that conversion to a dataframe is possible, the library author (say seaborn) can request conversion to dataframe in a unified manner.\r\n\r\nThe implementation of this is probably pretty simple as it requires \"only\" a special case in ``pd.DataFrame.__init__``. The main work is probably in adding it to developer documentation and publicizing it correctly.\r\n\r\ncc @jorisvandenbossche "},{"labels":["api",null],"text":"I see lots of code that goes like this\r\n\r\n```\r\nIn [17]: df = pd.DataFrame({'A': [1, 2, 3]})                                                                                                                                        \r\n\r\nIn [18]: df                                                                                                                                                                         \r\nOut[18]: \r\n   A\r\n0  1\r\n1  2\r\n2  3\r\n\r\nIn [19]: df.sort_values('A', ascending=False)                                                                                                                                       \r\nOut[19]: \r\n   A\r\n2  3\r\n1  2\r\n0  1\r\n\r\nIn [20]: df.sort_values('A', ascending=False).reset_index(drop=True)                                                                                                                \r\nOut[20]: \r\n   A\r\n0  3\r\n1  2\r\n2  1\r\n```\r\n\r\nmight be nice from an API / consistency perspective to add a ``ignore_index=False|True`` keyword to ``.sort_values()``, and ``.drop_duplicates()`` that does the reset in-line; this would give consistency similar to a ``pd.concat([......], ignore_index=True)`` operation which users are very familiar\r\n"},{"labels":["api",null,null,null],"text":"In https://github.com/pandas-dev/pandas/pull/29964 and https://github.com/pandas-dev/pandas/pull/29961(using NA in IntegerArray and BooleanArray), the question comes up how to handle `pd.NA`'s in conversion to numpy arrays.\r\n\r\nSuch conversion occurs mainly in `__array__` (for `np.(as)array(..)`) and `.astype()`. For example:\r\n\r\n```python\r\nIn [3]: arr = pd.array([1, 2, pd.NA], dtype=\"Int64\")  \r\n\r\nIn [4]: np.asarray(arr) \r\nOut[4]: array([1, 2, None/pd.NA/..?], dtype=object)\r\n\r\nIn [5]: arr.astype(float)  \r\nOut[5]: array([ 1.,  2., nan])  # <--- allow automatic NA to NaN conversion?\r\n```\r\n\r\nQuestions that come up here:\r\n\r\n- By default, when converting to object dtype, what \"NA value\" should be used? Before this was `NaN` or `None`, now it could logically be `pd.NA`. \r\n  A possible reason to choose None instead of pd.NA is that third party code that needs a numpy array will typically not be able to handle pd.NA while None is much more normal. On the other hand, there is also still time for such third party code to adapt. And it will probably be good to keep `list(arr)` (iteration/getitem) and `np.array(arr, dtype=object)` consisetnt.\r\n\r\n- When converting to a float dtype, are we fine to automatically convert `pd.NA` to `np.nan` ? Or do we think the user should explicitly opt in for this?\r\n\r\nWe will probably want to add a `to_numpy` to those Integer/BooleanArray to be able to make those choices explicit, eg with following signature:\r\n\r\n```\r\ndef to_numpy(self, dtype=object, na_value=...):\r\n    ... \r\n```\r\n\r\nwhere you can explicitly say which value to use for the NAs in the final numpy array (and the `Series.numpy` can then forward such keyword). \r\nThat way, a user can do `arr.to_numpy(dtype=object, na_value=None`) to get a numpy array with None instead of pd.NA, or `arr.to_numpy(dtype=float, na_value=np.nan)` to get a float array with NaNs.\r\n\r\nBut even if we have that function (which I think we should), the above questions about the *defaults* are still to be answered (eg for `__array__` we cannot have such a `na_value` keyword, so we need to make a default choice).\r\n\r\ncc @TomAugspurger @Dr-Irv "},{"labels":["api",null,null,null],"text":"From https://github.com/pandas-dev/pandas/pull/27929#discussion_r350469472. In `__getitem__` or  `Categorical.min(..)`, we always return `np.nan` as scalar missing value, regardless of the dtype:\r\n\r\n```\r\nIn [7]: cat = pd.Categorical([pd.Timestamp(\"2012\"), None], ordered=True)\r\n\r\nIn [8]: cat  \r\nOut[8]: \r\n[2012-01-01, NaT]\r\nCategories (1, datetime64[ns]): [2012-01-01]\r\n\r\nIn [9]: cat[1]   \r\nOut[9]: nan\r\n\r\nIn [10]: cat.min(skipna=False) \r\nOut[10]: nan\r\n\r\n```\r\n\r\nIn the above, this could also be `pd.NaT` instead? \r\n(similar issue will come up once we can use the EAs that use the new NA scalar in categoricals)\r\n\r\nHowever, `CategoricalDtype.na_value` now also returns `np.nan` (which should be consistent with what we return in the cases above):\r\n\r\n```\r\nIn [13]: cat.dtype.na_value \r\nOut[13]: nan\r\n```\r\n\r\nWe can of course let the `CategoricalDtype.na_value` be dependent on the `na_value` of the dtype of the categories. But I am not fully sure we want such values-dependent behaviour? "},{"labels":["api",null,null,null],"text":"Currently `pd.array` sometimes requires an explicit `dtype=...` to get one of our extension arrays (we'll infer for Period, Datetime, and Interval).\r\n\r\nThis proposal is to have it infer the extension type for\r\n\r\n* strings -> StringArray\r\n* boolean -> BooleanArray\r\n* integer -> IntegerArray\r\n\r\nAll of these currently return PandasArray.\r\n\r\nConcretely, we'll need to teach `infer_dtype` how not to infer `mixed` for a mix of strings / booleans and NA values, similar to how it handles integer-na\r\n\r\n```python\r\nIn [27]: lib.infer_dtype([True, None], skipna=False)\r\nOut[27]: 'mixed'\r\n\r\nIn [28]: lib.infer_dtype(['a', None], skipna=False)\r\nOut[28]: 'mixed'\r\n\r\nIn [29]: lib.infer_dtype([0, np.nan], skipna=False)\r\nOut[29]: 'integer-na'\r\n```\r\n\r\nand then handle those in `array`."},{"labels":["api",null,null],"text":"`df.plot(subplots=True)`  will create one subplot per column. Is there a way to group multiple columns on the same subplot (and leave the rest of the column separated)?\r\n\r\nI'd be happy to submit a PR if that's something you'd consider? In terms of API  `subplot` could accept a list of tuples where each tuple indicates which columns should be grouped together."},{"labels":["api",null,null],"text":"#### Problem description\r\nBasically these are performance tools in SQL to get analysis in multiple dimensions and they are missing in Pandas out of the box. Some of these can be achieved by a pivot table and melt/stack functions but being tools for analysis these functions should be a must and it also decreases the number of lines of code.\r\n\r\nGroup by Grouping set will help to rewrite the query with multiple groups by clauses combined with union statements into a single query. Cube is shorthand notation of grouping sets if the user chooses all the combinations of the fields listed in the cube clause \r\n\r\n```SQL Code\r\nSELECT\r\n    column1,\r\n    column2,\r\n    aggregate_function (column3)\r\nFROM\r\n    table_name\r\nGROUP BY\r\n    GROUPING SETS (\r\n        (column1, column2),\r\n        (column1),\r\n        (column2),\r\n        ()\r\n);\r\n\r\nSelect   column1,\r\n            column2,\r\n            column3,\r\n            column4,\r\n            aggregate_function (column5)\r\nfrom table\r\ngroup by column1, column2, cube (column3,column4)```\r\n\r\nCurrent way\r\n```pseudo code\r\n  a= <pandas dataframe>\r\n  a1 = a.groupby([column1]).sum(column5)\r\n  a2  = a.groupby([column1,column2]).sum(column5)\r\n   ...\r\n  an = a.groupby([column1,...,columnn]).sum(column5)\r\n result= union(a1,a2,......an)\r\n```\r\nExpected way \r\n```pseudo code\r\n  a= <pandas dataframe>\r\n  \r\n  gropby_cube1 = a.gropby([column1,column2]).cube([column3,.....,columnn]).sum(column5)\r\n   gropby_cube2 = a.gropby.cube([column1,column2,.....,columnn]).sum(column5)\r\n\r\n   gropby_sets1 = a.gropby.sets( {column1,column2} ,{column1,column2,column3} ,{}).sum(column5)\r\n   gropby_sets2 = a.gropby([column1,column2).sets({column1,column2,column3} ,{} ).sum(column5)\r\n\r\n   gropby_rollup1 = a.gropby.rollup({column1,column2,column3}).sum(column5)\r\n   gropby_rollup2 = a.gropby([column1,column2).rollup({column3} ).sum(column5)\r\n```\r\n\r\n"},{"labels":["api"],"text":"How can you create a copy of the DataFrame without copying the actual data, but having a new DataFrame that when updated (not in place) does not modify the original (\"shallow copy\")? And how is this expected to behave? \r\nI suppose that in technical terms, this would be a new BlockManager that references the same arrays?\r\n\r\nI ran in the above questions, and actually didn't know a clear answer. The context was: I wanted to replace one column of a DataFrame, but without modifying the original one. And so was wondering if I could do that without making a full copy of the DataFrame (as in theory this is not needed, and I just wanted to update one object column before serializing). \r\n\r\n---\r\n\r\nSo you can do something like this with `copy(deep=False)`. Let's explore this somewhat:\r\n\r\nMaking a normal (deep) and shallow copy:\r\n\r\n```\r\nIn [1]: df = pd.DataFrame({'a': [1, 2, 3], 'b': [.1, .2, .3]}) \r\n\r\nIn [2]: df_copy = df.copy() \r\n\r\nIn [3]: df_shallow = df.copy(deep=False)\r\n```\r\n\r\nModifying values in place works as expected: for the copy it does not change the original df, for the shallow copy it does:\r\n```\r\nIn [4]: df_copy.iloc[0,0] = 10  \r\n\r\nIn [5]: df_shallow.iloc[1,0] = 20  \r\n\r\nIn [6]: df    \r\nOut[6]: \r\n    a    b\r\n0   1  0.1\r\n1  20  0.2\r\n2   3  0.3\r\n```\r\n\r\nOverwriting a full column, however, becomes more tricky (due to our BlockManager ...):\r\n\r\n```\r\n# this updates the original df\r\nIn [7]: df_shallow['a'] = [10, 20, 30] \r\n\r\nIn [8]: df\r\nOut[8]: \r\n    a    b\r\n0  10  0.1\r\n1  20  0.2\r\n2  30  0.3\r\n\r\n# this does not update the original\r\nIn [9]: df_shallow['b'] = [100, 200, 300]  \r\n\r\nIn [10]: df_shallow  \r\nOut[10]: \r\n    a    b\r\n0  10  100\r\n1  20  200\r\n2  30  300\r\n\r\nIn [11]: df  \r\nOut[11]: \r\n    a    b\r\n0  10  0.1\r\n1  20  0.2\r\n2  30  0.3\r\n```\r\n\r\nThis is of course somewhat expected if you know the internals: if the new column is of the same dtype, it seems to modify the array of the block in place, while if it needs to create a new block (because the dtype changed on assignment), the reference with the old data is broken and it doesn't modify the original dataframe. \r\n\r\nWhile writing this down, I am realizing that my question is maybe more: *should assigning a column (`df['a'] =  ..`) be seen as an in-place modification of your dataframe that has impact through shallow copies?* \r\nBecause in reality, `df['a']` cannot always happen in place (if you are overwriting with a different dtype), this gives rather inconsistent and surprising behaviour depending on the dtypes.\r\n\r\n\r\n"},{"labels":["api",null,null,null],"text":"Since `pandas 0.25.0` we have [named aggregations](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.25.0.html#groupby-aggregation-with-relabeling). \r\n\r\nWhich works fine if you do aggregations on single columns. But what if you want to apply aggregations over multiple columns:\r\n\r\n**example**:\r\n\r\n```\r\n# example dataframe\r\ndf = pd.DataFrame(np.random.rand(4,4), columns=list('abcd'))\r\ndf['group'] = [0, 0, 1, 1]\r\n\r\n          a         b         c         d  group\r\n0  0.751462  0.572576  0.192957  0.921723      0\r\n1  0.070777  0.801548  0.601678  0.344633      0\r\n2  0.112964  0.361984  0.416241  0.785764      1\r\n3  0.380045  0.486494  0.000594  0.608759      1\r\n\r\n# aggregations on single columns\r\ndf.groupby('group').agg(\r\n             a_sum=('a', 'sum'),\r\n             a_mean=('a', 'mean'),\r\n             b_mean=('b', 'mean'),\r\n             c_sum=('c', 'sum'),\r\n             d_range=('d', lambda x: x.max() - x.min())\r\n)\r\n\r\n          a_sum    a_mean    b_mean     c_sum   d_range\r\ngroup                                                  \r\n0      0.947337  0.473668  0.871939  0.838150  0.320543\r\n1      0.604149  0.302074  0.656902  0.542985  0.057681\r\n```\r\nBut what if we want to calculate the `a.max() - b.max()` while aggregating. That does not seem to work. For example, something like this would make sense:\r\n```\r\ndf.groupby('group').agg(\r\n    diff_a_b=(['a', 'b'], lambda x: x['a'].max() - x['b'].max())\r\n)\r\n```\r\n---\r\nSo is it possible to do named aggregations on multiple columns? If not, is this in the pipeline for future releases?\r\n"},{"labels":["api",null],"text":"For a use case in pyarrow, I need to get the underlying values of a Series: an ExtensionArray if it is back with one, or otherwise the numpy array.\r\n\r\nIs there public API to get this? We have `Series.array`, but this always returns an ExtensionArray, and we have `Series.values`, but this return a numpy array for eg periods (for historical reasons).\r\n\r\nFor Index, we have the private `Index._values` described in the docstring as the \"best array representation\". And I think `Series._values` is somewhat similar.\r\n\r\nBut the question is: do we want a public way to get to this? I am personally not sure we should, as there are still dubious cases (like datetime64/timedelta64, for this one, `Index._values` and `Series._values` is actually different ...). \r\n\r\nBut if we don't add it, do we have a recommended way for external projects to do this? \r\n(basically it is something like the `extract_array(..., extract_numpy=True)` ?) \r\n\r\ncc @TomAugspurger @jbrockmendel @jreback \r\n\r\n"},{"labels":["api",null],"text":"I recently ran into this as well (but forgot to open an issue), and raised now by @alexiswl in https://github.com/pandas-dev/pandas/issues/18262#issuecomment-546746982\r\n\r\n`Series.item()` was a consequence of (historically) inheriting from np.ndarray, and was deprecated (like a set of other ndarray-inhertited methods/attributes) a while ago.\r\n\r\nWhile `.item()` could also be used to select the \"i-th\" element (`.item(i)`), and this use case is certainly redundant (not arguing here to get that aspect back), there is one use case where `item()` can actually be useful: if you do not pass *i*, the method returns the element of the Series *only* if it has one element, otherwise it errors.\r\n\r\nSuch a situation can typically occur if you use boolean indexing (or `query`) to select a single element. Eg in cases like `s[s == 'val']` or `df.loc[df['col1'] == 'val', 'col2']` where you know the condition *should* yield a single element. \r\nYou then typically want the scalar element as result, but those two code snippets give you a Series of one element. In those cases, you could use `item()` to retrieve it: `s[s == 'val'].item()`.\r\n\r\nI saw some people using `.item()` exactly for this use case, so wondering if it is worth to keep it for this (only the version without passing *i*).\r\n\r\nThe logical alternative is doing a `.iloc[0]`, but `.item()` has the advantage of guaranteeing there was actually only one result item.\r\n\r\n\r\n\r\n"},{"labels":["api",null,null,null],"text":"Hi,\r\nI've been using method chaining to write most of my data wrangling processes and one of the things that bother me a bit is to modify column names as part of the chain. \r\n\r\nWith a static list of column names, ```df.set_axis``` can do the work. But in the following cases, I have to use `df.columns=xxx` to modify the names. \r\n\r\n1. Header comes from a row\r\n2. Combine multi-indexed header into a single-index (e.g. concatenate levels)\r\n3. Expand single-indexed header to multi-indexed (e.g. split via some delimiter)\r\n\r\nI wonder if these could be achieved by allowing ```df.set_axis``` to take callables, something similar to ```df.assign```."},{"labels":["api",null],"text":"Working on #27138 I've found that ``MultiIndex`` keeps nan-likes in the levels, but encode them all to -1:\r\n\r\n```python\r\n>>> levels, codes = [[nan, None, pd.NaT, 128, 2]], [[0, -1, 1, 2, 3, 4]]\r\n>>> mi = pd.MultiIndex(levels, codes)\r\n>>> mi.codes[0]\r\n[-1, -1, -1, -1, 3, 4]\r\n>>> mi.levels[0]\r\nIndex([nan, None, NaT, 128, 2], dtype='object')\r\n```\r\n\r\nAll the MultiIndex nan-likes are encoded to -1, so it's not possible to decode them to their constituent values. So it's not possible to get more than one nan-like values out of the MultiIndex, so in this case ``None`` and ``NaT`` disappears when converting:\r\n\r\n```python\r\n>>> mi.to_frame()[0].array\r\n<PandasArray>\r\n[nan, nan, nan, nan, 128, 2]\r\nLength: 6, dtype: object\r\n```\r\n\r\nI think if nan-likes are all encoded to -1, it'd be more consistent to not have them in the levels, similarly to how ``Categorical`` does it already.\r\n\r\n```python\r\n>>> c = pd.Categorical(levels[0])\r\n>>> c.codes\r\narray([-1, -1, -1,  1,  0], dtype=int8)\r\n>>> c.categories\r\n>>> Int64Index([2, 128], dtype='int64')\r\n```\r\n\r\nIs there acceptance to change the MultiIndex API so we get nan-likes out of the labels? That would give them an API more similar to ``Categorical``.\r\n\r\n@pandas-dev/pandas-core."},{"labels":["api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n# prepare data\r\ns = pd.Series(np.arange(4), index=pd.MultiIndex.from_product([list('ab'), list('cd')]))\r\n# note the index 'i' is constant so that groupby would produce a single group\r\ndf = pd.DataFrame.from_dict({'i': [True, True, True, True], 'x': np.arange(4), 'y': list('abcd')}).set_index(['i'])\r\n\r\n# indexing: irreversible loss of dtype info\r\n# There is no way to compute 'dataframe' result from the 'series' one.\r\n>>> df.iloc[0].dtype                # series\r\ndtype('O')\r\n# vs\r\n>>> df.iloc[[0]].dtypes             # dataframe (desirable output)\r\nx     int64\r\ny    object\r\n\r\n# unstack: unexpected shape of double-unstack (should be one-row dataframe) \r\n>>> s.unstack()          # this produces expected output\r\n   c  d\r\na  0  1\r\nb  2  3\r\n>>> s.unstack().unstack()           # this should be a one-row dataframe, not series\r\nc  a    0\r\n   b    2\r\nd  a    1\r\n   b    3\r\ndtype: int64\r\n\r\n# aggregate; same loss of dtype info as in indexing\r\n>>> def get_1st_row(x):\r\n...:     return x.iloc[0]\r\n\r\n>>> df.aggregate(get_1st_row).dtype     # lost type info\r\ndtype('O')\r\n>>> df.groupby('i').aggregate(get_1st_row).dtypes   # desirable output\r\nx     int64\r\ny    object\r\ndtype: object\r\n>>> \r\n\r\n# doesn't play well with iterators.\r\n# There should be no difference between iteration over groups\r\n# in the groupby object, and the aggregation of the groupby object.\r\n# The following two computations are morally equivalent, yet\r\n# their results differ\r\n>>> df2 = pd.concat({k: df for k in range(2)})\r\n>>> df2.groupby(level=0).aggregate(get_1st_row)   # desirable output\r\n   x  y\r\n0  0  a\r\n1  0  a\r\n# vs\r\n>>> pd.concat((_df.aggregate(get_1st_row) for k, _df in df2.groupby(level=0)))  # unexpected output\r\nx    0\r\ny    a\r\nx    0\r\ny    a\r\ndtype: object\r\n```\r\n#### Problem description\r\n\r\nWhenever pandas is to return a single-row dataframe it, conventionally, returns a series instead.\r\nExamples of such operations include: a) indexing a single row, b) unstacking, c) dataframe aggregation, and d) groupby aggregation vs iteration over groups.\r\n\r\nThis leads to loss of dtype information, unexpected output shapes, and shape mismatch between two morally equivalent computations. \r\n\r\nMy proposal is to add a new dataframe-row class DFRow(DataFrame) and return an instance of that class instead of returning Series in indexing, aggregation, transposition, and unstack-ing.\r\nReturning a single-row DataFrame w/o casting it to DFRow would create ambiguity in unstack-ing:\r\nDFRow should not be unstack-able (just as Series is not stack-able), but a generic single-row DataFrame should be unstack-able until its index levels are exhausted, at which point unstack() should return DFRow.\r\n\r\n**UPDATE**\r\nIt turned out there is an existing way of controlling the shape of the output. Here is a workaround:\r\n```\r\n>>> df.aggregate([get_1st_row])\r\n             x  y\r\nget_1st_row  0  a\r\n>>> pd.concat((_df.aggregate([get_1st_row]) for k, _df in df2.groupby(level=0)))\r\n             x  y\r\nget_1st_row  0  a\r\nget_1st_row  0  a\r\n```"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nfrom numpy import nan\r\ndf = pd.DataFrame({'grp': [1, 1, 2, 2], 'y': [1, 0, 2, 5], 'z': [1, 2, nan, nan]})\r\ndf.groupby('grp').apply(lambda grp_df: grp_df.nlargest(1, 'z'))\r\n#        grp  y    z\r\n# grp               \r\n# 1   1    1  0  2.0\r\n# \r\n# (Group 2 is gone!)\r\n\r\n```\r\n#### Problem description\r\n\r\nWhen the values of the ordering variables are all missing, the `nlargest` and `nsmallest` methods return a zero-row dataframe. This behavior is particularly unexpected when applying over groups, since it silently omits groups with all-NaN values. I think it would be better to return the requested number of rows, with NaN as appropriate. \r\n\r\nPut differently, this is a case where `nlargest` differs from the corresponding `sort_values(...).head(...)` code.\r\n```python\r\n\r\ndf.groupby('grp').apply(lambda x: x.sort_values('z', ascending=False).head(1))\r\n#        grp  y    z\r\n# grp               \r\n# 1   1    1  0  2.0\r\n# 2   2    2  2  NaN\r\n```\r\n(I'm aware that a better way to write that `sort_values` line would be to skip the `apply` and write `df.sort_values('z', ascending=False).groupby('grp').head(1)`, which gives a similar result, but better index.)\r\n\r\nI've talked about grouped dataframes because that seems like a more pernicious problem, but the behavior is the same with ungrouped dataframes.\r\nThe problem is also the same for `nsmallest` and `nlargest`, and it doesn't matter if there are multiple ordering columns, as long as they're all NaN.\r\n\r\nRelated, but not identical issues:\r\nhttps://github.com/pandas-dev/pandas/issues/23993 (requesting nlargest and nsmallest methods for grouped dataframes)\r\nhttps://github.com/pandas-dev/pandas/issues/21426 (bug with unsigned integers)\r\nhttps://github.com/pandas-dev/pandas/issues/12694 (NaN in `Series.argsort`)\r\n\r\n\r\n#### Expected Output\r\n\r\n```python\r\n\r\ndf.groupby('grp').apply(lambda grp_df: grp_df.nlargest(1, 'z'))\r\n#        grp  y    z\r\n# grp               \r\n# 1   1    1  0  2.0\r\n# 2   2    2  2  NaN\r\n```\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.3.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.0.0-31-generic\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 0.25.1\r\nnumpy            : 1.17.2\r\npytz             : 2019.3\r\ndateutil         : 2.8.0\r\npip              : 19.2.3\r\nsetuptools       : 41.4.0\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.3\r\nIPython          : 7.8.0\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 2.2.4\r\nnumexpr          : 2.7.0\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 0.15.0\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.3.1\r\nsqlalchemy       : None\r\ntables           : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"It seems pandas `percent_rank` works like the `cume_dist` of SQL databases.\r\n\r\nAs `ibis-framework` tries to use the same pandas API as much as possible ... `ibis-framework` `percent_rank` is equal to `pandas`.\r\n\r\nAs in SQL databases there are these 2 operations, I need a way to implement the SQL `percent_rank`\r\n\r\nI don't know very well which path I should take. Is there an initial thoughts here: https://github.com/ibis-project/ibis/issues/1975\r\n\r\nI wonder if there was any discussion about this topic before.\r\n\r\nAny comment, recommendation or guidance would be very appreciated.\r\n"},{"labels":["api",null,null,null,null],"text":"#### Problem description\r\n\r\nIt would be to have `cut` as a series method so that we can method chain. Currently, we must use `pd.cut`, for example,\r\n\r\n```\r\nfilled_series = df.some_series.fillna(0)\r\npd.cut(filled_series, 10).mean()\r\n```\r\n\r\nit would be nice to do,\r\n\r\n```\r\ndf.some_series.fillna(0).cut(10).mean()\r\n```\r\n\r\nThis would also help hugely with tab completion.\r\n\r\n#### Expected Output\r\n\r\nTo have a tab-completed series method for `cut`."},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nseries = pd.Series(range(10))\r\nshould_be_none = series.rename('new_name', inplace=True)\r\nprint(should_be_none)\r\n```\r\n#### Problem description\r\n\r\nWhen a method is called with the `inplace` argument, the method should:\r\n- transform the series inplace\r\n- **return `None`**\r\n\r\nIn this case, the method does:\r\n- transform the series inplace\r\n- **returns the series transformed (not a copy)**\r\n\r\n#### Expected Output\r\n`None`\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.4.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : None.None\r\n\r\npandas           : 0.25.1\r\nnumpy            : 1.16.5\r\npytz             : 2019.3\r\ndateutil         : 2.8.0\r\npip              : 19.2.3\r\nsetuptools       : 41.4.0\r\nCython           : 0.29.13\r\npytest           : 5.2.1\r\nhypothesis       : None\r\nsphinx           : 2.2.0\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 1.2.1\r\nlxml.etree       : 4.4.1\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.3\r\nIPython          : 7.8.0\r\npandas_datareader: None\r\nbs4              : 4.8.0\r\nbottleneck       : 1.2.1\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.4.1\r\nmatplotlib       : 3.1.1\r\nnumexpr          : 2.7.0\r\nodfpy            : None\r\nopenpyxl         : 3.0.0\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.3.1\r\nsqlalchemy       : 1.3.9\r\ntables           : 3.5.2\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : 1.2.1\r\n\r\n</details>\r\n\r\n"},{"labels":["api",null,null,null],"text":"A lot of complexity in DataFrame._reduce/apply/groupby ops is driven by the numeric_only=None.  In these cases we try to apply the reduction to non-numeric dtypes and if it fails, exclude them.  This hugely complicated our exception handling.\r\n\r\nWe should consider removing that option and require users to subset the appropriate columns before doing their operations."},{"labels":["api",null],"text":"We currently have a public `Index.to_native_types()` method (https://dev.pandas.io/docs/reference/api/pandas.Index.to_native_types.html) that somehow formats the values of the index.\r\n\r\nExample:\r\n```\r\nIn [3]: pd.date_range(\"2012\", periods=3).to_native_types() \r\nOut[3]: array(['2012-01-01', '2012-01-02', '2012-01-03'], dtype=object)\r\n```\r\n\r\nI don't think this needs to be in the public API?"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\ndf = pd.DataFrame([\r\n    ['a', 1], ['a', 2], ['a', 3],\r\n    ['b', 1], ['b', 3], ['b', 5]\r\n], columns=['key', 'val'])\r\nprint(df.groupby('key').rolling(2).quantile(.4, interpolation='lower'))\r\n```\r\n#### Problem description\r\n\r\nThe keyword argument `interpolation` is ignored - no matter what value it is set to, the default (linear) interpolation is used.  This problem only applies to `RollingGroupBy.quantile` - `GroupBy.quantile` and `Rolling.quantile` both appear to work as expected.\r\n\r\n#### Expected Output\r\n\r\n```\r\n       val\r\nkey       \r\na   0  NaN\r\n    1  1.0\r\n    2  2.0\r\nb   3  NaN\r\n    4  1.0\r\n    5  3.0\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.4.final.0\r\npython-bits      : 64\r\nOS               : Darwin\r\nOS-release       : 17.7.0\r\nmachine          : x86_64\r\nprocessor        : i386\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 0.25.1\r\nnumpy            : 1.17.2\r\npytz             : 2019.2\r\ndateutil         : 2.8.0\r\npip              : 19.2.3\r\nsetuptools       : 41.2.0\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.1\r\nIPython          : 7.8.0\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : None\r\ntables           : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None"},{"labels":["api",null,null],"text":"Part of the discussion on missing value handling in https://github.com/pandas-dev/pandas/issues/28095, detailed proposal at https://hackmd.io/@jorisvandenbossche/Sk0wMeAmB.\r\n\r\n*if* we go for a new NA value, we also need to decide the behaviour of this value in comparison operations. And consequently, we also need to decide on the behaviour of boolean values with missing data in logical operations and indexing operations.  \r\nSo let's use this issue for that part of the discussion.\r\n\r\nSome aspects of this:\r\n\r\n- Behaviour in **comparison operations**: currently np.nan compares unequal (`value == np.nan -> False`, `values > np.nan -> False`, but we can also propagate missing values (`value == NA -> NA`, ...)\r\n-  Behaviour in **logical operations**: currently we always return False for `|` or `&` with missing data. But we could also use a \"three-valued logic\" like [Julia](https://docs.julialang.org/en/v1/manual/missing/index.html#Logical-operators-1) and SQL (this has, eg, `NA | True = True` or `NA & True = NA`).\r\n- Behaviour in **indexing**: currently you cannot do boolean indexing with a boolean series with missing values (which is object dtype right now). Do we want to change this? For example, interpret it as False (not select it) \r\n  (TODO: should check how other languages do this)\r\n\r\nJulia has a nice documentation page explain how they support [missing values](https://docs.julialang.org/en/v1/manual/missing/index.html), the above ideas largely match with that.\r\n\r\nBesides those behavioural API discussions, we also need to decide on how to approach this technically (boolean ExtensionArray with boolean numpy array + mask for missing values?) Shall we discuss that here as well, or keep that separate?\r\n\r\ncc @pandas-dev/pandas-core "},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\ndf = pd.DataFrame(data = {'Japanese' : 'こんにちは', 'English' : 'Hello'}, index = [0])\r\ndf.to_string()\r\n\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.1.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : None.None\r\n\r\npandas           : 0.25.0\r\nnumpy            : 1.16.2\r\npytz             : 2018.7\r\ndateutil         : 2.7.5\r\npip              : 18.1\r\nsetuptools       : 40.6.3\r\nCython           : 0.29.2\r\npytest           : 4.0.2\r\nhypothesis       : None\r\nsphinx           : 1.8.2\r\nblosc            : None\r\nfeather          : 0.4.0\r\nxlsxwriter       : 1.1.2\r\nlxml.etree       : 4.2.5\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : 2.8.3 (dt dec pq3 ext lo64)\r\njinja2           : 2.10\r\nIPython          : 7.2.0\r\npandas_datareader: None\r\nbs4              : 4.6.3\r\nbottleneck       : 1.2.1\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.2.5\r\nmatplotlib       : 3.0.2\r\nnumexpr          : 2.6.8\r\nodfpy            : None\r\nopenpyxl         : 2.5.12\r\npandas_gbq       : None\r\npyarrow          : 0.13.0\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.1.0\r\nsqlalchemy       : 1.2.15\r\ntables           : 3.4.4\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : 1.1.2\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"Slightly related to #28380 \r\n\r\nCurrently an issue in Dask https://github.com/dask/dask/issues/5294 for implementing Named Aggregation (introduced in pandas 0.25.0) is open. To implement this it needs to use `_normalize_keyword_aggregation` and `_is_multi_agg_with_relabel`. \r\n\r\nMaking it public would be useful for frameworks like [dask](https://github.com/dask/dask) [mars](https://github.com/mars-project/mars)\r\n\r\ncc: @TomAugspurger "},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nfrom functools import partial\r\n\r\nquant50 = partial(np.percentile, q=50)\r\nquant70 = partial(np.percentile, q=70)\r\n\r\ntest = pd.DataFrame({'col1': ['a', 'a', 'b', 'b', 'b'], 'col2': [1,2,3,4,5]})\r\n\r\ntest.groupby('col1').agg(\r\n    quantile_50=('col2', quant50),\r\n    quantile_70=('col2', quant70)\r\n)\r\n```\r\n#### Problem description\r\n\r\nI'm trying to calculate multiple percentiles, I get the following error: \"Function names must be unique, found multiple named percentile\"\r\n\r\nIt's an old problem https://github.com/pandas-dev/pandas/issues/7186 but I assumed that it should've been solved with the named aggregations. Now there is no need to deduce the column name from the function name, the column name was specified explicitly.\r\n\r\n#### Expected Output\r\n\r\ncolumn | quantile_50 | quantile_70\r\na | 1.5 | 2\r\nb | 4.0 | 3\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.6.9.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.1.11-200.fc29.x86_64\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 0.25.1\r\nnumpy            : 1.16.4\r\npytz             : 2017.3\r\ndateutil         : 2.7.2\r\npip              : 19.1.1\r\nsetuptools       : 39.1.0\r\nCython           : 0.28.1\r\npytest           : 3.3.1\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : 0.4.0\r\nxlsxwriter       : None\r\nlxml.etree       : 4.3.3\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10\r\nIPython          : 6.3.1\r\npandas_datareader: None\r\nbs4              : 4.7.1\r\nbottleneck       : 1.2.1\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.3.3\r\nmatplotlib       : 2.2.2\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 0.9.0\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.2.1\r\nsqlalchemy       : 1.1.18\r\ntables           : None\r\nxarray           : None\r\nxlrd             : 1.1.0\r\nxlwt             : None\r\nxlsxwriter       : None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\nimport string, pandas\r\n\r\np_csv = pandas.read_csv(my_dir/myFile), index_col=0)\r\n\r\nsepanames = sorted(p_csv[\"SEPARATOR\"].unique())\r\n\r\nfor i in range(0, 14):\r\n    print(i)\r\n    col = p_csv.columns.get_loc(\"SEPARATOR\") + 1 + i\r\n    p_csv.insert(col, \"SEPARATOR_\" + sepanames[i].upper(), p_csv[\"SEPARATOR\"].apply(lambda x: int(x == sepanames[i])))\r\n\r\np_csv.to_csv(\"my_dir/new_file.csv\")\r\n\r\n''' p_csv= pandas.read_csv((\"/myDir/myFile.csv\"), index_col=0)\r\n    pandas.get_dummies(p_csv, prefix=\"SEPARATOR_\", columns=\"SEPARATOR\")\r\n    p_csv.to_csv(\"/myDir/myNew.csv\")'''\r\n\r\nFILES:\r\nhttps://www.amazon.de/clouddrive/share/h37d1hqtrj5SrZTvKdrs9gXltVKUgo8Is9BxL8WH7Sf\r\n#### Problem description\r\n\r\nThis is all of my code. The quoted part is what I first tried, but after 20 minutes it ended through a sigkill without any result. The required files are available for download. I would think, that my code does the equivalent in this very case and it works just fine in under about a minute.\r\n\r\nPandas Version: 25.1.0\r\nPandas git version:  '171c71611886aab8549a8620c5b0071a129ad685'\r\n\r\n\r\n#### Expected Output\r\nNo error and changed csv file"},{"labels":[null,"api",null,null],"text":"Split from https://github.com/pandas-dev/pandas/pull/27899. How should we handle options to be passed to the `open` call (like `encoding`, `errors`).\r\n\r\nA few options\r\n\r\n* pass through `**kwargs`\r\n* a \"storage_options\" kwarg that's a dict with some specification\r\n\r\nWant to thing about things like S3, etc. too."},{"labels":["api",null],"text":"#### Code Sample\r\n\r\n```python\r\n\r\nimport pandas as pd\r\nanimals = pd.DataFrame({'kind': ['cat', 'dog', 'cat', 'dog'],\r\n                        'height': [9.1, 6.0, 9.5, 34.0],\r\n                        'weight': [7.9, 7.5, 9.9, 198.0]})\r\n\r\n# (1) - doesn't work\r\nanimals.groupby(\"kind\").rolling(1).agg(**{'total weight': pd.NamedAgg(column='weight', aggfunc=sum),\r\n                                          'min_weight': pd.NamedAgg(column='weight', aggfunc=min)})\r\n\r\n# (2) - works\r\nanimals.groupby(\"kind\").agg(**{'total weight': pd.NamedAgg(column='weight', aggfunc=sum),\r\n                               'min_weight': pd.NamedAgg(column='weight', aggfunc=min)})\r\n\r\n# works, but returns multiindex\r\nanimals.groupby(\"kind\").rolling(1).agg({'weight': {'total_weight': 'sum', 'min_weight': 'min'}})\r\n\r\n# (3) - this is what I'd want, expected output of (1)\r\nanimals.groupby(\"kind\").rolling(1).agg({'weight': {'total_weight': 'sum', 'min_weight': 'min'}}).droplevel(0, axis=1)\r\n\r\n```\r\n#### Problem description\r\n\r\nPandas version 0.25 provides new aggregate functionality through [NamedAgg](https://pandas-docs.github.io/pandas-docs-travis/whatsnew/v0.25.0.html#groupby-aggregation-with-relabeling). I would expect to be able to pass this in after a groupby as a `pd.apply` statement, and also after a groupby _and_ `pd.rolling`. \r\n\r\nHowever, when I try to pass in a dict of `NamedAgg` tuples as shown in (1) in the example code I get the following TypeError: `TypeError: aggregate() missing 1 required positional argument: 'arg'`\r\n\r\nIs this expected or am I incorrectly using this functionality? Thanks in advance!\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.4.final.0\r\npython-bits      : 64\r\nOS               : Darwin\r\nOS-release       : 18.7.0\r\nmachine          : x86_64\r\nprocessor        : i386\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : en_US.UTF-8\r\npandas           : 0.25.1\r\nnumpy            : 1.16.1\r\npytz             : 2018.5\r\ndateutil         : 2.7.3\r\npip              : 19.1.1\r\nsetuptools       : 41.0.1\r\nCython           : None\r\npytest           : 4.3.1\r\nhypothesis       : None\r\nsphinx           : 1.8.5\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : 0.9.2\r\npsycopg2         : 2.7.7 (dt dec pq3 ext lo64)\r\njinja2           : 2.10.1\r\nIPython          : 7.3.0\r\npandas_datareader: None\r\nbs4              : 4.7.1\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.0.3\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : 2.6.1\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.1.0\r\nsqlalchemy       : 1.3.3\r\ntables           : None\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : None\r\nxlsxwriter       : None\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"#### Summary\r\n\r\nThanks for making Pandas I have used it in a lot of projects! But now I have a problem.\r\n\r\nI have spent nearly 3 days trying to figure out how to **resample / upsample a Pandas MultiIndex** elegantly and correctly. I have read and tried numerous posts on StackOverflow and GitHub. My conclusion is that **I don't think this is supported very well in Pandas**. Let me explain what I want to do.\r\n\r\n#### Background\r\n\r\nI am currently building a Python API in collaboration with www.simfin.com that makes it very easy to download and use financial data (share-prices, fundamentals, etc.) for free. This will enable people to conduct and share financial research very easily. It works by downloading bulk-data in CSV files from the SimFin server and loading them in Pandas. The fundamental data such as Income Statements and Balance Sheets is usually indexed by the Ticker and Report Date which creates a Pandas DataFrame with a MultiIndex.\r\n\r\n#### Data Example\r\n\r\nLet us say we have a Pandas DataFrame `df` with this data:\r\n\r\n                             Revenue  Net Income (Common)\r\n    Ticker Report Date                                   \r\n    AAPL   2007-09-30    24578000000           3495000000\r\n           2008-09-30    37491000000           6119000000\r\n           2009-09-30    42905000000           8235000000\r\n           2010-09-30    65225000000          14013000000\r\n           2011-09-30   108249000000          25922000000\r\n           2012-09-30   156508000000          41733000000\r\n           2013-09-30   170910000000          37037000000\r\n           2014-09-30   182795000000          39510000000\r\n           2015-09-30   233715000000          53394000000\r\n           2016-09-30   215639000000          45687000000\r\n           2017-09-30   229234000000          48351000000\r\n           2018-09-30   265595000000          59531000000\r\n    AMZN   2007-12-31    14835000000            476000000\r\n           2008-12-31    19166000000            645000000\r\n           2009-12-31    24509000000            902000000\r\n           2010-12-31    34204000000           1152000000\r\n           2011-12-31    48077000000            631000000\r\n           2012-12-31    61093000000            -39000000\r\n           2013-12-31    74452000000            274000000\r\n           2014-12-31    88988000000           -241000000\r\n           2015-12-31   107006000000            596000000\r\n           2016-12-31   135987000000           2371000000\r\n           2017-12-31   177866000000           3033000000\r\n           2018-12-31   232887000000          10073000000\r\n    MSFT   2008-06-30    60420000000          17681000000\r\n           2009-06-30    58437000000          14569000000\r\n           2010-06-30    62484000000          18760000000\r\n           2011-06-30    69943000000          23150000000\r\n           2012-06-30    73723000000          16978000000\r\n           2013-06-30    77849000000          21863000000\r\n           2014-06-30    86833000000          22074000000\r\n           2015-06-30    93580000000          12193000000\r\n           2016-06-30    91154000000          20539000000\r\n           2017-06-30    96571000000          25489000000\r\n           2018-06-30   110360000000          16571000000\r\n\r\n#### Resample a single Ticker (DatetimeIndex)\r\n\r\nLet us first resample for a single ticker:\r\n\r\n    df.loc['MSFT'].resample('D').pad()\r\n\r\nThis works and the result is:\r\n\r\n                     Revenue  Net Income (Common)\r\n    Report Date                                  \r\n    2008-06-30   60420000000          17681000000\r\n    2008-07-01   60420000000          17681000000\r\n    2008-07-02   60420000000          17681000000\r\n    2008-07-03   60420000000          17681000000\r\n    2008-07-04   60420000000          17681000000\r\n\r\n#### Resample multiple Tickers (MultiIndex)\r\n\r\nLet us now try and resample for all tickers in the DataFrame. The `resample()` function takes an argument `level` which is supposed to work with a MultiIndex DataFrame:\r\n\r\n    df.resample('D', level='Report Date').pad()\r\n\r\nBut this apparently doesn't work for upsampling e.g. annual data to daily data, because we get this error message:\r\n\r\n    ValueError: Upsampling from level= or on= selection is not supported, use .set_index(...) to explicitly set index to datetime-like\r\n\r\nOne solution is to use `groupby()` (adapted from e.g. #13699):\r\n\r\n    df.reset_index('Ticker').groupby('Ticker').resample('D').pad()\r\n\r\nThis works, but it now has duplicated the Ticker both as an index and as a column:\r\n\r\n                       Ticker      Revenue  Net Income (Common)\r\n    Ticker Report Date                                         \r\n    AAPL   2007-09-30    AAPL  24578000000           3495000000\r\n           2007-10-01    AAPL  24578000000           3495000000\r\n           2007-10-02    AAPL  24578000000           3495000000\r\n           2007-10-03    AAPL  24578000000           3495000000\r\n           2007-10-04    AAPL  24578000000           3495000000\r\n\r\nWe can avoid one of them by adding the arg `group_keys=False`:\r\n\r\n    df.reset_index('Ticker').groupby('Ticker', group_keys=False).resample('D').pad()\r\n\r\nThis works, but now the Ticker is a data-column instead of an index:\r\n\r\n                Ticker      Revenue  Net Income (Common)\r\n    Report Date                                         \r\n    2007-09-30    AAPL  24578000000           3495000000\r\n    2007-10-01    AAPL  24578000000           3495000000\r\n    2007-10-02    AAPL  24578000000           3495000000\r\n    2007-10-03    AAPL  24578000000           3495000000\r\n    2007-10-04    AAPL  24578000000           3495000000\r\n\r\nTo get the original MultiIndex back with both Ticker and Report Date, we need to do:\r\n\r\n    df.reset_index('Ticker').groupby('Ticker', group_keys=False).resample('D').pad().reset_index().set_index(['Ticker', 'Report Date'])\r\n\r\nWhich produces the desired result:\r\n\r\n                            Revenue  Net Income (Common)\r\n    Ticker Report Date                                  \r\n    AAPL   2007-09-30   24578000000           3495000000\r\n           2007-10-01   24578000000           3495000000\r\n           2007-10-02   24578000000           3495000000\r\n           2007-10-03   24578000000           3495000000\r\n           2007-10-04   24578000000           3495000000\r\n\r\nBut this is so complicated that nobody can be expected to remember how to do it. So I would have to make a small helper-function that does all of this. But because the resampling method (pad, interpolate, etc.) is invoked through a function call on the groupby-object, my helper-function would get big and awkward if I want to allow different methods of resampling.\r\n\r\n#### Conclusion\r\n\r\nIt appears that upsampling a MultiIndex is not supported very well in Pandas, unless I have misunderstood how it is supposed to work.\r\n\r\nI think that by far the most elegant solution would be if the `resample()` function supported the `level` argument for upsampling, because the syntax and semantics would be very similar for upsampling DatetimeIndex and MultiIndex:\r\n\r\n    # DatetimeIndex\r\n    df.loc['MSFT'].resample('D').pad()\r\n\r\n    # MultiIndex\r\n    df.resample('D', level='Report Date').pad()\r\n\r\nI have taken a look at the Pandas source-code, but it is complicated and so sparsely documented, that it would take me forever to figure out how everything is connected and how it works, so I don't think I will be able to fix this myself. Is this something you could fix, because it would make it so much easier to upsample DataFrames with a MultiIndex?\r\n\r\nThanks!\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.6.8.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 4.15.0-60-generic\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 0.25.1\r\nnumpy            : 1.16.4\r\npytz             : 2019.2\r\ndateutil         : 2.8.0\r\npip              : 19.1.1\r\nsetuptools       : 41.0.1\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.1\r\nIPython          : 7.5.0\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.1.0\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.3.1\r\nsqlalchemy       : None\r\ntables           : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\n\r\n</details>"},{"labels":["api",null,null],"text":"Currently I am not able to use groupby on columns to produce groupings that contain combinations of same columns. Let's say I have a dataframe with columns A, B , C and D. I would like to create groups that contain [A,B], [A,C] and [B,D]. \r\n\r\nOne obvious way to implement this would be to allow as argument a dictionary of lists, where the keys are output labels and the lists are the column labels to be grouped. \r\n\r\nI am aware that currently grouping via dictionary works the opposite way, i.e. the key is the input label and the value is the output label, but at least to me this seems somewhat less useful than the reverse. "},{"labels":["api",null],"text":"I would like to request a method similar to `GroupBy.get_group(self, name, obj=None)` [(documentation)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.get_group.html), a method that can return a group by its index say call it as `GroupBy.get_nth_group(self, nth)`. \r\nIf there are 10 groups by calling `get_nth_group(4)` shall return the 4th group."},{"labels":["api",null],"text":"I'm trying to apply pd.to_datetime on an integer column to convert integers (including missing indicators) to datetimes. In order to preserve all the missing indicators, ideally, I would prefer setting the \"errors\" parameter to \"ignore\" to keep all the invalid parsings. But then I noticed that setting errors='ignore' not only preserved the missing indicators, but kept everything the same, even the dtype of the input column. See below for more details.  \r\n \r\n```python\r\nprint(dt_col)\r\n```\r\n<Outputs>\r\n0    20181223\r\n\r\n1    20161017\r\n2      -99998\r\n3    20171220\r\n4    20170511\r\nName: dt_col, dtype: int64\r\n</Outputs>\r\n\r\n```python\r\npd.to_datetime(dt_col, format='%Y%m%d', errors='ignore')\r\n```\r\n<Outputs>\r\n0    20181223\r\n\r\n1    20161017\r\n2      -99998\r\n3    20171220\r\n4    20170511\r\nName: dt_col, dtype: int64\r\n\r\n</Outputs>\r\n\r\n```python\r\npd.to_datetime(dt_col, format='%Y%m%d', errors='coerce')\r\n```\r\n<Outputs>\r\n0   2018-12-23\r\n\r\n1   2016-10-17\r\n2          NaT\r\n3   2017-12-20\r\n4   2017-05-11\r\nName: dt_col, dtype: datetime64[ns]\r\n\r\n</Outputs>\r\n\r\n\r\nPandas version: \r\n- pandas==0.25.1\r\n\r\nIs anyone else seeing the same behavior? "},{"labels":["api",null,null,null],"text":"I cleaned up my initial write up on the consistent missing values proposal (https://github.com/pandas-dev/pandas/issues/27825#issuecomment-520583911), and incorporated the items brought up in the last video chat. So I think it is ready for some more detailed discussion.\r\n\r\nThe last version of the full proposal can be found here: https://hackmd.io/@jorisvandenbossche/Sk0wMeAmB\r\n\r\nTL;DR:\r\n\r\n- I propose to introduce a new scalar (singleton) `pd.NA` that can be used as the missing value indicator (when accessing a single value, not necessarily how it is stored under the hood). \r\n- This can be used instead of `np.nan` or `pd.NaT` in new data types (eg nullable integers, potential string dtype)\r\n- Long term, we can see if there is a migration possible to use this consistently for all data types.\r\n\r\ncc @pandas-dev/pandas-core "},{"labels":["api",null],"text":"In tests.frame.test_arithmetic we test the DataFrame part of the following, but we do not test the Series behavior (at least not in that file)\r\n\r\n```\r\narr = np.array([np.nan, 1, 6, np.nan])\r\narr2 = np.array([2j, np.nan, 7, None])\r\nser = pd.Series(arr)\r\nser2 = pd.Series(arr2)\r\ndf = pd.DataFrame(ser)\r\ndf2 = pd.DataFrame(ser2)\r\n\r\nser < ser2  # raises TypeError, makes sense\r\ndf < df2  # raises TypeError, makes sense\r\n\r\nser.lt(ser2)  # raises TypeError, makes sense\r\n\r\n>>> df.lt(df2)\r\n       0\r\n0  False\r\n1  False\r\n2   True\r\n3  False\r\n```\r\n\r\nThe `df.lt(df2)` version (the \"flex\" op) masks positions where either `df` or `df2` is null.  That is fine, but why doesn't the `Series` version do the same thing?"},{"labels":["api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n# feature request\r\nschema = df.schema  # include all the index columns and the df.columns\r\nfor column in schema:\r\n    print(column.name, column.dtype)  # plus other attributes?\r\n```\r\n\r\nCurrently this is only available by using a pyarrow table, e.g.\r\n```python\r\nimport pyarrow as pa\r\ntable = pa.Table.from_pandas(df)\r\nschema = table.schema\r\nschema_types = [[s.name, str(s.type).upper()] for s in schema]\r\n```\r\n\r\nIt requires extra code to distinguish between index columns and other columns.\r\n```python\r\n# assuming schema from above pyarrow table\r\nschema_dict = dict([[s.name, str(s.type).upper()] for s in schema])\r\n# then iterate on df.index.names and df.columns to filter the schema_dict, e.g.\r\nschema_index = dict([[name, schema_dict[name]] for name in df.index.names])\r\n```\r\n\r\n#### Problem description\r\n\r\n- the df.info() is not iterable, it just prints information\r\n  - also, it has no dtypes for the index columns\r\n\r\n- the df.dtypes is a Series but it's not all columns, with index columns, e.g.\r\n\r\n```python\r\ntypes = df.dtypes\r\ntype(types)\r\n# <class 'pandas.core.series.Series'>\r\ntypes.index  # df.columns but not df.index columns\r\nfor idx in types.index:\r\n    print(idx, types[idx])\r\n```\r\n\r\n- feature request for an iterable schema data structure\r\n\r\n```python\r\nschema = df.schema  # include all the index columns and the df.columns\r\nfor column in schema:\r\n    print(column.name, column.dtype)\r\n\r\n# the column object may have other attributes too:\r\ncolumn.name  # str\r\ncolumn.dtype  # numpy.dtype ?\r\ncolumn.is_index  # boolean\r\n\r\ndf.schema.to_dict()  # column.name as keys, column object as value\r\ndf.schema.to_sql()   # just DDL output\r\n```\r\n\r\nSimilar schema are available if the DataFrame is serialized to parquet and read with pyarrow, but the pyarrow table schema is not iterable in the manner requested here.\r\n\r\nAlso note that `MultiIndex` has no dtypes, this feature request implicates new features for an index object."},{"labels":["api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> df = pd.DataFrame({'col': [['a', 'b'], ['c', 'd']]}, index=[0,0])\r\n>>> df.explode('col')\r\n  col\r\n0   a\r\n0   b\r\n0   c\r\n0   d\r\n0   a\r\n0   b\r\n0   c\r\n0   d\r\n```\r\n#### Problem description\r\nI'm not sure if this is expected behaviour. But it took me slightly off guard when I had a datetime index with occasionally identical timestamps. This is the output I expected:\r\n```python\r\n  col\r\n0   a\r\n0   b\r\n0   c\r\n0   d\r\n```\r\nThanks for your work on pandas! Apologies if this is actually normal behaviour.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.6.7.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 4.4.0-139-generic\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : en_US.UTF-8\r\nLANG             : en_US\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 0.25.0\r\nnumpy            : 1.14.3\r\npytz             : 2018.9\r\ndateutil         : 2.8.0\r\npip              : 19.0.2\r\nsetuptools       : 40.8.0\r\nCython           : None\r\npytest           : 4.5.0\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10\r\nIPython          : 7.3.0\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.0.2\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.2.1\r\nsqlalchemy       : None\r\ntables           : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"Follow-up on https://github.com/pandas-dev/pandas/issues/27775 and https://github.com/pandas-dev/pandas/pull/27818.\r\n\r\nShort recap of what those issues were about:\r\n\r\nCurrently, indexing into an Index with a 2D (or multiple D) indexer results in an \"invalid\" Index with an underlying ndarray:\r\n\r\n```\r\nIn [1]: idx = pd.Index([1, 2, 3])  \r\n\r\nIn [2]: idx2 = idx[:, None] \r\n\r\nIn [3]: idx2\r\nOut[3]: Int64Index([1, 2, 3], dtype='int64')\r\n\r\nIn [4]: idx2.values\r\nOut[4]: \r\narray([[1],\r\n       [2],\r\n       [3]])\r\n```\r\nSo from the repr it *looks* like a proper index, but the underlying values of an Index should always be 1D (such an invalid index will also lead to errors once you do operations on them).\r\n\r\nBefore pandas 0.25.0, the `shape` attribute of the index \"correctly\" returned the shape of the underlying values: `(3, 1)`, but in 0.25.0 this was changed to `(3,)` (only checking the length). This caused a regression matplotlib (https://github.com/pandas-dev/pandas/issues/27775), and will be \"fixed\" in 0.25.1 returning again the 2D shape of the underlying values (https://github.com/pandas-dev/pandas/pull/27818). Of course, this is only about the `shape` attribute, while the root cause is this invalid Index.\r\n\r\nI think it is clear that we should not allow such invalid Index object to exist. \r\nI currently know of two ways to end up such situation:\r\n\r\n* Passing a multidimensional array to the Index constructor (e.g. `pd.Index(np.random.randn(5, 5, 5))`. I think this is something we can deprecate and raise for later, and there is already an issue for this: https://github.com/pandas-dev/pandas/issues/27125\r\n* Indexing into an Index (e.g. `idx[:, None] `) -> this issue\r\n\r\nSo let's use **this issue** to discuss what to do for this second way: **a 2D indexing operation on a 1D object.**\r\n\r\nThis is relevant for the Index, but we should probably try to have it consistent with Series as well."},{"labels":["api",null],"text":"Can we document something like `DataFrame._constructor_sliced` as the public way to say that slicing a column from a pandas.DataFrame will return a pandas.Series object?\r\n\r\nProjects like Dask would like to work with any concrete DataFrame library (cudf being the second major dataframe implementation). Right now we resort to things like `type(dask.dataframe.DataFrame._meta.iloc[:, 0])` to get the concrete type of a slice. But that's fragile (e.g. a DataFrame with no columns).\r\n\r\nI'm not sure how useful this would be to users, so using an underscore-prefixed name may be reasonable."},{"labels":["api",null],"text":"```python\r\ndf = pd.DataFrame(0, index=[1,2,3,4], columns=['a', 'b', 'c'])\r\n# Starting from 0.25 groupby pad behaves like other groupby functions in removing the grouping column\r\ndf.groupby(df.a).mean()\r\n   b  c\r\na      \r\n0  0  0\r\ndf.groupby(df.a).pad()\r\n   b  c\r\n1  0  0\r\n2  0  0\r\n3  0  0\r\n4  0  0\r\n\r\n# However for other functions it is possible to keep the group column using as_index=False\r\ndf.groupby(df.a, as_index=False).mean()\r\n   a  b  c\r\n0  0  0  0\r\n# For ffill/bfill/pad instead the keyword as_index does not help as the column is not being used as the index anyway\r\ndf.groupby(df.a, as_index=False).pad()\r\n   b  c\r\n1  0  0\r\n2  0  0\r\n3  0  0\r\n4  0  0\r\n# There is no way of keeping the column, except creating a useless copy of the column series\r\ndf.groupby(df.a.copy()).pad()\r\n   a  b  c\r\n1  0  0  0\r\n2  0  0  0\r\n3  0  0  0\r\n4  0  0  0\r\n```\r\n#### Problem description\r\nStarting from 0.25 ffill behaves like other groupby functions in removing the group column. As far as I understand there is no non-hacky way of getting the old behaviour back. This is not consistent with what happens with other functions where the keyword argument as_index can be used to keep the grouping column.\r\n\r\nThe only way of having the old behaviour is to create a copy of the column. However this whole thing of view-dependent-behaviour is not only very confusing (at least to me) but it is also inefficient as it requires a useless copy. Moreover it is not a backward compatible solution.\r\n\r\nI would suggest to either add a keep_group_columns that works consistently with all groupy functions or to add a special keyword only for groupby functions that keep the original index."},{"labels":["api",null],"text":"From https://github.com/pandas-dev/pandas/issues/27553 and https://github.com/pandas-dev/pandas/issues/27522, I quickly looked at how consistent we are with defining errors and warnings in `pandas.errors`.\r\n\r\nSo we have a `pandas.errors` module which is meant to publicly expose our custom exceptions and warnings.\r\n\r\nSome are defined there such as PerformanceWarning, UnsortedIndexError, ParserError, ... (https://github.com/pandas-dev/pandas/blob/bb6135880e5e453d7701764b9f2e4ad3356a68d7/pandas/errors/__init__.py). \r\nBut many are not:\r\n\r\n* tslibs.parsing.DateParseError\r\n* tslibs.period.IncompatibleFrequency\r\n* pandas/_config/config.py: OptionError\r\n* pandas/core/base.py: DataError, GroupByError, SpeciicationError\r\n* pandas/core/common.py: SettingWithCopyError/Warning\r\n* pandas/core/computation/common/py: NameResolutionError\r\n* pandas/core/computation/engine.py: NumExprClobberingError\r\n* pandas/core/computation/ops.py: UndefinedVariableError\r\n* pandas/core/indexes/base.py: InvalidIndexError\r\n* pandas/coreindexing.py: IndexingError\r\n* pandas/io/clipboard/exceptions.py: PyperclipException\r\n* pandas/io/formats/css.py: CSSWarning\r\n* pandas/io/msgpack: several exceptions\r\n* pandas/io/pytables.py: lots of exceptions\r\n* pandas/io/sql.py: SQLALchemyRequired, DatabaseError\r\n* pandas/io/stata.py: several warnings\r\n\r\nDo we want to move them all to the public `pandas.errors` module? \r\nAre there reasons that some can / should not be moved? Eg should the cython ones be defined in the cython file for performance? \r\nAre there certain custom exceptions/warnings that we rather convert in a built-in one instead of publicly exposing them? \r\nDo we want to move all the io related ones? (those modules are somewhat public)"},{"labels":["api",null,null],"text":"Due to #23980 the following code now raises a ValueError since 0.25:\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nii = pd.IntervalIndex.from_tuples([(0, 10), (2, 12), (4, 14)])\r\npd.cut([5, 6], bins=ii)\r\n```\r\n#### Problem description\r\nBefore #23980 an IntervalIndex with overlapping columns could be used. It would return every Interval which is valid for the required data, which is obviously the correct solution.\r\n\r\nIn #23980 it was stated that this doesn't make sense in the context of cut. Unfortunately I missed the discussion over there (there really was None). I argue that by raising a value error we unnecessarily remove a valid feature: I use cut frequently as kind of a more versatile replacement to `pd.rolling` for overlapping non-equal sized custom windows.\r\n\r\nIf there is a smarter way to do this I am happy to learn about it. Otherwise we should at least give the option to use overlapping indices in cut. Thus I would recommend to raise a warning instead of an error here:\r\nhttps://github.com/pandas-dev/pandas/blob/0fd888c8b318d4041b0ae53c64715eca0d345dd3/pandas/core/reshape/tile.py#L247-L249\r\n\r\n\r\n#### Expected Output\r\nRaise a warning maybe (I am still not sure if this is necessary) and return:\r\n```python\r\n[(0, 10], (2, 12], (4, 14], (0, 10], (2, 12], (4, 14]]\r\nCategories (3, interval[int64]): [(0, 10] < (2, 12] < (4, 14]]\r\n```\r\n"},{"labels":[null,"api",null,null],"text":"I would like to propose that any pandas API that allows specification of a column name also works when specifying an index level name.  Today, this works in some places, but not all.  Here is a list of places where things work, and where things don't work and could be improved.  (The list is most likely incomplete, so additions are welcome).  References to existing issues are given when they already exist (and if I knew about them):\r\n\r\nHoping to include this in the roadmap #27478\r\n\r\nThings that work:\r\n- `DataFrame.query()` allows queries to use column names and index level names\r\n- `DataFrame.merge()` and `pd.merge()` allow both column names and index level names to be specified as key fields to use in the merge (but see below)\r\n- `DataFrame.groupby()` allows both column names and index level names to be mixed in the groupby list\r\n\r\nThings that could be improved:\r\n- Allow `.loc()` and `.getitem()` to specify index level names wherever it allows column names.\r\n- Modify `DataFrame.rename()` to allow renaming of index levels with a dict argument. (#20421)\r\n- Allow index names to be specified like columns in `pd.Grouper()` (#19542)\r\n- When merging on a subset of `MultiIndex` levels, preserve the levels not included in the merge.  (#13371 is somewhat related)\r\n- Have `.itertuples()` return a named tuple that includes index names (#27407) \r\n- Allow `.assign` to refer to columns that correspond to level names (although the `.loc` and `.getitem()` suggestion above might handle this\r\n\r\n"},{"labels":["api",null,null],"text":"This function I think gets a good amount of usage but people tend to import from `pandas.io.json`. ref #27522 I think we should move this to the top level\r\n\r\ncc @bhavaniravi "},{"labels":["api",null],"text":"There's the `dt` accessor on Series which operates on the Series values:\r\n``` \r\npd.Series([1,2,3]).dt.day \r\n#exists. But checks that series *values*  datetime64[ns]\r\n\r\npd.Series([1,2,3]).day\r\n# doesn't exist\r\n```\r\n\r\nThere's a top-level method `df.first`:\r\n```\r\nConvenience method for subsetting initial periods of time series data\r\nbased on a date offset.\r\n```\r\nwhich looks like it should live behind `dti` accessor on DataFrame.\r\n```\r\npd.DataFrame().first(0) \r\n# method is applicable only for datetime, but is in the top DataFrame namespace\r\n```\r\n\r\nThere's currently no datetime accessor on dataframe. Would it make sense to move it below a `dti` accessor on DataFrame?  \r\n\r\nThere's also an unfortunate unfortunate name collision between`Groupby().first`\r\n```\r\nCompute first of group values.\r\n```\r\nand `DataFrame.first`:\r\n```\r\nConvenience method for subsetting initial periods of time series data\r\nbased on a date offset.\r\n```\r\nAdding a `dti` accessor to DataFrame would clean that up as well.\r\n"},{"labels":["api",null,null],"text":"#### Code Sample\r\n\r\n```python\r\nimport pandas as pd\r\nfrom pandas.errors import OptionError \r\n# or from pandas import OptionError\r\noption = ('noexist', 1)\r\ntry:\r\n    pd.set_option(*option)\r\nexcept OptionError as e:\r\n    print('Ignore invaild option: {}'.format(option))\r\n```\r\n#### Problem description\r\n\r\n`OptionError` is raised by top-level function `pandas.set_option`, but can only be imported from private module `pandas._config.config`.\r\n\r\nSample code will failed when importing it from `pandas.errors`  :\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-62b92256ce5f> in <module>\r\n      1 import pandas as pd\r\n----> 2 from pandas.errors import OptionError # or from pandas import OptionError\r\n      3 option = ('noexist', 1)\r\n      4 try:\r\n      5     pd.set_option(*option)\r\n\r\nImportError: cannot import name 'OptionError' from 'pandas.errors' (/home/abc/.local/share/virtualenvs/abc-jIQt2SYy/lib/python3.7/site-packages/pandas/errors/__init__.py)\r\n```\r\n\r\n#### Expected Output\r\n\r\n```\r\nIgnored invaild option: ('noexist', 1)\r\n```\r\n\r\nExpected behavior : `OptionError` can be imported from `pandas.errors` or `pandas`. And other custom exceptions raised by top-level functions are also available in the same public namespace.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.3.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 4.4.0-17763-Microsoft\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : C.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 0.25.0\r\nnumpy            : 1.16.4\r\npytz             : 2019.1\r\ndateutil         : 2.8.0\r\npip              : 19.1.1\r\nsetuptools       : 41.0.1\r\nCython           : 0.29.12\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.1\r\nIPython          : 7.6.1\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.1.1\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : 2.6.2\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : None\r\ntables           : None\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : None\r\nxlsxwriter       : None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"we should use keyword only arguments for some of our functions that have large numbers of kwargs to make it harder to make mistakes in the calling conventions, a prime example is [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv#pandas.read_csv].\r\n\r\nlikely we want a signature\r\n\r\n```def read_csv(self, filepath_or_buffer, *, .......```\r\n\r\nIOW *all* args, except for the first should be kwargs.\r\n\r\n We could further modify a fair number of functions, so will treat this as a tracking issue."},{"labels":["api",null],"text":"https://github.com/pandas-dev/pandas/pull/27467#discussion_r305439508\r\n\r\nExamples:\r\n- `g.transform('invalid_name')`\r\n- `g.transform('agg')`, attribute exists but isn't appropriate agg/transformation.\r\n"},{"labels":["api",null],"text":"https://github.com/pandas-dev/pandas/pull/27461#discussion_r305168936\r\n\r\n> i think we need. way for EA to hook into this for an EA scalar\r\n> eg an IPaddress from cyberpandas could register a scalar i think\r\n\r\nBefore we move on this, I think we need to clarify in which situations we care about `lib.is_scalar(x)` vs the simpler `np.ndim(x) == 0`"},{"labels":["api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n\r\n# Function\r\ndef _filter(self, func):\r\n    return self[list(map(func, self))]\r\n\r\n# Set the attribute\r\nsetattr(pd.Index, \"filter\", _filter)\r\nindex = pd.Index([\"iris_1\", \"notiris_1\", \"iris_2\", \"other_thing\"])\r\n\r\n# New way\r\nfunc = lambda x:x.startswith(\"iris\")\r\nprint(\"New way:\", index.filter(func))\r\n\r\n# Old way\r\nprint(\"Old way:\", index[index.map(func)])\r\n\r\n# New way: Index(['iris_1', 'iris_2'], dtype='object')\r\n# Old way: Index(['iris_1', 'iris_2'], dtype='object')\r\n\r\n# Set the attribute\r\nsetattr(pd.Series, \"filter\", _filter)\r\nseries = pd.Series(list(\"imagine_more_complex_stuff_here_instead\"))\r\n\r\n# New way\r\nfunc = lambda x:x in [\"o\", \"_\"]\r\nprint(\"New way:\", series.filter(func))\r\n\r\n# Old way\r\nprint(\"Old way:\", series[series.map(func)])\r\n\r\n# New way: 7     _\r\n# 9     o\r\n# 12    _\r\n# 14    o\r\n# 20    _\r\n# 26    _\r\n# 31    _\r\n# dtype: object\r\n# Old way: 7     _\r\n# 9     o\r\n# 12    _\r\n# 14    o\r\n# 20    _\r\n# 26    _\r\n# 31    _\r\n# dtype: object\r\n```\r\n#### Problem description\r\n\r\nII always find myself writing really verbose code to filter my pandas indices or series.  I love the map method and would like to extend this concept to include `filter`."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\nCurrently, in https://github.com/pandas-dev/pandas/blob/master/pandas/core/reshape/merge.py\r\n```python\r\nelif validate in [\"many_to_many\", \"m:m\"]:\r\n            pass\r\n\r\n```\r\n#### Problem description\r\n\r\nI was expecting m:m to give me a warning/error if either the left or right merge keys are unique.\r\nIsn't that the purpose of specifying m:m? That is to ensure it is really a many to many merge.\r\n\r\n**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!\r\n\r\n**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.\r\n\r\nFor documentation-related issues, you can check the latest versions of the docs on `master` here:\r\n\r\nhttps://pandas-docs.github.io/pandas-docs-travis/\r\n\r\nIf the issue has not been resolved there, go ahead and file it in the issue tracker.\r\n\r\n#### Expected Output\r\n\r\nExpected an Error that keys are unique in either left/right/both and it is not a many-to-many merge when validate=m:m is specified.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.24.2\r\npytest: 4.3.1\r\npip: 19.0.3\r\nsetuptools: 40.8.0\r\nCython: 0.29.6\r\nnumpy: 1.16.2\r\nscipy: 1.2.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 7.4.0\r\nsphinx: 1.8.5\r\npatsy: 0.5.1\r\ndateutil: 2.8.0\r\npytz: 2018.9\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.5.1\r\nnumexpr: 2.6.9\r\nfeather: None\r\nmatplotlib: 3.0.3\r\nopenpyxl: 2.6.1\r\nxlrd: 1.2.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.1.5\r\nlxml.etree: 4.3.2\r\nbs4: 4.7.1\r\nhtml5lib: 1.0.1\r\nsqlalchemy: 1.3.1\r\npymysql: None\r\npsycopg2: 2.8.3 (dt dec pq3 ext lo64)\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\ngcsfs: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [10, 20, 30, 40]},\r\n            index=pd.MultiIndex.from_product([['a', 'b'], ['c', 'd']],\r\n                                                names=['ab', 'cd']))\r\n        df\r\nOut[2]:\r\n       x   y\r\nab cd\r\na  c   1  10\r\n   d   2  20\r\nb  c   3  30\r\n   d   4  40\r\n\r\nIn [3]: for it in df.itertuples():\r\n               print(it)\r\n\r\nPandas(Index=('a', 'c'), x=1, y=10)\r\nPandas(Index=('a', 'd'), x=2, y=20)\r\nPandas(Index=('b', 'c'), x=3, y=30)\r\nPandas(Index=('b', 'd'), x=4, y=40)\r\n```\r\n#### Problem description\r\nWhen iterating through a `DataFrame`, the names of the `Index` are lost.\r\n\r\nIt would be really convenient if when a `MultiIndex` is used, the names of the `MultiIndex` were included in the result of `itertuples()`.\r\n\r\nPropose to add named argument to `itertuples()` called `nameIndex` with default value `False` to retain current behavior, and `nameIndex=True` causing output as shown below.\r\n\r\n#### Expected Output\r\n```\r\nPandas(Index=Index(ab='a', cd='c'), x=1, y=10)\r\nPandas(Index=Index(ab='a', cd='d'), x=2, y=20)\r\nPandas(Index=Index(ab='b', cd='c'), x=3, y=30)\r\nPandas(Index=Index(ab='b', cd='d'), x=4, y=40)\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : b57d523b3006445170c865a6f5c8a5cde43e64f8\r\npython           : 3.7.3.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : None.None\r\n\r\npandas           : 0.25.0rc0+62.gb57d523b3\r\nnumpy            : 1.16.4\r\npytz             : 2019.1\r\ndateutil         : 2.8.0\r\npip              : 19.1.1\r\nsetuptools       : 41.0.1\r\nCython           : 0.29.11\r\npytest           : 5.0.0\r\nhypothesis       : 4.23.6\r\nsphinx           : 1.8.5\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 1.1.8\r\nlxml.etree       : 4.3.4\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.1\r\nIPython          : 7.6.1\r\npandas_datareader: None\r\nbs4              : 4.7.1\r\nbottleneck       : 1.2.1\r\nfastparquet      : 0.3.0\r\ngcsfs            : None\r\nlxml.etree       : 4.3.4\r\nmatplotlib       : 3.1.0\r\nnumexpr          : 2.6.9\r\nodfpy            : None\r\nopenpyxl         : 2.6.2\r\npandas_gbq       : None\r\npyarrow          : 0.11.1\r\npytables         : None\r\ns3fs             : 0.2.1\r\nscipy            : 1.2.1\r\nsqlalchemy       : 1.3.5\r\ntables           : 3.5.2\r\nxarray           : 0.12.1\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : 1.1.8\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"Could pandas include as a submodule \"SSPipe\" Package?\r\n\r\nhttps://pypi.org/project/sspipe/\r\nhttps://sspipe.github.io/\r\n\r\nDataFrame object already has a .pipe() method, but the objective is to make the \"pipe\" (Equivalent to R's magrittr %>% functional pipe) a standard that can be extrapolated to all Python objects and casuistics."},{"labels":["api",null],"text":"We defined validate_fill_value for DTA/TDA/PA and I'm thinking it may be worth requiring more generally and using on `ExtensionBlock`.  In particular, `ExtensionBlock._can_hold_element` ATM unconditionally returns `True`.  _can_hold_element isn't well-documented, but my intuition as to what it _should_ mean more or less matches what _validate_fill_value means."},{"labels":["api",null],"text":"I propose adding a ``MultiIndex._data`` that is of type ``List[Categorical]``, where all the underlying data of a MultiIndex would be stored. A ``multiIndex.array`` property would also be added, that accesses the ``_data``.\r\n\r\nThis has the advantage of collecting the data that is underlying MultiIndex into one data structure, that is human readable, and also makes access to zero-copy data very easy, e.g. would ``mi.array[1]`` return the data of the second level as a ``Categorical``, in a easy-to-read form. \r\n\r\nA ``MultiIndex`` could with the above changes be explained as just \"a container over a list of Categoricals\", which is easier to explain than the current mode. The ``MultiIndex`` could also be related to ``CategoricalIndex``, which is \"a container over a single Categorical\".\r\n\r\nThis change means that ``MultiIndex.levels`` will become a property that returns a ``FrozenList(cat.categories for cat in self._data)``, and  ``MultiIndex.codes`` will be a property that returns ``FrozenList(cat.codes for cat in self._data)``.\r\n\r\n``MultiIndex.array`` will be added and will simply be a property that returns a FrozenList of ``self._data``.\r\n\r\nPerformance will not be affected, as most operations would still go through ``MultiIndex.codes`` and ``MultiIndex.levels``.\r\n\r\n## Moving names from MultiIndex.levels to MultiIndex._names\r\n\r\nCurrently the levels' names are stored at each level's ``name`` attribute. This is not very compatible with extracting the categories from ``_data``. (the ``.categories`` is actually part of the dtype, which ideally should be immutable, so we shouldn't set or change its name attribute).\r\n\r\nTo make my suggestion practically possible, the level names should be stored in ``MultiIndex._names`` instead, and ``MultiIndex.names`` will become a property that reads from/writes to ``MultiIndex._names``. I think this change simplifies the  MultiIndex a bit, as data and names are dealt with separately. This is a small backward breaking change though.\r\n\r\nSo, I suggest making two PRs:\r\n\r\n1. Separating  the names from the levels (to be included in 0.25)\r\n2. Add ``_data``, ``array`` and change ``levels`` and ``codes`` into properties.\r\n\r\n"},{"labels":["api"],"text":"We can probably move `Series.__array_ufunc__` to `core/base.py` (once it's in)\r\n\r\nThe primary difference is that `ufunc(Index, Series)` should defer to `Series` by returning NotImplemented. This may be as easy as excluding `Series` from `Index._HANDLED_TYPES` and including `Index` in `Series._HANDLED_TYPES`."},{"labels":["api",null],"text":"From https://github.com/dask/dask/issues/5021,\r\n\r\nsuppose I have three ndarrays, different dtypes, and I'd like to place them in a DataFrame. However, one of more of the arrays is 2D.\r\n\r\nRight now, I'd probably do something like\r\n\r\n```python\r\nIn [18]: a = np.random.randn(10, 2)\r\n\r\nIn [19]: b = np.random.randn(10,)\r\n\r\nIn [20]: c = np.random.randint(0, 10, (10, 3))\r\n\r\nIn [21]: df1 = pd.DataFrame(a, columns=['a_1', 'a_2'])\r\n\r\nIn [22]: df2 = pd.DataFrame(b, columns=['b'])\r\n\r\nIn [23]: df3 = pd.DataFrame(c, columns=['c_1', 'c_2', 'c_3'])\r\n\r\nIn [24]: pd.concat([df1, df2, df2], axis=1)\r\nOut[24]:\r\n        a_1       a_2         b         b\r\n0 -0.006595  0.255718 -1.085109 -1.085109\r\n1  0.568423 -0.607054 -0.380154 -0.380154\r\n2  0.589928  1.920212 -0.296742 -0.296742\r\n...\r\n```\r\n\r\nWould we want to make that easier. Something like \r\n\r\n```python\r\npd.DataFrame.from_dict({\"a\": a, \"b\": b, \"c\": c})\r\n```\r\n\r\n* The length of all the input arrays must match.\r\n* For 2-d values in the dict, the output column names will be `<key>_<I>`.\r\n\r\nThere are a few issues (non-string keys, etc.), but this may be worth doing.\r\n"},{"labels":["api",null,null],"text":"edit: see https://github.com/pandas-dev/pandas/issues/27108#issuecomment-527633246\r\n\r\n---\r\n\r\nI'd like to be able to have an index, and ensure that no operation introduces duplicates.\r\n\r\n```python\r\nidx = pd.Index(..., allow_duplicates=False)\r\ns = pd.Series(..., index=idx)\r\n```\r\n\r\nFrom here, any pandas operation that introduces duplicates (e.g. `s.loc[['a', 'a']]`) would raise, rather than return an Index with two values."},{"labels":["api",null,null],"text":"We don't have a good method of testing for equality between EA's\r\n\r\n```\r\nIn [19]: from pandas.tests.extension.decimal.array import DecimalArray                                                                                                                                                                                           \r\n\r\nIn [20]: from decimal import Decimal                                                                                                                                                                                                                             \r\n\r\nIn [21]: x = DecimalArray([Decimal('1'),Decimal('Nan')])                                                                                                                                                                                                         \r\n\r\nIn [22]: x == x                                                                                                                                                                                                                                                  \r\nOut[22]: array([ True, False])\r\n\r\nIn [23]: x = pd.Series([1,np.nan], dtype='Int64').array                                                                                                                                                                                                          \r\n\r\nIn [24]: x == x                                                                                                                                                                                                                                                  \r\nOut[24]: array([ True, False])\r\n\r\n```\r\n\r\nThese *happen* to work with Series now because the null checks are handled at a higher level.\r\n```\r\nIn [26]: x.equals(x)                                                                                                                                                                                                                                             \r\nOut[26]: True\r\n\r\nIn [27]: x = pd.Series(DecimalArray([Decimal('1'),Decimal('Nan')]))                                                                                                                                                                                              \r\n\r\nIn [28]: x.equals(x)                                                                                                                                                                                                                                             \r\nOut[28]: True\r\n```\r\nwe could provide a default implementation that should just work in the presence of NA.\r\n\r\n```\r\ndef equals(self, other):\r\n    return ((self == other) | (self.isna() == other.isna())).all()\r\n```\r\n\r\nactually we should *also* implement a default ``__eq__`` "},{"labels":["api",null],"text":"I've searched the documentation and old issues and I can't find anything on the reason for this unusual choice. The original historic PR was #2922. \r\n\r\nOne user raised the question before  in #14900 and aggressively shut down by jeff without an answer.\r\n\r\nIt has caused bugs in my code in the past, and I've seen it do the same for other people, \r\nIt is the cause of subtle bugs seen in the [wild](https://gist.github.com/betatim/c59039682d92fab89859358e8c585313), https://github.com/pandas-dev/pandas/issues/26959#issuecomment-504456480.  It's inconsistency with python slice conventions is confusing to newbies who ask on [SO](https://stackoverflow.com/questions/55187559/why-is-loc-slicing-in-pandas-inclusive-of-stop-contrary-to-typical-python-slic) but no explanation is given. \r\n\r\nEvery [pandas tutorial](https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c) has to mention this special case:\r\n```\r\n.loc includes the last value with slice notation. In other data containers such as Python lists, the last value is excluded.\r\n```\r\n\r\nusers often need to slice something with `closed='left'` behavior, and try [to add it](https://github.com/pandas-dev/pandas/issues/12398) in similar situations where it isn't available.\r\n\r\nThis behavior is fully documented. That's not the problem. For example,\r\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html says\r\n\r\n```\r\nNote that contrary to usual python slices, both the start and the stop are \r\nincluded, when present in the index! See \"Slicing with labels\".).\r\n```\r\n\r\nThe \"Slicing with labels\" section documents the behavior, but gives no reason why the choice to break with python conventions was taken.\r\n\r\nAnother user who asked this question was given a workaround which is much more cumbersome compared to the convenience of `.loc` in https://github.com/pandas-dev/pandas/issues/16571#issuecomment-305599270.\r\n\r\nHe points to [DataFrame.between_time](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.between_time.html), which has kwds for requesting this behavior, but infuriatingly, accepts only times and not datetimes.\r\n\r\nA  few related cookbook entries were added \r\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/cookbook.html#dataframes\r\nwhich explains\r\n```\r\nThere are 2 explicit slicing methods, with a third general case\r\n\r\n    Positional-oriented (Python slicing style : exclusive of end)\r\n    Label-oriented (Non-Python slicing style : inclusive of end)\r\n```\r\nthis again documents how `loc` works, but again offers no reason why pandas originally broke with python conventions. \r\n\r\nIn every case I've seen someone ask \"why is label-based slicing right-inclusive?\" the answer has always been \"because it's label based, not position based\", which doesn't really explain anything.\r\n\r\nThe same issue exists with string based partial time slicing such like `df.loc[:\"2018\"]`,\r\nwhich will include rows with `2018-01-01` prefix.\r\n\r\nSo, over the years several people have found this undesirable and/or have tried to find out why, but with an hour's worth of gooling and reading, I can't find an explanation ever have been given.\r\n\r\nI'm not saying there's no good reason, I understand that indexing can get complicated, and mixed cases are tricky, etc'. But I want to understand why this was necessary or desirable, and why making `.loc` pythonic was unfeasible.\r\n\r\nI'll be opening a very small POC PR for discussion in a few minutes, which adds a new indexer called `locs`. <del>It is far from complete, but it seems to do exactly what I want for the single-index case, in what I've tried so far.</del> it passes all the equivalent tests loc does.\r\n\r\nTo summarize:\r\n- it's been asked before, but not answered.\r\n- the behavior is documented, but is surprising and never explained.\r\n- It's not immediately obvious why it's impossible to implement the pythonic version.\r\n\r\nSo I ask, why is `.loc` right-inclusive?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"},{"labels":["api",null],"text":"Categorical.copy passes `values=self._codes.copy()`, and doesn't have a `deep=False` kwarg (which the base ExtensionArray does).  This is causing trouble in #26954.  Is this intentional @TomAugspurger?\r\n\r\nPandas-internal EAs with `copy` behavior not respecting `deep` kwarg:\r\n- [ ] Categorical (no kwarg at all)\r\n- [ ] DatetimeLike (ignores kwarg)\r\n- [ ] PandasArray\r\n\r\nIntegerArray, IntervalArray, and SparseArray look correct.\r\n"},{"labels":["api",null,null],"text":"Hello,\r\n\r\nI noticed in doc https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.strftime.html that `DatetimeIndex` have a `dt` attribute which can help to convert a `Timestamp` to string.\r\n\r\n```python\r\n>>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\r\n...                     periods=3, freq='s')\r\n>>> rng.strftime('%B %d, %Y, %r')\r\nIndex(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\r\n       'March 10, 2018, 09:00:02 AM'],\r\n      dtype='object')\r\n```\r\n\r\n(or `rng.to_series().dt.strftime('%B %d, %Y, %r')`)\r\n\r\nA similar method should exist for `TimedeltaIndex` to help to convert it to string (it's especially useful to output an Excel files without \"0 days\" for each index when each timedelta is below 1 day)\r\n\r\n```python\r\n>>> rng = pd.timedelta_range(start=\"0h\", freq=\"15min\", periods=3)\r\nTimedeltaIndex(['00:00:00', '00:15:00', '00:30:00'], dtype='timedelta64[ns]', freq='15T')\r\n>>> rng.dt.strftime(\"%M:%S\")\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-48-1f408f75ecf6> in <module>\r\n----> 1 rng.dt.strftime(\"%m:%s\")\r\n\r\nAttributeError: 'TimedeltaIndex' object has no attribute 'dt'\r\n\r\n```\r\n\r\nI have found a bad method to do this:\r\n\r\n```python\r\nrng.map(lambda td: time.strftime(\"%M:%S\", time.gmtime((td.to_pytimedelta().total_seconds()))))\r\n```\r\n\r\nunfortunatelly it doesn't support the \"f\" specifier for microsecond (I only need in fact millisecond)\r\n\r\n```python\r\n>>> rng = pd.to_timedelta([\"00:01:02.345\", \"00:02:03.456\"])\r\nTimedeltaIndex(['00:01:02.345000', '00:02:03.456000'], dtype='timedelta64[ns]', freq=None)\r\n>>> rng.map(lambda td: time.strftime(\"%M:%S.%f\", time.gmtime((td.to_pytimedelta().total_seconds()))))\r\nIndex(['01:02.f', '02:03.f'], dtype='object')\r\n```\r\n\r\nit could be nice to simply be able to do\r\n\r\n```python\r\nrng.to_series().dt.strftime(\"%M:%S.%f\")\r\n```\r\n\r\nor naming attribute `td` (instead of `dt`) to avoid confusion between `DatetimeIndex` and `TimedeltaIndex`\r\n\r\n```python\r\nrng.to_series().td.strftime(\"%M:%S.%f\")\r\n```\r\n\r\nor even simplier\r\n\r\n```python\r\nrng.strftime(\"%M:%S.%f\")\r\n```\r\n\r\nI think a workaround to achieve this is (with positive `Timedelta`) to add epoch and so manage `Timestamp` instead.\r\n\r\n```python\r\n>>> rng = pd.to_timedelta([\"00:01:02.345\", \"00:02:03.456\"])\r\nTimedeltaIndex(['00:01:02.345000', '00:02:03.456000'], dtype='timedelta64[ns]', freq=None)\r\n>>> rng = rng + pd.to_datetime(0)\r\nDatetimeIndex(['1970-01-01 00:01:02.345000', '1970-01-01 00:02:03.456000'], dtype='datetime64[ns]', freq=None)\r\nrng = rng.strftime(\"%H:%M.%f\")\r\nIndex(['00:01.345000', '00:02.456000'], dtype='object')\r\n>>> rng.str[:-3]\r\nIndex(['00:01.345', '00:02.456'], dtype='object')\r\n```\r\n\r\nBut I'm still wondering how to manage negative `Timedelta` such as\r\n\r\n```python\r\n>>> rng = pd.to_timedelta([\"00:01:02.345\", \"-00:02:03.456\"])\r\nTimedeltaIndex(['00:01:02.345000', '-1 days +23:57:56.544000'], dtype='timedelta64[ns]', freq=None)\r\n```\r\n\r\nAny idea?\r\n\r\nKind regards\r\n\r\n\r\nPS : related issue https://github.com/pandas-dev/pandas/issues/17232 (about display of negative Timedelta)"},{"labels":["api"],"text":"```python\r\nIn [19]: import pandas as pd\r\n    ...: s=pd.Series([1,2])\r\n    ...: s.floor() \r\nAttributeError: 'Series' object has no attribute 'floor'\r\n```"},{"labels":["api"],"text":"Overlaps with builtin `filter`.\r\n\r\nCould use e.g. https://pypi.org/project/flake8-builtins/ to check for other cases like this, xref #22122"},{"labels":["api",null],"text":"#### Code Sample\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport scipy.io\r\n\r\n# Two lists\r\nl1 = [1,2]\r\nl2 = [4,5]\r\n# Numpy arrays\r\na1 = np.array(l1)\r\na2 = np.array(l2)\r\n# Dictionary of arrays\r\nd = {'a':a1,'b':a2}\r\n\r\n# Constructing DataFrame from dictionary\r\ndf = pd.DataFrame.from_dict(d,orient='index')\r\nprint(df)\r\n```\r\nThe above code gives,\r\n\r\n```python\r\n 0  1\r\na  1  2\r\nb  4  5\r\n```\r\nIf we change the length of the lists,\r\n\r\n```python\r\n# Two lists of unequal length\r\nl1 = [1,2]\r\nl2 = [4,5,6]\r\n<rest code same as above>\r\n```\r\nThen we get,\r\n```python\r\n   0  1    2\r\na  1  2  NaN\r\nb  4  5  6.0\r\n```\r\nBut now if we want to reshape the numpy arrays (for whatever processing needs)\r\n```python\r\n# Two lists of unequal length\r\nl1 = [1,2]\r\nl2 = [4,5,6]\r\n\r\n# Numpy arrays with reshape\r\na1 = np.array(l1).reshape(1,-1)\r\na2 = np.array(l2).reshape(1,-1)\r\n\r\n# Dictionary of arrays\r\nd = {'a':a1,'b':a2}\r\n\r\n# Constructing DataFrame from dictionary\r\ndf = pd.DataFrame.from_dict(d,orient='index')\r\nprint(df)\r\n```\r\nHere we get,\r\n```python\r\n             0\r\na     [[1, 2]]\r\nb  [[4, 5, 6]]\r\n```\r\n**This is ACCEPTABLE.**\r\nBut check what happens if the list lengths are equal,\r\n```python\r\n# Two lists of equal length\r\nl1 = [1,2]\r\nl2 = [4,5]\r\n\r\n# Numpy arrays with reshape\r\na1 = np.array(l1).reshape(1,-1)\r\na2 = np.array(l2).reshape(1,-1)\r\n\r\n# Dictionary of arrays\r\nd = {'a':a1,'b':a2}\r\n\r\n# Constructing DataFrame from dictionary\r\ndf = pd.DataFrame.from_dict(d,orient='index')\r\n```\r\nWe get the error,\r\n\r\n```python\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-49-b17a697beafc> in <module>()\r\n      1 # Constructing DataFrame from dictionary\r\n----> 2 df = pd.DataFrame.from_dict(d,orient='index')\r\n\r\nc:\\programdata\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py in from_dict(cls, data, orient, dtype)\r\n    898             raise ValueError('only recognize index or columns for orient')\r\n    899 \r\n--> 900         return cls(data, index=index, columns=columns, dtype=dtype)\r\n    901 \r\n    902     def to_dict(self, orient='dict', into=dict):\r\n\r\nc:\\programdata\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py in __init__(self, data, index, columns, dtype, copy)\r\n    383                 else:\r\n    384                     mgr = self._init_ndarray(data, index, columns, dtype=dtype,\r\n--> 385                                              copy=copy)\r\n    386             else:\r\n    387                 mgr = self._init_dict({}, index, columns, dtype=dtype)\r\n\r\nc:\\programdata\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py in _init_ndarray(self, values, index, columns, dtype, copy)\r\n    511         # by definition an array here\r\n    512         # the dtypes will be coerced to a single dtype\r\n--> 513         values = _prep_ndarray(values, copy=copy)\r\n    514 \r\n    515         if dtype is not None:\r\n\r\nc:\\programdata\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py in _prep_ndarray(values, copy)\r\n   6255         values = values.reshape((values.shape[0], 1))\r\n   6256     elif values.ndim != 2:\r\n-> 6257         raise ValueError('Must pass 2-d input')\r\n   6258 \r\n   6259     return values\r\n\r\nValueError: Must pass 2-d input\r\n```\r\n\r\n#### Problem description\r\nThe **`DataFrame.from_dict()` does not work properly when the dictionary contains Numpy arrays as values which have been reshaped**. If the arrays are of unequal length, the DataFrame is created (with a single column) with the arrays as an object. But if they are equal, it throws an unexpected error.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"Triggered by the regression reported in https://github.com/pandas-dev/pandas/issues/26206  and my attempt to fix it (https://github.com/pandas-dev/pandas/pull/26848), I looked a bit into how our different constructors handle different cases of invalid datetimes.\r\n\r\nOut of bound datetimes (cases: a non-ns unit numpy datetime64 array, datetime.datetime objects, strings):\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th></th>\r\n      <th>M8[D] out of bound</th>\r\n      <th>datetime.datetime out of bound</th>\r\n      <th>list of M8[D] scalars out of bound</th>\r\n      <th>string out of bound</th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>dateframe</th>\r\n      <td>datetime64[ns] 1842-11-22 0 ...</td>\r\n      <td>object</td>\r\n      <td>object</td>\r\n      <td>object</td>\r\n    </tr>\r\n    <tr>\r\n      <th>datetimeindex</th>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>OutOfBoundsDatetime</td>\r\n    </tr>\r\n    <tr>\r\n      <th>index</th>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>object</td>\r\n      <td>object</td>\r\n      <td>object</td>\r\n    </tr>\r\n    <tr>\r\n      <th>index (explicit dtype)</th>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>OutOfBoundsDatetime</td>\r\n    </tr>\r\n    <tr>\r\n      <th>pd.to_datetime</th>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>OutOfBoundsDatetime</td>\r\n      <td>OutOfBoundsDatetime</td>\r\n    </tr>\r\n    <tr>\r\n      <th>series</th>\r\n      <td>datetime64[ns] 1842-11-22 0 ...</td>\r\n      <td>object</td>\r\n      <td>object</td>\r\n      <td>object</td>\r\n    </tr>\r\n    <tr>\r\n      <th>series (explicit dtype)</th>\r\n      <td>datetime64[ns] 1842-11-22 0 ...</td>\r\n      <td>datetime64[ns] 1842-11-22 0 ...</td>\r\n      <td>datetime64[ns] 1842-11-22 0 ...</td>\r\n      <td>datetime64[ns] 1842-11-22 0 ...</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\nSome remarks here:\r\n\r\n* The above table is for master (and the same for 0.24). However, on pandas 0.22, the case of out of bounds datetime64 array raised an error in all cases (so also for Series(..), that's https://github.com/pandas-dev/pandas/issues/26206).\r\n* For `to_datetime`, all the OutOfBounds are expected, as the default is to error.\r\n* For the ones with a specified dtype (either explicit dtype passed or DatetimeIndex), we also expect an error. So DatetimeIndex and Index(.., dtype='M8[ns]') are fine, but `Series(.., dtype='M8[ns])` is clearly broken (gives an incorrect date for all cases)\r\n* The inconsistencies are mainly when no dtype is enforced. In some cases (eg for datetime.datetime objects) we return a object dtype Index/Series, in other cases we raise an error. \r\n\r\nThe last item is the **main question**: when inferring (no dtype enforced) and in case the data are clearly timestamps (either numpy datetime64 or datetime.datetime), should we raise an error or return object dtype.\r\n\r\nNote that for datetime.datetime, returning object dtype might be more logical as it means returning the input as is. While for datetime64[non-ns], it actually means converting a numpy dtype to object data.\r\n\r\n"},{"labels":["api",null,null,null],"text":"In #26414 we splitted the pandas plotting module into a general plotting framework able to call different backends and the current matplotlib backends. The idea is that other backends can be implemented in a simpler way, and be used with a common API by pandas users.\r\n\r\nThe API defined by the current matplotlib backend includes the objects listed next, but this API can probably be simplified. Here is the list with questions/proposals:\r\n\r\nNon-controversial methods to keep in the API (They provide the `Series.plot(kind='line')`... functionality):\r\n- LinePlot\r\n- BarPlot\r\n- BarhPlot\r\n- HistPlot\r\n- BoxPlot\r\n- KdePlot\r\n- AreaPlot\r\n- PiePlot\r\n- ScatterPlot\r\n- HexBinPlot\r\n\r\nPlotting functions provided in pandas (e.g. `pandas.plotting.andrews_curves(df)`)\r\n- andrews_curves\r\n- autocorrelation_plot\r\n- bootstrap_plot\r\n- lag_plot\r\n- parallel_coordinates\r\n- radviz\r\n- scatter_matrix\r\n- table\r\n\r\nShould those be part of the API and other backends should also implement them? Would it make sense to convert to the format `.plot` (e.g. `DataFrame.plot(kind='autocorrelation')`...)? Does it make sense to keep out of the API, or move to a third-party module?\r\n\r\nRedundant methods that can possibly be removed:\r\n- hist_series\r\n- hist_frame\r\n- boxplot\r\n- boxplot_frame\r\n- boxplot_frame_groupby\r\n\r\nIn the case of `boxplot`, we currently have several ways of generating a plot (calling mainly the same code):\r\n1. `DataFrame.plot.boxplot()`\r\n2. `DataFrame.plot(kind='box')`\r\n3. `DataFrame.boxplot()`\r\n4. `pandas.plotting.boxplot(df)`\r\n\r\nPersonally, I'd deprecate number 4, and for number 3, deprecate or at least not require a separate `boxplot_frame` method in the backend, but try to reuse `BoxPlot` (for number 3 comments, same applies to `hist`).\r\n\r\nFor `boxplot_frame_groupby`, didn't check in detail, but not sure if `BoxPlot` could be reused for this?\r\n\r\nFunctions to register converters:\r\n- register\r\n- deregister\r\n\r\nDo those make sense for other backends? \r\n\r\nDeprecated in pandas 0.23, to be removed:\r\n- tsplot\r\n\r\nTo see what each of these functions do in practise, it may be useful this notebook by @liirusuk: https://github.com/python-sprints/pandas_plotting_library/blob/master/AllPlottingExamples.ipynb\r\n\r\nCC: @pandas-dev/pandas-core @tacaswell, @jakevdp, @philippjfr, @PatrikHlobil"},{"labels":["api",null,null,null],"text":"I'd like to be able to _prepend_ indices to existing ones using `.set_index()`, in the same vein as the `append` argument.\r\n\r\nThis avoid having to do the awkward alternatives:\r\n- `.reset_index().set_index(...)`\r\n- `.set_index(append=True).swaplevel(...)` or `.set_index(append=True).reorder_levels(...)`\r\n\r\nMy first suggestion is to add a `prepend` keyword, mutually exclusive with `append`, but discussion is welcomed.\r\n\r\nAssociated PR: https://github.com/pandas-dev/pandas/pull/26724"},{"labels":["api",null],"text":"Currently, to extend pandas `Series`, `DataFrame` and `Index` with user-defined methods, we use accessors in the next way:\r\n\r\n```python\r\n@pandas.api.extensions.register_series_accessor('emoji')\r\nclass Emoji:\r\n    def __init__(self, data):\r\n        self.data = data\r\n\r\n    def is_monkey(self):\r\n        \"\"\"\r\n        This would create `Series().emoji.is_monkey`\r\n        \"\"\"\r\n        return self.data.isin(['🙈', '🙉', '🙊'])\r\n```\r\n\r\nWhile this works well, I think there are two problems with this approach:\r\n- The API looks somehow intimidating, and it's not well known. I think because `pandas.api.extensions.register_series_accessor` is too long and lives in `pandas.api`, separate of functionality most users know.\r\n- It's not possible to register methods directly (`Series().is_monkey` instead of `Series().emoji.is_monkey`)\r\n\r\nI think all the projects extending pandas I've seen, simply \"inject\" the methods (except the ones implemented by pandas maintainers). For example:\r\n- https://github.com/PatrikHlobil/Pandas-Bokeh/blob/master/pandas_bokeh/__init__.py#L20\r\n- https://github.com/nalepae/pandarallel/blob/master/pandarallel/pandarallel.py#L52\r\n\r\nWhat I propose is to have a easier/simpler API for the user. To be specific, this is the syntax I'd like when extending `Series`...\r\n\r\n```python\r\nimport pandas\r\n\r\n@pandas.Series.extend('emoji')\r\nclass Emoji:\r\n    def __init__(self, data):\r\n        self.data = data\r\n\r\n    def is_monkey(self):\r\n        \"\"\"\r\n        This would create `Series().emoji.is_monkey`\r\n        \"\"\"\r\n        return self.data.isin(['🙈', '🙉', '🙊'])\r\n\r\n@pandas.Series.extend(namespace='emoji')\r\ndef is_monkey(data):\r\n    \"\"\"\r\n    This would also create `Series().emoji.is_monkey`\r\n    \"\"\"\r\n    return data.isin(['🙈', '🙉', '🙊'])\r\n\r\n@pandas.Series.extend\r\nclass Emoji:\r\n    def __init__(self, data):\r\n        self.data = data\r\n\r\n    def is_monkey(self):\r\n        \"\"\"\r\n        This would directly create `Series().is_monkey`\r\n        \"\"\"\r\n        return self.data.isin(['🙈', '🙉', '🙊'])\r\n\r\n@pandas.Series.extend\r\ndef is_monkey(data):\r\n    \"\"\"\r\n    This would create `Series().emoji.is_monkey`\r\n    \"\"\"\r\n    return data.isin(['🙈', '🙉', '🙊'])\r\n```\r\n\r\nThis would make things much easier for the user, because:\r\n- The name `pandas.Series.extend` is much easier to remember\r\n- A single function can be used (without creating a class)\r\n- A direct method of `Series`... can be created\r\n\r\nCC: @pandas-dev/pandas-core "},{"labels":["api",null,null],"text":"Is there is any standard method to replace flat columns with hierarchical while merge the dataframes?\r\n\r\nI mean something like:\r\n\r\n\r\n```python\r\ndef columns_as_multiindex(df, inplace=False):\r\n    \"\"\"Replace flat (:obj:`pandas.Index`) columns with hierarchical (:obj:`pandas.MultiIndex`).\r\n\r\n    Parameters\r\n    ----------\r\n    df : pandas.DataFrame\r\n        Dataframe wich contains the columns to be converted to MultiIndex.\r\n    inplace : bool (default False)\r\n        If ``True``, do operation inplace and return ``None``.\r\n\r\n    Returns\r\n    -------\r\n    pandas.DataFrame or None\r\n        Returns dataframe with modifed columns or ``None`` (depends on `inplace` parameter value).\r\n\r\n    Examples\r\n    --------\r\n    >>> df0 = pd.DataFrame([[1,2], [10, 20], [100, 200]], columns=[\"A\", \"B\"])\r\n    >>> df0\r\n    ...      A    B\r\n    ... 0    1    2\r\n    ... 1   10   20\r\n    ... 2  100  200\r\n\r\n    >>> columns = pd.MultiIndex.from_tuples([(\"C\", \"C0\"), (\"D\", \"D0\")])\r\n    >>> df1 = pd.DataFrame([[1,3], [10, 30], [100, 300]], columns=columns)\r\n    >>> df1\r\n    ...      C    D\r\n    ...     C0   D0\r\n    ... 0    1    3\r\n    ... 1   10   30\r\n    ... 2  100  300\r\n\r\n    Merging ``df0`` with ``df1`` returns the dataframe contains tuples in the column names\r\n\r\n    >>> df = pd.merge(df0, df1, left_on=\"A\", right_on=[(\"C\", \"C0\")])\r\n    >>> df\r\n    ...      A    B  (C, C0)  (D, D0)\r\n    ... 0    1    2        1        3\r\n    ... 1   10   20       10       30\r\n    ... 2  100  200      100      300\r\n\r\n    It's a bit ugly. Let's beautify the header\r\n\r\n    >>> columns_as_multiindex(df)\r\n    >>> df\r\n    ...      A    B    C    D\r\n    ...               C0   D0\r\n    ... 0    1    2    1    3\r\n    ... 1   10   20   10   30\r\n    ... 2  100  200  100  300\r\n    \"\"\"\r\n    columns = df.columns.tolist()\r\n    columns = [(c,) if isinstance(c, str) or not np.iterable(c) else c for c in columns]\r\n    max_len = max([len(c) for c in columns])\r\n    columns = [tuple(c + (\"\",) * (max_len - len(c))) for c in columns]\r\n    columns = pd.MultiIndex.from_tuples(columns)\r\n    if inplace:\r\n        df.columns = columns\r\n    else:\r\n        df_new = df.copy()\r\n        df_new.columns = columns\r\n        return df_new\r\n```\r\n\r\n**UPD 2019-06-13** A bit more robust function on input data.\r\n"},{"labels":["api"],"text":"I've made a PR to remove ``NDFrame.select``, see #26641. That method was deprecated in 0.21.\r\n\r\nThere was a discussion in #12401, where many participants agreed that removing the old ``select`` and having ``NDFrame.filter`` renamed to ``NDFrame.select`` would be a better naming scheme. I agree on that for several of the reasons mentioned in the thread.\r\n\r\nWhat do people think of such a name change and a related deprecation of the name (not functionality) ``.filter``?\r\n"},{"labels":["api",null],"text":"#### Code Sample\r\n```python\r\ndf = pd.DataFrame(np.random.rand(4,4))\r\n\r\nwhere = ''\r\nwith pd.HDFStore('test.h5') as store:\r\n    store.put('df', df, 't')\r\n    store.select('df', where = where)\r\n```\r\n#### Problem description\r\nWanted to be able construct \"by hands\" and save `where` condition for later, so declare it as variable. But some times constructed `where` becomes empty and code throws an error. \r\n\r\n```python-traceback\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n\r\n  File \"<ipython-input-101-48181c3b59fb>\", line 6, in <module>\r\n    store.select('df', where = where)\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/io/pytables.py\", line 740, in select\r\n    return it.get_result()\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/io/pytables.py\", line 1518, in get_result\r\n    results = self.func(self.start, self.stop, where)\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/io/pytables.py\", line 733, in func\r\n    columns=columns)\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/io/pytables.py\", line 4254, in read\r\n    if not self.read_axes(where=where, **kwargs):\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/io/pytables.py\", line 3443, in read_axes\r\n    self.selection = Selection(self, where=where, **kwargs)\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/io/pytables.py\", line 4815, in __init__\r\n    self.terms = self.generate(where)\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/io/pytables.py\", line 4828, in generate\r\n    return Expr(where, queryables=q, encoding=self.table.encoding)\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/core/computation/pytables.py\", line 548, in __init__\r\n    self.terms = self.parse()\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/core/computation/expr.py\", line 766, in parse\r\n    return self._visitor.visit(self.expr)\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/core/computation/expr.py\", line 331, in visit\r\n    return visitor(node, **kwargs)\r\n\r\n  File \"/home/beforeflight/Coding/Python/_venvs_/main/lib/python3.7/site-packages/pandas/core/computation/expr.py\", line 335, in visit_Module\r\n    raise SyntaxError('only a single expression is allowed')\r\n\r\n  File \"<string>\", line unknown\r\nSyntaxError: only a single expression is allowed\r\n```\r\n\r\n#### Expected Output\r\nWhen empty string is passed to `where` - just select whole DataFrame. It may be easily achieved by changing last statement to `store.select('df', where = where if where else None)`. But it would be better to add this checking  inside pandas, so user may not worry about it all the times using selection from HDF with `where`.\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 5.0.0-16-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.24.2\r\npytest: 4.5.0\r\npip: 19.1.1\r\nsetuptools: 41.0.1\r\nCython: 0.29.7\r\nnumpy: 1.16.3\r\nscipy: 1.2.1\r\npyarrow: None\r\nxarray: 0.12.1\r\nIPython: 7.2.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.8.0\r\npytz: 2019.1\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.5.1\r\nnumexpr: 2.6.9\r\nfeather: None\r\nmatplotlib: 3.0.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml.etree: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10.1\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\ngcsfs: None\r\n</details>\r\n"},{"labels":["api"],"text":"Followup to https://github.com/pandas-dev/pandas/pull/26399\r\n\r\n```python\r\nIn [2]: df = pd.DataFrame({\"A\": [1, 2, 1, 2], \"B\": [1, 2, 3, 4]})\r\n\r\nIn [3]: df\r\nOut[3]:\r\n   A  B\r\n0  1  1\r\n1  2  2\r\n2  1  3\r\n3  2  4\r\n\r\nIn [4]: df.agg(foo=(\"B\", \"sum\"))\r\n```\r\n\r\nExpected Output\r\n\r\n```python\r\nIn [13]: df.agg({\"B\": {\"foo\": \"sum\"}})\r\n/Users/taugspurger/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/frame.py:6284: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\r\n  result, how = self._aggregate(func, axis=axis, *args, **kwargs)\r\nOut[13]:\r\n      B\r\nfoo  10\r\n```\r\n\r\n\r\nwithout the warning. Similar for `Series.agg`\r\n\r\n```python\r\nIn [16]: df.B.agg({\"foo\": \"sum\"})  # allow foo=\"sum\"\r\nOut[16]:\r\nfoo    10\r\nName: B, dtype: int64\r\n```\r\n"},{"labels":["api",null],"text":"We currently don't allow duplicate function names in the list passed too `.groupby().agg({'col': [aggfuncs]})`. This is painful with multiple lambdas, which all have the name `<lambda>`\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\ndf\r\nIn [2]: df = pd.DataFrame({\"A\": ['a', 'a'], 'B': [1, 2], 'C': [3, 4]})\r\n\r\nIn [3]: df.groupby(\"A\").agg({'B': [lambda x: 0, lambda x: 1]})\r\n```\r\n\r\n```pytb\r\n---------------------------------------------------------------------------\r\nSpecificationError                        Traceback (most recent call last)\r\n~/sandbox/pandas/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs)\r\n    483                 try:\r\n--> 484                     result = _agg(arg, _agg_1dim)\r\n    485                 except SpecificationError:\r\n\r\n~/sandbox/pandas/pandas/core/base.py in _agg(arg, func)\r\n    434                 for fname, agg_how in arg.items():\r\n--> 435                     result[fname] = func(fname, agg_how)\r\n    436                 return result\r\n\r\n~/sandbox/pandas/pandas/core/base.py in _agg_1dim(name, how, subset)\r\n    417                                              \"in aggregation\")\r\n--> 418                 return colg.aggregate(how, _level=(_level or 0) + 1)\r\n    419\r\n\r\n~/sandbox/pandas/pandas/core/groupby/generic.py in aggregate(self, func_or_funcs, *args, **kwargs)\r\n    771             ret = self._aggregate_multiple_funcs(func_or_funcs,\r\n--> 772                                                  (_level or 0) + 1)\r\n    773         else:\r\n\r\n~/sandbox/pandas/pandas/core/groupby/generic.py in _aggregate_multiple_funcs(self, arg, _level)\r\n    834                     'Function names must be unique, found multiple named '\r\n--> 835                     '{}'.format(name))\r\n    836\r\n\r\nSpecificationError: Function names must be unique, found multiple named <lambda>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nSpecificationError                        Traceback (most recent call last)\r\n<ipython-input-3-2aa02bdc2edd> in <module>\r\n----> 1 df.groupby(\"A\").agg({'B': [lambda x: 0, lambda x: 1]})\r\n\r\n~/sandbox/pandas/pandas/core/groupby/generic.py in aggregate(self, arg, *args, **kwargs)\r\n   1344     @Appender(_shared_docs['aggregate'])\r\n   1345     def aggregate(self, arg=None, *args, **kwargs):\r\n-> 1346         return super().aggregate(arg, *args, **kwargs)\r\n   1347\r\n   1348     agg = aggregate\r\n\r\n~/sandbox/pandas/pandas/core/groupby/generic.py in aggregate(self, func, *args, **kwargs)\r\n    174                             \"'(column, aggfunc).\")\r\n    175\r\n--> 176         result, how = self._aggregate(func, _level=_level, *args, **kwargs)\r\n    177         if how is None:\r\n    178             return result\r\n\r\n~/sandbox/pandas/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs)\r\n    487                     # we are aggregating expecting all 1d-returns\r\n    488                     # but we have 2d\r\n--> 489                     result = _agg(arg, _agg_2dim)\r\n    490\r\n    491             # combine results\r\n\r\n~/sandbox/pandas/pandas/core/base.py in _agg(arg, func)\r\n    433                 result = OrderedDict()\r\n    434                 for fname, agg_how in arg.items():\r\n--> 435                     result[fname] = func(fname, agg_how)\r\n    436                 return result\r\n    437\r\n\r\n~/sandbox/pandas/pandas/core/base.py in _agg_2dim(name, how)\r\n    424                 colg = self._gotitem(self._selection, ndim=2,\r\n    425                                      subset=obj)\r\n--> 426                 return colg.aggregate(how, _level=None)\r\n    427\r\n    428             def _agg(arg, func):\r\n\r\n~/sandbox/pandas/pandas/core/groupby/generic.py in aggregate(self, arg, *args, **kwargs)\r\n   1344     @Appender(_shared_docs['aggregate'])\r\n   1345     def aggregate(self, arg=None, *args, **kwargs):\r\n-> 1346         return super().aggregate(arg, *args, **kwargs)\r\n   1347\r\n   1348     agg = aggregate\r\n\r\n~/sandbox/pandas/pandas/core/groupby/generic.py in aggregate(self, func, *args, **kwargs)\r\n    174                             \"'(column, aggfunc).\")\r\n    175\r\n--> 176         result, how = self._aggregate(func, _level=_level, *args, **kwargs)\r\n    177         if how is None:\r\n    178             return result\r\n\r\n~/sandbox/pandas/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs)\r\n    542             return self._aggregate_multiple_funcs(arg,\r\n    543                                                   _level=_level,\r\n--> 544                                                   _axis=_axis), None\r\n    545         else:\r\n    546             result = None\r\n\r\n~/sandbox/pandas/pandas/core/base.py in _aggregate_multiple_funcs(self, arg, _level, _axis)\r\n    588                     colg = self._gotitem(col, ndim=1,\r\n    589                                          subset=obj.iloc[:, index])\r\n--> 590                     results.append(colg.aggregate(arg))\r\n    591                     keys.append(col)\r\n    592                 except (TypeError, DataError):\r\n\r\n~/sandbox/pandas/pandas/core/groupby/generic.py in aggregate(self, func_or_funcs, *args, **kwargs)\r\n    770             # but not the class list / tuple itself.\r\n    771             ret = self._aggregate_multiple_funcs(func_or_funcs,\r\n--> 772                                                  (_level or 0) + 1)\r\n    773         else:\r\n    774             cyfunc = self._is_cython_func(func_or_funcs)\r\n\r\n~/sandbox/pandas/pandas/core/groupby/generic.py in _aggregate_multiple_funcs(self, arg, _level)\r\n    833                 raise SpecificationError(\r\n    834                     'Function names must be unique, found multiple named '\r\n--> 835                     '{}'.format(name))\r\n    836\r\n    837             # reset the cache so that we\r\n\r\nSpecificationError: Function names must be unique, found multiple named <lambda>\r\n```\r\n\r\nI propose that we mangle the names somehow\r\n\r\n```python\r\nIn [2]: df = pd.DataFrame({\"A\": ['a', 'a'], 'B': [1, 2], 'C': [3, 4]})\r\n\r\nIn [3]: df.groupby(\"A\").agg({'B': [lambda x: 0, lambda x: 1]})\r\nOut[3]:\r\n         B\r\n  <lambda> <lambda 1>\r\nA\r\na        0          1\r\n```\r\n\r\nThat adds a `1`, `2`, ... to all subsequent lambdas in the same MI level. It doesn't change the first. Do we want `<lambda 0>` for the first?\r\n\r\nAs a side-effect, this enables multiple lambdas per column with the new keyword aggregation\r\n\r\n```python\r\nIn [4]: df.groupby(\"A\").agg(b=('B', lambda x: 0), c=('B', lambda x: 1))\r\nOut[4]:\r\n   b  c\r\nA\r\na  0  0\r\n```\r\n\r\n---\r\n\r\nI have a WIP started. Will do for 0.25."},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nDF = pandas.DataFrame\r\nS = pandas.Series\r\nll = [list(range(5)) for i in range(10)]\r\nprint(DF(ll).shape)\r\nld = [{j: j for j in range(5)} for i in range(10)]\r\nprint(DF(ld).shape)\r\ndd = {i: {j: j for j in range(5)} for i in range(10)}\r\nprint(DF(dd).shape)\r\ndl = {i: list(range(5)) for i in range(10)}\r\nprint(DF(dl).shape)\r\nls = [S(list(range(5))) for i in range(10)]\r\nprint(DF(ls).shape)\r\nds = {i: S(list(range(5))) for i in range(10)}\r\nprint(DF(ds).shape)\r\nss = S([S(list(range(5))) for i in range(10)])\r\nprint(DF(ss).shape)\r\nsl = S([list(range(5)) for i in range(10)])\r\nprint(DF(sl).shape)\r\nsd = S([{j: j for j in range(5)} for i in range(10)])\r\nprint(DF(sd).shape)\r\n```\r\n#### Problem description\r\nThere are kind of two separate issues here...\r\n\r\n1. It seems strange to me that when a two dimensional object is passed to the DataFrame constructor, whether or not the object is interpreted as row by col or col by row is determined by whether or not the outer object is a dict or a list. If the outer object is a Dict, an MxN object will create an NxM dataframe. If the outer object is a list, an MxN object will create an MxN dataframe\r\n\r\n2. It seems strange to me that the DataFrame constructor works well with lists of series and dicts of series but doesn't work with series of dicts, series of lists, or series of series.\r\n\r\n#### Expected Output\r\nNaively, I would expect the resultant DataFrame to be structured in the same way regardless of whether the object passed to the DataFrame constructor is a dict of lists, a dict of dicts, a list of lists, etc... All of the above calls to pandas.DataFrame should return equivalent dataframes IMO.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.8.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 142 Stepping 10, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.24.2\r\npytest: None\r\npip: 19.0.3\r\nsetuptools: 41.0.0\r\nCython: None\r\nnumpy: 1.16.3\r\nscipy: 1.2.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 7.4.0\r\nsphinx: None\r\npatsy: 0.5.1\r\ndateutil: 2.8.0\r\npytz: 2019.1\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: 2.6.2\r\nxlrd: 1.2.0\r\nxlwt: None\r\nxlsxwriter: 1.1.7\r\nlxml.etree: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\ngcsfs: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"Assume we have a Categorical, and want to convert to a dense array (not encoded). We have `np.asarray(..)` and the `to_dense()` method (which uses asarray under the hood):\r\n\r\n```\r\nIn [1]: cat = pd.Categorical(['a', 'b', 'a'])\r\n\r\nIn [2]: np.asarray(cat) \r\nOut[2]: array(['a', 'b', 'a'], dtype=object)\r\n\r\nIn [3]: cat.to_dense() \r\nOut[3]: array(['a', 'b', 'a'], dtype=object)\r\n```\r\n\r\nIn addition, we also have `get_values`:\r\n```\r\nIn [4]: cat.get_values() \r\nOut[4]: array(['a', 'b', 'a'], dtype=object)\r\n```\r\n\r\n`get_values` is mostly the same, with the exception that returns an Index for datetime/period/timedelta, and an object array for integers if there are missing values instead of float array:\r\n\r\n```\r\nIn [10]: cat = pd.Categorical(pd.date_range(\"2012\", periods=3))\r\n\r\nIn [11]: cat.to_dense()\r\nOut[11]: \r\narray(['2012-01-01T00:00:00.000000000', '2012-01-02T00:00:00.000000000',\r\n       '2012-01-03T00:00:00.000000000'], dtype='datetime64[ns]')\r\n\r\nIn [12]: cat.get_values()\r\nOut[12]: DatetimeIndex(['2012-01-01', '2012-01-02', '2012-01-03'], dtype='datetime64[ns]', freq='D')\r\n```\r\n\r\nWith the result that it preserves somewhat more the dtype (although only specifically for datetime-like, it will not do it for any EA)\r\n\r\nWhile looking into the deprecation of `get_values` (https://github.com/pandas-dev/pandas/pull/26409), I was wondering: do we want some method to actually get a \"dense\" version of the array, but with the exact same dtype? (so returning an EA in case the categories have an extension dtype)\r\n\r\nAnd should we deprecate `to_dense()` ?\r\n\r\n"},{"labels":["api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [1]: class Test:\r\n   ...:     def __ror__(self, other):\r\n   ...:         print('here')\r\n   ...:         return self\r\n   ...: \r\n\r\nIn [2]: {} | Test()\r\nhere\r\nOut[2]: <__main__.Test at 0x7f37a1601ba8>\r\n\r\nIn [3]: import pandas\r\n\r\nIn [4]: pandas.DataFrame() | Test()\r\nOut[4]: \r\nEmpty DataFrame\r\nColumns: []\r\nIndex: []\r\n```\r\n#### Problem description\r\n\r\nI would expect DataFrame to return `NotImplemented` in this case, so we can properly delegate to `__ror__`.\r\n\r\n#### Expected Output\r\n\r\n```\r\nOut[3]: <__main__.Test at 0x7f37a1601ba8>\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nIn [4]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.14.67-ts1\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.utf8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.24.2\r\npytest: 4.4.1\r\npip: 19.0.3\r\nsetuptools: 40.8.0\r\nCython: 0.29.6\r\nnumpy: 1.15.4\r\nscipy: 1.2.1\r\npyarrow: 0.11.1\r\nxarray: None\r\nIPython: 7.4.0\r\nsphinx: 2.0.1\r\npatsy: 0.5.1\r\ndateutil: 2.8.0\r\npytz: 2018.9\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.5.1\r\nnumexpr: 2.6.9\r\nfeather: None\r\nmatplotlib: 3.0.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: 1.1.5\r\nlxml.etree: None\r\nbs4: 4.7.1\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: 0.2.0\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\ngcsfs: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"## Code Sample\r\n\r\n**[IN]**\r\n```python\r\nimport pandas as pd\r\nnow = pd.Timestamp.utcnow()\r\nhelp(now.isoformat)\r\n```\r\n**[OUT]**\r\n    Help on method isoformat in module pandas._libs.tslibs.timestamps:\r\n    \r\n    isoformat(sep='T') method of pandas._libs.tslibs.timestamps.Timestamp instance\r\n        [sep] -> string in ISO 8601 format, YYYY-MM-DDT[HH[:MM[:SS[.mmm[uuu]]]]][+HH:MM].\r\n        sep is used to separate the year from the time, and defaults to 'T'.\r\n        timespec specifies what components of the time to include (allowed values are 'auto', 'hours', 'minutes', 'seconds', 'milliseconds', and 'microseconds').\r\n    \r\n    \r\n\r\n**[IN]**\r\n```python\r\nnow.isoformat(timespec=\"seconds\")\r\n```\r\n\r\n**[OUT]**\r\n    ---------------------------------------------------------------------------\r\n\r\n    TypeError                                 Traceback (most recent call last)\r\n\r\n    <ipython-input-8-3626478eeb62> in <module>\r\n    ----> 1 now.isoformat(timespec=\"seconds\")\r\n    \r\n\r\n    pandas\\_libs\\tslibs\\timestamps.pyx in pandas._libs.tslibs.timestamps.Timestamp.isoformat()\r\n    \r\n\r\n    TypeError: isoformat() got an unexpected keyword argument 'timespec'\r\n\r\n\r\n**[IN]**\r\n```python\r\nnow.to_pydatetime().isoformat(timespec=\"seconds\")\r\n```\r\n\r\n\r\n\r\n**[OUT]**\r\n    '2019-04-18T07:38:50+00:00'\r\n\r\n\r\n\r\n## Problem description\r\n\r\nThe help method has given me the feeling that I can use timespec just like I can with standard datetime objects (since python 3.6).\r\n\r\n## Expected Output\r\n\r\nWe could mimic standard datetime.isoformat behavior, or at least remove this false information from the help message.\r\n\r\n\r\n## Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 142 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_FR.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.24.2\r\npytest: 4.4.1\r\npip: 19.0.3\r\nsetuptools: 41.0.0\r\nCython: None\r\nnumpy: 1.16.2\r\nscipy: 1.2.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 7.4.0\r\nsphinx: 2.0.1\r\npatsy: 0.5.1\r\ndateutil: 2.8.0\r\npytz: 2019.1\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 3.0.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml.etree: 4.3.3\r\nbs4: 4.7.1\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10.1\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\ngcsfs: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"You can currently do this:\r\n\r\n```\r\nIn [2]: pdt = pd.PeriodDtype(freq='D')\r\n\r\nIn [3]: pdt\r\nOut[3]: period[D]\r\n\r\nIn [4]: pdt.freq = 'F'  \r\n\r\nIn [5]: pdt  \r\n...\r\nAttributeError: 'str' object has no attribute 'freqstr'\r\n```\r\n\r\nI think in general it would be good to make those dtype specific attributes not settable (this is already the case for DatetimeTZDtype.unit/tz, but not yet for the above case of PeriodDtype, and also not eg for IntervalDtype.subtype). "},{"labels":["api",null],"text":"This may be useful to downstream projects (e.g. https://github.com/apache/arrow/pull/3868).\r\n\r\nInitially, I think the idea was for RangeIndex to be a drop-in optimized replacement for the equivalent Int64Index. Other libraries might want public access to the same optimizations."},{"labels":["api"],"text":"Hi, first of all thanks for the amazing library everyone!\r\n\r\nThis is a minor issue I'm sure, but I find it impossible to remember the name of the `.pct_change()` function.\r\n\r\nPartly this is because of confusion with the existing .diff() function which is easy to remember - I always try things like:\r\n- pctdiff()\r\n- pct_diff()\r\n- percent_diff()\r\n\r\nNormally after a couple of goes I admit defeat and Google it yet again :)\r\n\r\nSo this is a request for an additional function with the name `.pct_diff()` (or whatever spelling is preferred) that replicates/calls `.pct_change()`, in order to standardise the API.\r\n\r\nThanks!"},{"labels":["api",null,null],"text":"In my trying to use pd.Intervals/pd.IntervalIndex (which are terrific additions), I've become somewhat perplexed with the default of `closed='right'`. Python slicing is inclusive-exclusive, so I thought `pd.Interval`s might have the same default. I imagine there is a reason for this that I haven't quite understood.\r\n\r\nI tried searching through the issue tracker for more information but wasn't really successful ... although I imagine it's pretty likely that I've missed the relevant issue."},{"labels":["api",null,null],"text":"Consider an ordered Categorical with missing values:\r\n\r\n```\r\nIn [32]: cat = pd.Categorical(['a', np.nan, 'b', 'a'], ordered=True)\r\n\r\nIn [33]: cat.min()\r\nOut[33]: nan\r\n\r\nIn [34]: cat.max()\r\nOut[34]: 'b'\r\n\r\nIn [35]: cat.min(numeric_only=True)\r\nOut[35]: 'a'\r\n\r\nIn [36]: cat.max(numeric_only=True)\r\nOut[36]: 'b'\r\n\r\nIn [37]: cat.min(numeric_only=False)\r\nOut[37]: nan\r\n\r\nIn [38]: cat.max(numeric_only=False)\r\nOut[38]: 'b'\r\n```\r\n\r\nSo from the observation above (and from the code: https://github.com/pandas-dev/pandas/blob/a89e19d59e0bff2d02e4647af1904e2c9701dd5f/pandas/core/arrays/categorical.py#L2199), it seems that `numeric_only` means that only the actual categories should be considered, and not the missing values (so codes that are not -1).\r\n\r\nThis struck me as strange, for the following reasons:\r\n\r\n* The fact that -1 is used as the code for missing data is rather an implementation detail, but now actually determines min/max behaviour (missing value is always the minimum, but never the maximum, unless there are only missing values)\r\n\r\n* This behaviour is different than the default for other data types in pandas, which is skipping missing values by default:\r\n\r\n    ```\r\n    In [1]: s = pd.Series([1, np.nan, 2, 1])  \r\n\r\n    In [2]: s.min()\r\n    Out[2]: 1.0\r\n\r\n    In [3]: s.astype(pd.CategoricalDtype(ordered=True)).min()\r\n    Out[3]: nan\r\n\r\n    In [5]: s.min(skipna=False)\r\n    Out[5]: nan\r\n    ```\r\n\r\n* The keyword in pandas to determine whether NaNs should be skipped or not for reductions is `skipna=True/False`, not `numeric_only` (this also means the `skipna` keyword for categorical series is broken / has no effect). \r\n  Apart from that, the name \"numeric_only\" is also strange to me to mean this (and is also not documented).\r\n\r\n* The `numeric_only` keyword in reductions methods of DataFrame actually means something entirely different: should full columns be excluded from the result based on their dtype. \r\n  \r\n    ```\r\n    In [63]: cat = pd.Categorical(['a', np.nan, 'b', 'a'], ordered=True)\r\n\r\n    In [64]: pd.Series(cat).min(numeric_only=True)\r\n    Out[64]: 'a'\r\n\r\n    In [65]: pd.DataFrame({'cat': cat}).min(numeric_only=True)\r\n    Out[65]: Series([], dtype: float64)\r\n    ```\r\n\r\nFrom the above list, I don't see a good reason for having `numeric_only=False` as 1) the default behaviour and 2) altogether as an option (instead of skipna). But it seems this was implemented rather from the beginning that Categoricals were introduced.\r\n\r\n*Am I missing something?* \r\n*Is there a reason we don't skip NaNs by default for Categorical?* \r\n\r\nWould it be an idea to deprecate `numeric_only` in favor of `skipna` and deprecate the default?\r\n\r\ncc @jreback @jankatins \r\n\r\n\r\n"},{"labels":["api",null],"text":"#### Problem description\r\n\r\nxref: #24782 \r\n\r\nSee the comment in https://github.com/pandas-dev/pandas/pull/24819\r\n"},{"labels":["api",null],"text":"cc @mrocklin for dask.dataframe visibility\r\n\r\nI'm one of the developers of https://github.com/rapidsai/cudf and we're working on adding GPU-accelerated file readers / writers to our library. It seems most of the standard formats are covered quite nicely in the Pandas API, but ORC isn't. Before we went off defining our own API I wanted to open a discussion for defining what that API would look like so we can be consistent with the Pandas and Pandas-like community.\r\n\r\nAt the top level, I imagine it would look almost identical to Parquet in something like the following:\r\n```\r\ndef read_orc(path, engine='auto', columns=None, **kwargs):\r\n    \"\"\"\r\n    Load an orc object from the file path, returning a DataFrame.\r\n\r\n    Parameters\r\n    ----------\r\n    path : string\r\n        File path\r\n    columns : list, default=None\r\n        If not None, only these columns will be read from the file.\r\n    engine : {'auto', 'pyarrow'}, default 'auto'\r\n        Orc library to use. If 'auto', then the option\r\n        ``io.orc.engine`` is used. The default ``io.orc.engine``\r\n        behavior is to use 'pyarrow'.\r\n    kwargs are passed to the engine\r\n\r\n    Returns\r\n    -------\r\n    DataFrame\r\n    \"\"\"\r\n    ...\r\n\r\n\r\ndef to_orc(self, fname, engine='auto', compression='snappy', index=None,\r\n           partition_cols=None, **kwargs):\r\n    \"\"\"\r\n    Write a DataFrame to the binary orc format.\r\n\r\n    This function writes the dataframe as a `orc file\r\n    <https://orc.apache.org/>`_. You can choose different orc\r\n    backends, and have the option of compression. See\r\n    :ref:`the user guide <io.orc>` for more details.\r\n\r\n    Parameters\r\n    ----------\r\n    fname : str\r\n        File path or Root Directory path. Will be used as Root Directory\r\n        path while writing a partitioned dataset.\r\n    engine : {'auto', 'pyarrow'}, default 'auto'\r\n        Orc library to use. If 'auto', then the option\r\n        ``io.orc.engine`` is used. The default ``io.orc.engine``\r\n        behavior is to use 'pyarrow'.\r\n    compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\r\n        Name of the compression to use. Use ``None`` for no compression.\r\n    index : bool, default None\r\n        If ``True``, include the dataframe's index(es) in the file output.\r\n        If ``False``, they will not be written to the file. If ``None``,\r\n        the behavior depends on the chosen engine.\r\n    partition_cols : list, optional, default None\r\n        Column names by which to partition the dataset\r\n        Columns are partitioned in the order they are given\r\n    **kwargs\r\n        Additional arguments passed to the orc library. See\r\n        :ref:`pandas io <io.orc>` for more details.\r\n    \"\"\"\r\n    ...\r\n```\r\n    "},{"labels":["api",null,null,null],"text":"#### Problem description\r\n`read_table()` got deprecated in favour of using `read_csv()`.\r\n\r\nUsing `read_csv()` to read tab/space delimited files is counter-intuitive. According to the docs and the related issues, both share the same code and it is not clear why the one function should be preferred over the other, and that change may even break existing code.\r\n\r\nIn my point of view `read_table()` is the more general function and `read_csv()` is a special case. Why would you deprecate (and then remove) the more useful function? It is already annoying to use `to_csv()` to write space/tab delimited files. And as I can see it, it comes down to two lines of code.\r\n\r\n#### Proposed solution\r\nKeep both functions as they are (un-deprecate `read_table()`) or rename the function to have a more general name like `read_txt()` (as in `numpy.genfromtxt()`) or similar."},{"labels":["api",null,null],"text":"https://github.com/pandas-dev/pandas/pull/25063 made the 0.24.x `sort=True` behavior be enabled by `sort=None`. This allows us to use `sort=True` to mean \"always sort\".\r\n\r\nSo rather than raising\r\n\r\n```pytb\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: idx = pd.Index([1, 0])\r\n\r\nIn [3]: idx.union(idx, sort=True)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-92d6bafc02d6> in <module>\r\n----> 1 idx.union(idx, sort=True)\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in union(self, other, sort)\r\n   2319         Index(['a', 'b', 'c', 'd', 1, 2, 3, 4], dtype='object')\r\n   2320         \"\"\"\r\n-> 2321         self._validate_sort_keyword(sort)\r\n   2322         self._assert_can_do_setop(other)\r\n   2323\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in _validate_sort_keyword(self, sort)\r\n   2263         if sort not in [None, False]:\r\n   2264             raise ValueError(\r\n-> 2265                 \"The 'sort' keyword only takes the values of \"\r\n   2266                 f\"None or False; {sort} was passed.\"\r\n   2267             )\r\n\r\nValueError: The 'sort' keyword only takes the values of None or False; True was passed.\r\n```\r\n\r\nWe would instead return\r\n\r\n```\r\nInt64Index([0, 1], dtype='int64')\r\n```\r\n\r\nThis is a (hopefully) exhaustive list of special cases not sorted when `sort=None`.\r\n\r\n* `union`:\r\n  - equal\r\n  - length-0\r\n  - incomparable\r\n* intersection\r\n  - equal\r\n  - incomparable\r\n* difference\r\n  - incomparable\r\n* symmetric_difference\r\n  - incomparable"},{"labels":["api",null,null],"text":"https://github.com/pandas-dev/pandas/blob/1700680381bdbfbc1abe9774f96881801b24d6ca/pandas/core/generic.py#L10159-L10166\r\n\r\nRunning this code  :\r\n\r\n` pd.DataFrame({'B': [1,20,400,1,1,1]}).ewm(span=2,min_periods=2,adjust=False).mean().values.flatten()`\r\nI  am getting\r\n`array([ nan, 13.66666667, 271.22222222,  91.07407407, 31.02469136,  11.00823045])`\r\n\r\nAs I understand the first item should be the simple moving average with window equals to span ( here it is 2 ). See for example see this link : https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_averages\r\n\r\n> There are three steps to calculating an exponential moving average (EMA). First, calculate the simple moving average for the initial EMA value. An exponential moving average (EMA) has to start somewhere, so a simple moving average is used as the previous period's EMA in the first calculation. \r\n\r\nso instead of 13.66666 it should be (20+1)/2 =>10.5\r\n\r\nAfterwards the formula is :\r\n![image](https://user-images.githubusercontent.com/3207061/52234883-5ae5ea00-28cb-11e9-90c3-d6ce312d27c8.png)\r\n\r\nThe formula also can be found here : http://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html  when adjust is set to False .\r\n\r\n\r\nWhat do you think ?\r\n\r\nThanks,\r\nBoris"},{"labels":["api",null,null],"text":"Python 3.6 added a `fold` argument in `datetime.datetime` to disambiguate DST transition times that occur twice (in wall time).\r\n\r\nhttps://docs.python.org/3/library/datetime.html#datetime-objects.\r\n\r\nTechnically `Timestamp` will accept the argument in 3.6, but it's not formally documented or tested.\r\nAdditionally since we will still be supporting 3.5 after dropping 2.7, we can add/handle a fold argument directly in the `Timestamp` constructor as well.\r\n"},{"labels":["api",null],"text":"```python\r\nIn[2]: import pandas as pd\r\n  ...: import numpy as np\r\n  ...: pd.__version__\r\nOut[2]: u'0.23.4'\r\n\r\nIn[3]: ts = pd.Series([np.nan, 1., 2., 3., np.nan, 4., np.nan])\r\nIn[4]: ts.pct_change(fill_method = None)\r\nOut[4]: \r\n0    NaN\r\n1    NaN\r\n2    1.0\r\n3    0.5\r\n4    NaN\r\n5    NaN\r\n6    NaN\r\ndtype: float64\r\n\r\nIn[5]: ts.pct_change(fill_method = 'pad')\r\nOut[5]: \r\n0         NaN\r\n1         NaN\r\n2    1.000000\r\n3    0.500000\r\n4    0.000000\r\n5    0.333333\r\n6    0.000000\r\ndtype: float64\r\n\r\nIn[6]: ts.pct_change(fill_method = 'pad').mask(ts.isnull())\r\nOut[6]: \r\n0         NaN\r\n1         NaN\r\n2    1.000000\r\n3    0.500000\r\n4         NaN\r\n5    0.333333\r\n6         NaN\r\ndtype: float64\r\n```\r\n\r\nHello, \r\n\r\nAfter recently updating my version, I noticed a change in behavior of pct_change with missing data. This is related to https://github.com/pandas-dev/pandas/issues/19873 . \r\n\r\nFirst example without fill_method is as expected. The second example is the result now and the third is what it used to be. I think the user should be able to choose if she prefers the second or third behavior. I agree that the second example is correct, as it forward fills as expected, but if the time series is a stock price for example, returns on missing days (holidays) were not 0, which can bias some statistics. \r\n\r\nI would suggest adding a new parameter, like skipna. I could not find any solution with existing parameters, if I missed something please let me know.\r\n\r\nThanks\r\n\r\n\r\n"},{"labels":["api",null,null],"text":"Prior to https://github.com/pandas-dev/pandas/pull/24521 (i.e., for pandas 0.23 and earlier), pandas.Index.intersection would not sort values:\r\n```\r\n# pandas 0.23.3\r\nIn [21]: pd.Index(['c', 'b', 'a']).intersection(['b', 'a'])\r\nOut[21]: Index(['b', 'a'], dtype='object')\r\n```\r\nNow it does:\r\n```\r\nIn [28]: pd.Index(['c', 'b', 'a']).intersection(['b', 'a'])\r\nOut[28]: Index(['a', 'b'], dtype='object')\r\n```\r\n\r\nThis turned up in a failure in xarray's test suite (not a real bug): https://github.com/pydata/xarray/issues/2717\r\n\r\nIt's fine to change this for consistency, but a deprecation cycle would probably make sense so users aren't surprised by the behavior of their code changing, e.g., we could default to `sort=None` and issue a FutureWarning for now."},{"labels":["api",null],"text":"Related to the discussion happening in https://github.com/pandas-dev/pandas/pull/24674#issuecomment-454816310, further xref https://github.com/pandas-dev/pandas/issues/24773, https://github.com/pandas-dev/pandas/issues/22384\r\n\r\nThe question being posed here is what the *return type* of `ExtensionArray.astype(..)` should be. \r\n\r\nCurrently, it can be both a numpy.ndarray or an ExtensionArray. And the `astype` method in the base class is actually advertised as *\"Cast to a NumPy array with 'dtype'\"* (this is not fully correct, as currently casting eg a DatetimeArray to period dtype will give a PeriodArray EA, not a numpy array). \r\n\r\nHowever, this gives some discussion about what eg `DatetimeArray.astype(\"datetime64[ns]\")` should return, as we actually have a DatetimeArray EA that supports that dtype. \r\nYou get similar inconsistencies / dubious cases in eg `DatetimeArray.astype('int64')` which currently returns an int64 ndarray, while `pd.array(DatetimeArray(), dtype='int64')` will give a PandasArray[int64].\r\n\r\nSo one idea would be to restrict `ExtensionArray` to be a method to convert from one type of ExtensionArray to another type of ExtensionArray, i.e. the output type would be expected to always be an ExtensionArray.\r\n(similarly, in the end, as how `ndarray.astype` only gives other ndarrays, or `pyarrow.Array.cast` only gives other pyarrow arrays; there are other (more explicit) methods to convert to a numpy array)\r\n\r\nIn practice this would mean returning a PandasArray for numpy dtypes instead of a numpy ndarray. Regarding storing the result of it in a Series/DataFrame should not change, because then we unpack such a PandasArray anyway.\r\n\r\ncc @TomAugspurger @jreback @jbrockmendel @shoyer @h-vetinari "},{"labels":["api",null,null],"text":"Hello,\r\n\r\nMy name is Tomer, and I am a developer working at Microsoft.\r\nI am part of [Azure Data Explorer](https://azure.microsoft.com/en-us/services/data-explorer/), and maintaining our [python packages](https://github.com/Azure/azure-kusto-python).\r\nWe have many customers that are using pandas, and we were thinking of how to ease their usage.\r\nWe would like to expose/contribute something similar to the following:\r\n\r\nimport pandas as pd\r\npd.read_azure_data_explorer(\"query\", con=connection_string)\r\n\r\nI think that adding the ability to read Azure Data Explorer directly into pandas could potentially help many of our customers who are accustomed to pandas.\r\nI would like to hear what do you guys think about that.\r\n\r\nThanks,\r\nTomer"},{"labels":["api",null,null],"text":"#### Code Sample\r\n\r\n```\r\ndata = {'id': [101, 203, 645, 101, 392, 546, 203],\r\n        'items': ['glass', 'table', 'laptop', 'charger', 'mouse', 'phone', 'screen']}\r\n\r\ndf = pd.DataFrame(data)\r\n\r\n\r\nid  | items\r\n--  | --\r\n101 | glass\r\n203 | table\r\n645 | laptop\r\n101 | charger\r\n392 | mouse\r\n546 | phone\r\n203 | screen\r\n\r\n\r\n\r\n# Method to aggregate and keep the values of the aggregated rows.\r\ndf = df.groupby('id').items.apply(list).reset_index()\r\n\r\ndf['items_string'] = df['items'].apply(lambda x: ' '.join(map(str, x)))\r\n```\r\n#### Problem description\r\n\r\nI was wondering if its possible to make this a standard method, by keeping values of columns in string format while aggregating the data, so no data is lost. \r\n\r\nFor exampe: ```df.grouby('id').keep_values('items')```\r\n\r\nI know this is possible with the code I provided above, but there should be more concise way right?\r\nI dont have enough knowledge to provide the code for the solution, but I hope my idea is understood.\r\n\r\n#### Expected Output\r\n```\r\nid  | items_string\r\n--  | --\r\n101 | glass charger\r\n203 | table screen\r\n392 | mouse\r\n546 | phone\r\n645 | laptop\r\n\r\n```\r\n\r\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.core.dtypes.common.is_list_like((2,3))\r\nOut[2]: True\r\n```\r\n\r\n#### Problem description\r\n\r\nWe discussed several times the fact that tuples in pandas should _not_ be considered collections of things, but rather\r\n- ``MultiIndex`` keys, or\r\n- combinations of keys, e.g. one for each dimension of a ``DataFrame``\r\n\r\n(simple way to discriminate: if you could easily add an element, it is a collection; if instead the number of elements is somewhat hardcoded, it is not).\r\n\r\nIt is perfectly natural, and would solve problems/hacks such as\r\n\r\nhttps://github.com/pandas-dev/pandas/commit/32ee9732b823448b87848f6bcaefdc762868999c#diff-1e79abbbdd150d4771b91ea60a4e1cc7R2701\r\n\r\nhttps://github.com/pandas-dev/pandas/pull/24697#issuecomment-453078627\r\n\r\n... and many others, to change the behavior of ``is_list_like``, which is used in many places.\r\n\r\nSee #23061 for a similar fix (although the similarity breaks whereas ``set``s are _intrinsically_ different from a ``list``, while for ``tuple``s it is a design decision).\r\n\r\nI do expect some tests to break, and I also expect that in some cases, we'll want to preserve backwards compatibility... but at least let's set a sane default.\r\n\r\n#### Expected Output\r\n\r\n``False``\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 040f06f731a09fc6e0663cada6697f6602b36f1d\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-8-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.24.0.dev0+1282.g040f06f73\r\npytest: 3.5.0\r\npip: 9.0.1\r\nsetuptools: 39.2.0\r\nCython: 0.28.4\r\nnumpy: 1.14.3\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.5.0\r\ndateutil: 2.7.3\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.2.2.post1634.dev0+ge8120cf6d\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml.etree: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\ngcsfs: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"`tz_localize` is implemented in NDFrame. But, AFAICT, `tz_localize` doesn't and can't work on a DataFrame\r\n\r\n\r\n```python\r\nIn [14]: df = pd.DataFrame({\"A\": pd.date_range('2000', periods=4)})\r\n\r\nIn [15]: df.tz_localize('UTC')\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-15-4390d0c58a16> in <module>\r\n----> 1 df.tz_localize('UTC')\r\n\r\n~/sandbox/pandas/pandas/core/generic.py in tz_localize(self, tz, axis, level, copy, ambiguous, nonexistent)\r\n   9374             if level not in (None, 0, ax.name):\r\n   9375                 raise ValueError(\"The level {0} is not valid\".format(level))\r\n-> 9376             ax = _tz_localize(ax, tz, ambiguous, nonexistent)\r\n   9377\r\n   9378         result = self._constructor(self._data, copy=copy)\r\n\r\n~/sandbox/pandas/pandas/core/generic.py in _tz_localize(ax, tz, ambiguous, nonexistent)\r\n   9354                     ax_name = self._get_axis_name(axis)\r\n   9355                     raise TypeError('%s is not a valid DatetimeIndex or '\r\n-> 9356                                     'PeriodIndex' % ax_name)\r\n   9357                 else:\r\n   9358                     ax = DatetimeIndex([], tz=tz)\r\n\r\nTypeError: index is not a valid DatetimeIndex or PeriodIndex\r\n```\r\n\r\nAnyone know why? OK with removing?"},{"labels":["api"],"text":"working on this now."},{"labels":["api",null,null,null,null,null,null,null],"text":"The constructor(s) for DatetimeArray are a bit messy right now, so let's step back a bit to lay out what we want out of them.\r\n\r\nWhat do we want out of our init? I'd like the following constraints:\r\n\r\n1. `data` is never copied unless explicitly requested with `copy=True`.  The values in `data` are never coerced. This means no lists (copy), and no ndarrays of values that can be coerced to datetime64[ns] (no object-dtype strings, Timestamps, etc.). We do allow unboxing data from a Series / Index / DatetimeArray, and we do allow viewing i8 data as M8[ns].\r\n2. The signature matches across all DTA classes: `values, dtype, freq, copy`\r\n3. It's fast. There are two wrinkles here\r\n  a.) I didn't (and many users probably don't) appreciate the performance impact of passing `freq=` to DTI / DTA. (ballpark: 5x slower for creating). Everything else is relatively cheap to check, the most expensive thing is probably timezone normalization which I think is unavoidable.\r\n  b.) Frequency inference. Right now it's disallowed. Should we allow it? Is this expensive?\r\n\r\nIf possible, I'd prefer to avoid defining `DatetimeArray.__new__`, for two main reasons\r\n\r\n1. Maintainability: defining `__new__` complicates pickle, which makes for relatively difficult debugging sessions in the future\r\n2. Aesthetics: Python already has a way for initializing classes (`__init__`), so all else equal I'd prefer to use that instead of `__new__` + `_simple_new`\r\n\r\n---\r\n\r\nSome concretish TODOs:\r\n\r\n1. Investigate validation-checking code between `DatetimeArray.__init__` and `sequence_to_dt64ns` (checking user-provided freq / dtype / tz vs. those properties on DatetimeArray `values`)\r\n2. Implement `freq` validation (blocked by \r\nhttps://github.com/pandas-dev/pandas/issues/24555 and maybe \r\nhttps://github.com/pandas-dev/pandas/issues/24562)\r\n3. Standardize `DatetimeArray._simple_new` and the `__init__`. Right now `_simple_new` takes `_simple_new(cls, values, freq=None, tz=None)`. Changing that `tz` to `dtype` should lets use share more code between TDA/DTA/PeriodArray."},{"labels":["api",null,null],"text":"Master currently has an (undocumented) (maybe-) API-breaking change from 0.23.4 when passed integer values\r\n\r\n0.23.4\r\n\r\n```python\r\nIn [2]: i8data = np.arange(5) * 3600 * 10**9\r\n\r\nIn [3]: pd.DatetimeIndex(i8data, tz=\"US/Central\")\r\nOut[3]:\r\nDatetimeIndex(['1970-01-01 00:00:00-06:00', '1970-01-01 01:00:00-06:00',\r\n               '1970-01-01 02:00:00-06:00', '1970-01-01 03:00:00-06:00',\r\n               '1970-01-01 04:00:00-06:00'],\r\n              dtype='datetime64[ns, US/Central]', freq=None)\r\n```\r\n\r\nMaster\r\n\r\n```python\r\nIn [3]: pd.DatetimeIndex(i8data, tz=\"US/Central\")\r\nOut[3]:\r\nDatetimeIndex(['1969-12-31 18:00:00-06:00', '1969-12-31 19:00:00-06:00',\r\n               '1969-12-31 20:00:00-06:00', '1969-12-31 21:00:00-06:00',\r\n               '1969-12-31 22:00:00-06:00'],\r\n              dtype='datetime64[ns, US/Central]', freq=None)\r\n\r\n```\r\n\r\n---\r\n\r\nAttempt to explain the behavior: In 0.23.4, passing an `ndarray[i8]` was equivalent to passing `data.view(\"M8[ns]\")`\r\n\r\n```python\r\n# 0.23.4\r\nIn [4]: pd.DatetimeIndex(i8data.view(\"M8[ns]\"), tz=\"US/Central\")\r\nOut[4]:\r\nDatetimeIndex(['1970-01-01 00:00:00-06:00', '1970-01-01 01:00:00-06:00',\r\n               '1970-01-01 02:00:00-06:00', '1970-01-01 03:00:00-06:00',\r\n               '1970-01-01 04:00:00-06:00'],\r\n              dtype='datetime64[ns, US/Central]', freq=None)\r\n```\r\n\r\nOn master, integer values are treated as unix timestamps, while M8[ns] values are treated as wall-times in the given timezone.\r\n\r\n```python\r\n# master\r\nIn [4]: pd.DatetimeIndex(i8data.view(\"M8[ns]\"), tz=\"US/Central\")\r\nOut[4]:\r\nDatetimeIndex(['1970-01-01 00:00:00-06:00', '1970-01-01 01:00:00-06:00',\r\n               '1970-01-01 02:00:00-06:00', '1970-01-01 03:00:00-06:00',\r\n               '1970-01-01 04:00:00-06:00'],\r\n              dtype='datetime64[ns, US/Central]', freq=None)\r\n```\r\n\r\n---\r\n\r\n**Reason for the change**\r\n\r\nThere are four cases of interest:\r\n\r\n```\r\nIn [4]: arr = np.arange(5) * 24 * 3600 * 10**9\r\nIn [5]: tz = 'US/Pacific'\r\n\r\nIn [6]: a = pd.DatetimeIndex(arr, tz=tz)\r\nIn [7]: b = pd.DatetimeIndex(arr.view('M8[ns]'), tz=tz)\r\nIn [8]: c = pd.DatetimeIndex._simple_new(arr, tz=tz)\r\nIn [9]: d = pd.DatetimeIndex._simple_new(arr.view('M8[ns]'), tz=tz)\r\n\r\nIn [10]: a\r\nOut[10]: \r\nDatetimeIndex(['1970-01-01 00:00:00-08:00', '1970-01-02 00:00:00-08:00',\r\n               '1970-01-03 00:00:00-08:00', '1970-01-04 00:00:00-08:00',\r\n               '1970-01-05 00:00:00-08:00'],\r\n              dtype='datetime64[ns, US/Pacific]', freq=None)\r\n\r\nIn [11]: b\r\nOut[11]: \r\nDatetimeIndex(['1970-01-01 00:00:00-08:00', '1970-01-02 00:00:00-08:00',\r\n               '1970-01-03 00:00:00-08:00', '1970-01-04 00:00:00-08:00',\r\n               '1970-01-05 00:00:00-08:00'],\r\n              dtype='datetime64[ns, US/Pacific]', freq=None)\r\n\r\nIn [12]: c\r\nOut[12]: \r\nDatetimeIndex(['1969-12-31', '1970-01-01', '1970-01-02', '1970-01-03',\r\n               '1970-01-04'],\r\n              dtype='datetime64[ns, US/Pacific]', freq=None)\r\n\r\nIn [13]: d\r\nOut[13]: \r\nDatetimeIndex(['1969-12-31', '1970-01-01', '1970-01-02', '1970-01-03',\r\n               '1970-01-04'],\r\n              dtype='datetime64[ns, US/Pacific]', freq=None)\r\n```\r\n\r\nIn 0.23.4 we have `a.equals(b)` and `c.equals(d)` but no way to pass data in a way that was constructor-neutral.  In master we now have `a` match `c` and `d`.  At some point in the refactoring process we changed that, but off the top of my head I don't remember when or if this was the precise motivation or just a side-benefit.\r\n\r\nBTW _simple_new was also way too much:\r\n\r\n```\r\n        if getattr(values, 'dtype', None) is None:\r\n            # empty, but with dtype compat\r\n            if values is None:\r\n                values = np.empty(0, dtype=_NS_DTYPE)\r\n                return cls(values, name=name, freq=freq, tz=tz,\r\n                           dtype=dtype, **kwargs)\r\n            values = np.array(values, copy=False)\r\n\r\n        if is_object_dtype(values):\r\n            return cls(values, name=name, freq=freq, tz=tz,\r\n                       dtype=dtype, **kwargs).values\r\n        elif not is_datetime64_dtype(values):\r\n            values = _ensure_int64(values).view(_NS_DTYPE)\r\n```\r\n\r\n---\r\n\r\n**Was this documented**?\r\n\r\nhttp://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.html mentions that it's \"represented internally as int64\".\r\n\r\nThe (imprecise) type on `data` is \"Optional datetime-like data\"\r\n\r\nI don't see anything in http://pandas.pydata.org/pandas-docs/stable/timeseries.html suggesting that integers can be passed to DatetimeIndex."},{"labels":["api",null],"text":"#### Code Sample\r\n\r\n```python\r\ndf.to_csv(file_path)\r\ndf = pd.read_csv(file_path)\r\n```\r\n#### Problem description\r\n\r\nCurrently, the default CSV writing behaviour is to write the index column.  The default reading behaviour, however, is to assume there is no index column in the file. This is not intuitive when writing and reading files.\r\n\r\nThe expected behaviour is that a file which is written without any index option it should be able to be read without any index option. However, the default writing and reading behaviour results in an unnamed column.\r\n\r\nOne fix is to change the default index_col for read_csv, another option is to change the default index boolean for to_csv. The former is probably preferable as it preserves information."},{"labels":["api",null,null],"text":"In https://github.com/pandas-dev/pandas/pull/24395/ @jschendel mentioned that \r\n\r\n> I think this could be restricted to repeat(self, repeats, axis=None) but I opted to match on the least restrictive function signature instead.\r\n\r\nI agree and I think we should do that."},{"labels":["api",null],"text":"#### Example\r\n\r\nexample.csv:\r\n```\r\ncol1,col2\r\n1,a\r\n2,b\r\n3,c\r\n,\r\n```\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# Runs OK, but would be useful to have an option to ignore null rows, as the integers are forced to floats.\r\npd.read_csv(\"example.csv\")\r\n\r\n# \"ValueError: Integer column has NA values in column 0\"\r\npd.read_csv(\"example.csv\", dtype={\"col1\": np.int, \"col2\": \"object\"})\r\n```\r\n#### Problem description\r\n\r\nI have encountered several cases of CSV files with one or more \"empty\" rows at the bottom - i.e. valid rows with the correct number of delimited fields, but without any data in the fields. This seems to be a \"feature\" of CSV exports from programs like Excel in some circumstances. The problem often doesn't get detected until run in Python, as users don't typically inspect the CSV files with a text editor (nor would I expect them to!).\r\n\r\nSo far, I have been unable to find any option within `pd.read_csv()` that allows these rows to be ignored. `skipfooter` requires the number of null lines to be known, which can only be done by reading through the entire file separately before handing it off to Pandas, and I'd prefer not to have to create a wrapper around every call to `pd.read_csv()`.\r\n\r\nWhen reading such a file with `pd.read_csv()` without dtypes specified, the data is read, although with a couple of minor issues. Columns that would normally be parsed as integers are instead parsed as floats to allow NA values to exist, and the null rows need to be removed manually.\r\n\r\nHowever, if using `pd.read_csv(..., dtype={...})`, an exception is thrown if the columns have a dtype without an NA representation - integers being the most obvious. I can't see any way to get Pandas to read the file with specified dtypes in this circumstance without preprocessing the file outside of Pandas. \r\n\r\n#### Expected Output\r\n\r\n```\r\n>>> pd.read_csv(\"example.csv\", ignore_empty=True)\r\n   col1 col2\r\n0   1      a\r\n1   2      b\r\n2   3      c\r\n>>> pd.read_csv(\"example.csv\", ignore_empty=True, dtype={\"col1\": np.int, \"col2\": \"object\"})\r\n   col1 col2\r\n0   1      a\r\n1   2      b\r\n2   3      c\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.15.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.22.0\r\npytest: None\r\npip: 10.0.1\r\nsetuptools: 40.6.2\r\nCython: 0.29\r\nnumpy: 1.15.4\r\nscipy: 0.19.1\r\npyarrow: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: 0.5.0\r\ndateutil: 2.7.3\r\npytz: 2018.5\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.2.3\r\nopenpyxl: None\r\nxlrd: 1.1.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.7.5 (dt dec pq3 ext lo64)\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":""},{"labels":["api",null],"text":"In #23752, the MultiIndex signature was changed. Compared to 0.23.4, the only change is that ``labels`` has been changed to ``codes``.\r\n\r\nNow that the signature is being changed anyway, I've started to think about if this is even the right signature:\r\n\r\nI think ``codes`` should actually be an implementation detail, and an improved signature would be using ``data`` for the first parameter, similarly to how ``data`` is the first parameter in the signature for ``CategoricalIndex``. So a signauture like this:\r\n\r\n```\r\n>>> inspect.signature(pd.MultiIndex)  # proposed signature\r\n<Signature (data=None, levels=None, sortorder=None, names=None, dtype=None, copy=False, name=None, verify_integrity=True, _set_identity=True)>\r\n```\r\n\r\nI think would be better.\r\n\r\n``data`` could then accept codes, but could also accept other types of data, that could be used to construct a ``MultiIndex``. For example:\r\n\r\n```python\r\n>>> pd.MultiIndex(data=[[1,0, 1, 0], [0,1,0,1]], levels=[['a', 'b'], ['x', 'y']])\r\nMultiIndex([('b', 'x'),  # repr after #22511\r\n            ('a', 'y'),\r\n            ('b', 'x'),\r\n            ('a', 'y')],\r\n           )\r\n>>> pd.MultiIndex({'a': [1,2,3], 'v': ['a', 'd', 'q']})\r\nMultiIndex([(1, 'a'),\r\n            (2, 'd'),\r\n            (3, 'q')],\r\n            names=['a', 'b']\r\n           )\r\n```\r\n\r\nIn the first example, I use the current initalisation method, and in the second I show a initalisation with a dict, similar to how a DataFrame is initalized with a dict.\r\n\r\nI think this could make the initialisation of MultiIndex more similar to the ones for the other pandas objects, and make MultiIndexes more friendly to use for users.\r\n"},{"labels":["api",null,null],"text":"Creating an issue of my comment on the PR that deprecated PeriodIndex add/sub with integers (https://github.com/pandas-dev/pandas/pull/22535):\r\n\r\n@jbrockmendel what was actually the rationale of deprecating it for Periods as well? \r\nThe issue that is closed by this (https://github.com/pandas-dev/pandas/issues/21939, cc @jreback @mroeschke) discusses it for DatetimeIndex/TimedeltaIndex, but I think says to keep it for PeriodIndex for now. \r\n\r\nIn any case, for PeriodIndex, this was an explicitly documented behaviour (in several places throughout the docs), and the documentation is now no longer up to date (you can check the doc build to see the deprecation warnings being raised). \r\n\r\nBut instead of updating the docs, I think we should maybe also reconsider the deprecation (given that is was a clearly documented behaviour). Or at least provide a clear alternative way.\r\n\r\n(and to be clear: I fully agree with the deprecation for DatetimeIndex / TimedeltaIndex!)\r\n"},{"labels":["api",null],"text":"While adding the keyword and functionality (beyond the cython backend) is only gonna happen past v.0.24, I think the EA interface should be adapted to allow for the possibility of the `return_inverse`-kwarg.\r\n\r\nThe default implementation could just recommend to start with\r\n``` \r\nif return_inverse:\r\n    raise NotImplementedError('this array type does not yet support `return_inverse=True`')\r\n```\r\n\r\nOf course, this could be added along with the other changes to `.unique`, but I was wondering if this should already be prepared before v.0.24 so that EA authors (who will probably expect the EA interface to be more stable with v.0.24) can already prepare for it.\r\n\r\n@jreback @TomAugspurger "},{"labels":["api",null,null,null],"text":"This is coming out of a discussion that has stalled #22225 (which is about adding `.set_index` to Series, see #21684). The discussion has shifted away from what capabilities a putative `Series.set_index` should have, but what capabilities `df.set_index` has *currently*.\r\n\r\nThe main issue (for @jreback) is that `df.set_index` takes arrays:\r\n> > @jreback: There were several attempts to have DataFrame.set_index take an array as well, but these never got off the ground.\r\n\r\n> @h-vetinari: I'm not sure *when*, but they certainly did get off the ground:\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> pd.__version__\r\n'0.23.4'\r\n>>>\r\n>>> df = pd.DataFrame(np.random.randint(0, 10, (4, 4)), columns=list('abcd'))\r\n>>> df.set_index(['a',          # label\r\n...               df.index,     # Index\r\n...               df.b ** 2,    # Series\r\n...               df.b.values,  # ndarray\r\n...               list('ABCD'), # list\r\n...               'c'])         # label again\r\n              b  d\r\na   b      c\r\n0 0 0  2 A 1  0  2\r\n8 1 1  4 B 4  1  4\r\n3 2 25 5 C 8  5  5\r\n0 3 9  7 D 2  3  7\r\n```\r\n\r\nFurther on:\r\n> @jreback: @h-vetinari you are confusing the purpose of `.set_axis`. [...] The problem with `.set_index` on a DataFrame with an array is that it technically can work with an array and not keys. (meaning its not unambiguous)\r\n\r\nI don't think I am confusing them. If I want to set the `.index`-attribute of a Series/DataFrame, then using `.set_index` is the most reasonable name by far. If anything, `set_axis` should be a superset of `set_index` (and a putative `set_columns`), that just switches between the two based on the `axis`-kwarg.\r\n\r\nMore than that, the current capabilities of `df.set_index` are a proper superset of `df.set_axis(axis=0)`**, in that it's possible to fill `keys` with *only* `Series`/`Index`/`ndarray`/`list` etc.:\r\n```\r\n>>> df.set_index(pd.Index(df.a))  # same result as Series directly below\r\n>>> df.set_index(df.a) \r\n   a  b  c  d\r\na\r\n0  0  0  1  2\r\n8  8  1  4  4\r\n3  3  5  8  5\r\n0  0  3  2  7\r\n>>> df.set_index(df.a.values)  # same result as list directly below\r\n>>> df.set_index([[0, 8, 3, 0]])\r\n   a  b  c  d\r\n0  0  0  1  2\r\n8  8  1  4  4\r\n3  3  5  8  5\r\n0  0  3  2  7\r\n```\r\n** there is one caveat, in that lists (and only lists; out of all containers) need to be wrapped in another list, i.e. `df.set_index([[0, 8, 3, 0]])` instead of `df.set_index([0, 8, 3, 0])`. This is the heart of the ambiguity that @jreback mentioned above (because a list is interpreted as a list of column keys).\r\n\r\nSumming up:\r\n* [x] `set_index` is the most natural name for setting the `.index`-attribute\r\n* [x] `df.set_index` *should* be able to process list-likes (as it currently does; this is the source of the ambiguity of the list case).\r\n* [ ] `df.set_axis` should be able to do everything that `df.set_index` does, and just switch between operating on index/columns based on the `axis`-kwarg (after all, `index` and `columns` are the two axes of a DF).\r\n  * [ ] it could be considered to add a method `set_columns` on a `DataFrame` \r\n  * [ ] The `axis`-kwarg of `set_axis` should just switch between the behaviour of `set_index` (i.e. dealing with keys *and* array-likes) and `set_columns`.\r\n* [ ] `Series.set_index` should support the same signature as `df.set_index`, with the exception of the `drop`-keyword (which only makes sense for column labels).\r\n* [ ] For Series, the `set_index` and `set_axis` methods should be exactly the same.\r\n\r\nSince I can't tag @pandas-dev/pandas-core, here are a few individual tags: @jreback @TomAugspurger @jorisvandenbossche @gfyoung @WillAyd @jbrockmendel @jschendel @toobaz.\r\n\r\nEDIT: Forgot to add an xref from @jreback:\r\n> @h-vetinari we had quite some discussion about this: #14829\r\nand never reached resolution. This is an API question.\r\n\r\nIn that issue, there's discussion largely around `.rename`, and how to make that method more consistent. Also discussed was potentially introducing `.relabel`, as well as `.set_columns`."},{"labels":["api",null],"text":"Hello,\r\n\r\n#### Problem description\r\n\r\nThe problem is that pandas' ``Timestamp`` raises ``TypeError`` when compared with unsupported types (from pandas' point of view), where I think it should return ``NotImplemented`` so that comparisons are delegated to the other object (if supported). See https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/tslibs/timestamps.pyx#L247\r\n\r\nThe current behaviour prevents other objects to deal with comparisons with ``Timestamp``. For instance, I developed a library that provides arithmetic operations for intervals composed of arbitrary comparable objects. In the context of this library, I defined two specific objects corresponding to negative and positive infinities (that are respectively lower and greater than \"everything else\"). When these objects are compared with pandas' ``Timestamp``, a ``TypeError`` is raised, preventing the comparison to be delegated to my \"infinities\". \r\n\r\n#### Code Sample\r\n\r\n```python\r\n# Let's define infinity, or any object whose magic methods support Timestamp\r\nclass Inf:\r\n    def __lt__(self, o): return False\r\n    def __le__(self, o): return isinstance(o, Inf)\r\n    def __gt__(self, o): return not isinstance(o, Inf)\r\n    def __ge__(self, o): return True\r\n    def __eq__(self, o): return isinstance(o, Inf)\r\n    def __ne__(self, o): return not self == o  # Required for Python 2\r\n    def __repr__(self): return '+inf'\r\n\r\n# Import pandas and create a timestamp\r\nimport pandas as pd\r\ntimestamp = pd.Timestamp('2018-11-30')\r\n\r\n# Comparison works if compared in *that* order, because magic method is called on Inf\r\nassert Inf() > timestamp\r\nassert not (Inf() < timestamp)\r\n\r\n# ... but not when magic method is called on Timestamp\r\nassert timestamp < Inf()\r\n```\r\n\r\n... raises the following:\r\n\r\n```\r\npandas/_libs/tslib.pyx in pandas._libs.tslib._Timestamp.__richcmp__()\r\nTypeError: Cannot compare type 'Timestamp' with type 'Inf'\r\n```\r\n\r\n#### Expected Behaviour\r\n\r\n```TypeError``` not raised, assertion holds.\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.18.17-300.fc29.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_BE.UTF-8\r\nLOCALE: fr_BE.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: 3.7.4\r\npip: 9.0.3\r\nsetuptools: 40.2.0\r\nCython: None\r\nnumpy: 1.14.0\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.1.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 1.0.1\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n\r\n\r\nAlso tested on the latest 0.23.4. "},{"labels":["api",null,null],"text":"Potential feature, could `df.to_sql()` return the indexes of the inserted rows? Something like [knex.js Query Builder](https://knexjs.org/#Builder-insert)\r\n\r\n```python\r\ninserted = df.to_sql(con=engine, name=example_table, if_exists='replace', index=False)\r\n```\r\n#### Problem description\r\nThis would allow for more quickly knowing how many rows have been entered from your dataframe\r\n\r\n#### Expected Output\r\nSomething like `inserted = 2` if 2 rows have been added. Even better would be `inserted = [24, 25]` where inserted is now a list of the inserted primary keys"},{"labels":["api",null,null],"text":"leftover from https://github.com/pandas-dev/pandas/pull/23623\r\n\r\n1. Signature for `.to_numpy()`: @jorisvandenbossche proposed `copy=True`, which I think is good. Beyond that, we may want to control the \"fidelity\" of the conversion. Should `Series[datetime64[ns, tz]].to_numpy()` be an ndarray of Timestamp objets or an ndarray of dateimte64[ns] normalized to UTC (by default, and should we allow that to be controlled)? Can we hope for a set of keywords appropriate for all subtypes, or do we need to allow `kwargs`? *Perhaps* `to_numpy(copy=True, dtype=None)` will suffice?\r\n\r\n2. Make `.array` always an ExtensionArray (via @shoyer). This gives pandas a bit more freedom going forward, since the *type* of `.array` will be stable if / when we flip over to Arrow arrays by default. We'll just swap out the data backing the ExtensionArray. A generic \"NumpyBackedExtensionArray\" is pretty easy to write (I had one in cyberpandas). My main concern here is that it makes the statement \"`.array` is the actual data stored in the Series / Index\" falseish, but that's OK.\r\n\r\n3. Revert the breaking changes to `Series.values` for `period` and `interval` dtype data (cc @jschendel)? I think we should do this.\r\n\r\n```python\r\nIn [3]: sper = pd.Series(pd.period_range('2000', periods=4))\r\n\r\nIn [4]: sper.values  # on master this is the PeriodArray\r\nOut[4]:\r\narray([Period('2000-01-01', 'D'), Period('2000-01-02', 'D'),\r\n       Period('2000-01-03', 'D'), Period('2000-01-04', 'D')], dtype=object)\r\n\r\nIn [5]: sper.array\r\nOut[5]:\r\n<PeriodArray>\r\n['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04']\r\nLength: 4, dtype: period[D]\r\n```\r\n\r\nIn terms of LOC, it's a simple change\r\n\r\n```diff\r\n@@ -1984,6 +1984,16 @@ class ExtensionBlock(NonConsolidatableMixIn, Block):\r\n         return blocks, mask\r\n\r\n\r\n+class ObjectValuesExtensionBlock(ExtensionBlock):\r\n+    \"\"\"Block for Interval / Period data.\r\n+\r\n+    Only needed for backwards compatability to ensure that\r\n+    Series[T].values is an ndarray of objects.\r\n+    \"\"\"\r\n+    def external_values(self, dtype=None):\r\n+        return self.values.astype(object)\r\n+\r\n+\r\n class NumericBlock(Block):\r\n     __slots__ = ()\r\n     is_numeric = True\r\n@@ -3004,6 +3014,8 @@ def get_block_type(values, dtype=None):\r\n\r\n     if is_categorical(values):\r\n         cls = CategoricalBlock\r\n+    elif is_interval_dtype(dtype) or is_period_dtype(dtype):\r\n+        cls = ObjectValuesExtensionBlock\r\n```\r\n\r\nThere are a couple other places (like `Series._ndarray_values`) that assume \"extension dtype means `.values` is an ExtensionArray\", which I've surfaced on my DatetimeArray branch. We'll need to update those to use `.array` anyway.\r\n\r\n---\r\n\r\n- [x] `Series.to_numpy()` signature\r\n- [ ] `Series.array` is always an EA\r\n- [x] Revert breaking changes to `Series.values` for Period / Interval (#24163)"},{"labels":["api",null],"text":"In #22511 and #23752 I work on some changes to ``MultiIndex``. It came up in discussion that the consteuction of MultiIndex shold be changed, so that initialising with a list of tuples should be accepted:\r\n\r\n```python\r\n>>>pd.MultiIndex([('a', 1)\r\n...               ('b', 2)])\r\nMultiIndex([('a', 1)\r\n            ('b', 2)])\r\n```\r\n\r\nI'm also thinking that MultiIndex could accept dicts when initialising, and it would work similarly, except the dict keys would become the values for ``MultiIndex.names``, and the dict values would be level values for the MultiIndex.\r\n"},{"labels":["api",null],"text":"Issue is described at https://github.com/pandas-dev/pandas/pull/23832#pullrequestreview-177106215\r\n\r\nShould we parse columns that are used to construct date columns or not?"},{"labels":["api",null],"text":"For TZ-aware DatetimeIndex, `to_series()` drops the timezone unless we pass `keep_tz=True`\r\n\r\n```\r\n>>> dti = pd.date_range('1977-04-15', periods=3, freq='MS', tz='US/Hawaii')\r\n\r\n>>> dti.to_series().to_frame().dtypes\r\n0    datetime64[ns]\r\n\r\n>>> dti.to_frame().dtypes\r\n0    datetime64[ns, US/Hawaii]\r\n```\r\n\r\nIIRC there has been discussion about deprecating the need for `keep_tz`; this should be another reason to do so."},{"labels":["api",null,null],"text":"Following from #23167...\r\n\r\nThe current checks for the `.str`-constructor regarding `MultiIndex` is (roughly)\r\n```\r\nif isinstance(data, MultiIndex) and data.nlevels > 1:\r\n    raise ...\r\n```\r\n\r\nMeaning the constructor passes for `MultiIndex` with a single level, but essentially *all* methods fail or produce garbage:\r\n```\r\nidx = pd.Index(['aaa', 'bb', 'c'])\r\nmi = pd.MultiIndex.from_arrays([idx])\r\n>>> mi.str.len()\r\nInt64Index([1, 1, 1], dtype='int64')  # compare idx.str.len() == Int64Index([3, 2, 1], dtype='int64')\r\n>>> mi.str.cat()\r\n[...]\r\nNotImplementedError: initializing a Series from a MultiIndex is not supported\r\n>>> mi.str.startswith('a')\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.upper()\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.islower()\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.split()\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.find('a')\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.ljust(10)\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.repeat(3)\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.slice(1, 2)\r\nIndex([(), (), ()], dtype='object')  # compare idx.str.slice(1, 2) == Index(['a', 'b', ''], dtype='object')\r\n>>> mi.str.zfill(10)\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.wrap(2)\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.normalize('NFC')\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.index('')\r\n[...]\r\nValueError: tuple.index(x): x not in tuple\r\n>>> mi.str.get(1)\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n>>> mi.str.contains('a')\r\nFloat64Index([nan, nan, nan], dtype='float64')\r\n```\r\n\r\nMy original plan in #23167 was just to disable `MultiIndex.str` regardless of the number of levels, but @toobaz brought up the point (in a side discussion in #23670) that:\r\n> Sorry, naive question, but what is the problem with just running `.str` on the result of `self.get_level_values(0)`?\r\n\r\nThis would, of course, work without problem. The main question that arises from this issue:\r\n* should the `.str`-accessor be enabled for `MultiIndex` at all?\r\n* if yes, should it return an `Index` or a 1-level `MultiIndex`?\r\n\r\nPS. As another link to #23670, one could *maybe* consider enabling `.str` for *all* `MultiIndex`, by operating on `MultiIndex.to_flat_index()` in those cases. This might be interesting for example for easy joining of the `MultiIndex`-levels with `.str.join`."},{"labels":["api",null,null],"text":"At the moment the following raises:\r\n\r\n```\r\n>>> dti = pd.date_range('2016-01-01', periods=3, tz='US/Central')\r\n>>> pd.DatetimeIndex(dti, tz='Asia/Tokyo')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/pandas/core/indexes/datetimes.py\", line 413, in __new__\r\n    raise TypeError(msg.format(data.tz, tz))\r\nTypeError: data is already tz-aware US/Central, unable to set specified tz: Asia/Tokyo\r\n```\r\n\r\nIt isn't clear to me that raising is the right thing to do; shouldn't this just be equivalent to `dti.tz_convert('Asia/Tokyo')`?  Or is this ambiguous for some reason?"},{"labels":["api",null,null],"text":"I wouldn't be surprised if there is already an issue about this, but couldn't directly find one.\r\n\r\nWhen doing a subselection of columns on a DataFrameGroupBy object, both a plain list (so a tuple within the `__getitem__` [] brackets) as the double square brackets (a list inside the `__getitem__` [] brackets) seems to work:\r\n\r\n```\r\nIn [6]: df = pd.DataFrame(np.random.randint(10, size=(10, 4)), columns=['a', 'b', 'c', 'd'])\r\n\r\nIn [8]: df.groupby('a').sum()\r\nOut[8]: \r\n    b   c   d\r\na            \r\n0   0   5   7\r\n3  18   6  12\r\n4  16   6   9\r\n6  10  11  11\r\n9   3   3   0\r\n\r\nIn [9]: df.groupby('a')['b', 'c'].sum()\r\nOut[9]: \r\n    b   c\r\na        \r\n0   0   5\r\n3  18   6\r\n4  16   6\r\n6  10  11\r\n9   3   3\r\n\r\nIn [10]: df.groupby('a')[['b', 'c']].sum()\r\nOut[10]: \r\n    b   c\r\na        \r\n0   0   5\r\n3  18   6\r\n4  16   6\r\n6  10  11\r\n9   3   3\r\n```\r\n\r\nPersonally I find this `df.groupby('a')['b', 'c'].sum()` a bit strange, and inconsistent with how DataFrame indexing works.\r\n\r\nOf course, on a DataFrameGroupBy you don't have the possible confusion with indexing multiple dimensions (rows, columns), but still.\r\n\r\ncc @jreback @WillAyd "},{"labels":["api",null,null],"text":"We need to better describe the exact semantics of `_ndarray_values`: what is it expected to return and how it is used.\r\n\r\nCurrenlty it is defined on the ExtensionArray, but mentioned it is not part of the \"official\" interface:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/712fa945c878eaed18f79d4cf99ed91e464d51b1/pandas/core/arrays/base.py#L687-L697\r\n\r\nOne Series/Index, the property will either give you what `EA._ndarray_values` gives, or the underlying ndarray:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/712fa945c878eaed18f79d4cf99ed91e464d51b1/pandas/core/base.py#L768-L780\r\n\r\n---\r\n\r\nWhat it currently is for the EAs:\r\n\r\n* Categorical: integer codes\r\n* IntegerArray: the integer `_data`, so but losing any information about missing values\r\n* PeriodArray: the integer ordinals\r\n* IntervalIndex: object array of Interval objects\r\n\r\n---\r\n\r\nFor what it is currently used (this needs to be better looked at, copying now from https://github.com/pandas-dev/pandas/issues/19954#issuecomment-436374598, quoting Tom here):\r\n\r\n- Index.itemsize (deprecated)\r\n- Index.strides (deprecated)\r\n- Index._engine\r\n- Index set ops\r\n- Index.insert\r\n- DatetimeIndex.unique\r\n- MultiIndex.equals\r\n- pytables._convert_index (shared across integer and period)\r\n\r\nThere are a few other uses (mostly datetime / timedelta / period) that could maybe uses asi8 instead. I'm not familiar enough with indexing to know whether that can operate on something other than ndarrays. In theory, EAs can implement the buffer protocol, which would get the data to cython. But I don't know what ops would be required when we're down there.\r\n\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame({'a': ['x' * 250]})\r\ndf.to_stata('test')\r\n```\r\n\r\n#### Problem description\r\n\r\nThe last line above raises an exception:\r\n\r\n```\r\nValueError:\r\nFixed width strings in Stata .dta files are limited to 244 (or fewer)\r\ncharacters.  Column 'a' does not satisfy this restriction.\r\n```\r\n\r\nbut is solved with:\r\n\r\n```py\r\ndf.to_stata('test', version=117)\r\n```\r\n\r\nThis functionality (writing in `dta` format 117) was added in version 0.23. In my opinion, the Stata writer should automatically switch to version 117 if one of the columns is wider than 244 characters. At the least, the error message should be changed to note that as of version 0.23, it's possible to write long strings to Stata files by adding `version=117`.\r\n\r\nI'd be happy to submit a PR if this functionality is desired.\r\n\r\n#### Expected Output\r\n\r\nStata file written to disk.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n```\r\npd.show_versions()\r\nNo module named 'dask'\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.7.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 18.2.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.24.0.dev0+948.g82120016e\r\npytest: 3.10.0\r\npip: 18.1\r\nsetuptools: 40.5.0\r\nCython: 0.29\r\nnumpy: 1.15.4\r\nscipy: 1.1.0\r\npyarrow: 0.11.1\r\nxarray: 0.10.9\r\nIPython: 7.1.1\r\nsphinx: 1.8.1\r\npatsy: 0.5.1\r\ndateutil: 2.7.5\r\npytz: 2018.7\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.4\r\nnumexpr: 2.6.8\r\nfeather: None\r\nmatplotlib: 3.0.1\r\nopenpyxl: 2.5.9\r\nxlrd: 1.1.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 1.1.2\r\nlxml: 4.2.5\r\nbs4: 4.6.3\r\nhtml5lib: 1.0.1\r\nsqlalchemy: 1.2.13\r\npymysql: 0.9.2\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: 0.1.6\r\nfastparquet: 0.1.6\r\npandas_gbq: None\r\npandas_datareader: None\r\ngcsfs: 0.1.2\r\n```\r\n\r\n</details>\r\n\r\ncc: @bashtage \t"},{"labels":["api",null,null],"text":"In #23167, I'm trying consistently infer the dtype of the underlying Series/Index while calling the constructor of the `.str`-accessor. For testing this thoroughly, I wanted to build a parametrized fixture that returns an ndarray for all the dtypes that `lib.infer_dtype` can infer. I based myself on the list in the docstring, but found the following:\r\n* the docstring mentions `'complex'` as a possible outcome, but this does not work (instead returning `'mixed'`)\r\n```\r\n>>> lib.infer_dtype([1+1j, 2+2j])\r\n'mixed'\r\n>>> lib.infer_dtype([np.complex128(1+1j)])\r\n'mixed'\r\n```\r\nand I don't believe it's actually possible to achieve this, given the code.\r\n* the docstring mentions `'timedelta64'`, but this similarly does not work (returning `'timedelta'`; and can't be hit either, IMO)\r\n```\r\n>>> lib.infer_dtype([np.timedelta64(1, 'D')])\r\n'timedelta'\r\n```\r\n* the docstring *does not* mention `'interval'`, but that *is* a possible outcome:\r\n```\r\n>>> lib.infer_dtype([pd.Interval(0, 1), pd.Interval(0, 2)])\r\n'interval'\r\n```\r\n\r\nSo it needs to be discussed if `'complex'`/`'timedelta64'` should be added to the code or removed from the docstring, and vice versa for `'interval'`."},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> ix = pd.Index([1,2])\r\n>>> eix = pd.Index([])\r\n>>> pi = pd.PeriodIndex(['19910905', '19910906'], freq='D')\r\n\r\n# Pair 1\r\n>>> pi.union(eix)\r\nValueError: can only call with other PeriodIndex-ed objects\r\n>>> eix.union(pi)\r\nPeriodIndex(['1991-09-05', '1991-09-06'], dtype='period[D]', freq='D')\r\n\r\n# Pair 2\r\n>>> pi.union(ix)\r\nValueError: can only call with other PeriodIndex-ed objects\r\n>>> ix.union(pi)\r\nIndex([1, 2, 1991-09-05, 1991-09-06], dtype='object')\r\n```\r\n#### Problem description\r\nConceptually I would imagine a union operation to be commutative. I was just wondering if there was an deliberate rationale behind not implementing pd.Index._assert_can_do_setop to only fail if the complementary self._assert_can_do_setop also fails.\r\n\r\nThis behavior also leads to some unexpected behaviors in `pd.concat`. For example:\r\n\r\n    >>> df1 = df1 = pd.DataFrame([[1,2,3],[1,2,3]], index=pd.PeriodIndex(['19910905', '19910906'], freq='D'))\r\n    >>> df2 = pd.DataFrame()\r\n    >>> pd.concat([df1, df2], axis=1, keys=['a', 'b'])\r\n    ValueError: can only call with other PeriodIndex-ed objects\r\n    >>> pd.concat([df2, df1], axis=1, keys=['a', 'b'])\r\n    Works!\r\n\r\nAdditionally (and perhaps this should be raised as a separate issue) should the specific implementation of `pd.PeriodIndex._assert_can_do_setop` not raise if the `other` index is empty? Since `pd.Index([]).union(<instance of pd.PeriodIndex>)` results in an instance of `pd.PeriodIndex`."},{"labels":["api",null],"text":"There are a few places where we need an empty array-like of the same type+dtype as self.  e.g. in #23431 I used `self._data[:0]`.  A dedicated `empty()` method might pretty that up.\r\n\r\nhttps://github.com/pandas-dev/pandas/pull/23431#discussion_r229905762\r\n\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nindex = pd.MultiIndex.from_tuples(((0,1), (0, 2)))\r\nindex.set_levels(map(int, (3,)), level=0)\r\n```\r\n#### Problem description\r\nRaises\r\n```python\r\nTypeError: 'map' object is not subscriptable\r\n```\r\nWorkaround by wrapping map with list.\r\n\r\n#### Expected Output\r\nMultiIndex(levels=[[3], [1, 2]],\r\n           labels=[[0, 0], [0, 1]])\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.18.14-200.fc28.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.23.4\r\npytest: 3.8.2\r\npip: 10.0.1\r\nsetuptools: 40.4.3\r\nCython: None\r\nnumpy: 1.15.2\r\nscipy: 1.1.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 7.0.1\r\nsphinx: 1.8.1\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.5\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"#### keep=\"all\" as parameter option in `duplicated`\r\n\r\nMy view is that a more intuitive parametrization of the `duplicated` function would be that `keep='all'` should allow you to keep all, since `keep=False` kind of doesn't make any sense.\r\n\r\nI have started on a pull request but had not realised how tightly coupled the `drop_duplicates` and `duplicated` functions are (but it makes sense of course) - I guess this is why the choice of `keep=False` was made in the first place.\r\n\r\nI am of the view that `keep=\"all\"` in the `drop_duplicates` method should raise an error, because again, that would be counter-intuitive behaviour.\r\n"},{"labels":["api"],"text":"In DatetimeIndex/TimedeltaIndex arithmetic operations sometimes we return `type(self)(values, freq=\"infer\")` and other times we return `self._shallow_copy(values, freq=None)`.  I don't see any clear logic guiding when to do which.  Anyone else know?"},{"labels":["api",null],"text":"To split off the discussion on the constructors from https://github.com/pandas-dev/pandas/issues/23185, to have a more focussed discussion about that here. Also going further on the discussion we were having in https://github.com/pandas-dev/pandas/pull/23140/files#r225218594\r\n\r\nSo topic of this issue: how should the different constructors look like for the internal EAs and the Index classes based on those EAs (specifically for the datetimelike ones).\r\n\r\n### Index constructors\r\n\r\nI think the for the Index constructors, there is not *that* much discussion. \r\nWe have:\r\n\r\n- default `Index(..)` (`__new__` or `__init__`): this is quite overloaded for some of the index classes, but that's the way it is now since they are exposed to the user.\r\n- `_simple_new`: I think we agree that for those (from Tom's comment here https://github.com/pandas-dev/pandas/pull/23093#pullrequestreview-164015080), it should basically simply get the EA array and potentially a name:\r\n\r\n    ```\r\n    @classmethod\r\n    def _simple_new(cls, values, name=None):\r\n        # type: (Union[ndarray, ExtensionArray], Optional[Any]) -> Index\r\n        result = object.__new__(cls)\r\n        result._data = values\r\n        result.name = name\r\n        result._reset_identity()\r\n        return result\r\n    ```\r\n\r\n- `_shallow_copy` and `_shallow_copy_with_infer` might need another look to propose something.\r\n\r\n\r\n\r\n### Array constructors\r\n\r\nThe default Index constructors mix a lot of different things (which is what partly lead to the suite of other constructors), and I personally don't think this is something we necessarily need to repeat for the Array constructors. \r\n\r\nIn the discussion related to \r\n\r\nEach Array type might have it specific constructors (like we have `IntervalArray.from_breaks` and others), but I think that in the discussion we were having in https://github.com/pandas-dev/pandas/pull/23140/files#r225218594, there are 3 clearly defined use case that are generic for the different dtypes. Constructing from:\r\n\r\n1) physical values (ordinals + freq for Period, datetime64 + optional tz for Datetime, int ndarray + mask for IntegerArray)\r\n2) extension Array (i.e. accept itself)\r\n3) array of scalars (eg an object ndarray of Period or Timestamp objects)\r\n\r\nFor this last item, we already `_from_sequence` for exactly this as part of the EA interface. \r\n\r\nSo one option is simply accept all of those three things in the main Array `__init__`, another option is to have separate constructors for them. I think this is what the discussion is mainly about?\r\n\r\nI see the following advantages of keeping them separate (or at least keep the third item separate):\r\n\r\n- Code clarity throughout the Array implementation: To quote Tom from (https://github.com/pandas-dev/pandas/pull/23140#discussion_r225371045): \r\n  > From the WIP PeriodArray PR, I found that having to think carefully about what type of data I had forced some clarity in the code. I liked having to explicitly reach for that _from_periods constructor.\r\n- Keep concerns separated inside the constructor -> code clarity in the constructors itself\r\n- We already decided that we will not rely on the default constructor in the EA interface but rather have a specific `_from_sequence`, `_from_factorized`, so we cannot use it anyway in places that need to deal with EAs in general\r\n\r\nAlso note that this is basically what we have for the new IntegerArray. It's `__init__` only accepts a ndarray of integers + mask, and there is a separate function `integer_array` that provides a more general purpose constructor (from list, from floats, detecting NaNs as missing values, etc ..), which is then used in `_from_sequence`.\r\n\r\ncc @TomAugspurger @jreback @jbrockmendel "},{"labels":["api",null,null,null],"text":"#### Problem description\r\nWhen creating a `DatetimeIndex` partially-bounded, i.e., only specifying either `start` or `end` the use of a `closed` parameter different from `None` was intended to be disallowed. This behaviour is bugged as shown in #23198. #23199 solves this bug.\r\n\r\nBefore #23199, the DatetimeIndex was successfully created. The result contained one less than the requested `periods` in some cases as shown in #23176.\r\n\r\nAfter #23199, a `ValueError` will be raised as it seemed to be intended.\r\n\r\nThe proposal is to allow `closed` parameters different to `None` for partially-bounded `DatetimeIndex`es. The behaviour should be as defined in the following table\r\n\r\nPeriods | Ommited parameter | Closed | Current number of items | After #23199 | Proposed number of items\r\n-|-|-|-|-|-\r\nN | `end` | `None` | N | N | N\r\nN | `end` | `\"left\"` | N-1 | `ValueError` | N\r\nN | `end` | `\"right\"` | N | `ValueError` | N\r\nN | `start` | `None` | N | N | N\r\nN | `start` | `\"left\"` | N | `ValueError` | N\r\nN | `start` | `\"right\"` | N-1 | `ValueError` | N"},{"labels":["api"],"text":"Lets say that I have a Pandas dataframe with an index name that overlaps with a column name:\r\n\r\n    import pandas as pd\r\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]}, index=[4, 5 ,6])\r\n    df.index.name = 'a'\r\n    df\r\n    \r\n    \r\n       a  b\r\n    a      \r\n    4  1  4\r\n    5  2  5\r\n    6  3  6\r\n\r\nAnd I sort by the column name `'a'`\r\n\r\n    >>> df.sort_values(by=['a'])\r\n    FutureWarning: 'a' is both an index level and a column label.\r\n    Defaulting to column, but this will raise an ambiguity error in a future version\r\n\r\n       a  b\r\n    a      \r\n    4  1  4\r\n    5  2  5\r\n    6  3  6\r\n\r\nThe warning makes total sense.  I understand what is going on.  However, I'm not sure how I should improve the situation to make it safe.  Ideally I'm looking for something like \r\n\r\n    df.sort_values(by=['a'], i_really_mean_columns=True)\r\n\r\nIs there a way to do this?\r\n\r\nOne answer is \"don't have dataframes with overlapping names\" but unfortunately I'm not entirely in charge of these names (my users are).  Another answer is \"temporarily rename your index, then name it back when you're done\" but this opens me up to some concurrency/threading safety issues.  Currently my best approach is to copy the dataframe, rename the index, do the operation."},{"labels":["api",null,null,null],"text":"Right now `SparseArray.astype(numpy_dtype)` is sparse:\r\n\r\n```python\r\nIn [6]: a = pd.SparseArray([0, 1, 0, 1])\r\n\r\nIn [7]: a.astype(np.dtype('float'))\r\nOut[7]:\r\n[0, 1.0, 0, 1.0]\r\nFill: 0\r\nIntIndex\r\nIndices: array([1, 3], dtype=int32)\r\n```\r\n\r\nThis is potentially confusing. I did it to match the behavior of SparseSeries, but we may not want that."},{"labels":["api",null,null,null],"text":"This has confused me for a while, but we apparently (sometimes?) allow a `fill_value` whose dtype is not the same as `sp_values.dtype`\r\n\r\n```python\r\nIn [20]: a = pd.SparseArray([1, 2, 3], fill_value=1.0)\r\n\r\nIn [21]: a\r\nOut[21]:\r\n[1.0, 2, 3]\r\nFill: 1.0\r\nIntIndex\r\nIndices: array([1, 2], dtype=int32)\r\n\r\nIn [22]: a.sp_values.dtype\r\nOut[22]: dtype('int64')\r\n```\r\n\r\nThis can lead to confusing behavior when doing operations.\r\n\r\nI suspect a primary motivation was supporting sparse integer values with `NaN` for a fill value. We should investigate what's tested, part of the API, and useful to users."},{"labels":["api",null,null],"text":"```\r\npd.TimedeltaIndex(['1D']) == 1\r\n[...]\r\nTypeError: cannot compare a TimedeltaIndex with type int\r\n\r\npd.TimedeltaIndex(['1D']) == 1.\r\n[...]\r\nTypeError: cannot compare a TimedeltaIndex with type float\r\n```"},{"labels":["api",null],"text":"The name of the function `is_list_like` from `pandas.core.dtypes.common` suggests that whatever gets `True` returned will be \"like a list\". As such, it's used in a handful of places - e.g. I got asked to use it in #20347, and now again in #22486. What I found out in the latter one is that it's true for sets:\r\n\r\n```\r\n>>> from pandas.core.dtypes.common import is_list_like\r\n>>> is_list_like({1, 2, 3})\r\nTrue\r\n```\r\n\r\nThis has some uncomfortable consequences - for `str.cat` it's a bug (#23009), and for `df.set_index` it would be too.\r\n\r\n@jreback asked me to try out removing `set` from `is_list_like` (https://github.com/pandas-dev/pandas/pull/22486#issuecomment-428159769), so this issue is following up on that.\r\n> @h-vetinari why don't you try (separate PR) excluding set from is_list_like and see what the implications of that are.\r\n\r\nThere are some rare cases like `.isin`, where sets should also be included. I'd say to have a looser definition `is_set_like` that's basically `is_list_like or set`. Alternatively, one could think of `is_list_like(strict=False)` to include sets.\r\n\r\nAnother question is if this needs a deprecation cycle and how?"},{"labels":["api",null,null],"text":"This supersedes #22721.\r\n\r\nPandas is trying to straddle many different chasms, which leads to undesirable behaviour on the fringes. For the purpose of this issue, I'm talking mainly about\r\n1. supporting python 2/3 (will be over soon...)\r\n1. being largely based on numpy's type system\r\n\r\nFrom the first point, we have the inconsistent handling of str vs. bytes, so having the Series-concatenator work with bytes is a necessity in Python 2.\r\n\r\nMostly due to the second point, there's no proper string dtype, it's just hiding in the `object` dtype. I started #22721 as a side issue which came up while refactoring in #22725. Then I got told that:\r\n> We do NOT handle bytes in `.str` if you want to add tests and raise, pls do so, but not going to 'make it work better'. It is amazingly confusing and causes all sorts of errors. We probably don't have explicit checks on this (though I *thought* that we always infer on the strings that must be string/unicode and *never* bytes).\r\n\r\nHowever, it works already -- the `Series.str`-accessor already checks that it can only be called on an object column, but there's not much more it can do (not least because inspecting every element of a Series would be very performance-intense). Consequently, `.str.cat` currently *does* work on bytes data, and easily at that:\r\n```\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> s = pd.Series(np.array(list('abc'), 'S1').astype(object))\r\n>>> t = pd.Series(np.array(list('def'), 'S1').astype(object))\r\n>>> s.str.cat(t, sep=b'')\r\n0    b'ad'\r\n1    b'be'\r\n2    b'cf'\r\ndtype: object\r\n>>> s.str.cat(t, sep=b',')\r\n0    b'a,d'\r\n1    b'b,e'\r\n2    b'c,f'\r\ndtype: object\r\n```\r\n\r\nLong story short - this issue supersedes #22721, and should serve as a long term goal to disable `.str` once Python 2 gets dropped and/or there is a string dtype."},{"labels":["api",null],"text":"In #20347, I was asked to allow list-like inputs for the `.str.cat` methods for Series. However, I didn't know that `is_listlike({'a', 'b', 'c'})` is True (for sets) until I stumbled over it in #22486.\r\n\r\nThese objects should imo clearly be disallowed, lest users shoot themselves in the foot massively:\r\n```\r\n>>> s = pd.Series(['a', 'b', 'c'])\r\n>>> s.str.cat({'a', 'b', 'c'})\r\n0    ac\r\n1    bb\r\n2    ca\r\ndtype: object\r\n```\r\n\r\nI'm not even sure this should go through a deprecation cycle - IMO it's just a plain ol' bug that should be fixed ASAP."},{"labels":["api",null],"text":"I frequently come into situations where I have to break up piping, because piping doesn't work on attributes:\r\n\r\n```python\r\n>>> new_df = (df.groupby(...)\r\n...             .pipe(...)\r\n>>> new_df.index.name = 'index_name'\r\n>>> new_df.columns = pd.CategoricalIndex(new_df.columns)\r\n>>> new_df = new_df.pipe(...)  # and so on...\r\n```\r\n\r\nI think it would be cleaner if NDFrame had a ``setattr`` method that returns ``self``. Then the above would become:\r\n\r\n```python\r\n>>> new_df = (df.groupby(...)\r\n...             .pipe(...)\r\n...             .setattr('index.name', 'index_name')\r\n...             .setattr('columns', lambda x: pd.CategoricalIndex(x.columns))\r\n...             .pipe(...)  # and so on...\r\n...             )\r\n```\r\n\r\nThe first argument to ``.setattr`` is a string and can be dot-seperated, and the second parameter is called with ``self`` as its first argument if a callable.\r\n\r\nI think this would make piping cleaner in many cases.\r\n\r\nOpinions?"},{"labels":["api",null,null],"text":"Hello,\r\nInitially the string methods, like replace, lower, zfill, strip etc etc.. are restricted to Series use only.\r\nIt would be good if a parameter is put to use it on data frames too. Methods like strip won't affect numeric columns since they wount be having spaces already. But if there is a method which can affect a numeric column, it can be excluded using exclude parameter (which should be added). \r\nA simple way of doing it is demonstrated below. \r\n\r\n```\r\nimport pandas as pd\r\ndata=pd.read_csv(\"nba.csv\")\r\ndata\r\ndtypes=data.dtypes.astype(str)\r\nfor columns in data.columns:\r\n    data[columns]=data[columns].astype(str)\r\n    data[columns]=data[columns].str.replace(\" \",\"\")\r\n    data[columns]=data[columns].astype(dtypes[columns])\r\ndata\r\n```\r\nIn this example, the method is working fine with Series of all dtypes. And after successfully applying method, the columns are converted back to their original dtype.\r\n\r\nIf this issue is approved, I would like to work and contribute to this feature."},{"labels":["api",null,null],"text":"From the dev-chat https://github.com/pandas-dev/pandas/issues/22274#issuecomment-425211774 and a rehash of https://github.com/pandas-dev/pandas/issues/20633\r\n\r\nConclusion: \r\n\r\n- Make the offset alias `'D'` and `offsets.Day` always operate as a calendar day.\r\n\r\nTechnical implication:\r\n* `Day` will need to subclass `DateOffset` instead of `Tick`\r\n\r\nOperations that will change behavior (may be missing some):\r\n1. `Day` arithmetic with Timestamp/DTI/datetime Series/DataFrame\r\n2. `DatetimeIndex.shift`\r\n3. Any use of `'D'` with Timedelta/TDI/timedelta Series/DataFrame\r\n4. Tick arithmetic with `Day`\r\n\r\nDeprecation Procedures (could use some input here)\r\n1. None\r\n2. None\r\n3. `DeprecationWarning` that users should use `'24 H'` instead of `'D'` and allow users to use `'D'` for v0.24.0\r\n4.  None\r\n\r\ncc @pandas-dev/pandas-core "},{"labels":["api",null,null,null],"text":"These occur naturally with `.groupby(extensionarray)` and `Seres[extensionarray].value_counts`. We should define a public API so that we don't have to convert to object in these cases.\r\n\r\nYou'd likely end up with an `Index` with a non-object dtype.\r\n\r\n---\r\n\r\n*more radically* this kind of removes the *need* for all our Index subclasses, aside from MultiIndex. But we can think about that separately from the interface.\r\n\r\nI don't think this is blocking for 1.0, but it may not be too much effort."},{"labels":["api",null],"text":"This mostly affects \r\n\r\n1. IntervalArray\r\n2. PeriodArray\r\n3. DatetimeArray (and maybe TimedeltaArray if we do that)\r\n4. SparseArray\r\n5. IntegerArray\r\n\r\nCategorical is already public, so let's leave that out.\r\n\r\n---\r\n\r\nA few questions\r\n\r\n1. do we allow users to construct these directly (via a set of `to_*_array` methods, or a top-ish-level `pd.array([...], dtype)` method)?\r\n2. Do users see these when they do `.values` or any operation returning an array (`.unique`, probably others)?"},{"labels":["api",null],"text":"#### Problem description\r\n\r\nInitializing a dataset using a `set` type of object results in pandas building a cartesian product. Which isn't great b/c:\r\n\r\n- It's totally unexpected\r\n- it's extremely inefficient and at first will just block the console until you figure out why\r\n- it's not mentioned in the docs, it's not part of the pandas cheatsheet, most of the SO answers, etc. E.g. it's **astounishing**.\r\n\r\n#### Example\r\n```python\r\nimport pandas as pd\r\n\r\ncol1 = set([1, 2, 3])\r\ncol2 = [2, 3, 4]\r\npd.DataFrame({\r\n    'col1': set(col1),\r\n    'col2': col2\r\n})\r\n#         col1  col2\r\n# 0  {1, 2, 3}     2\r\n# 1  {1, 2, 3}     3\r\n# 2  {1, 2, 3}     4\r\n#\r\n# ^ wait, what?...\r\n```\r\n\r\n#### Expected Behaviour:\r\n```python\r\npd.DataFrame({\r\n    'col1': list(col1),\r\n    'col2': col2\r\n})\r\n#    col1  col2\r\n# 0     1     2\r\n# 1     2     3\r\n# 2     3     4\r\n```\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.15.0-24-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\npandas: 0.23.1\r\npytest: 3.7.1\r\npip: 18.0\r\nsetuptools: 39.1.0\r\nCython: 0.28.3\r\nnumpy: 1.14.5\r\nscipy: 1.1.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.4.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.4.4\r\nnumexpr: 2.6.5\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: 1.2.8\r\npymysql: None\r\npsycopg2: 2.7.5 (dt dec pq3 ext lo64)\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"Is this required? Right now https://github.com/pandas-dev/pandas/blob/a03d9535b16a6d5441334ef2e698d72778cf8115/pandas/core/algorithms.py#L708-L711 assumes that it's implemented, but we don't document / implement it by default, so we end up with an AttributeError at runtime."},{"labels":["api",null],"text":"The state of the various flavours of `.unique` as of `v0.23`:\r\n- `[pd/Series/Index].unique` does not have `keep`-kwarg\r\n- `Series.unique` returns array, `Series.drop_duplicates` returns `Series`. Returning a plain `np.ndarray` is quite unusual for a `Series` method, and furthermore the differences between these closely-related methods are confusing from a user perspective, IMO\r\n- same point for `Index`\r\n- `DataFrame.unique` does not exist, but is a much more natural candidate (from the behaviour of numpy, resp. `Series/Index`) than `.drop_duplicates`\r\n- `pd.unique` chokes on 2-dimensional data\r\n- no `return_inverse`-kwarg for any of the `.unique` variants; see #4087 (milestoned since 0.14), #21357\r\n\r\nI originally wanted to add `df.unique(..., return_inverse=True|False)` for #21357, but got directed to add it to `duplicated` instead. After slow progress over 3 months in #21645 (PR essentially finished since 2), @jorisvandenbossche brought up the - justified (IMO) - feedback that: \r\n> I think my main worry is that we are adding a `return_inverse` keyword which actually does not return the inverse for that function (it does return the inverse for another function), and that it is in name similar to numpy's keyword, but in usage also different.\r\n\r\nand\r\n> [...] it might make sense to add this to `pd.unique` / `Series.unique` as well? (not necessarily at the same time; or might actually be an easier starter)\r\n\r\nThis prompted me to have another look at the situation with `.unique`, and I found the list of the above inconsistencies. To resolve them, I suggest to:\r\n- [ ] Change return type for `[Series/Index].unique` to be same as caller (deprecation cycle by introducing `raw=None` which at first defaults to True?)\r\n- [ ] Add `keep`-kwarg to `[Series/Index].unique` (make `.unique` a wrapper around `.drop_duplicates`?)\r\n- [ ] Add `df.unique` (as thin wrapper around `.drop_duplicates`?)\r\n- [ ] Add `keep`-kwarg to `pd.unique` and dispatch to `DataFrame/Series/Index` as necessary\r\n- [ ] Add `return_inverse`-kwarg to all of them (and add to EA interface); under the hood by exposing the same kwarg to `duplicated` and `drop_duplicates` as well\r\n- [ ] (something for later) solve #21720 (treatment of `np.nan/None` in `df.duplicated` inconsistent vs. Series behaviour)\r\n\r\nEach point is essentially self-contained and independent of the others, but of course they make more sense together.\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\ncurrent     -> query_config[\"aggregation\"] = \"5min\"\r\nproposed -> query_config[\"aggregation\"] = \"PT5M\"\r\n\r\nresampled_dataframe = dataframe.resample(query_config[\"aggregation\"], how = query_config[\"aggregation_type\"], closed = query_config[\"closed\"])\r\n```\r\n\r\n#### Problem description\r\n\r\nhttps://en.wikipedia.org/wiki/ISO_8601#Durations\r\n\r\nCurrent behavior uses non standardized markup for time duration, wich can cause errors and confusion. Implementing ISO8601 duration grantees standardized solution for time duration in PANDAS. It does not have to replace the current system rather extend it.\r\n\r\n\r\n"},{"labels":["api",null],"text":"The state of `update/combine_first` in `v0.23`:\r\n* `.update` signature does not match between DataFrame/Series (#22358)\r\n* `df.update` has a `join`-kwarg that only supports `left`, although the source code itself notes:\r\n  ```# TODO: Support other joins``` (#21855)\r\n* `.update` is one of the (very) few pandas-methods that's inplace by default, but does not have an `inplace`-kwarg (#22286)\r\n* `.combine_first` is effectively (the not-yet-implemented) `.update(join='outer')`, has an awkward, non-standard name, and much fewer capabilities than `.update`.  (#21859)\r\n\r\nI tried to make some steps towards #21855 and #21859 by adding an `inplace`-kwarg to `df.update` in #22286, which has been stalled in discussion whether `update` should ever be inplace at all, resp. how to move away from inplacing generally.\r\n\r\nToday, some headway was made with the comment by @jreback:\r\n> So we have `.update` (in-place defaults) and `.combine_first` which is not very standard terminology.\r\nIn an ideal world I think adding `.coalesce` is probably the right thing to do (does R use this term?).\r\nwhich is basically a rename of `.combine_first`, and deprecate `.update`.\r\n\r\nwhich I'm strongly in favour of (with the caveat that it should use the capabilities of `update`; I suggested something similar in #21855; would also solve most of the discussion there). And yes, dplyr uses \"coalesce\", which itself is inspired by SQL: https://cran.r-project.org/web/packages/dplyr/dplyr.pdf#page.15\r\n\r\nThis discussion is opened on the advice of @jreback, who would like to involve:\r\n> [...] to get some more commentary on this, esp from @jorisvandenbossche and @TomAugspurger (and some off-line discussions that I had with @cpcloud )\r\n\r\nAlso tagging the other participants of #21855: @gfyoung @toobaz \r\n\r\nSumming up this proposal:\r\n1. Add `.coalesce` to `generic.py`, à la:\r\n ```def coalesce(self, other, join='left', overwrite=True, filter_func=None, raise_conflict=False):```\r\n which is _not_ inplace and inherited by DataFrame/Series\r\n1. support different joins, at least: `join='left'|'outer'|'inner'|'right'` (most of the discussion in #21855 is about potentially allowing different joins for different axes, and which keywords to use for that).\r\n1. (potentially; not essential to the proposal) slowly deprecate `.update` and `.combine_first`"},{"labels":["api",null,null,null,null],"text":"Sometimes we want to apply background_gradient to some columns according to values in another column. There's no easy way to do this currently. It can be made possible by adding an optional values parameter. And when the values parameter is specified, it is used to generate background colors. \r\n"},{"labels":["api",null],"text":"I recently had a colleague that ran into a nasty bug, due to the fact that on Python >= 3.6 we no longer sort the columns when creating a DataFrame from a dict.\r\n\r\nEg on Python 3.5:\r\n\r\n```\r\nIn [1]: df = pd.DataFrame({'B': [1, 2], 'A': [3, 4]})\r\n\r\nIn [2]: df\r\nOut[2]: \r\n   A  B\r\n0  3  1\r\n1  4  2\r\n```\r\n\r\nOn Python 3.6/3.7 this would keep the column order as `['B', 'A']`. \r\n\r\nIf you write code that depends on the ordering of columns, like `df.columns = ['other', 'name']` (not the most robust way, but people do that), or like `df.iloc[..]`, you will silently get different results, leading to bugs.\r\n\r\n---\r\n\r\nOpening the issue mainly to discuss to see if we can somewhat alleviate such problem and what other people think (did other people similar problems), not necessarily to propose to completely revert the change (that would also be annoying given that is already out for some time).\r\n\r\nBut we could think about raising a warning in the specific case of using dict on Python >= 3.6 where the sorted column order and dict keys order does not match. \r\nBut the annoying thing is that there is no easy way then for people to silence the warning, apart from adding a new keyword we then later would deprecate itself.\r\n\r\ncc @topper-123 @TomAugspurger \r\n\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nprint(\"Pandas version :  \", pd.__version__)\r\n\r\ndef print_dict(fn):\r\n    print(fn.__name__)\r\n    print(fn(dict(C=[3], B=[2], A=[1])))\r\n\r\nwith pd.option_context('display.max_rows', 10, 'display.max_columns', 5):\r\n    print_dict(pd.DataFrame)\r\n    print_dict(pd.DataFrame.from_dict)\r\n    print_dict(pd.DataFrame.from_records)\r\n```\r\n#### Problem description\r\n\r\npandas 0.22 gave consistent output when creating a DataFrame passing a dictionary using any of `DataFrame` constructor, `DataFrame.from_dict` and `DataFrame.from_records`. pandas 0.23 changed the behaviour of creating from dictionaries to respect key ordering in Python version 3.6+ for both the `DataFrame` constructor and `DataFrame.from_dict` but not using `DataFrame.from_records`. \r\n\r\n\r\n#### Output 0.23\r\nDifferent ordering of columns depending on construction method\r\n\r\n```\r\nPandas version :   0.23.4\r\nDataFrame\r\n   C  B  A\r\n0  3  2  1\r\nfrom_dict\r\n   C  B  A\r\n0  3  2  1\r\nfrom_records\r\n   A  B  C\r\n0  1  2  3\r\n```\r\n\r\n#### Output 0.22\r\nConsistent ordering of columns (sorted keys)\r\n\r\n```\r\nPandas version :   0.22.0\r\nDataFrame\r\n   A  B  C\r\n0  1  2  3\r\nfrom_dict\r\n   A  B  C\r\n0  1  2  3\r\nfrom_records\r\n   A  B  C\r\n0  1  2  3\r\n```\r\n\r\n#### Expected Output (my local build including #22687)\r\nConsistent ordering of columns (dict key order respected)\r\n\r\n```\r\nPandas version :   0.24.0.dev0+570.g434910bcb\r\nDataFrame\r\n   C  B  A\r\n0  3  2  1\r\nfrom_dict\r\n   C  B  A\r\n0  3  2  1\r\nfrom_records\r\n   C  B  A\r\n0  3  2  1\r\n```\r\n"},{"labels":["api",null,null],"text":"In lots of places, pandas does something like `if np.any(arr.isna())`, which is wasteful as we have to create an ndarray of booleans just to check whether there are any `True` values.\r\n\r\nSome arrays, like Arrow, know ahead of time whether there are any NAs in the array. Would it make sense to expose an API for an array saying whether they have any missing values?\r\n\r\nWith indexes, we work around this by caching a `_hasnans` value. That wouldn't work for mutable arrays."},{"labels":["api",null],"text":"From discussion started in #22639:\r\n\r\nCurrently, `pd.read_csv` has two booleans\r\n\r\n* **error_bad_lines** : boolean, default True\r\n  Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these “bad lines” will dropped from the DataFrame that is returned.\r\n* **warn_bad_lines** : boolean, default True\r\n  If error_bad_lines is False, and warn_bad_lines is True, a warning for each “bad line” will be output.\r\n\r\nThis is confusing (what happens if both are `True`), and not in line with other `errors`-kwargs that are all around the place. Clearer would be something like: `error_bad_lines = {'raise'|'warn'|'ignore'}`, and removing `warn_bad_lines`."},{"labels":[null,"api",null,null,null],"text":"*This ticket is an outgrowth of a discussion in pull request #22587*\r\n\r\nBy [my rough count](https://docs.google.com/spreadsheets/d/1h_iQ5Pexs5SuCT2pfbN8OH6ld6ZyvWqTEaH2W-xkEJE/edit?usp=sharing), the [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) method has nearly 50 keyword arguments.\r\n\r\nOf those, 32 arguments are made up or two or more words. Twenty of those multi-word arguments use an underscore to mark the space between words, like  `skip_blank_lines` and `parse_dates`. Twelve do not, like `chunksize` and `lineterminator`. \r\n\r\nIt is my opinion this is a small flaw in pandas' API, and that the library would benefit by standardizing how spaces are handled. It would make pandas more legible and consistent, and therefore easier for users of all experience levels.\r\n\r\nI have taught pandas to dozens of newbies across the country and I can testify from experience that small variations in the naming style of commonly used methods introduces unnecessary frustration, and \r\ncan even reduce user confidence in the quality of the overall product.\r\n\r\nAs a frequent user of pandas, I can also attest that the inconsistencies require me, someone who uses the library daily, to routinely consult the documentation to ensure I use the proper kwarg naming style.\r\n\r\nI am sympathetic to the desire to maintain backwards compatibility, which I believe could be managed with deprecation warnings that, if included, could be temporary, and ultimately removed in a future version, much in the way `sort_values` was introduced.\r\n\r\nSince the underscore method of handling word breaks is more common and more legible, I propose it be adopted. All existing multi-word arguments without an underscore would need to be modified. You can find an experimental patch of the `skiprows` kwargs, and considerable support from other users for pursuing this type of change, in #22587.\r\n\r\nIf that pull request is ultimately merged, and the maintainers agree with the larger goal I've tried to articulate here, I would be pleased to lead an effort to expand whatever design pattern is agreed upon to other keyword arguments across the library."},{"labels":["api",null,null,null],"text":"I'm very often working with `df.groupby.apply()`, and there are many confusing (sometimes wrong) aspects about the behaviour of the output, particularly regarding what happens with the index of the output. `v.0.23` cleaned up big parts of the `apply` API, but there's still a lot left...\r\n\r\nIdeally, I wish there'd be a sort of matrix (not necessarily in the following form) in the documentation - and implemented by the API - along the following lines\r\n\r\nFor `as_index=True`:\r\n```\r\nfunction output   |  result type  |  (multi-)index levels |  groupby-cols  |  columns\r\n--------------------------------------------------------------------------------------------\r\nscalar            |    Series     |    groupby-columns    |      n/a       |  none\r\nSeries            |   DataFrame   |    groupby-columns    |     dropped    |  index (union) of Series\r\nDataFrame         |   DataFrame   |   gb-cols + df.index  |     dropped    |  columns (union) of DFs\r\nnp.ndarray 1-dim  |   DataFrame   |  to dicuss / raise ?  |      n/a       |  to dicuss / raise ?\r\nnp.ndarray 2-dim  |   DataFrame   |  to dicuss / raise ?  |      n/a       |  to dicuss / raise ?\r\nIndex             |  MultiIndex?  |   gb-cols + output    |      n/a       |  n/a\r\n```\r\nFor `as_index=False`:\r\n```\r\nfunction output   |  result type  |  (multi-)index levels |  groupby-cols  |  columns\r\n--------------------------------------------------------------------------------------------\r\nscalar            |   DataFrame?  |      RangeIndex       |      n/a       |  gb-cols + output?\r\nSeries            |   DataFrame   |      RangeIndex       |      kept      |  gb-cols + index of Series?\r\nDataFrame         |   DataFrame   |  to dicuss / raise ?  |      kept      |  gb-cols + columns of DFs\r\nnp.ndarray 1-dim  |   DataFrame   |  to dicuss / raise ?  |      n/a       |  to dicuss / raise ?\r\nnp.ndarray 2-dim  |   DataFrame   |  to dicuss / raise ?  |      n/a       |  to dicuss / raise ?\r\nIndex             |    Series?    |  to dicuss / raise ?  |      n/a       |  n/a\r\n```\r\n\r\nCurrently, the behaviour is much, much more complicated / inconsistent / wrong. I'm trying to fill corresponding tables with the current behaviour and some issue xrefs, but it's by far not complete yet:\r\n\r\nFor `as_index=True`:\r\n```\r\nfunction output   |  result type  |  (multi-)index levels |  groupby-cols  |  columns\r\n--------------------------------------------------------------------------------------------\r\nscalar            |    Series     |    groupby-columns    |      n/a       |  none\r\nSeries (same idx) |   DataFrame   |    groupby-columns    |     kept?!     |  index of Series\r\nSeries (diff idx) |    Series?!   |  gb-cols + output.idx |      n/a       |  none?!\r\ngroup as-is       |   DataFrame   |    original index?!   |     kept?!     |  original columns\r\ngroup selection   |   DataFrame   |  gb-cols + output.idx |     kept?!     |  original columns\r\nDataFrame         |   DataFrame   |  gb-cols + output.idx |      n/a       |  columns (union) of DFs\r\nnp.ndarray 1-dim  |    Series?!   |   groupby-columns     |      n/a       |  none\r\nnp.ndarray 2-dim  |    Series?!   |   groupby-columns     |      n/a       |  none\r\nIndex             |    Series?!   |   groupby-columns     |      n/a       |  none #22541\r\n```\r\nFor `as_index=False`:\r\n```\r\nfunction output   |  result type  |  (multi-)index levels |  groupby-cols  |  columns\r\n--------------------------------------------------------------------------------------------\r\nscalar            |    Series     |      RangeIndex       |      n/a       |  none\r\nSeries (same idx) |   DataFrame   |      RangeIndex       |     kept       |  index of Series\r\nSeries (diff idx) |    Series?!   | RngIdx + output.idx?! |      n/a       |  none?!\r\ngroup as-is       |   DataFrame   |    original index?!   |     kept       |  original columns\r\ngroup selection   |   DataFrame   | RngIdx + output.idx?! |     kept       |  original columns\r\nDataFrame         |   DataFrame   | RngIdx + output.idx?! |      n/a       |  columns (union) of DFs\r\nnp.ndarray 1-dim  |    Series?!   |      RangeIndex       |      n/a       |  none\r\nnp.ndarray 2-dim  |    Series?!   |      RangeIndex       |      n/a       |  none\r\nIndex             |    Series?!   |      RangeIndex       |      n/a       |  none #22541\r\n```\r\n\r\nSome xrefs: #20420, #22541, #22542, #22546"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pytest # Not run via pytest -- just used for exception testing\r\n\r\ndef create_df(total, index=None):\r\n  dma = [501, 501, 501, 501, 501, 501, 502, 502, 502, 502, 502, 502]\r\n  size = [1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2]\r\n  age = ['20-25', '30-35', '40-45', '20-25', '30-35', '40-45',\r\n         '20-25', '30-35', '40-45', '20-25', '30-35', '40-45']\r\n  df = pd.DataFrame()\r\n  df['dma'] = dma\r\n  df['size'] = size\r\n  df['age'] = age\r\n  df['total'] = total\r\n\r\n  df10 = df.copy()\r\n  df10.total = 10 * df.total\r\n\r\n  df.set_index(index, inplace=True)\r\n  return df\r\n\r\ndef run_test(index, value, use_df_index, expected_exception=None, expected_df=None):\r\n  total = np.array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],)\r\n  df = create_df(total, index)\r\n  df_10 = create_df(10 * total, index)\r\n\r\n  def run():\r\n    if use_df_index:\r\n      df.loc[df.index==value, 'total'] = df_10.loc[df_10.index==value, 'total']\r\n    else:\r\n      df.loc[value, 'total'] = df_10.loc[value, 'total']\r\n\r\n  if expected_exception:\r\n    with pytest.raises(expected_exception):\r\n      run()\r\n  else:\r\n    run()\r\n    pd.testing.assert_frame_equal(df, expected_df)\r\n\r\n\r\ntotal = np.array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],)\r\n\r\nexpected_df = create_df(\r\n  np.array([10., 10., 10., 1., 1., 1., 10., 10., 10., 1., 1., 1.],),\r\n  'size')\r\nrun_test('size', 1, False, expected_exception=ValueError)  # A1\r\nrun_test('size', 1, True, expected_df=expected_df)         # A2 *\r\nrun_test('size', (1,), False, expected_df=expected_df)     # B1 *\r\nrun_test('size', (1,), True, expected_df=expected_df)      # B2 *\r\n\r\nexpected_df = create_df(\r\n  np.array([10., 1., 1., 10., 1., 1., 10., 1., 1., 10., 1., 1.],),\r\n  'age')\r\nWRONG_DF = create_df(total, 'age')\r\nrun_test('age', '20-25', False, expected_exception=ValueError)     # A1\r\nrun_test('age', '20-25', True, expected_df=expected_df)            # A2 *\r\nrun_test('age', ('20-25',), False, expected_exception=ValueError)  # B1\r\nrun_test('age', ('20-25',), True, expected_df=WRONG_DF)            # B2\r\n\r\nexpected_df = create_df(\r\n  np.array([10., 1., 1., 1., 1., 1., 10., 1., 1., 1., 1., 1.],),\r\n  ['size', 'age'])\r\nrun_test(['size', 'age'], (1, '20-25'), False, expected_df=expected_df)    # B1 *\r\nrun_test(['size', 'age'], (1, '20-25'), True, expected_exception=KeyError) # B2\r\n```\r\n#### Problem description\r\n\r\nWhen assigning via the loc parameter, I'm running into issues with using a string index.  The example shows various attempts at using loc to assign with different indices: a single int column, a single string column, and a two-column index.\r\n\r\nThe variations of attempts are commented as:\r\n- for single column indexes, use a flat value (A) or a tuple (B).  Multi-column indexes only use tuple (B).\r\n- use `df.loc[df.index==value, column]` (1) vs `df.loc[value, column]` (2)\r\n\r\n#### Expected Output\r\n\r\nI'd like to use a single variation for all index types (but it seems no single method works).  Ideally, it would be 'B2', but that does not work for a string-based index. \r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.0.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.7.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.23.4\r\npytest: 3.7.2\r\npip: 18.0\r\nsetuptools: 40.2.0\r\nCython: None\r\nnumpy: 1.15.1\r\nscipy: None\r\npyarrow: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.5\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: 1.2.11\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"Using this for sparse in https://github.com/pandas-dev/pandas/pull/22325, it'd be good to have generally."},{"labels":["api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n# two frames w/similar indexes\r\ndf_1 = pd.DataFrame(np.eye(4))\r\ndf_2 = pd.DataFrame(np.eye(4)*-1)\r\n\r\n# reverse, sort -> indexes are kept reversed\r\ndf_1.iloc[::-1].align(df_2.iloc[::-1], axis=0, join=\"outer\")\r\n\r\n# one frame has a different index\r\ndf_2 = pd.DataFrame(np.eye(4) * -1, index=range(1, 5))\r\n\r\n# reverse, sort -> indexes are sorted\r\ndf_1.iloc[::-1].align(df_2.iloc[::-1], axis=0, join=\"outer\")\r\n\r\n```\r\n#### Problem description\r\n\r\nInconsistent behavior, makes it impossible to use structures of the form:\r\n```\r\ndef func(x, arg):\r\n    if arg < 0:\r\n        return func(arg.iloc[::-1])\r\n    return ...\r\n```\r\nas the order gets switched without user's consent.\r\n\r\n#### Expected Output\r\nindexes are kept in the original order \r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\npandas: 0.23.1\r\npytest: 2.9.2\r\npip: 9.0.1\r\nsetuptools: 23.0.0\r\nCython: 0.24\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\npyarrow: None\r\nxarray: 0.9.3\r\nIPython: 4.2.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.2.2\r\nnumexpr: 2.6.1\r\nfeather: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.2\r\nlxml: 3.6.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nsqlalchemy: 1.1.13\r\npymysql: 0.7.9.None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"The actual implementation of ``pandas.Index.shift`` for a datetime-like index takes a differently named parameter for the number of shifts than the base method.\r\n\r\nIt should be named ``periods``, which would also be consistent with the ``pandas.DataFrame.shift`` or ``pandas.Series.shift`` methods. But for a DatetimeIndex it's ``n``.\r\n\r\nSee:\r\nhttps://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/indexes/base.py#L2586\r\nvs.\r\nhttps://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/indexes/datetimelike.py#L1021\r\n\r\nWith a DatetimeIndex, following the documentation leads to a\r\n```\r\nTypeError: shift() got an unexpected keyword argument 'periods'\r\n```\r\n\r\nIf this is somehow intended, then it should be stated so in the docstring for ``pandas.Index.shift``.\r\n\r\npandas version is 0.23.4\r\n"},{"labels":["api",null],"text":"ATM `pd.Timestamp.utcnow()` returns a UTC-localized `Timestamp` object, but `pd.Timestamp.utcfromtimestamp()` does not.  As a result, the following round-trip fails:\r\n\r\n```\r\nunow = pd.Timestamp.utcnow()\r\nresult = pd.Timestamp.utcfromtimestamp(unow.timestamp())\r\nassert unow == result  # <-- TypeError because unow is tzaware and result is tznaive\r\n```\r\n\r\nChanging this would mean breaking with the stdlib, but our `utcnow` already breaks with the stdlib (see also #22450)"},{"labels":["api"],"text":"Right now, `Series.shift(0)` will just return the series. Shifting for all other periods induces a copy:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: a = pd.Series([1, 2])\r\n\r\nIn [3]: a.shift(1) is a\r\nOut[3]: False\r\n\r\nIn [4]: a.shift(0) is a\r\nOut[4]: True\r\n```\r\n\r\nShould we defensively copy on `0` as well, for a consistent user experience?\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/e669fae0762d901e61f7af84fc3b5181848d257d/pandas/core/generic.py#L8084-L8086"},{"labels":[null,"api",null],"text":"Currently, there's\r\n\r\n```DataFrame.update(other, join='left', overwrite=True, filter_func=None, raise_conflict=False)```\r\nand \r\n```Series.update(other)```\r\n\r\nI think Series should have the same keywords/capabilities, and ideally share the implementation, probably in `generic.py`. Relevant also in light of #22286 and #21855."},{"labels":["api",null,null],"text":"I would like to request a `pandas.Series.query()` method that works identically to [`pandas.DataFrame.query()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html).\r\n\r\nI have a large multi-indexed series that I would like to split into training and testing data for an ML research project. A minimal working example would be:\r\n```python\r\nimport pandas as pd\r\n\r\nyears = range(2002, 2018)\r\nfields = range(1, 5)\r\n\r\nindex = pd.MultiIndex.from_product(\r\n    [years, fields], names=['year', 'field'])\r\n\r\nseries = pd.Series(index=index)\r\n```\r\nWhat I would like to be able to do is split this series into 2010 data and _not_ 2010 data. Accessing 2010 data is very easy:\r\n```python\r\ntest_data = series[2010]\r\n```\r\nAccessing _not_ 2010 data is very hard. This is the shortest method I've found so far:\r\n```python\r\ntrain_data = series.to_frame().query('year != 2010')[0]\r\n```\r\nSince `pandas.Series` doesn't support `query()`, I have to convert it to a DataFrame and then back into a Series. Is there any reason why `pandas.Series` doesn't support `query` directly?"},{"labels":["api",null,null],"text":"Creeping up to this in https://github.com/pandas-dev/pandas/pull/22345\r\n\r\nA few questions\r\n\r\n1. What should this look like for EA authors? What helpers can / should we provide?\r\n2. How does this affect users? Specifically, assuming IntegerArray implements reductions, what's the dtype here?\r\n\r\n```python\r\nIn [11]: df = pd.DataFrame({\"A\": ['a', 'b', 'a'], \"B\": pd.core.arrays.IntegerArray([1, 2, 3])})\r\n\r\nIn [12]: df.groupby(\"A\").B.min().dtype\r\n```\r\n\r\nIs it `int64`, or should we preserve `Int64`?\r\n\r\nWe can also discuss transforms ( cumulative operations like cumsum, maybe things like `.shift`)."},{"labels":["api",null,null],"text":"#### Code Sample\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnp.random.seed(2357)\r\n\r\ndf = pd.DataFrame(np.random.normal(size=(100, 3)))\r\ns = pd.Series(np.random.randn(50))\r\n\r\ndf.corrwith(s)\r\n```\r\n#### Problem description\r\n\r\nIt might be desirable to have `corrwith` return a warning in situations like the above where the alignment of the two data sets is suspect (the two objects only happen to have an overlapping index).  Perhaps the warning should be triggered unless the two objects have the right shape *and* the indexes are equal?\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.5.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.23.0\r\npytest: 3.5.1\r\npip: 18.0\r\nsetuptools: 39.1.0\r\nCython: 0.28.2\r\nnumpy: 1.14.4\r\nscipy: 1.1.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.4.0\r\nsphinx: 1.7.4\r\npatsy: 0.5.0\r\ndateutil: 2.7.3\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.3\r\nnumexpr: 2.6.5\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: 2.5.3\r\nxlrd: 1.1.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 1.0.4\r\nlxml: 4.2.1\r\nbs4: 4.6.0\r\nhtml5lib: 1.0.1\r\nsqlalchemy: 1.2.7\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Problem description\r\n\r\nThe docs for ``DataFrame.groupby`` signature start with:\r\n```\r\nby : mapping, function, label, or list of labels\r\n    Used to determine the groups for the groupby.\r\n```\r\n... but the code assumes that lists of mappings or functions can also be passed, and this is also tested, although with limited enthusiasm:\r\nhttps://github.com/pandas-dev/pandas/blob/0370740034978d3a63d4b8e5e2c96ff54e7e08ba/pandas/tests/groupby/test_grouping.py#L667\r\n... and consistency (apparently that code path _is_ used somewhere else):\r\nhttps://github.com/pandas-dev/pandas/blob/0370740034978d3a63d4b8e5e2c96ff54e7e08ba/pandas/tests/groupby/test_grouping.py#L732\r\n\r\n#### Expected Output\r\n\r\nEither we disable/deprecate the possibility of passing lists of mappings, ore we document it.\r\n\r\nI guess the latter is the desired outcome, since the code does not support the feature \"by chance\". Still I wanted to double check with @pandas-dev/pandas-core because\r\n- it is not a killer feature, as it is really easy to pass a single lambda that does the same job of a list of mappings (and more, like applying different mappings to specific levels of the index)\r\n- removing it would allow us to simplify the code quite a bit (e.g. #22257 wouldn't have happened)\r\n- it is probably not much used\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-6-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.24.0.dev0+437.g33d70efb5\r\npytest: 3.5.0\r\npip: 9.0.1\r\nsetuptools: 39.2.0\r\nCython: 0.28.4\r\nnumpy: 1.14.3\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.5.0\r\ndateutil: 2.7.3\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.2.2.post1634.dev0+ge8120cf6d\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\ngcsfs: None\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"This is a proposal to replace #20633.\r\n\r\n`offsets.Day` (`'D'`) is documented to represent calendar day; however, `Day` arithmetic and usage of the `'D'` offset alias (e.g. `df.resample('D')`, `timedelta_range(..., freq='D')`, etc.) currently respects absolute time (i.e. `Day` acts like `Timedelta(days=1)`). This usage is ingrained pretty deeply in a lot of methods and operations and is fairly difficult to walk back to respect the notion of calendar day as stated in the docs.\r\n\r\nInstead, I propose to keep `Day` as is (a timedelta-like frequency like `Hour` (`'H'`)) and add a new frequency `CalendarDay`, `'CD'`. It would act very similarly to `DateOffset(day=1)` but act like a frequency. Thoughts?\r\n"},{"labels":["api",null],"text":"We have a few extension arrays that build off one or more ndarrays. You'll often want to get the underlying NumPy dtype for that array.\r\n\r\n* IntegerArray uses `.type`\r\n* IntervalArray uses `.subtype`\r\n\r\nIntervalArray can't use `.type`, since that has to be `Interval`. So IntegerArray will need to alias subtype to type.\r\n\r\nAre we happy with the name `subtype`?\r\n"},{"labels":["api",null],"text":"ndarray.take accepts scalars, and returns a scalar. We should probably make that part of the interface, or document that we don't support it.\r\n\r\n\r\n```python\r\nIn [18]: np.array([1, 2]).take(0)\r\nOut[18]: 1\r\n```\r\n\r\nCategorical currently returns an invalid categorical:\r\n\r\n```\r\nIn [19]: res = pd.Categorical([0, 1]).take(0)\r\n\r\nIn [20]: type(res)\r\nOut[20]: pandas.core.arrays.categorical.Categorical\r\n```\r\n\r\n```pytb\r\nIn [21]: res\r\nOut[21]: ---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/Envs/pandas-dev/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    700                 type_pprinters=self.type_printers,\r\n    701                 deferred_pprinters=self.deferred_printers)\r\n--> 702             printer.pretty(obj)\r\n    703             printer.flush()\r\n    704             return stream.getvalue()\r\n\r\n~/Envs/pandas-dev/lib/python3.6/site-packages/IPython/lib/pretty.py in pretty(self, obj)\r\n    398                         if cls is not object \\\r\n    399                                 and callable(cls.__dict__.get('__repr__')):\r\n--> 400                             return _repr_pprint(obj, self, cycle)\r\n    401\r\n    402             return _default_pprint(obj, self, cycle)\r\n\r\n~/Envs/pandas-dev/lib/python3.6/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\r\n    693     \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\r\n    694     # Find newlines and replace them with p.break_()\r\n--> 695     output = repr(obj)\r\n    696     for idx,output_line in enumerate(output.splitlines()):\r\n    697         if idx:\r\n\r\n~/sandbox/pandas/pandas/core/base.py in __repr__(self)\r\n     80         Yields Bytestring in Py2, Unicode String in py3.\r\n     81         \"\"\"\r\n---> 82         return str(self)\r\n     83\r\n     84\r\n\r\n~/sandbox/pandas/pandas/core/base.py in __str__(self)\r\n     59\r\n     60         if compat.PY3:\r\n---> 61             return self.__unicode__()\r\n     62         return self.__bytes__()\r\n     63\r\n\r\n~/sandbox/pandas/pandas/core/arrays/categorical.py in __unicode__(self)\r\n   1942         \"\"\" Unicode representation. \"\"\"\r\n   1943         _maxlen = 10\r\n-> 1944         if len(self._codes) > _maxlen:\r\n   1945             result = self._tidy_repr(_maxlen)\r\n   1946         elif len(self._codes) > 0:\r\n\r\nTypeError: len() of unsized object\r\n```\r\n\r\n- IntervalArray.take fails on `take`\r\n- SparseArray allows it.\r\n"},{"labels":["api",null,null],"text":"Pandas seems to ignore the extra (invalid) parameters. For e.g.\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\ndf=pd.read_excel('myfile.xlsx', some_dummy_param=True)\r\n```\r\nNote that some_dummy_param does not throw an error.\r\n\r\n#### Problem description\r\n\r\nIs there any way to make sure only valid parameters are passed to read_excel method?\r\n\r\n#### Expected Output\r\nSince there is no such parameter called \"some_dummy_param\", I should get an error:\r\n\r\n> TypeError: __init__() got an unexpected keyword argument 'some_dummy_param'\r\n"},{"labels":["api",null],"text":"To pivot, there's currently:\r\n\r\n`pd.pivot_table / df.pivot_table`\r\nhttps://github.com/pandas-dev/pandas/blob/d30c4a0696d5fbdc3c7ce36a9b9b19224a557e09/pandas/core/reshape/pivot.py#L28\r\n\r\n`df.pivot`\r\nhttps://github.com/pandas-dev/pandas/blob/d30c4a0696d5fbdc3c7ce36a9b9b19224a557e09/pandas/core/reshape/reshape.py#L386\r\n\r\n`pivot_simple` (Importable at the top level as `pd.pivot`)\r\nhttps://github.com/pandas-dev/pandas/blob/d30c4a0696d5fbdc3c7ce36a9b9b19224a557e09/pandas/core/reshape/reshape.py#L411\r\n\r\nI'm not sure why `pd.pivot` currently doesn't mirror `df.pivot` like how `pd.pivot_table` mirrors `df.pivot_table`, but it would clearer if these were in sync. "},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\ns = pd.Series([0.9999,2,3,4.0001])\r\nbins = 4\r\nhist_data = s.value_counts(bins=bins, sort=False)\r\n\r\n\"\"\"hist_data is shown below\r\n(0.996, 1.75]    1\r\n(1.75, 2.5]      1\r\n(2.5, 3.25]      1\r\n(3.25, 4.0]      1\r\ndtype: int64\r\n\"\"\"\r\n#The following statement should be true (according to the data returned by Series.value_counts\r\nhist_data.index[-1].right >= s.iloc[-1]\r\n\r\n```\r\n#### Problem description\r\n\r\nvalue_counts returns a series.  The index of that series contains the histogram bin limits.  As shown in the code, hist_data shows that there is 1 value in the range (3.25, 4.0], but that isn't the case. 4.0001 is not in that range.  It appears that the bin limits are truncated to 3 decimal places.  This does not appear to be a problem on the minimum value in the series as 0.9999 does lie in the range (0.996, 1.75].\r\n\r\n#### Expected Output\r\nTrue \r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.6.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.23.3\r\npytest: 3.6.1\r\npip: 18.0\r\nsetuptools: 39.2.0\r\nCython: None\r\nnumpy: 1.14.5\r\nscipy: 1.1.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.4.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.5\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 1.0.1\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"Based on the SciPy sprint discussions, and the discussions on related issues, seems like `from_records` should be the pandas way to create a `DataFrame` from row based data.\r\n\r\nThe current signature is next:\r\n- ```def from_records(cls, data, index=None, exclude=None, columns=None, coerce_float=False, nrows=None):```\r\n\r\nAnd it currently supports input `data` as:\r\n- dict\r\n- numpy.ndarray\r\n- DataFrame\r\n- Iterable of list\r\n- Iterable of tuple\r\n- Iterable of dict\r\n\r\nWhat I propose is to make `from_records` only work when `data` is an iterable of array-like (list, tuple, np.array...) or an iterable of dict. And deprecate the other cases (dict and DataFrame). After searching on GitHub repos and blogs, couldn't find cases where it's used with dict or DataFrame, and IMO the DataFrame constructor is a better way for those cases. This would make the code simpler.\r\n\r\nThen, I'd add a new parameter `dtypes` expecting a list or a dict with the dtypes of the new DataFrame. With this, and for the case when `data` is a generator, and `nrows` and `dtypes` are specified, we wouldn't need to exhaust the generator and load it to Python structures. Meaning that we'd just need to allocate the DataFrame memory, and there wouldn't be any intermediate memory requirements.\r\n\r\nRelated issues: #5902, #2305, #2193, #4464, #1794, #13818.\r\n\r\n@wesm, @jreback, @jorisvandenbossche any comments? Are you ok with this approach and the proposed changes to the API?\r\n\r\n\r\n\r\n\r\n\r\n"},{"labels":["api",null,null],"text":"We should make SparseArray a proper ExtensionArray.\r\n\r\nIt seems like this will be somewhat difficult to do *properly* when SparseArray subclasses ndarray. Basic things like `np.asarray(sparse_array)` don't match the required ExtensionArray API (https://github.com/pandas-dev/pandas/issues/14167). Fixing this, especially when we subclass ndarray, is going to be difficult. I can't override the behavior of `np.asarray(sparse_array)` in Python.\r\n\r\nSo, some questions\r\n\r\n1. Do people *rely* on SparseArray being an ndarray subclass?\r\n2. Do we want to make a clean break, or introduce deprecations for things that will need changing (but with no clear upgrade path)?\r\n\r\nMy current preference is to just break things, but I don't use sparse. SparseArray would compose an ndarray of dense values and a `SparseIndex`, but it would no longer subclass ndarray.\r\n\r\nCCing some people who seem to use pandas' sparse: @hexgnu @kernc @Licht-T "},{"labels":["api",null],"text":"#### Proposal  description\r\n\r\nDataclasses were added in Python 3.7.\r\n\r\nIt would be nice for pandas to support dataclasses. For example could be possible to construct dataframe from by calling `.from_dataclasses` or just `.DataFrame(data=dataclass_list)`. There should be also possibility to do `.to_dataclasses`.\r\n\r\n#### Expected Behaviour\r\n\r\n```python\r\nfrom dataclasses import dataclass\r\nimport pandas as pd\r\n\r\n@dataclass\r\nclass SimpleDataObject(object):\r\n  field_a: int\r\n  field_b: str\r\n\r\ndataclass_object1 = SimpleDataObject(1, 'a')\r\ndataclass_object2 = SimpleDataObject(2, 'b')\r\n>>> asd\r\n\r\n# Dataclasses to DataFrame\r\ndf = pd.from_dataclasses([dataclass_object1, dataclass_object2])\r\ndf.dtypes == ['field_a', 'field_b']\r\n>>> True\r\ndf.dtypes == ['int', 'str']\r\n>>> True\r\n\r\n# Dataclasses to DataFrame\r\ndf = pd.DataFrame(data=[dataclass_object1, dataclass_object2])\r\ndf.dtypes == ['field_a', 'field_b']\r\n>>> True\r\ndf.dtypes == ['int', 'str']\r\n>>> True\r\n\r\n# DataFrame to Dataclasses\r\ndf = pd.DataFrame(columns=['field_a', 'field_b'], data=[[1, 'a'], [2, 'b']])\r\ndataclass_list = df.to_dataclasses()\r\ndataclass_list == [dataclass_object1, dataclass_object2]\r\n>>> True\r\n```"},{"labels":["api",null,null,null,null],"text":"For clarification, by \"interval-point\" joins I mean joining an `IntervalIndex`/`IntervalArray` against the point values contained in the intervals, e.g. joining a numeric `IntervalIndex` against a `Float64Index`.  I want to keep this discussion separate from interval-interval merges for the time being.\r\n\r\nFor example, the following `join` does not currently work (and likewise `merge` with column data):  \r\n```python\r\nIn [2]: df1 = pd.DataFrame({'A': [10, 20, 30]}, index=pd.interval_range(0, 3))\r\n\r\nIn [3]: df2 = pd.DataFrame({'B': ['foo', 'bar', 'baz', 'qux']},\r\n   ...:                    index=[0.5, 1, 2.71828, 3.14159])\r\n\r\nIn [4]: df1\r\nOut[4]: \r\n         A\r\n(0, 1]  10\r\n(1, 2]  20\r\n(2, 3]  30\r\n\r\nIn [5]: df2\r\nOut[5]: \r\n           B\r\n0.50000  foo\r\n1.00000  bar\r\n2.71828  baz\r\n3.14159  qux\r\n\r\nIn [6]: df1.join(df2)\r\nOut[6]: \r\n         A    B\r\n(0, 1]  10  NaN\r\n(1, 2]  20  NaN\r\n(2, 3]  30  NaN\r\n```\r\n\r\nI think the behavior of such a `join`/`merge` is straight forward for left/right joins, but is a little bit less clear for inner/outer joins.  For inner (outer) joins one takes the intersection (union) of both indexes as the resulting index values.  This makes sense when both indexes contain the same type of objects, but this is not the case for interval-point joins.  I can't think of a consistent way to handle inner/outer joins, and not entirely if they even make sense.  A few options:\r\n\r\n- Do not support inner/outer interval-point joins\r\n- For inner joins:\r\n  - always keep the intervals and filter any non-matches?\r\n  - default to the left index and filter any non-matches?\r\n- For outer joins:\r\n  - union any non-matching point values for an `object` dtype?\r\n     - obviously non-performant and a bit weird\r\n  - coerce non-matching points to degenerate intervals (left == right) and union for an interval dtype?\r\n- Use a new API for non-exact interval joins?\r\n\r\nI'm leaning towards just using the existing API not supporting inner/outer for the time being, but would appreciate any thoughts."},{"labels":["api",null,null],"text":"I always found the mechanics of `combine_first` very unintuitive, and constantly need to look into the docs to see what's happening. I haven't checked the git history, but it seems that the method was a direct response from wesm to a SO question (https://stackoverflow.com/a/9794891). In particular, I think this would be much more intuitive to do with `df.update`, which is a subset of what #21855 proposes -- it introduces `join='outer'` for `DataFrame.update` (currently, only `'left'` is supported, but even the source code notes `# TODO: Support other joins`).\r\n\r\nWith that new option, `df1.combine_first(df2)` would be the same as `df1.update(df2, join='outer', overwrite=False)`, only that `combine_first` has much fewer options and controls (i.e. `filter_func` and `raise_conflict)`. The only difference is that `df.update` currently returns None, see #21858.\r\n\r\nSince it's quite a well-established function, the deprecation cycle would maybe have to be longer than usual, but I think the `update` variant is much cleaner, as well as more versatile, than this single-purpose function."},{"labels":["api",null],"text":"Currently, `df.update` follows the convention of `dict.update` to return None and update inplace. This is against the prevailing trend (and philosopy?) of pandas to move away from `inplace`. See for example (one among many...) @TomAugspurger's response in #21841:\r\n> Generally, we're moving away from inplace operations. It's confusing whether inplace means no copy or not. Reindex, by definition, can't be inplace unless the index is the same.\r\nWe recommend chaining your method calls, and hope to provide better memory control in the future.\r\n\r\nThe `update`-method of perfoms an important function regardless of whether it returns the object or None, and so should IMO be enabled to work in chained operations as well.\r\n\r\nFirst step there would be adding an `inplace`-argument with default `True`, and then -- *potentially* -- transitioning with a longish deprecation cycle towards `inplace=False`.\r\n\r\nRelevant xrefs: #21855 #21859"},{"labels":["api"],"text":"Under #18262 a `FutureWarning` was added suggesting that existing code like this:\r\n\r\n    pd.DataFrame.from_items(x)\r\n\r\nShould be changed to this:\r\n\r\n    import collections\r\n    pd.DataFrame.from_dict(collections.OrderedDict(x))\r\n\r\nThe fact that `from_items()` appeared only 6 times (now 8 times) in a Stack Overflow search was used as partial justification for removing it.  But if you search on GitHub, [`pd.DataFrame.from_items()`](https://github.com/search?q=pd.DataFrame.from_items&type=Code) appears more than 15,000 times in Python--almost half as many as `from_records()`!\r\n\r\nWe should celebrate the fact that this function doesn't cause enough confusion to appear often on Stack Overflow.  But it does occur (a lot!) in real code, and deprecating it is a mistake.\r\n\r\nIf constructing a temporary `OrderedDict` around items is the best way to construct a DataFrame, Pandas should implement that as a short function called `DataFrame.from_items()`, rather than asking thousands of people to busy themselves to accommodate this unnecessary API change.\r\n\r\nI recommend removing the FutureWarning, and retaining this widely-used, longstanding function.\r\n\r\nFor reference, the `FutureWarning` starts in 0.23 and looks like this:\r\n\r\n> FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\r\n\r\n"},{"labels":["api",null],"text":"Currently we don't specify what the behaviour should be for `ExtensionArray.argsort` when there are missing values. \r\nThis is not a huge problem because the `Series.sort_values` deals with the missing values itself (only argsorts the non-missing data), but still we should pin down and test the behaviour.\r\n\r\nI suppose we should follow numpy's example here and put them last (which is also consistent with the default behaviour of `sort_values`):\r\n\r\n```\r\nIn [114]: a = np.array([1, 3, 2, np.nan, 4, np.nan])\r\n\r\nIn [115]: a.argsort()\r\nOut[115]: array([0, 2, 1, 4, 3, 5])\r\n\r\n```\r\n\r\n\r\n\r\n"},{"labels":["api",null],"text":"One a DataFrameGroupBy object, you can already select some of the columns you want to apply your functions on, but apart from that, I would not expect it to influence the output structure:\r\n\r\n```\r\nIn [56]: df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar'],\r\n    ...:                    'B' : np.random.randn(4),\r\n    ...:                    'C' : np.random.randn(4)})\r\n    ...:                    \r\n\r\nIn [57]: \r\n\r\nIn [57]: gr = df.groupby('A')\r\n\r\nIn [58]: gr.agg({'B': {'r': np.sum}, 'C': {'r2': np.sum}})\r\n/home/joris/scipy/pandas/pandas/core/groupby/groupby.py:4650: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\r\n  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\r\nOut[58]: \r\n            C         B\r\n           r2         r\r\nA                      \r\nbar -0.371496  1.230346\r\nfoo  0.091779  2.487224\r\n\r\nIn [59]: gr[['C', 'B']].agg({'B': {'r': np.sum}, 'C': {'r2': np.sum}})\r\n/home/joris/scipy/pandas/pandas/core/groupby/groupby.py:4650: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\r\n  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\r\nOut[59]: \r\n           r2         r\r\nA                      \r\nbar -0.371496  1.230346\r\nfoo  0.091779  2.487224\r\n```\r\n\r\nSo in the above, doing `gr[['C', 'B']]` causes the final result not to have the MultiIndex, which IMO is very surprising.\r\n\r\nThe above example if for a deprecated case, so not sure how important it is (didn't directly see a similar case for a non-deprecated call), but when we clean-up the deprecations, we should certainly check this.\r\n\r\ncc @jreback @WillAyd "},{"labels":["api",null,null,null,null],"text":"xref #20700 "},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nseries = pd.Series([1, 2, 3], pd.date_range('201701010000', '201701010030', freq='15min'))\r\nprint(pd.infer_freq(series.index))\r\n# Error\r\npd.Timedelta(pd.infer_freq(series.index))\r\n# While this works\r\npd.Timedelta(series.index.freq)\r\n```\r\n#### Problem description\r\n\r\npd.Timedelta constructor works with e.g. '15min' but not with '15T',\r\nwhile pd.infer_freq() returns '15T' and pd.Series.index.freq returns '15min'.\r\nThis leads to errors.\r\n\r\n#### Expected Output\r\n\r\nSame behaviour of pd.Timedelta() for 'min' and 'T' offset alias or deprecation of 'T' as an offset alias.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.4.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 142 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\npandas: 0.23.1\r\npytest: 3.6.2\r\npip: 10.0.1\r\nsetuptools: 39.2.0\r\nCython: None\r\nnumpy: 1.14.5\r\nscipy: 1.1.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.4.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.5\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"Proposal: expose `CategoricalDtype` in the top-level `pandas` namespace.\r\n\r\nAs of pandas 0.23.0, `.astype(..., categories=...)` raises a `FutureWarning` but `CategoricalDtype`, the recommended alternative, is still buried in `pandas.api.types`. This makes its use considerably more verbose than the old interface:\r\n\r\n```\r\n# previous use (prior to 0.23.0)\r\ndf['col'] = df['col'].astype('category', categories=my_categories)\r\n\r\n# current use (0.23.0)\r\nfrom pandas.api.types import CategoricalDtype\r\ncat = CategoricalDtype(my_categories)\r\ndf['col'] = df['col'].astype(cat)\r\n\r\n# proposed (0.24+)\r\ndf['col'] = df['col'].astype(pd.CategoricalDtype(my_categories))\r\n```\r\n"},{"labels":["api",null],"text":"I assumed that in eg `get_level_values`, `stack`, `unstack`, ... (which all boil down to the behaviour of `_get_level_number`) using an integer to specify the level, would always mean positional because we have no way to disambiguate between position or name in those cases. \r\n\r\nBut, this seems very inconsistent:\r\n\r\n```\r\nIn [40]: mi = pd.MultiIndex.from_product([[0, 1], [2, 3], [4, 5]], names=[1, 1, 2])\r\n\r\nIn [41]: mi._get_level_number(0)     # <--- positional\r\nOut[41]: 0\r\n\r\nIn [43]: mi._get_level_number(1)     # <--- positional (as name should raise an error given duplicates)\r\nOut[43]: 1\r\n\r\nIn [44]: mi._get_level_number(2)     # <--- positional / name is the same\r\nOut[44]: 2\r\n\r\nIn [45]: mi = pd.MultiIndex.from_product([[0, 1], [2, 3], [4, 5]], names=[2, 1, 0])\r\n\r\nIn [46]: mi._get_level_number(0)     # <--- name\r\nOut[46]: 2\r\n\r\nIn [47]: mi._get_level_number(2)     # <--- name\r\nOut[47]: 0\r\n\r\nIn [48]: mi = pd.MultiIndex.from_product([[0, 1], [2, 3], [4, 5]], names=[2, 1, 1])\r\n\r\nIn [49]: mi._get_level_number(2)     # <--- name\r\nOut[49]: 0\r\n\r\nIn [50]: mi._get_level_number(1)     # <--- positional (as name should raise an error given duplicates)\r\nOut[50]: 1\r\n\r\nIn [51]: mi._get_level_number(0)     # <--- positional\r\nOut[51]: 0\r\n\r\nIn [52]: mi = pd.MultiIndex.from_product([[0, 1], [2, 3], [4, 5]], names=[1, 0, 1])\r\n\r\nIn [53]: mi._get_level_number(1)     # <--- positional (as name should raise an error given duplicates)\r\nOut[53]: 1\r\n\r\nIn [54]: mi._get_level_number(0)     # <--- name\r\nOut[54]: 1\r\n\r\nIn [55]: mi = pd.MultiIndex.from_product([[0, 1], [2, 3], [4, 5]], names=[0, 0, 1])\r\n\r\nIn [56]: mi._get_level_number(0)     # <--- positional (as name should raise an error given duplicates)\r\nOut[56]: 0\r\n\r\nIn [57]: mi._get_level_number(1)     # <--- name\r\nOut[57]: 2\r\n\r\nIn [58]: mi._get_level_number(2)     # <--- positional\r\n```\r\n\r\n\r\nAm I missing something? Was this discussed before?\r\n\r\ncc @toobaz\r\n"},{"labels":["api",null,null],"text":"xref https://github.com/pandas-dev/pandas/pull/21584#discussion_r198317876\r\n\r\nThis method would allow for changing the `closed` value of an existing `IntervalIndex`.\r\n\r\nExample usage:\r\n```python\r\nIn [2]: index = pd.interval_range(0, 3, closed='both')\r\n\r\nIn [3]: index\r\nOut[3]:\r\nIntervalIndex([[0, 1], [1, 2], [2, 3]]\r\n              closed='both',\r\n              dtype='interval[int64]')\r\n\r\nIn [4]: index.set_closed('neither')\r\nOut[4]:\r\nIntervalIndex([(0, 1), (1, 2), (2, 3)]\r\n              closed='neither',\r\n              dtype='interval[int64]')\r\n```"},{"labels":["api",null,null],"text":"The string detection routines are a bit odd, they work with arrays & not list-likes. We should change the impl to ``infer_dtype`` which already does the correct things.\r\n\r\n```\r\n[5]: from pandas.core.dtypes.common import is_string_like_dtype, is_string_dtype\r\n\r\nIn [2]: is_string_like_dtype(np.array([b'foo']))                  \r\nOut[2]: True\r\n\r\n# should work\r\nIn [3]: is_string_like_dtype([b'foo'])          \r\nOut[3]: False\r\n\r\n# should work\r\nIn [4]: is_string_like_dtype(np.array(['foo', 'bar'], dtype=object))\r\nOut[4]: False\r\n\r\nIn [6]: is_string_dtype(np.array(['foo', 'bar'], dtype=object))                    \r\nOut[6]: True\r\n\r\n# should work\r\nIn [7]: is_string_dtype(['foo', 'bar'])                        \r\nOut[7]: False\r\n```\r\nWe should also consolidate these into a single ``is_string_dtype`` if possible (this might be tricky though).\r\n\r\n"},{"labels":["api",null],"text":"Currently, ``Categorical.unique`` and ``CategoricalIndex.unique`` drop unused categories:\r\n\r\n```python\r\n>>> categories = ['very good', 'good', 'neutral', 'bad', 'very bad']\r\n>>> cat = pd.Categorical(['good','good', 'bad', 'bad'], categories=categories, ordered=True)\r\n>>> cat\r\n[good, good, bad, bad]\r\nCategories (5, object): [very good < good < neutral < bad < very bad]\r\n>>> cat.unique()\r\n[good, bad]\r\nCategories (2, object): [good < bad]  # unused categories dropped\r\n```\r\n\r\nSo, ``.unique()`` both uniquefies and drops unused categories (does two things in one operation)\r\n\r\nOften, even if you want to uniquefy values, you still want to control whether to drop unused categories or not. So ``Categorical/CategoricalIndex.unique`` should IMO keep all categories, and categories should be dropped in a seperate action. So, this would be a better API:\r\n\r\n```python\r\n>>> cat.unique()\r\n[good, bad]\r\nCategories (5, object): [very good < good < neutral < bad < very bad]    # unused not dropped\r\n```\r\n\r\nIf you want to drop unused categories, you should do it explicitly like so: ``cat.unique().remove_unused_categories()``.\r\n\r\nThe proposed API is also faster, as dropping unused categories requires recoding the categories/codes, which is potentially expensive.\r\n"},{"labels":["api",null,null],"text":"https://github.com/pandas-dev/pandas/pull/21486 is implementing support for `axis=None` for compatibility with NumPy 1.15's `all` / `any`.\r\n\r\nThe basic idea is to have `df.op(axis=None)` be the same as `np.op(df, axis=None)` for reductions like `sum`, `mean`, etc.\r\n\r\nGetting there poses a backwards-compatibility challenge. Currently for some reduction functions like `np.sum` we interpret `axis=None` as\r\n\r\n```python\r\nIn [11]: df = pd.DataFrame({\"A\": [1, 2]})\r\n\r\nIn [12]: np.sum(df)\r\nOut[12]:\r\nA    3\r\ndtype: int64\r\n```\r\n\r\nThe best I option I see is to just warn that the behavior will change in the future, without providing a way to achieve the behavior today. Then users can update things like `np.sum(df)` to `np.sum(df, axis=0)`. In a later version we'll make the actual change to interpret `axis=None` like NumPy."},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nset1 = ['a', 'b', 'c']\r\nset2 = ['1', '2', '3']\r\nset3 = ['test']\r\n\r\ntuple_index = [(s1, s2) for s1 in set1 for s2 in set2]\r\ndf = pd.DataFrame(index=tuple_index, columns=set3)\r\n\r\ndf.loc[('a', '1'), 'test'] = 5\r\nprint(df)  # Shows assignment occurs\r\nprint(df.loc[('a', '1'), 'test'])  # Raises KeyError: \"None of [('a', '1')] are in the [index]\"\r\n\r\n```\r\n#### Problem description\r\n\r\nIndexing with tuples is permitted only for setting but not getting.\r\n\r\n#### Expected Output\r\n\r\nShould either:\r\n1. Refuse to set the item because using tuple indicies is a silly idea, see #20991\r\n2. Successfully get the item.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.22.0\r\npytest: 3.2.3\r\npip: 10.0.1\r\nsetuptools: 38.4.0\r\nCython: 0.27.3\r\nnumpy: 1.14.2\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.3.1\r\nsphinx: None\r\npatsy: 0.5.0\r\ndateutil: 2.7.0\r\npytz: 2018.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 4.2.1\r\nbs4: 4.6.0\r\nhtml5lib: 1.0.1\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\nCredit @jameshowse for finding."},{"labels":["api",null,null],"text":"See #21427 \r\n\r\nThe issue is that a (non-`Day`) `Tick` object with `normalize=True` breaks addition monotonicity/associativity:\r\n\r\n```\r\nnow = pd.Timestamp.now()\r\ntick = pd.offsets.Minute(n=3, normalize=True)\r\n\r\n>>> now\r\nTimestamp('2018-06-11 17:38:10.135400')\r\n>>> now + tick\r\nTimestamp('2018-06-11 00:00:00')\r\n\r\n>>> now + 64*tick + 64*tick\r\nTimestamp('2018-06-11 00:00:00')\r\n>>> now + 128*tick\r\nTimestamp('2018-06-12 00:00:00')\r\n```\r\n\r\nThe `Day` tick doesn't have the same problem, but I'd prefer to disallow it rather than make it a double-special-case.  (See #20633 for more discussion about how `Day` should behave)"},{"labels":["api",null,null,null],"text":"#### Code Sample\r\n\r\n```python\r\nnano_time = '1518071940360000000'\r\nprint(pd.to_datetime(pd.to_numeric(nano_time), unit=\"ns\")) # Works\r\nprint(pd.to_datetime(nano_time, unit=\"ns\")) # OverflowError: Python int too large to convert to C long\r\n```\r\n#### Problem description\r\n\r\nIt appears the `to_numeric` function is not suited for very long numbers in string form, which is a problem for parsing nano-second based times. The method should be smarter in processing its input.\r\n\r\n#### Expected Output\r\n\r\nNot an error.\r\n\r\n`Timestamp('2018-02-08 06:39:00.360000')`\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.23.0\r\npytest: 3.0.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.2.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.3\r\nbs4: 4.6.0\r\nhtml5lib: 0.999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"\r\n\r\n```python\r\n    # in lexsort_indexers() which is called in sort_index()\r\n    # the original codes\r\n    # if isinstance(orders, bool):\r\n    #     orders = [orders] * len(keys)\r\n    # elif orders is None:\r\n    #     orders = [True] * len(keys)\r\n    # the changed codes\r\n    if orders:\r\n        orders = [True] * len(keys)\r\n    elif orders is None:\r\n        orders = [True] * len(keys)\r\n    else:\r\n        orders = [False] * len(keys)\r\n\r\n```\r\n#### Problem description\r\n\r\nSince there is \"1==True\" and \"0==False\" in Python,  I usually use 0/1 to replace False/True which is more convenient (Maybe this is not a good habit). In most case, it works as expected because the judge sentence is usually \r\n```python\r\n       if bool:\r\n              ……\r\n       else:\r\n             ……\r\n```\r\n\r\nAs usual, I wrote \"dataframe.sort_index(level=0, axis=1, ascending=0)\" in my codes. However, there came an error this time. I checked the code and then found that the value of 'ascending' was passed to a variable named 'orders' which is a parameter of function \"lexsort_indexer()\" in sorting.py. \r\nThe codes in lexsort_indexer() : \r\n```python\r\ndef lexsort_indexer(keys, orders=None, na_position='last'):\r\n    from pandas.core.categorical import Categorical\r\n    labels = []\r\n    shape = []\r\n    if isinstance(orders, bool):\r\n        orders = [orders] * len(keys)\r\n    elif orders is None:\r\n        orders = [True] * len(keys)\r\n    ……\r\n```\r\nSince 0 is neither bool type or None, th orders was unchanged and then there is an error. In order to make the code more compatible, I change the judge sentence as follows:\r\n```python\r\n    if orders:\r\n        orders = [True] * len(keys)\r\n    elif orders is None:\r\n        orders = [True] * len(keys)\r\n    else:\r\n        orders = [False] * len(keys)\r\n```\r\nWill this suggestion be accepted in the next version of pandas?\r\n\r\n#### Expected Output\r\ndataframe.sort_index(level=0, axis=1, ascending=0)\r\n0 error\r\n\r\n"},{"labels":["api",null,null,null,null],"text":"Haven't investigated:\r\n\r\n```\r\npandas/tests/series/indexing/test_datetime.py::test_getitem_median_slice_bug\r\n  /Users/taugspurger/sandbox/pandas/pandas/core/internals.py:312: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\r\n    return self.values[slicer]\r\n  /Users/taugspurger/sandbox/pandas/pandas/core/indexes/datetimelike.py:429: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\r\n    result = getitem(key)\r\n\r\npandas/tests/series/indexing/test_indexing.py::test_basic_getitem_setitem_corner\r\n  /Users/taugspurger/sandbox/pandas/pandas/core/internals.py:312: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\r\n    return self.values[slicer]\r\n  /Users/taugspurger/sandbox/pandas/pandas/core/indexes/datetimelike.py:429: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\r\n    result = getitem(key)\r\n```"},{"labels":["api",null,null],"text":"I hadn't even noticed that `resolution` was a `datetime` property until now, but it returns `timedelta(0, 0, 1)`.  `Timestamp` doesn't override that.  I expect the correct thing to do is to return `Timedelta(nanoseconds=1)`.  Pretty low priority."},{"labels":["api",null,null,null],"text":"```\r\ndti = pd.date_range('2016-01-01', periods=3)\r\npi = dti.to_period('D')\r\n\r\n>>> pi.is_monotonic\r\nTrue\r\n>>> pi[::-1].is_monotonic\r\nFalse\r\n```\r\n"},{"labels":["api",null,null],"text":"My understanding is that `Index[object]` was originally written with strings in mind, which is why ops like `__sub__` and `__neg__` don't make sense.  Since we've got more non-string cases (e.g. #21314), what if instead these were defined something like:\r\n\r\n```\r\ndef __neg__(self):\r\n    return Index([-x for x in self])\r\n\r\ndef __sub__(self, other):  # hand-wave appropriate handling for scalar vs vector other\r\n    return Index([x - other for x in self])\r\n```\r\n\r\nThe string cases would still raise, but for things like Decimal and DateOffset we could get the \"natural\" behavior.  Thoughts?"},{"labels":["api",null],"text":"During the implementation of non-numpy backed ExtensionArrays I quite often run into the case where it is simpler for me to write a complete re-implementation of the method defined on `pd.Series` instead of using the current implementation that only delegates part of the work. It would probably make sense to introduce some sort of delegation mechanism, either we continue the delegation like in https://github.com/pandas-dev/pandas/blob/4274b840e64374a39a0285c2174968588753ec35/pandas/core/base.py#L1041 or we could possibly add really general interface like NumPy's `__array_ufunc__`: https://docs.scipy.org/doc/numpy/reference/arrays.classes.html#numpy.class.__array_ufunc__\r\n\r\nMy use case where this arises currently is coming from https://github.com/pandas-dev/pandas/issues/21296 and `pd.Series.argsort` but I expect that there will be much more cases in this direction while I continue to implement the ExtensionArray interface for Arrow Arrays."},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas\r\n\r\nclass List(list):\r\n    pass\r\n\r\npandas.DataFrame(List([List([1,2,3]), List([4,5,6])]))\r\n```\r\n```\r\nTypeError: Argument 'rows' has incorrect type (expected list, got List)\r\n```\r\n\r\n#### Problem description\r\n\r\nIt seems it is not possible to create a DataFrame from subclasses of lists. The code seems to have a too strict check.\r\n\r\n#### Expected Output\r\n\r\nIt should create a DataFrame like it would with regular lists.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.13.0-41-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: 3.3.0\r\npip: 9.0.1\r\nsetuptools: 39.1.0\r\nCython: 0.28.2\r\nnumpy: 1.14.3\r\nscipy: 1.0.0\r\npyarrow: 0.9.0\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.7.4\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"Now that we support merging `on` combination of column names and index levels, this should work\r\n\r\n```python\r\nIn [40]: a = pd.DataFrame({\"A\": [1, 2, 3, 4]}, index=pd.MultiIndex.from_product([['a', 'b'], [0, 1]], names=['outer', 'inner']))\r\n\r\nIn [41]: b = pd.Series([1, 2, 3, 4], index=pd.MultiIndex.from_product([['a', 'b'], [1, 2]], names=['outer', 'inner']), name='B')\r\n\r\nIn [42]: pd.merge(a, b, on=['outer', 'inner'])\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-42-337c5a9e9f8f> in <module>()\r\n----> 1 pd.merge(a, b, on=['outer', 'inner'])\r\n\r\n~/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/reshape/merge.py in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\r\n     58                          right_index=right_index, sort=sort, suffixes=suffixes,\r\n     59                          copy=copy, indicator=indicator,\r\n---> 60                          validate=validate)\r\n     61     return op.get_result()\r\n     62\r\n\r\n~/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/reshape/merge.py in __init__(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\r\n    524         if not isinstance(right, DataFrame):\r\n    525             raise ValueError('can not merge DataFrame with instance of '\r\n--> 526                              'type {right}'.format(right=type(right)))\r\n    527\r\n    528         if not is_bool(left_index):\r\n\r\nValueError: can not merge DataFrame with instance of type <class 'pandas.core.series.Series'>\r\n```\r\n\r\nShould be the same as\r\n\r\n```python\r\nIn [39]: pd.merge(a, b.to_frame(), on=['outer', 'inner'])\r\nOut[39]:\r\n             A  B\r\nouter inner\r\na     1      2  1\r\nb     1      4  3\r\n```"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.Index([1, 2, 3]).droplevel([])\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-9172aa27c42a> in <module>()\r\n----> 1 pd.Index([1, 2, 3]).droplevel([])\r\n\r\nAttributeError: 'Int64Index' object has no attribute 'droplevel'\r\n```\r\n\r\n#### Problem description\r\n\r\nClearly the call above doesn't make too much sense, but there is no reason not to support it, and can help [write cleaner code](https://github.com/pandas-dev/pandas/pull/21016#issuecomment-389572566).\r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [2]: pd.Index([1, 2, 3]).droplevel([])\r\nOut[2]: Int64Index([1, 2, 3], dtype='int64')\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: e033c0616158d3ba974456b4f84810492936b1fe\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-6-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.24.0.dev0+10.ge033c0616.dirty\r\npytest: 3.5.0\r\npip: 9.0.1\r\nsetuptools: 39.0.1\r\nCython: 0.25.2\r\nnumpy: 1.14.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.5.0\r\ndateutil: 2.7.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api"],"text":"xref https://github.com/pandas-dev/pandas/pull/17361\r\n\r\n```\r\nIn [24]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]}, index=pd.Index([1, 2], name='a'))\r\n\r\nIn [25]: df.sort_values(['a', 'b'])\r\n/Users/taugspurger/.virtualenvs/pandas-dev/bin/ipython:1: FutureWarning: 'a' is both an index level and a column label.\r\nDefaulting to column, but this will raise an ambiguity error in a future version\r\n  #!/Users/taugspurger/Envs/pandas-dev/bin/python3\r\nOut[25]:\r\n   a  b\r\na\r\n1  1  3\r\n2  2  4\r\n```\r\n\r\nWhat should the user do in this situation? Should we provide a keyword to disambiguate? A literal like `pd.ColumnLabel('a')` or `pd.IndexName('a')`? Or do we require that they rename an index or column? \r\nRight now, they're essentially stuck with the last one. If we want to discourage that, then I suppose that's OK. But it's somewhat common to end up with overlapping names, from e.g. a groupby.\r\n\r\ncc @jmmease "},{"labels":["api",null,null,null],"text":"This is a bit strange.\r\n\r\n```python\r\nIn [3]: pd.DataFrame({\"A\": pd.Categorical([1, 2, 2, 2, 3])})\r\nOut[3]:\r\n   A\r\n0  1\r\n1  2\r\n2  2\r\n3  2\r\n4  3\r\n\r\nIn [4]: df = pd.DataFrame({\"A\": pd.Categorical([1, 2, 2, 2, 3])})\r\n\r\nIn [5]: df.median()\r\nOut[5]:\r\nA    2.0\r\ndtype: float64\r\n\r\nIn [6]: df.A.median()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-6-15196eaacc89> in <module>()\r\n----> 1 df.A.median()\r\n\r\n~/sandbox/pandas/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs)\r\n   9587                                       skipna=skipna)\r\n   9588         return self._reduce(f, name, axis=axis, skipna=skipna,\r\n-> 9589                             numeric_only=numeric_only)\r\n   9590\r\n   9591     return set_function_name(stat_func, name, cls)\r\n\r\n~/sandbox/pandas/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\r\n   3220         return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\r\n   3221                                 numeric_only=numeric_only,\r\n-> 3222                                 filter_type=filter_type, **kwds)\r\n   3223\r\n   3224     def _reindex_indexer(self, new_index, indexer, copy):\r\n\r\n~/sandbox/pandas/pandas/core/arrays/categorical.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\r\n   2065         if func is None:\r\n   2066             msg = 'Categorical cannot perform the operation {op}'\r\n-> 2067             raise TypeError(msg.format(op=name))\r\n   2068         return func(numeric_only=numeric_only, **kwds)\r\n   2069\r\n\r\nTypeError: Categorical cannot perform the operation median\r\n```\r\n\r\nAnyone know whether that's intentional?"},{"labels":["api",null],"text":"Is there a historical reason why an Int64Index (defaulting to represent epoch timestamps) cannot be converted to a DatetimeIndex?\r\n\r\n```\r\nIn [7]: pd.__version__\r\nOut[7]: '0.23.0rc2+27.geff1faf.dirty'\r\n\r\nIn [8]: val = [pd.Timestamp('2018-01-01').value]\r\n\r\nIn [9]: pd.DatetimeIndex(pd.Index(val))\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-9-77aedfdf1582> in <module>()\r\n----> 1 pd.DatetimeIndex(pd.Index(val))\r\n\r\n/mnt/c/Users/Matt Roeschke/Projects/pandas-mroeschke/pandas/core/indexes/datetimes.py in __new__(cls, data, freq, start, end, periods, tz, normalize, closed, ambiguous, dayfirst, yearfirst, dtype, copy, name, verify_integrity)\r\n    439             # must be integer dtype otherwise\r\n    440             if isinstance(data, Int64Index):\r\n--> 441                 raise TypeError('cannot convert Int64Index->DatetimeIndex')\r\n    442             if data.dtype != _INT64_DTYPE:\r\n    443                 data = data.astype(np.int64)\r\n\r\nTypeError: cannot convert Int64Index->DatetimeIndex\r\n\r\n# Expected behavior of In[9]\r\nIn [10]: pd.DatetimeIndex(pd.Index(val).values)\r\nOut[10]: DatetimeIndex(['2018-01-01'], dtype='datetime64[ns]', freq=None)\r\n```"},{"labels":["api",null],"text":"Here are 2 ways to drop rows from a pandas data-frame based on a condition:\r\n\r\n1. `df = df[condition]`\r\n\r\n2. `df.drop(df[condition].index, axis=0, inplace=True)`\r\n\r\nThe first one does not do it *inplace*, right?\r\n\r\nThe second one does not work as expected when the index is not unique, so the user would need to `reset_index()` then `set_index()` back. \r\n\r\n\r\n**Question**\r\nWould it be possible to have column dropping based directly on the condition?\r\ne.g.\r\n`df.drop(condition, axis=0, inplace=True)`\r\n"},{"labels":["api",null,null],"text":"I haven't quite replaced cyberpandas' take with pandas' yet. I think one reason is that I need to be able to pass axis through to `pandas.core.algorithms.take`\r\n\r\nWe'll have to be careful. The default for `take_nd` is 0, while for `ndarray.take` it's None. I think we want the default to be 0.\r\n\r\nI still have to figure out one other thing, but then hopefully things will work.\r\n"},{"labels":["api",null,null,null],"text":"cc @h-vetinari \r\n\r\nSome requests for simplification. Most of the complexity seems to be in the code inferring what kind of regime we're in (one or more alignable objects vs. one or more unalignable objects vs. mix). Being strict here (after deprecating) or requiring additional user input may be worthwhile from a maintenance perspective.\r\n\r\n- [ ] https://github.com/pandas-dev/pandas/pull/20347/files/3f77b80b4ca7c8b04894b00bc08e980b62b97b97#diff-4627140a96de4a6390883e22e983f1b2\r\n- [ ] https://github.com/pandas-dev/pandas/pull/20347/files/3f77b80b4ca7c8b04894b00bc08e980b62b97b97#diff-4627140a96de4a6390883e22e983f1b2\r\n- [ ] https://github.com/pandas-dev/pandas/pull/20347#discussion_r185459889\r\n- [ ] https://github.com/pandas-dev/pandas/pull/20347#discussion_r185460287\r\n- [ ] https://github.com/pandas-dev/pandas/pull/20347#discussion_r185460641\r\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n* `pd.read_excel()`\r\n```python\r\n>>> import pandas as pd\r\n>>> pd.read_excel('sampledata.xlsx', sheet_name='Sheet2')\r\n      a   b       c\r\n0  this  is  sheet2\r\n>>> pd.read_excel('sampledata.xlsx', sheetname='Sheet2')\r\n/Users/<myname>/.pyenv/versions/miniconda3-latest/envs/py36/envs/py36/lib/python3.6/site-packages/pandas/util/_decorators.py:118: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\r\n  return func(*args, **kwargs)\r\n      a   b       c\r\n0  this  is  sheet2\r\n```\r\n\r\n* `ExcelFile.parse()`\r\n```python\r\n>>> import pandas as pd\r\n>>> xlsx_file=pd.ExcelFile('sampledata.xlsx')\r\n>>> xlsx_file.sheet_names\r\n['Sheet1', 'Sheet2', 'Sheet3']\r\n>>> xlsx_file.parse(sheet_name='Sheet2')\r\n      a   b       c\r\n0  this  is  sheet2\r\n>>> xlsx_file.parse(sheetname='Sheet2')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/<myname>/.pyenv/versions/miniconda3-latest/envs/py36/envs/py36/lib/python3.6/site-packages/pandas/io/excel.py\", line 327, in parse\r\n    **kwds)\r\nTypeError: _parse_excel() got multiple values for keyword argument 'sheetname'\r\n```\r\n\r\n#### Problem description\r\n\r\n* The document says ExcelFile.parse() is \"Equivalent to read_excel(ExcelFile, ...)\", but when using argument `sheetname`,which is deprecated, these two gives different results.\r\n  * pd.read_excel() works with `FutureWarning`, but ExcelFile.parse() gives `TypeError` instead.\r\n\r\n#### Expected Output\r\n\r\n ExcelFile.parse() should raise `FutureWarning` and use the value of `sheetname` as that of `sheet_name`\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.4.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: ja_JP.UTF-8\r\nLOCALE: ja_JP.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: 3.3.2\r\npip: 9.0.1\r\nsetuptools: 38.4.0\r\nCython: 0.27.3\r\nnumpy: 1.14.0\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.6.6\r\npatsy: 0.5.0\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: None\r\nmatplotlib: 2.1.2\r\nopenpyxl: 2.4.10\r\nxlrd: 1.1.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.1\r\nbs4: 4.6.0\r\nhtml5lib: 1.0.1\r\nsqlalchemy: 1.2.1\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"#### well, my function's input is ndarray-like and it's output is a scaler. The question is I have no method to apply this function on rolling object.\r\n\r\n```python\r\nfrom sklearn.decomposition import PCA\r\n\r\ndf = pd.DataFrame({\"A\": [1, 2, 3, 4, 5, 6, 7, 8], \"B\": [2, 3, 4, 5, 6, 7, 8, 9]})\r\n\r\ndef first_variance(X):\r\n    pca = PCA(n_components=1)\r\n    pca.fit(X)\r\n    return pca.explained_variance_\r\n\r\ndf.rolling(4).apply(first_variance)\r\n\r\n```\r\n#### The domo is incorrect because the \"apply\" method only can apply by index or column.\r\n"},{"labels":["api",null],"text":"leftover from https://github.com/pandas-dev/pandas/pull/20772\r\n\r\nIIUC, the proposal is for `datetime_index.set_freq(new_freq)` to be an alternative to `datetimeindex.freq = new_freq` (please correct me if I'm wrong).\r\n\r\nThis would also apply to TimedeltaIndex."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.DataFrame([1, 2], columns=range(3))\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in create_block_manager_from_blocks(blocks, axes)\r\n   4844                 blocks = [make_block(values=blocks[0],\r\n-> 4845                                      placement=slice(0, len(axes[0])))]\r\n   4846 \r\n\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in make_block(values, placement, klass, ndim, dtype, fastpath)\r\n   3192 \r\n-> 3193     return klass(values, ndim=ndim, placement=placement)\r\n   3194 \r\n\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in __init__(self, values, placement, ndim)\r\n    124                 'Wrong number of items passed {val}, placement implies '\r\n--> 125                 '{mgr}'.format(val=len(self.values), mgr=len(self.mgr_locs)))\r\n    126 \r\n\r\nValueError: Wrong number of items passed 1, placement implies 3\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-4ad51ebcfae4> in <module>()\r\n----> 1 pd.DataFrame([1, 2], columns=range(3))\r\n\r\n/home/nobackup/repo/pandas/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)\r\n    403                 else:\r\n    404                     mgr = self._init_ndarray(data, index, columns, dtype=dtype,\r\n--> 405                                              copy=copy)\r\n    406             else:\r\n    407                 mgr = self._init_dict({}, index, columns, dtype=dtype)\r\n\r\n/home/nobackup/repo/pandas/pandas/core/frame.py in _init_ndarray(self, values, index, columns, dtype, copy)\r\n    536             values = maybe_infer_to_datetimelike(values)\r\n    537 \r\n--> 538         return create_block_manager_from_blocks([values], [columns, index])\r\n    539 \r\n    540     @property\r\n\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in create_block_manager_from_blocks(blocks, axes)\r\n   4852         blocks = [getattr(b, 'values', b) for b in blocks]\r\n   4853         tot_items = sum(b.shape[0] for b in blocks)\r\n-> 4854         construction_error(tot_items, blocks[0].shape[1:], axes, e)\r\n   4855 \r\n   4856 \r\n\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in construction_error(tot_items, block_shape, axes, e)\r\n   4829         raise ValueError(\"Empty data passed with indices specified.\")\r\n   4830     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\r\n-> 4831         passed, implied))\r\n   4832 \r\n   4833 \r\n\r\nValueError: Shape of passed values is (1, 2), indices imply (3, 2)\r\n```\r\n\r\n#### Problem description\r\n\r\n(From https://github.com/pandas-dev/pandas/pull/18626#issuecomment-378068742 )\r\n\r\n#18819 (now fixed) disabled a call such as ``pd.Series([1], index=range(3))`` - the same result can be obtained with ``pd.Series(1, index=range(3)``, which is less ambiguous.\r\n\r\nIn principle, the same reasoning should lead us to disable ``pd.DataFrame([[1, 2]], index=range(3))``. But that can't be replaced as comfortably, because ``pd.DataFrame([1, 2], index=range(3))`` aligns vertically - and this couldn't be otherwise, as 1d objects are treated as ``Series``, and ``Series`` in ``DataFrames`` are mainly columns, not rows. Moreover, this is probably quite used in existing code, and also in tests:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/6cacdde5630c593999059833b516e1fec60aaf72/pandas/tests/frame/test_apply.py#L139\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/6cacdde5630c593999059833b516e1fec60aaf72/pandas/tests/indexes/test_multi.py#L3248\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/6cacdde5630c593999059833b516e1fec60aaf72/pandas/tests/reshape/test_reshape.py#L499\r\n\r\nSo I think the best way to proceed is:\r\n- allow 1d objects to be broadcasted horizontally (not just aligned vertically)\r\n- clearly document the above, and the fact that 2d objects of length 1 are broadcasted vertically instead\r\n\r\n#### Expected Output\r\n\r\n``` python\r\nIn [3]: pd.DataFrame([[1]*3, [2]*3], columns=range(3))\r\nOut[3]: \r\n   0  1  2\r\n0  1  1  1\r\n1  2  2  2\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nIn [3]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 7ec74e5f7b1f9a379b318153da88092cccb855cc\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-6-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.23.0.dev0+798.g7ec74e5f7\r\npytest: 3.5.0\r\npip: 9.0.1\r\nsetuptools: 39.0.1\r\nCython: 0.25.2\r\nnumpy: 1.14.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.5.0\r\ndateutil: 2.7.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"If I add two `Series` objects, both of which contain the same duplicate label, it appears that I get a result in which every possible combination for that label appears. In the below: `s1` has the label `b` twice, `s2` has the label `b` thrice, and the result has the label `b` six times.\r\n\r\n```python\r\nPython 3.6.5 (default, Mar 29 2018, 15:37:32) \r\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import pandas as pd\r\n>>> s1 = pd.Series(range(3), list('abb'))\r\n>>> s2 = pd.Series(range(4), list('abbb'))\r\n>>> s1 + s2\r\na    0\r\nb    2\r\nb    3\r\nb    4\r\nb    3\r\nb    4\r\nb    5\r\ndtype: int64\r\n```\r\n\r\nThat doesn't seem an unreasonable behaviour, but it's not applied consistently. If instead the second series has the same number of occurrences, then the data entries are added elementwise:\r\n```python\r\n>>> s3 = pd.Series(range(3), list('abb'))\r\n>>> s4 = pd.Series(range(3), list('abb'))\r\n>>> s3 + s4\r\na    0\r\nb    2\r\nb    4\r\ndtype: int64\r\n```\r\n\r\nThat seems to make the \"outer product\" behaviour in the first example somewhat dangerous, because any code that depends on it is at risk of giving inconsistent results if it happens to get a dataset where the numbers of the various labels match exactly. Should the second behaviour be altered to be consistent with the first? Or should maybe the first behaviour become an error (after a suitable deprecation period)?\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.5.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: None\r\npip: 10.0.0\r\nsetuptools: 39.0.1\r\nCython: 0.28.2\r\nnumpy: 1.14.2\r\nscipy: 1.0.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.3.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 1.0.1\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> start = pd.Timestamp('2008-01-02 07:51:37.999477')\r\n>>> end = start + pd.Timedelta('2 hours')\r\n>>> pd.date_range(start, end, periods=1000)    # Intuitively a linearly spaced time series\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-69-2304a28824c6>\", line 1, in <module>\r\n    pd.date_range(start, end, periods=1000)\r\n\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\datetimes.py\", line 2057, in date_range\r\n    closed=closed, **kwargs)\r\n\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 118, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\datetimes.py\", line 324, in __new__\r\n    ambiguous=ambiguous)\r\n\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\datetimes.py\", line 421, in _generate\r\n    raise ValueError('Of the three parameters: start, end, and '\r\n\r\nValueError: Of the three parameters: start, end, and periods, exactly two must be specified\r\n```\r\n#### Problem description\r\n\r\nI need a DatetimeIndex object to later use as index in a Series. DatetimeIndex should start at `start`, end at `end` and have a fixed number of elements (1000). Intuitively, this should work with `pd.date_range`, but it doesn't, and I haven't found a good explanation about why this is the case.\r\nI have found a workaround on Stackoverflow (https://stackoverflow.com/questions/25796030/how-can-i-use-pandas-date-range-to-obtain-a-time-series-with-n-specified-perio) that does work:\r\n```python\r\n>>> start = pd.Timestamp('2008-01-02 07:51:37.999477')\r\n>>> end = start + pd.Timedelta('2 hours')\r\n>>> pd.to_datetime(np.linspace(start.value, end.value, 1000))\r\nDatetimeIndex(['2008-01-02 07:51:37.999476992',\r\n               '2008-01-02 07:51:45.206684160',\r\n               '2008-01-02 07:51:52.413891328',\r\n               '2008-01-02 07:51:59.621098496',\r\n               '2008-01-02 07:52:06.828305920',\r\n               '2008-01-02 07:52:14.035513088',\r\n               '2008-01-02 07:52:21.242720256',\r\n               '2008-01-02 07:52:28.449927424',\r\n               '2008-01-02 07:52:35.657134592',\r\n               '2008-01-02 07:52:42.864341760',\r\n               ...\r\n               '2008-01-02 09:50:33.134612224',\r\n               '2008-01-02 09:50:40.341819392',\r\n               '2008-01-02 09:50:47.549026560',\r\n               '2008-01-02 09:50:54.756233728',\r\n               '2008-01-02 09:51:01.963440896',\r\n               '2008-01-02 09:51:09.170648064',\r\n               '2008-01-02 09:51:16.377855488',\r\n               '2008-01-02 09:51:23.585062656',\r\n               '2008-01-02 09:51:30.792269824',\r\n               '2008-01-02 09:51:37.999476992'],\r\n              dtype='datetime64[ns]', length=1000, freq=None)\r\n```\r\n\r\n#### Expected Output\r\n```python\r\n>>> start = pd.Timestamp('2008-01-02 07:51:37.999477')\r\n>>> end = start + pd.Timedelta('2 hours')\r\n>>> pd.date_range(start, end, periods=1000)\r\nDatetimeIndex(['2008-01-02 07:51:37.999476992',\r\n               '2008-01-02 07:51:45.206684160',\r\n               '2008-01-02 07:51:52.413891328',\r\n               '2008-01-02 07:51:59.621098496',\r\n               '2008-01-02 07:52:06.828305920',\r\n               '2008-01-02 07:52:14.035513088',\r\n               '2008-01-02 07:52:21.242720256',\r\n               '2008-01-02 07:52:28.449927424',\r\n               '2008-01-02 07:52:35.657134592',\r\n               '2008-01-02 07:52:42.864341760',\r\n               ...\r\n               '2008-01-02 09:50:33.134612224',\r\n               '2008-01-02 09:50:40.341819392',\r\n               '2008-01-02 09:50:47.549026560',\r\n               '2008-01-02 09:50:54.756233728',\r\n               '2008-01-02 09:51:01.963440896',\r\n               '2008-01-02 09:51:09.170648064',\r\n               '2008-01-02 09:51:16.377855488',\r\n               '2008-01-02 09:51:23.585062656',\r\n               '2008-01-02 09:51:30.792269824',\r\n               '2008-01-02 09:51:37.999476992'],\r\n              dtype='datetime64[ns]', length=1000, freq=None)\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.4.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 44 Stepping 2, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en\r\nLOCALE: None.None\r\n\r\npandas: 0.22.0\r\npytest: 3.3.2\r\npip: 9.0.1\r\nsetuptools: 38.4.0\r\nCython: 0.27.3\r\nnumpy: 1.14.2\r\nscipy: 1.0.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.6.6\r\npatsy: 0.5.0\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: None\r\nmatplotlib: 2.1.2\r\nopenpyxl: 2.4.10\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.1\r\nbs4: 4.6.0\r\nhtml5lib: 0.9999999\r\nsqlalchemy: 1.2.1\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api"],"text":"Hi,\r\n\r\nI would like to map a function expecting N arguments to a DataFrame of N columns. Today to do that I do:\r\n\r\n```python\r\ndf = pd.DataFrame({\"A\": [1, 2, 3, 4], \"B\": [5, 6, 7, 8]})\r\ndef f(x,y):\r\n     return x*y\r\n\r\ndf[['A', 'B']].apply(lambda x: f(*x), axis=1) \r\n```\r\nSo the `map` method for DataFrame would be:\r\n\r\n```python\r\ndef map(self, f):\r\n    return self.apply(lambda x: f(*x), axis=1)\r\n```\r\nif I am right. Then `df.map(f)` would produce a Serie.\r\n\r\nNote that with 1 colummn and a 1 argument function `map` would work as `numpy.map` does.\r\n\r\nThanks,\r\n\r\nOlivier.\r\n\r\nps: I edited this post after the talk I had with Tom, so it must seem strange now :smiley: "},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.Series([1,2,3]).loc[[i for i in (4,5)]]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-2-aba9eb9eea33> in <module>()\r\n----> 1 pd.Series([1,2,3]).loc[[i for i in (4,5)]]\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)\r\n   1370 \r\n   1371             maybe_callable = com._apply_if_callable(key, self.obj)\r\n-> 1372             return self._getitem_axis(maybe_callable, axis=axis)\r\n   1373 \r\n   1374     def _is_scalar_access(self, key):\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)\r\n   1829                     raise ValueError('Cannot index with multidimensional key')\r\n   1830 \r\n-> 1831                 return self._getitem_iterable(key, axis=axis)\r\n   1832 \r\n   1833             # nested tuple slicing\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_iterable(self, key, axis)\r\n   1109 \r\n   1110         if self._should_validate_iterable(axis):\r\n-> 1111             self._has_valid_type(key, axis)\r\n   1112 \r\n   1113         labels = self.obj._get_axis(axis)\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _has_valid_type(self, key, axis)\r\n   1683                         raise KeyError(\r\n   1684                             u\"None of [{key}] are in the [{axis}]\".format(\r\n-> 1685                                 key=key, axis=self.obj._get_axis_name(axis)))\r\n   1686                     else:\r\n   1687 \r\n\r\nKeyError: 'None of [[4, 5]] are in the [index]'\r\n\r\nIn [3]: pd.Series([1,2,3]).loc[(i for i in (4,5))]\r\nOut[3]: \r\n4   NaN\r\n5   NaN\r\ndtype: float64\r\n\r\n\r\n```\r\n#### Problem description\r\n\r\nSince we convert iterators to lists anyway...\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/1e4e04bf47417aadaf11c7d55c206508f2899fa5/pandas/core/indexing.py#L1124\r\n\r\n... we might as well do the conversion as soon as possible (i.e., in ``__getitem__``), and simplify the code by only handling list-likes which have a length. I would also consider changing ``is_list_like`` to return ``False`` for iterators, or provide it with a ``has_len=False`` argument.\r\n\r\nIt would also solve this other, less important, difference:\r\n```\r\nIn [2]: pd.Series([1,2,3]).loc[[i for i in (2,5)]]\r\n/usr/bin/ipython3:1: FutureWarning: \r\nPassing list-likes to .loc or [] with any missing label will raise\r\nKeyError in the future, you can use .reindex() as an alternative.\r\n\r\nSee the documentation here:\r\nhttps://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\r\n  #! /bin/sh\r\nOut[2]: \r\n2    3.0\r\n5    NaN\r\ndtype: float64\r\n\r\nIn [3]: pd.Series([1,2,3]).loc[(i for i in (2,5))]\r\nOut[3]: \r\n2    3.0\r\n5    NaN\r\ndtype: float64\r\n```\r\n\r\n... and probably others.\r\n\r\n#### Expected Output\r\n\r\nExactly the same for lists and iterators.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: d04b7464dcc20051ef38ac2acda580de854d3e01\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-6-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.23.0.dev0+754.gd04b7464d.dirty\r\npytest: 3.5.0\r\npip: 9.0.1\r\nsetuptools: 39.0.1\r\nCython: 0.25.2\r\nnumpy: 1.14.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.5.0\r\ndateutil: 2.7.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample\r\n\r\n```python\r\nIn [61]: df_test = pd.DataFrame({\"A\":[1,1,2,2],\"B\":[1,1,1,1]})\r\n\r\nIn [62]: df_test.groupby(\"B\").rank(method=\"dense\", ascending=True, pct=False, na_option='top')\r\nOut[62]:\r\n     A\r\n0  1.0\r\n1  1.0\r\n2  2.0\r\n3  2.0\r\n\r\nIn [63]: df_test.groupby(\"B\").rank(method=\"dense\", ascending=True, pct=True, na_option='top')\r\nOut[63]:\r\n      A\r\n0  0.25\r\n1  0.25\r\n2  0.50\r\n3  0.50\r\n```\r\n#### Problem description\r\n``pd.groupby.rank`` result does not scale to 100% when method is \"dense\". \r\n#### Expected Output\r\n``` python\r\nIn [65]: df_test['A'].rank(method=\"dense\", ascending=True, pct=True, na_option='top')\r\nOut[65]:\r\n0    0.5\r\n1    0.5\r\n2    1.0\r\n3    1.0\r\nName: A, dtype: float64\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 448124c138dc39001638aacd68f253b1034d7f04\r\npython: 3.6.4.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: zh_CN.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.23.0.dev0+743.g448124c13\r\npytest: 3.3.2\r\npip: 9.0.1\r\nsetuptools: 38.4.0\r\nCython: 0.27.3\r\nnumpy: 1.14.0\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.6.7\r\npatsy: 0.5.0\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: None\r\nmatplotlib: 2.1.2\r\nopenpyxl: 2.4.10\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.1\r\nbs4: 4.6.0\r\nhtml5lib: 1.0.1\r\nsqlalchemy: 1.2.1\r\npymysql: 0.7.11.None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"`DatetimeIndex.offset` and `DatimeIndex.freq` refer to the same thing, and are used interchangeably throughout the code for `DatetimeIndex`:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/4a344972722cc3c27250cbc8e382472b13e66bde/pandas/core/indexes/datetimes.py#L1729-L1732\r\n\r\nIs there a reason for having both?  \r\n\r\n`TimedeltaIndex` and `PeriodIndex` only have `freq`, so it seems like we could keep `freq` for consistency, deprecate `DatetimeIndex.offset`, and modify the `DatetimeIndex` code to only use `freq`."},{"labels":["api",null,null],"text":"xref #3599. Should prob not do this kind of inference.\r\n\r\n```\r\nIn [83]: df1 = DataFrame([{\"val1\": 1, \"val2\" : 20}, {\"val1\":1, \"val2\": 19}, \r\n    ...:                  {\"val1\":2, \"val2\": 27}, {\"val1\":2, \"val2\": 12}])\r\n    ...:                  \r\n\r\nIn [84]: df2 = DataFrame([{\"val1\": 1, \"val2\" : 20}, {\"val1\":1, \"val2\": 19}, \r\n    ...:                  {\"val1\":1, \"val2\": 27}, {\"val1\":1, \"val2\": 12}])\r\n    ...:                  \r\n\r\nIn [85]: df1.groupby('val1').apply(lambda x: x.val2-x.val2.mean())\r\nOut[85]: \r\nval1   \r\n1     0    0.5\r\n      1   -0.5\r\n2     2    7.5\r\n      3   -7.5\r\nName: val2, dtype: float64\r\n\r\nIn [86]: df2.groupby('val1').apply(lambda x: x.val2-x.val2.mean())\r\nOut[86]: \r\nval2    0    1    2    3\r\nval1                    \r\n1     0.5 -0.5  7.5 -7.5\r\n\r\nIn [87]: df2.groupby('val1', squeeze=True).apply(lambda x: x.val2-x.val2.mean())\r\nOut[87]: \r\n0    0.5\r\n1   -0.5\r\n2    7.5\r\n3   -7.5\r\nName: 1, dtype: float64\r\n```\r\n\r\n[87] should a) have the correct index\r\nbut this should just work w/o the ``squeeze`` kwarg"},{"labels":["api",null,null,null],"text":"xref issues\r\n\r\n- [ ] #20596\r\n- [ ] #16980 \r\n- [ ] #8774\r\n\r\nAs I understand the offset classes, they are supposed to respect transitions in \"wall time\" instead of absolute time. Currently the `Day` offset is currently defined to be 24 hours instead of 1 calendar day which is problematic to respect \"wall time\" wrt DST transitions.\r\n\r\n```\r\nIn [25]: foo\r\nOut[25]: Timestamp('2016-10-30 00:00:00+0300', tz='Europe/Helsinki') # DST change on this day\r\n\r\nIn [26]: foo + pd.tseries.offsets.DateOffset(weeks=1)\r\nOut[26]: Timestamp('2016-11-06 00:00:00+0200', tz='Europe/Helsinki')\r\n\r\nIn [27]: foo + pd.tseries.offsets.Week()\r\nOut[27]: Timestamp('2016-11-06 00:00:00+0200', tz='Europe/Helsinki') # respects calendar transition\r\n\r\nIn [28]: foo + pd.Timedelta(weeks=1)\r\nOut[28]: Timestamp('2016-11-05 23:00:00+0200', tz='Europe/Helsinki')\r\n\r\nIn [29]: foo + pd.tseries.offsets.DateOffset(days=1)\r\nOut[29]: Timestamp('2016-10-31 00:00:00+0200', tz='Europe/Helsinki')\r\n\r\nIn [30]: foo + pd.tseries.offsets.Day()\r\nOut[30]: Timestamp('2016-10-30 23:00:00+0200', tz='Europe/Helsinki') # does not respects calendar transition\r\n\r\nIn [31]: foo + pd.Timedelta(days=1)\r\nOut[31]: Timestamp('2016-10-30 23:00:00+0200', tz='Europe/Helsinki')\r\n```\r\n\r\nFrom prior issues, #20596, #16980, #8774, it seems like many are confused that 1 day = 24 hours when the DST transition comes up and defining 1 day = 1 calendar day would be more intuitive and consistent with other offsets. @"},{"labels":["api",null],"text":"Do we want to support this? If so, how?\r\n\r\n```python\r\nIn [22]: from pandas.tests.extension.decimal.array import DecimalArray, decimal, DecimalDtype\r\n\r\nIn [23]: ser = pd.Series(['1.0', '1.1'])\r\nIn [24]: ser.astype(...)\r\n```\r\n\r\nSome issues:\r\n\r\n1. unlikely, but multiple extension types could have the same name, so `ser.astype('decimal')` might be ambiguous. Let's refuse to guess, and require that an instance or subclass of `ExtensionDtype` be provided. So `DecimalDtype`\r\n2. How to go from dtype to array type? (we can currently go array to dtype, but not the other way around)\r\n3. How to get the Array's \"parser\" (something like `to_decimal`).\r\n\r\nA solution to 2 is to require that an `ExtensionDtype` have a property `array_type` that returns the class it's a type for.\r\n\r\n```python\r\nclass DecimalDtype(ExtensionDtype):\r\n    @property\r\n    def array_type(self): return DecimalArray\r\n```\r\n\r\nI think that's fine. It means you can't use the same type for multiple arrays, but whatever.\r\n\r\nA solution to 3 is a bit harder... Having an arbitrary \"try to parse this sequence of things, maybe coercing / erroring\" might be generally useful (e.g. reading in from a CSV). Thoughts here? I don't think this needs to block 0.23."},{"labels":["api",null,null],"text":"I was reviewing the PR on the DatetimeIndex docstring, and the list of parameters is horribly complex because you can both use it by passing `data` (with accompanying keywords like `copy`, `dtype`, ..) and by generating a range with optional `start`, `end`, `periods`. \r\n\r\nSince for generating ranges, we have the special purpose `pandas.date_range`, do we need to keep this functionality in the `DatetimeIndex` constructor as well?\r\n\r\nOf course deprecating it will annoy people who use it (although I *think* the majority of usage of DatetimeIndex passed `data`), so the question is whether it is worth it."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: s1 = pd.Series([1,2,3], index=pd.Index([1,2,3], name=\"T\"))\r\n\r\nIn [3]: s1\r\nOut[3]:\r\nT\r\n1    1\r\n2    2\r\n3    3\r\ndtype: int64\r\n\r\nIn [4]: s2 = pd.Series([10,20,30], index=pd.Index([1,2,3], name=\"Time\"))\r\n\r\nIn [5]: s2\r\nOut[5]:\r\nTime\r\n1    10\r\n2    20\r\n3    30\r\ndtype: int64\r\n\r\nIn [6]: s1.align(s2)\r\nOut[6]:\r\n(T\r\n 1    1\r\n 2    2\r\n 3    3\r\n dtype: int64, Time\r\n 1    10\r\n 2    20\r\n 3    30\r\n dtype: int64)\r\n\r\nIn [7]: s3 = pd.Series([1,2,4], index=pd.Index([1,2,4], name=\"T\"))\r\n\r\nIn [8]: s3\r\nOut[8]:\r\nT\r\n1    1\r\n2    2\r\n4    4\r\ndtype: int64\r\n\r\nIn [9]: s3.align(s2)\r\nOut[9]:\r\n(1    1.0\r\n 2    2.0\r\n 3    NaN\r\n 4    4.0\r\n dtype: float64, 1    10.0\r\n 2    20.0\r\n 3    30.0\r\n 4     NaN\r\n dtype: float64)\r\n\r\nIn [10]: s4 = pd.Series([10,20,30], index=pd.Index([1,2,3], name=\"T\"))\r\n\r\nIn [11]: s4\r\nOut[11]:\r\nT\r\n1    10\r\n2    20\r\n3    30\r\ndtype: int64\r\n\r\nIn [12]: s3.align(s4)\r\nOut[12]:\r\n(T\r\n 1    1.0\r\n 2    2.0\r\n 3    NaN\r\n 4    4.0\r\n dtype: float64, T\r\n 1    10.0\r\n 2    20.0\r\n 3    30.0\r\n 4     NaN\r\n dtype: float64)\r\n\r\nIn [15]: s1 + s2\r\nOut[15]:\r\nT\r\n1    11\r\n2    22\r\n3    33\r\ndtype: int64\r\n\r\nIn [16]: s2 + s1\r\nOut[16]:\r\nTime\r\n1    11\r\n2    22\r\n3    33\r\ndtype: int64\r\n\r\n```\r\n#### Problem description\r\n\r\nIt's not clear if this is a bug or a feature, and if the latter, then the behavior should be documented.\r\n\r\nIn the first example `s1.align(s2)`, the two series have the same index values, but the names of the respective index for each series is different.  In this case, `s1.align(s2)` returns the two series, preserving the names of the respective indexes.\r\n\r\nIn the second example, `s3.align(s2)`, the two series have different index values, and the names of the respective index for each series is different.   In this case, `s3.align(s2)` returns the two series, but the names of the indices have disappeared.\r\n\r\nIn the third example, `s3.align(s4)`, the two series have different index values, and the names of the respective index for each series is the same.   In this case, `s3.align(s4)` returns the two series, and the names of each index are the same name.\r\n\r\nSo in the first two cases, the names are different, but in the first case, the names are preserved, while in the second case, the names are lost.\r\n\r\nIn the last two cases, the index values are the different, but the names are preserved if they are the same, otherwise they are lost.\r\n\r\nFinally, the last 2 examples, involving addition show some asymmetries due to this issue with `Series.align()` with respect to name handling when adding two series with different names.  In this case, `s1+s2` and `s2+s1` have indices with different names, which seems a bit odd.\r\n\r\n#### Expected Output\r\n\r\nThis is not clear to me.  Either\r\n1. If the names are different, then return no names on the corresponding indexes\r\n2. If the names are different, then raise an Exception\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 6c1ab7f2c0ec13237c383f5c485d92e8ce158b14\r\npython: 3.6.4.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.23.0.dev0+685.g6c1ab7f2c\r\npytest: 3.3.2\r\npip: 9.0.1\r\nsetuptools: 38.4.0\r\nCython: 0.27.3\r\nnumpy: 1.14.0\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.6.6\r\npatsy: 0.5.0\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: None\r\nmatplotlib: 2.1.2\r\nopenpyxl: 2.4.10\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.1\r\nbs4: 4.6.0\r\nhtml5lib: 1.0.1\r\nsqlalchemy: 1.2.1\r\npymysql: 0.7.11.None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"I think I got a bug here.\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame({\"A\": [1, 2, 3, 4, 5], \"B\": [3.125, 4.12, 3.1, 6.2, 7.],\"C\":['a','a','b','b','a']})\r\na=df.pivot(index=\"A\",columns=\"C\",values=\"B\")\r\nb=a.copy()\r\ndef func(x):\r\n    x['a']=3\r\na.apply(func,axis=1)\r\nb.apply(func,axis=1)\r\n\r\nIn [2]: a\r\nOut[2]:\r\nC    a    b\r\nA\r\n1  3.0  NaN\r\n2  3.0  NaN\r\n3  3.0  3.1\r\n4  3.0  6.2\r\n5  3.0  NaN\r\n\r\nIn [3]: b\r\nOut[3]:\r\nC      a    b\r\nA\r\n1  3.125  NaN\r\n2  4.120  NaN\r\n3    NaN  3.1\r\n4    NaN  6.2\r\n5  7.000  NaN\r\n```\r\nFirst, I get `a` after call pivot on the  dataframe `df`, and `b,` which is a copy of `a`. Then I call apply on `a` and `b` respectively. Now we can see the `a` and `b` is different.\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '0.23.0.dev0+657.g01882ba5b'\r\n\r\nIn [3]: df1 =  pd.DataFrame({'v1' : range(12)}, index=pd.MultiIndex.from_product([list('abc'),list('xy'),[1,2]], names=['abc','xy','num']))\r\n   ...: df1\r\n   ...:\r\nOut[3]:\r\n            v1\r\nabc xy num\r\na   x  1     0\r\n       2     1\r\n    y  1     2\r\n       2     3\r\nb   x  1     4\r\n       2     5\r\n    y  1     6\r\n       2     7\r\nc   x  1     8\r\n       2     9\r\n    y  1    10\r\n       2    11\r\n\r\nIn [4]: df2 = pd.DataFrame({'v2': [100*i for i in range(1,7)]}, index=pd.MultiIndex.from_product([list('abc'), list('xy')],names=['abc','xy']))\r\n\r\nIn [5]: df2\r\nOut[5]:\r\n         v2\r\nabc xy\r\na   x   100\r\n    y   200\r\nb   x   300\r\n    y   400\r\nc   x   500\r\n    y   600\r\n\r\nIn [6]: df1.merge(df2, on=['abc','xy'])  # 'num' disappears\r\nOut[6]:\r\n        v1   v2\r\nabc xy\r\na   x    0  100\r\n    x    1  100\r\n    y    2  200\r\n    y    3  200\r\nb   x    4  300\r\n    x    5  300\r\n    y    6  400\r\n    y    7  400\r\nc   x    8  500\r\n    x    9  500\r\n    y   10  600\r\n    y   11  600\r\n\r\nIn [7]: df1.reset_index().merge(df2, on=['abc','xy']) # This preserves 'num'\r\nOut[7]:\r\n   abc xy  num  v1   v2\r\n0    a  x    1   0  100\r\n1    a  x    2   1  100\r\n2    a  y    1   2  200\r\n3    a  y    2   3  200\r\n4    b  x    1   4  300\r\n5    b  x    2   5  300\r\n6    b  y    1   6  400\r\n7    b  y    2   7  400\r\n8    c  x    1   8  500\r\n9    c  x    2   9  500\r\n10   c  y    1  10  600\r\n11   c  y    2  11  600\r\n\r\nIn [8]: df1.merge(df2, on='xy')  # 'abc' and 'num' disappear\r\nOut[8]:\r\n    v1   v2\r\nxy\r\nx    0  100\r\nx    0  300\r\nx    0  500\r\nx    1  100\r\nx    1  300\r\nx    1  500\r\nx    4  100\r\nx    4  300\r\nx    4  500\r\nx    5  100\r\nx    5  300\r\nx    5  500\r\nx    8  100\r\nx    8  300\r\nx    8  500\r\nx    9  100\r\nx    9  300\r\nx    9  500\r\ny    2  200\r\ny    2  400\r\ny    2  600\r\ny    3  200\r\ny    3  400\r\ny    3  600\r\ny    6  200\r\ny    6  400\r\ny    6  600\r\ny    7  200\r\ny    7  400\r\ny    7  600\r\ny   10  200\r\ny   10  400\r\ny   10  600\r\ny   11  200\r\ny   11  400\r\ny   11  600\r\n\r\n```\r\n#### Problem description\r\n\r\nIt seems that the new feature implemented in #17484 that allows merging on a combination of columns and index levels can drop index levels, which is really non-intuitive.  In the first example, the index level named \"num\" gets dropped, while in the last example, both \"abc\" and \"xy\" are dropped.\r\n\r\nIf this is the desired behavior, then it needs to be carefully documented.\r\n\r\nN.B. There is also an error in the docs of merging.rst that says this feature was introduced in v.0.22, but it will be introduced in v0.23\r\n\r\nI'm guessing @jmmease will need to look at this.\r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [6]: df1.merge(df2, on=['abc','xy'])\r\nOut[6]:\r\n            v1   v2\r\nabc xy num\r\na   x  1     0  100\r\n       2     1  100\r\n    y  1     2  200\r\n       2     3  200\r\nb   x  1     4  300\r\n       2     5  300\r\n    y  1     6  400\r\n       2     7  400\r\nc   x  1     8  500\r\n       2     9  500\r\n    y  1    10  600\r\n       2    11  600\r\n\r\nIn [8]: df1.merge(df2, on='xy')\r\nOut[8]:\r\n   abc_x  num  v1 abc_y   v2\r\nxy\r\nx      a    1   0     a  100\r\nx      a    1   0     b  300\r\nx      a    1   0     c  500\r\nx      a    2   1     a  100\r\nx      a    2   1     b  300\r\nx      a    2   1     c  500\r\nx      b    1   4     a  100\r\nx      b    1   4     b  300\r\nx      b    1   4     c  500\r\nx      b    2   5     a  100\r\nx      b    2   5     b  300\r\nx      b    2   5     c  500\r\nx      c    1   8     a  100\r\nx      c    1   8     b  300\r\nx      c    1   8     c  500\r\nx      c    2   9     a  100\r\nx      c    2   9     b  300\r\nx      c    2   9     c  500\r\ny      a    1   2     a  200\r\ny      a    1   2     b  400\r\ny      a    1   2     c  600\r\ny      a    2   3     a  200\r\ny      a    2   3     b  400\r\ny      a    2   3     c  600\r\ny      b    1   6     a  200\r\ny      b    1   6     b  400\r\ny      b    1   6     c  600\r\ny      b    2   7     a  200\r\ny      b    2   7     b  400\r\ny      b    2   7     c  600\r\ny      c    1  10     a  200\r\ny      c    1  10     b  400\r\ny      c    1  10     c  600\r\ny      c    2  11     a  200\r\ny      c    2  11     b  400\r\ny      c    2  11     c  600\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.4.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.23.0.dev0+657.g01882ba5b\r\npytest: 3.4.0\r\npip: 9.0.1\r\nsetuptools: 38.5.1\r\nCython: 0.25.1\r\nnumpy: 1.14.1\r\nscipy: 1.0.0\r\npyarrow: 0.8.0\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.7.1\r\npatsy: 0.5.0\r\ndateutil: 2.6.1\r\npytz: 2018.3\r\nblosc: 1.5.1\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: None\r\nmatplotlib: 2.2.0\r\nopenpyxl: 2.5.0\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.1\r\nbs4: 4.6.0\r\nhtml5lib: 1.0.1\r\nsqlalchemy: 1.2.5\r\npymysql: 0.8.0\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: 0.1.3\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"Related to https://github.com/pandas-dev/pandas/issues/18262, but since this is a very specific class of attributes, thought to open a specific issue about it.\r\n\r\nWe have a bunch of attributes on the `Series` class that stem from the time it was a numpy array subclass, and now just pass through the attribute of the underlying numpy array. It are typically attributes describing the data layout specific to the numpy array, which I don't think necessarily makes sense for a Series:\r\n\r\n- `Series.base`\r\n- `Series.data`\r\n- `Series.strides`\r\n- `Series.itemsize`\r\n- `Series.flags`\r\n\r\nand potentially also:\r\n\r\n- `Series.real` and `Series.imag`\r\n\r\nSo deprecating those can potentially remove 7 entries from the Series namespace.\r\n\r\nAre there good reasons to keep them? Is this somehow useful for \"compatibility\" (writing code that works for both series as numpy array) \r\n(I personally can't think of a usecase where you would want one of the above, unless you explicitly know will deal with numpy arrays)\r\n\r\nOne of the problems might be that if we refer users to `Series.values.<attribute>` that this will depend on the underlying array type if that will work or not (eg if `.values` starts giving an ExtensionArray, it will also not have those attributes)\r\n\r\ncc @shoyer I don't think you kept those for \"compatibility\" in DataArray in xarray? \r\nAnd in dask I think `itemsize`, `real` and `imag` is provided."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '0.22.0'\r\n\r\nIn [3]: s1 = pd.Series(range(8),\r\n   ...:                index=pd.MultiIndex.from_product([list('ab'),\r\n   ...:                                                  list('xy'),\r\n   ...:                                                  [1,2]],\r\n   ...:                                                  names=['ab','xy','num'])\r\n   ...: )\r\n   ...:\r\n\r\nIn [4]: s1\r\nOut[4]:\r\nab  xy  num\r\na   x   1      0\r\n        2      1\r\n    y   1      2\r\n        2      3\r\nb   x   1      4\r\n        2      5\r\n    y   1      6\r\n        2      7\r\ndtype: int64\r\n\r\nIn [5]:\r\n\r\nIn [5]: s2 = pd.Series([100*(i+1) for i in range(4)],\r\n   ...:                index=pd.MultiIndex.from_product([list('ab'),\r\n   ...:                                                  list('xy')],\r\n   ...:                                                  names=['ab','xy']))\r\n   ...:\r\n\r\nIn [6]: s2\r\nOut[6]:\r\nab  xy\r\na   x     100\r\n    y     200\r\nb   x     300\r\n    y     400\r\ndtype: int64\r\n\r\nIn [7]: s1.loc[pd.IndexSlice[:,:,1]] = -1  # This works as expected\r\n\r\nIn [8]: s1\r\nOut[8]:\r\nab  xy  num\r\na   x   1     -1\r\n        2      1\r\n    y   1     -1\r\n        2      3\r\nb   x   1     -1\r\n        2      5\r\n    y   1     -1\r\n        2      7\r\ndtype: int64\r\n\r\nIn [9]: s3 = s1.loc[pd.IndexSlice[:,:,1]] + s2 # This works as expected\r\n\r\nIn [10]: s3\r\nOut[10]:\r\nab  xy\r\na   x      99\r\n    y     199\r\nb   x     299\r\n    y     399\r\ndtype: int64\r\n\r\nIn [11]: s1.loc[pd.IndexSlice[:,:,1]] = s2 # This works differently in v0.22.0 and v0.23 (dev)\r\n\r\nIn [12]: s1\r\nOut[12]:\r\nab  xy  num\r\na   x   1      NaN\r\n        2      1.0\r\n    y   1      NaN\r\n        2      3.0\r\nb   x   1      NaN\r\n        2      5.0\r\n    y   1      NaN\r\n        2      7.0\r\ndtype: float64\r\n\r\nIn [13]: s1.loc[pd.IndexSlice['a',:,:]] = -2 # This works as expected\r\n\r\nIn [14]: s1\r\nOut[14]:\r\nab  xy  num\r\na   x   1     -2.0\r\n        2     -2.0\r\n    y   1     -2.0\r\n        2     -2.0\r\nb   x   1      NaN\r\n        2      5.0\r\n    y   1      NaN\r\n        2      7.0\r\ndtype: float64\r\n\r\nIn [15]: s4 = pd.Series([1000*i for i in range(1,5)], index=pd.MultiIndex.from_pr\r\n    ...: oduct([list('xy'),[1,2]], names=['xy','num']))\r\n    ...:\r\n\r\nIn [16]: s4\r\nOut[16]:\r\nxy  num\r\nx   1      1000\r\n    2      2000\r\ny   1      3000\r\n    2      4000\r\ndtype: int64\r\n\r\nIn [17]: s5 = s1.loc[pd.IndexSlice['a',:,:]] + s4 # This fails\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-17-cc0933ef6ebd> in <module>()\r\n----> 1 s5 = s1.loc[pd.IndexSlice['a',:,:]] + s4 # This fails\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py in wrapper(left, right, name, na_op)\r\n    716             return NotImplemented\r\n    717\r\n--> 718         left, right = _align_method_SERIES(left, right)\r\n    719\r\n    720         converted = _Op.get_op(left, right, name, na_op)\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py in _align_method_SERIES(left, right, align_asobject)\r\n    645                 right = right.astype(object)\r\n    646\r\n--> 647             left, right = left.align(right, copy=False)\r\n    648\r\n    649     return left, right\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py in align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)\r\n   2605                                          fill_value=fill_value, method=method,\r\n   2606                                          limit=limit, fill_axis=fill_axis,\r\n\r\n-> 2607                                          broadcast_axis=broadcast_axis)\r\n   2608\r\n   2609     def rename(self, index=None, **kwargs):\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py in align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)\r\n   5728                                       copy=copy, fill_value=fill_value,\r\n   5729                                       method=method, limit=limit,\r\n-> 5730                                       fill_axis=fill_axis)\r\n   5731         else:  # pragma: no cover\r\n   5732             raise TypeError('unsupported type: %s' % type(other))\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py in _align_series(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)\r\n   5797                 join_index, lidx, ridx = self.index.join(other.index, how=join,\r\n   5798                                                          level=level,\r\n-> 5799                                                          return_indexers=True)\r\n   5800\r\n   5801             left = self._reindex_indexer(join_index, lidx, copy)\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in join(self, other, how, level, return_indexers, sort)\r\n   3101             else:\r\n   3102                 return self._join_multi(other, how=how,\r\n-> 3103                                         return_indexers=return_indexers)\r\n   3104\r\n   3105         # join on the level\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in _join_multi(self, other, how, return_indexers)\r\n   3199                              \"overlapping names\")\r\n   3200         if len(overlap) > 1:\r\n-> 3201             raise NotImplementedError(\"merging with more than one level \"\r\n   3202                                       \"overlap on a multi-index is not \"\r\n   3203                                       \"implemented\")\r\n\r\nNotImplementedError: merging with more than one level overlap on a multi-index is not implemented\r\n\r\nIn [18]: s1.loc[pd.IndexSlice['a',:,:]]  = s4 # This puts in NaN rather than s4 values\r\n\r\nIn [19]: s1\r\nOut[19]:\r\nab  xy  num\r\na   x   1      NaN\r\n        2      NaN\r\n    y   1      NaN\r\n        2      NaN\r\nb   x   1      NaN\r\n        2      5.0\r\n    y   1      NaN\r\n        2      7.0\r\ndtype: float64\r\n```\r\n#### Problem description\r\n\r\nThis is a bit related to #10440 .  In the above code, if we slice where we fix the value of the third level, then we can change the slice to a constant.  We can also add that slice to a Series that has an Index that matches the first 2 levels.  \r\n\r\nIn v0.22.0 of pandas, the result of the lines \r\n```\r\ns1.loc[pd.IndexSlice[:,:,1]] = s2\r\ns1\r\n```\r\nis (as shown above)\r\n```\r\nab  xy  num\r\na   x   1      NaN\r\n        2      1.0\r\n    y   1      NaN\r\n        2      3.0\r\nb   x   1      NaN\r\n        2      5.0\r\n    y   1      NaN\r\n        2      7.0\r\ndtype: float64\r\n```\r\nBut in the development version 0.23 of pandas, the \"correct\" result is given:\r\n```\r\nab  xy  num\r\na   x   1      100\r\n        2        1\r\n    y   1      200\r\n        2        3\r\nb   x   1      300\r\n        2        5\r\n    y   1      400\r\n        2        7\r\ndtype: int64\r\n```\r\n\r\nSo I then would expect that the last 2 examples, using `s4` , would work in v0.23 development, because the only difference is that I am fixing the value of the first level in the slice, as opposed to the last level of the slice.  But in both of those cases, I get this error (independent of the pandas version):\r\n```\r\nNotImplementedError: merging with more than one level overlap on a multi-index is not implemented\r\n```\r\n\r\nSo there is a bit of an inconsistency in that a slice that fixes the last level allows the addition and assignment operations to work (and it is better with v0.23 development version than in v0.22 because the `NaN` values go away), but a slice that fixes the first level does not allow the operations to work.\r\n\r\nI'm not sure if this is a bug, or by design, or if the documentation needs to be clarified as to which type of slicing will allow the \"setting\" operation to work as expected.  There is a line in the docs (http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers) that says \"You can use a right-hand-side of an alignable object as well.\"  At least to me, it's not clear what objects are considered \"alignable\".\r\n\r\nIn any case, the expected behavior should be clear in the documentation, and, IMHO, if you fix the value of the first index or the last index, the behavior should be consistent.  \r\n\r\n#### Expected Output\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '0.22.0'\r\n\r\nIn [3]: s1 = pd.Series(range(8),\r\n   ...:                index=pd.MultiIndex.from_product([list('ab'),\r\n   ...:                                                  list('xy'),\r\n   ...:                                                  [1,2]],\r\n   ...:                                                  names=['ab','xy','num'])\r\n   ...: )\r\n   ...:\r\n\r\nIn [4]: s1\r\nOut[4]:\r\nab  xy  num\r\na   x   1      0\r\n        2      1\r\n    y   1      2\r\n        2      3\r\nb   x   1      4\r\n        2      5\r\n    y   1      6\r\n        2      7\r\ndtype: int64\r\n\r\nIn [5]:\r\n\r\nIn [5]: s2 = pd.Series([100*(i+1) for i in range(4)],\r\n   ...:                index=pd.MultiIndex.from_product([list('ab'),\r\n   ...:                                                  list('xy')],\r\n   ...:                                                  names=['ab','xy']))\r\n   ...:\r\n\r\nIn [6]: s2\r\nOut[6]:\r\nab  xy\r\na   x     100\r\n    y     200\r\nb   x     300\r\n    y     400\r\ndtype: int64\r\n\r\nIn [7]: s1.loc[pd.IndexSlice[:,:,1]] = -1  # This works as expected\r\n\r\nIn [8]: s1\r\nOut[8]:\r\nab  xy  num\r\na   x   1     -1\r\n        2      1\r\n    y   1     -1\r\n        2      3\r\nb   x   1     -1\r\n        2      5\r\n    y   1     -1\r\n        2      7\r\ndtype: int64\r\n\r\nIn [9]: s3 = s1.loc[pd.IndexSlice[:,:,1]] + s2 # This works as expected\r\n\r\nIn [10]: s3\r\nOut[10]:\r\nab  xy\r\na   x      99\r\n    y     199\r\nb   x     299\r\n    y     399\r\ndtype: int64\r\n\r\nIn [11]: s1.loc[pd.IndexSlice[:,:,1]] = s2 # This works differently in v0.22.0 and v0.23 (dev)\r\n\r\nIn [12]: s1\r\nOut[12]:\r\nab  xy  num\r\na   x   1      NaN\r\n        2      1.0\r\n    y   1      NaN\r\n        2      3.0\r\nb   x   1      NaN\r\n        2      5.0\r\n    y   1      NaN\r\n        2      7.0\r\ndtype: float64\r\n\r\nIn [13]: s1.loc[pd.IndexSlice['a',:,:]] = -2 # This works as expected\r\n\r\nIn [14]: s1\r\nOut[14]:\r\nab  xy  num\r\na   x   1     -2.0\r\n        2     -2.0\r\n    y   1     -2.0\r\n        2     -2.0\r\nb   x   1      NaN\r\n        2      5.0\r\n    y   1      NaN\r\n        2      7.0\r\ndtype: float64\r\n\r\nIn [15]: s4 = pd.Series([1000*i for i in range(1,5)], index=pd.MultiIndex.from_pr\r\n    ...: oduct([list('xy'),[1,2]], names=['xy','num']))\r\n    ...:\r\n\r\nIn [16]: s4\r\nOut[16]:\r\nxy  num\r\nx   1      1000\r\n    2      2000\r\ny   1      3000\r\n    2      4000\r\ndtype: int64\r\n\r\nIn [17]: s5 = s1.loc[pd.IndexSlice['a',:,:]] + s4 # This should not fail\r\n\r\nIn[18]: s5\r\nOut[18]:\r\nxy num\r\nx  1   998\r\n   2    1998\r\ny  1    2998\r\n    2    3998\r\n\r\nIn[19]: s1.loc[pd.IndexSlice['a',:,:]]  = s4 # This should not set NaN\r\n\r\nIn[20]: s1\r\nab  xy  num\r\na   x   1    1000\r\n        2     2000\r\n    y   1    3000\r\n        2     4000\r\nb   x   1      NaN\r\n        2      5.0\r\n    y   1      NaN\r\n        2      7.0\r\ndtype: float64\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.4.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.22.0\r\npytest: 3.3.2\r\npip: 9.0.1\r\nsetuptools: 38.4.0\r\nCython: 0.27.3\r\nnumpy: 1.14.0\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.6.6\r\npatsy: 0.5.0\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: None\r\nmatplotlib: 2.1.2\r\nopenpyxl: 2.4.10\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.1\r\nbs4: 4.6.0\r\nhtml5lib: 1.0.1\r\nsqlalchemy: 1.2.1\r\npymysql: 0.7.11.None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api"],"text":"At geopandas some tests started failing with pandas master:\r\n\r\n```\r\nIn [8]: from geopandas import GeoSeries\r\n   ...: from shapely.geometry import Point\r\n\r\nIn [9]: p = Point(1, 2)\r\n\r\nIn [10]: GeoSeries(p, index=['a', 'b', 'c', 'd'])\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-9a0cacdb2179> in <module>()\r\n----> 1 GeoSeries(p, index=['a', 'b', 'c', 'd'])\r\n\r\n/home/joris/scipy/geopandas/geopandas/geoseries.py in __new__(cls, data, index, crs, **kwargs)\r\n     96                 name = kwargs.get('name', None)\r\n     97             else:\r\n---> 98                 s = pd.Series(data, index=index, **kwargs)\r\n     99                 # prevent trying to convert non-geometry objects\r\n    100                 if s.dtype != object and not s.empty:\r\n\r\n/home/joris/scipy/pandas/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)\r\n    253                             'Length of passed values is {val}, '\r\n    254                             'index implies {ind}'\r\n--> 255                             .format(val=len(data), ind=len(index)))\r\n    256                 except TypeError:\r\n    257                     pass\r\n\r\nValueError: Length of passed values is 1, index implies 4\r\n```\r\n\r\npreviously this replicated the single point multiple times, just as `pd.Series(1, index=['a', 'b', 'c', 'd'])` gives a Series with four 1's.\r\n\r\nThis is related to https://github.com/pandas-dev/pandas/pull/19714, which removed the broadcasting of 1-length lists in the Series constructor (pd.Series([1], index=['a', 'b', 'c', 'd'])\r\n\r\nThe reason that geopandas converted the geometry to single element lists, is because geometries are convertable to array (and some are also iterable), and hence not seen as a 'scalar' by pandas (added 4 years ago: https://github.com/geopandas/geopandas/pull/70). \r\n\r\nIt still works when you do not pass an index:\r\n\r\n```\r\nIn [36]: GeoSeries(p)\r\nOut[36]:\r\n0    POINT (1 2)\r\ndtype: object\r\n```\r\n\r\nNote there is also some inconsistency within pandas itself:\r\n\r\n```\r\nIn [39]: pd.Series(p)\r\nOut[39]: \r\n0    POINT (1 2)\r\ndtype: object\r\n\r\nIn [40]: pd.Series(p, index=['a', 'b', 'c', 'd'])\r\n...\r\nValueError: Wrong number of items passed 2, placement implies 4\r\n```\r\n\r\n(because in the first case when no index is specifed,  `p` is converted to `[p]` before passing it to `_sanitize_array`, it works, but in the seconds case `_sanitize_array` converts the point `p` to `np.array[1, 2])` (array of its coordinates))"},{"labels":["api",null,null],"text":"As a way to make dropping an index level possible in an method chain.\r\n\r\n```python\r\n>>> df = pd.DataFrame(np.arange(12).reshape(4, 3), columns=['a', 'b', 'c']).set_index(['a', 'b'])\r\n>>> df.droplevel(0)\r\n```\r\n\r\nroughly equivalent to \r\n\r\n```python\r\ndf2 = df.copy()\r\ndf2.index = df.index.droplevel(0)\r\ndf2\r\n```\r\n\r\ncc @twiecki "},{"labels":["api",null,null],"text":"In several docstring PRs for Period datetime properties, we ran into the confusion about how the date/time of those attributes are determined (start or end ?). Eg see discussion in https://github.com/pandas-dev/pandas/pull/20277#discussion_r173651389\r\n\r\nSmall example to illustrate:\r\n\r\n```\r\nIn [30]: p1 = pd.Period('2017-01-01', freq='D')\r\n\r\nIn [31]: p2 = pd.Period('2017-01-01', freq='M')\r\n\r\nIn [32]: p1\r\nOut[32]: Period('2017-01-01', 'D')\r\n\r\nIn [33]: p2\r\nOut[33]: Period('2017-01', 'M')\r\n\r\nIn [34]: p1.start_time\r\nOut[34]: Timestamp('2017-01-01 00:00:00')\r\n\r\nIn [35]: p2.start_time\r\nOut[35]: Timestamp('2017-01-01 00:00:00')\r\n\r\nIn [36]: p1.day\r\nOut[36]: 1\r\n\r\nIn [37]: p2.day\r\nOut[37]: 31\r\n```\r\n\r\nThe discussion raised from how to describe the summary of such an attribute: \"The day of the month\" -> but which day of the period span? -> should this be \"The day of the month of the start of the Period\" ? -> ah, no, because it is not always the start, it depends on the frequency.\r\n\r\nIn the above example, `M` is actually the freq string for \"MonthEnd\", and the datetime-properties apparently then use the end as date to calculate those properties.\r\n\r\nQuestions:\r\n\r\n- How best to document this? Can we use a certain phrase in all docstrings?\r\n- Is there a way to know, given a certain `freq`, what the \"anchor point\" is? (using anchor point here, don't know if we have existing terminology for that) A way to know if the `freq` is a \"<some-freq>End\" ?\r\n- It's rather confusing behaviour, is this actually the behaviour we want?\r\n\r\ncc @jreback @jbrockmendel @sinhrks "},{"labels":["api",null,null],"text":"While reviewing: https://github.com/pandas-dev/pandas/pull/20198, I notices this was defined in the `DatetimeProperties` itself:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/78ded2585efb3e4b5164af91c85927b99298f026/pandas/core/indexes/accessors.py#L114-L143\r\n\r\nIs there any reason that we have it like that, while all others are just wrapped DatetimeIndex attributes/methods? \r\n\r\nBecause that means this returns (a bit strangly) an ndarray instead of a Series of the datetime.datetime values (making it inconsistent with other methods)\r\n\r\ncc @jbrockmendel @jreback"},{"labels":["api",null,null,null],"text":"``Index.strftime`` should be returning an ``Index`` and not an array, we do this for other accessors. see https://github.com/pandas-dev/pandas/pull/20103/files#r173622263\r\n\r\n\r\n\r\n```\r\nIn [6]: pd.date_range('20130101',periods=3).hour\r\nOut[6]: Int64Index([0, 0, 0], dtype='int64')\r\n\r\nIn [7]: pd.date_range('20130101',periods=3).strftime('%Y%m%d')\r\nOut[7]: array(['20130101', '20130102', '20130103'], dtype='<U8')\r\n```"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\ndf = pd.DataFrame.from_dict({'A': {(42, 'foo', 4): 0, (42, 'foo', 9): 0},\r\n 'B': {(42, 'foo', 4): 0, (42, 'foo', 9): 0},\r\n 'C': {(42, 'foo', 4): 0, (42, 'foo', 9): 0},\r\n 'D': {(42, 'foo', 4): 0, (42, 'foo', 9): 0},\r\n 'E': {(42, 'foo', 4): 0, (42, 'foo', 9): 0},\r\n 'F': {(42, 'foo', 4): 0, (42, 'foo', 9): 0},\r\n 'G': {(42, 'foo', 4): 1, (42, 'foo', 9): 1}})\r\n\r\nprint(df.reindex(pd.RangeIndex(-1, 11), level=2, fill_value=0))\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n          A  B  C  D  E  F  G\r\n42 foo 4  0  0  0  0  0  0  1\r\n       9  0  0  0  0  0  0  1\r\n```\r\n\r\n#### Problem description\r\n\r\nI would expect that the resulting dataframe have 12 rows, with indices `(42, 'foo', i)` for `i` from `-1` to `10`.\r\n\r\nApologies if I am just misunderstanding something. I haven't managed to find any information in the docs on the reason for the current behavior.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.4.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.14.24-1-MANJARO\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 38.5.1\r\nCython: 0.27.3\r\nnumpy: 1.14.0\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2018.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.1.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 1.0.1\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n```\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"Many thanks for the excellent software. This report is about behavior I did not expect. Not sure if it is a bug or not.\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> s = pd.Series([10, 20, 30, 'a', 'a', 'b', 'a'])\r\n>>> print(s)\r\n0    10\r\n1    20\r\n2    30\r\n3     a\r\n4     a\r\n5     b\r\n6     a\r\ndtype: object\r\n>>> print(s.replace('a', None))\r\n0    10\r\n1    20\r\n2    30\r\n3    30\r\n4    30\r\n5     b\r\n6     b\r\ndtype: object\r\n>>> print(s.replace({'a': None}))\r\n0      10\r\n1      20\r\n2      30\r\n3    None\r\n4    None\r\n5       b\r\n6    None\r\ndtype: object\r\n```\r\n#### Problem description\r\n\r\nThis behavior was unexpected for me. I would have assumed that these two lines would produce the same output:\r\n```python\r\ns.replace('a', None)\r\ns.replace({'a': None})\r\n```\r\n\r\nIn my particular use case, I was actually looking to just replace `'a'`with `None` and therefore did `s.replace('a', None)`. I did not check output carefully and therefore ended up with some very strange behavior down the line in my data analysis.\r\n\r\nNot sure if this is to be considered a bug or not. Docs are not entirely clear on what is intended behavior. Possible solutions could include\r\n\r\n* Describe behavior in docs (the filling behavior is barely described at all).\r\n* Hint that something like `s.replace('a', numpy.nan)` might be a better option.\r\n* Change API to require a more explicit opt-in for filling.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.4.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-116-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 38.5.1\r\nCython: None\r\nnumpy: 1.14.0\r\nscipy: None\r\npyarrow: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2018.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"Dask would like to inherit the accessors registered by `register_*_acessor`. This would be much easier if the `regiser_*_accessor` methods added the name of the accessor to a (private) class variable.\r\n\r\nIt seems like we have use of it in pandas as well: https://github.com/pandas-dev/pandas/pull/19960/files#r171815351"},{"labels":["api",null,null],"text":"Discussed briefly on the call today, but we should go through things formally.\r\n\r\nWhat should the return type of `Series[extension_array].values` and `Index[extension_array].values` be? I believe the two options are\r\n\r\n1. Return the ExtensionArray backing it (e.g. like what Categorical does)\r\n2. Return an ndarray with some information loss / performance cost\r\n   - e.g. like Series[datetimeTZ].values -> datetime64ns at UTC\r\n   - e.g. Series[period].values -> ndarray[Period objects]\r\n\r\n## Current State\r\n\r\nNot sure how much weight we should put on the current behavior, but for reference:\r\n\r\ntype        | Series.values           | Index.values\r\n----------- | ----------------------- | ------------\r\ndatetime    | datetime64ns            | datetime64ns\r\ndatetime-tz | datetine64ns(UTC&naive) | datetime64ns(UTC&naive)\r\ncategorical | Categorical             | Categorical\r\nperiod      | NA                      | ndarray[Period objects]\r\ninterval    | NA                      | ndarray[Interval objects]\r\n\r\n<details>\r\n\r\n```python\r\nIn [5]: pd.Series(pd.date_range('2017', periods=1)).values\r\nOut[5]: array(['2017-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\r\n\r\nIn [6]: pd.Series(pd.date_range('2017', periods=1, tz='US/Eastern')).values\r\nOut[6]: array(['2017-01-01T05:00:00.000000000'], dtype='datetime64[ns]')\r\n\r\nIn [7]: pd.Series(pd.Categorical([1])).values\r\nOut[7]:\r\n[1]\r\nCategories (1, int64): [1]\r\n\r\nIn [8]: pd.Series(pd.SparseArray([1])).values\r\nOut[8]:\r\n[1]\r\nFill: 0\r\nIntIndex\r\nIndices: array([0], dtype=int32)\r\n\r\nIn [9]: pd.date_range('2017', periods=1).values\r\nOut[9]: array(['2017-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\r\n\r\nIn [10]: pd.date_range('2017', periods=1, tz='US/Central').values\r\nOut[10]: array(['2017-01-01T06:00:00.000000000'], dtype='datetime64[ns]')\r\n\r\nIn [11]: pd.period_range('2017', periods=1, freq='D').values\r\nOut[11]: array([Period('2017-01-01', 'D')], dtype=object)\r\n\r\nIn [12]: pd.interval_range(start=0, periods=1).values\r\nOut[12]: array([Interval(0, 1, closed='right')], dtype=object)\r\n\r\nIn [13]: pd.CategoricalIndex([1]).values\r\nOut[13]:\r\n[1]\r\nCategories (1, int64): [1]\r\n```\r\n\r\n</details>\r\n\r\nIf we decide to have the return values be ExtensionArrays, we'll need to discuss\r\nto what extent they're part of the public API.\r\n\r\nRegardless of the choice for `.values`, we'll probably want to support the other\r\nuse case (maybe just by documenting \"call `np.asarray` on it). Internally, we\r\nhave `._values` (\"best\" array, ndarray or EA) and `._ndarray_values` (always an\r\nndarray).\r\n\r\n\r\ncc @jreback @jorisvandenbossche @jschendel @jbrockmendel @shoyer @chris-b1 "},{"labels":["api",null],"text":"Sometimes it is useful to flatten all levels of a multi-index. For example, when pivoting data into a wide format, the new columns are generally multi-indexed. However, when exporting to CSV, sometimes it might be desirable to have only one header row.\r\n\r\nAFAIK, there is no dedicated method to flatten an existing multi-index. Assuming I want to combine the multi-index levels with an underscore, I would do this to get one header row:\r\n\r\n```\r\ndf.columns = [\"_\".join(v) for v in df.columns.values]\r\n````\r\n\r\nThis is not really obvious, and maybe a dedicated `MultiIndex` method should be added, e.g. `df.columns.flatten(sep=\"_\", inplace=False)`. What do you think?"},{"labels":["api",null],"text":"I don't think that pandas should make any assumptions on how subclasses `__init__` method. Currently we assume that the first positional argument accepts an instance of the class or a sequence of the class's scalar type.\r\n\r\nI'd rather break that into two distinct methods.\r\n\r\n```python\r\n@classmedthod\r\ndef from_extension_array(cls, extension_array: ExtensionArray) -> ExtensionArray:\r\n    \"\"\"Construct a new ExtensionArray from an instance of the same type\r\n\r\n    Parameters\r\n    ----------\r\n    extension_array : ExtensionArray\r\n        An instance of 'cls'\r\n\r\n    Returns\r\n    -------\r\n    ExtensionArray\r\n    \"\"\"\r\n\r\n\r\nScalarType = ExtensionArray.dtype.type\r\n\r\n@classmethod\r\ndef from_scalars(cls, scalars: Sequence[ScalarType]) -> ExtensionArray:\r\n    \"\"\"Construct a new ExtensionArray from a sequence of the scalar type\r\n\r\n    Parameters\r\n    ----------\r\n    scalars : Sequence[ScalarType]\r\n        A sequence of cls.dtype.type, the scalar type for this array\r\n\r\n    Returns\r\n    -------\r\n    ExtensionArray\r\n    \"\"\"\r\n```\r\n\r\nI think these should be abstract. For many subclasses, a simple `cls(arg)` should suffice."},{"labels":["api",null,null],"text":"As a follow-up to the changes in DataFrame.apply (https://github.com/pandas-dev/pandas/pull/18577), we should make sure `Series.apply` behaves consistently with that as well.\r\n\r\nI think the default behaviour is OK (it already did what we now do by default for DataFrame: expand Series, keep lists or arrays as scalars, see below). \r\n\r\nBut we could *in principle* also add the same control over the output type by adding a similar `result_type` keyword.\r\n\r\nCurrent behaviour:\r\n\r\n```\r\nIn [42]: s = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\r\n\r\nIn [44]: s.apply(lambda x: [1, 2])\r\nOut[44]: \r\na    [1, 2]\r\nb    [1, 2]\r\nc    [1, 2]\r\nd    [1, 2]\r\ndtype: object\r\n\r\nIn [45]: s.apply(lambda x: pd.Series([1, 2]))\r\nOut[45]: \r\n   0  1\r\na  1  2\r\nb  1  2\r\nc  1  2\r\nd  1  2\r\n\r\nIn [46]: s.apply(lambda x: pd.Series([1, 2], index=['A', 'B']))\r\nOut[46]: \r\n   A  B\r\na  1  2\r\nb  1  2\r\nc  1  2\r\nd  1  2\r\n\r\nIn [47]: s.apply(lambda x: np.array([1, 2]))\r\nOut[47]: \r\na    [1, 2]\r\nb    [1, 2]\r\nc    [1, 2]\r\nd    [1, 2]\r\ndtype: object\r\n```"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\nLet's assume the following subclassing case:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n# Define a subclass of Series\r\nclass ExtendedSeries( pd.Series ):\r\n     _metadata = ['_reference']\r\n\r\n     def __init__( self, *args, **kwargs ):\r\n        reference = kwargs.pop('reference', {})\r\n        super(ExtendedSeries, self).__init__(*args, **kwargs)\r\n        self._reference = reference\r\n\r\n    @property\r\n    def _constructor(self):\r\n        return ExtendedSeries\r\n\r\n# Define a subclass of DataFrame that slices into the ExtendedSeries class\r\nclass ExtendedFrame( pd.DataFrame ):\r\n\r\n     _metadata = ['_reference']\r\n\r\n    def __init__( self, *args, **kwargs ):\r\n        reference = kwargs.pop('reference', {})\r\n        super(ExtendedFrame, self).__init__(*args, **kwargs)\r\n        self._reference = reference\r\n\r\n    @property\r\n    def _constructor(self):\r\n        return ExtendedFrame\r\n    @property\r\n    def _constructor_sliced(self):\r\n        return ExtendedSeries\r\n\r\n```\r\n#### Problem description\r\n\r\nThis works fine, but **does not allow to transfer extended metadata** from the `ExtendedFrame` to the `ExtendedSeries`.  \r\nAs far as I understand, writing the `_constructor_sliced` as follows should work: \r\n```python\r\nimport pandas as pd\r\n\r\n# Define a subclass of DataFrame that slices into the ExtendedSeries class\r\nclass ExtendedFrame( pd.DataFrame ):\r\n\r\n     _metadata = ['_reference']\r\n\r\n    def __init__( self, *args, **kwargs ):\r\n        reference = kwargs.pop('reference', {})\r\n        super(ExtendedFrame, self).__init__(*args, **kwargs)\r\n        self._reference = reference\r\n\r\n    @property\r\n    def _constructor(self):\r\n        return ExtendedFrame\r\n    @property\r\n    def _constructor_sliced(self):\r\n        a = ExtendedSeries([], reference=self._reference)\r\n        return a.__init__\r\n```\r\nthis would allow to first set the metadata and then return the object to initialice its data. Isn't it?  \r\nBut defining it this way gives errors in `core/frame:2166` and `core/frame:2563`. In both cases this is due to the call `self._constructor_sliced._from_array()`.   \r\n\r\nSeeing that `Series.from_array` has been labeled as deprecated and that `Series._from_array` calls the class' constructor. Couldn't it be possible to just change the two instances of `self._constructor_sliced._from_array()` to `self._constructor_sliced()`?  \r\nIf I'm seeing this correctly, wouldn't this change allow for this level of flexibility in subclassing without affecting the regular functionality?\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.21.1\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 38.4.0\r\nCython: None\r\nnumpy: 1.14.0\r\nscipy: 0.18.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.4.1\r\nsphinx: 1.6.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.1.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.5.1\r\nhtml5lib: 0.999999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample\r\n\r\n```python\r\ndf = pd.DataFrame({\"date\": ['10000101', '20180220']})\r\n\r\n# Timestamp limitations correctly raise exception\r\npd.to_datetime(df.date, errors='raise')\r\n...\r\nOutOfBoundsDatetime: Out of bounds nanosecond timestamp: 1000-01-01 00:00:00\r\n\r\n# Timestamp limitations correctly coerce to NaT\r\npd.to_datetime(df.date, errors='coerce')\r\n...\r\n0          NaT\r\n1   2018-02-20\r\nName: date, dtype: datetime64[ns]\r\n\r\n# errors=`ignore` incorrectly results in datetime like object\r\npd.to_datetime(df.date, errors='ignore', format=\"%Y%m%d\")\r\n0    1000-01-01 00:00:00\r\n1    2018-02-20 00:00:00\r\nName: date, dtype: object\r\n```\r\n#### Problem description\r\n\r\nI believe that when `errors='ignore'`, and the timestamp limitations are violated by a datum, the result [should be the original input](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html) – not a datetime like object.\r\n\r\n#### Expected Output\r\n\r\n```python\r\npd.to_datetime(df.date, errors='ignore', format=\"%Y%m%d\")\r\n0    '10000101'\r\n1    2018-02-20 00:00:00\r\nName: date, dtype: object\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.7.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.21.0\r\npytest: 2.9.2\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nfeather: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\ndf = pd.concat([pd.DataFrame({'x':np.random.randn(3), 'gp': 'a'}), pd.DataFrame({'x':np.random.randn(3), 'gp': 'b'})])\r\ndf.groupby('gp')['x'].cumsum(skipna=False)\r\n```\r\n#### Problem description\r\n\r\nDataFrame.cumsum(skipna=False) works. \r\nBut DataFrame.groupby['var'].cumsum(skipna=False) does not work. \r\n\r\nas seen from the above codes. Error message says numpy operations are not valid with groupby. This is inconsistent behavior, and is in conflict with docs http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.core.groupby.DataFrameGroupBy.cumsum.html\r\n\r\n"},{"labels":["api",null,null,null],"text":"Returning a categorical feels more natural to me\r\n\r\n```python\r\nIn [11]: pd.factorize(pd.Categorical(['a', 'a', 'c']))\r\nOut[11]: (array([0, 0, 1]), array([0, 1]))\r\n```\r\n\r\nThat's kind of what we do for a `DatetimeIndex` with TZ:\r\n\r\n```python\r\nIn [10]: pd.factorize(pd.Series(pd.DatetimeIndex(['2017', '2017'], tz='US/Eastern')))\r\nOut[10]:\r\n(array([0, 0]),\r\n DatetimeIndex(['2017-01-01 00:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None))\r\n```"},{"labels":["api",null],"text":"Index op naming and pinning is inconsistent:\r\n\r\n - no `Index.__rsub__`, https://github.com/pandas-dev/pandas/blob/master/pandas/core/indexes/base.py#L2200\r\n- `__iadd__, __isub__` aren't getting pinned to `cls` https://github.com/pandas-dev/pandas/blob/master/pandas/core/indexes/base.py#L3998\r\n- `__radd__, __rpow__, __rmul, ...` aren't getting the correct names (not such a big deal)\r\n- numeric subclass methods names:\r\n\r\n```\r\n>>> pd.Int64Index.__add__.__name__\r\n'_evaluate_numeric_binop'\r\n>>> pd.Float64Index.__mul__.__name__\r\n'_evaluate_numeric_binop'\r\n>>> pd.DatetimeIndex.__radd__.__name__\r\n'__add__'\r\n```"},{"labels":["api",null,null,null,null],"text":"```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pd.Categorical(['a', 'b', None]).fillna({2: 'a'})\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-099b4b7d5652> in <module>()\r\n----> 1 pd.Categorical(['a', 'b', None]).fillna({2: 'a'})\r\n\r\n~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/util/_decorators.py in wrapper(*args, **kwargs)\r\n    136                 else:\r\n    137                     kwargs[new_arg_name] = new_arg_value\r\n--> 138             return func(*args, **kwargs)\r\n    139         return wrapper\r\n    140     return _deprecate_kwarg\r\n\r\n~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/arrays/categorical.py in fillna(self, value, method, limit)\r\n   1661                 raise TypeError('\"value\" parameter must be a scalar, dict '\r\n   1662                                 'or Series, but you passed a '\r\n-> 1663                                 '\"{0}\"'.format(type(value).__name__))\r\n   1664\r\n   1665         return self._constructor(values, categories=self.categories,\r\n\r\nTypeError: \"value\" parameter must be a scalar, dict or Series, but you passed a \"dict\"\r\n```\r\n\r\nI suppose that mappings don't make sense for `Categorical.fillna`\r\n\r\nAlso... this seems strange...\r\n\r\n```python\r\nIn [10]: pd.Categorical(['a', 'b', None]).fillna(pd.Series(['a'], index=[0, 1, 2]))\r\nOut[10]:\r\n[a, a, a]\r\nCategories (2, object): [a, b]\r\n```\r\n\r\nWe shouldn't be affecting valid values here.\r\n\r\nPerhaps best to just raise for all series / dict like? Unless I'm missing something."},{"labels":["api",null,null],"text":"In pandas 0.22 and earlier, we passing multiple values for `y` worked by accident. The code clearly assumed that `y` was a scalar, but things usually worked, perhaps with an incidental warning:\r\n\r\n```python\r\nIn [12]: df = pd.DataFrame(np.random.uniform(size=(10, 2)), columns=['a', 'b'])\r\n\r\nIn [15]: df.plot(x='c', y=['a', 'b'])\r\n/Users/taugspurger/miniconda3/envs/pandas-0.21.0/lib/python3.6/site-packages/pandas/plotting/_core.py:1714: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\r\n  series.name = label\r\nOut[15]: <matplotlib.axes._subplots.AxesSubplot at 0x10f83b198>\r\n```\r\n\r\nOn master, this will currently raise. I think that we should explicitly support list-likes for `y`.\r\n\r\nTagging for 0.23 since I think this should be a blocker.\r\n\r\nxref https://github.com/pandas-dev/pandas/pull/18695 where we made the change."},{"labels":["api",null,null,null],"text":"Just so things don't get lost\r\n\r\n- [x] Interface, ExtensionBlock, and Internals (#19268)\r\n- [x] Series and DataFrame can hold extension arrays (#19520)\r\n- [x] overridable `assert_*_equal` (https://github.com/pandas-dev/pandas/pull/19863)\r\n- [x] unique (#19869)\r\n- [x] fillna (#19909)\r\n- [x] `__setitem__` works properly (https://github.com/pandas-dev/pandas/pull/19907)\r\n- [x] Document everything: (#19936)\r\n- [x] argsort, sort_values (https://github.com/pandas-dev/pandas/pull/19957)\r\n- [x] factorize (https://github.com/pandas-dev/pandas/pull/20361)\r\n- [x] groupby works properly (blocked by factorize)\r\n- [x] IntervalArray: (https://github.com/TomAugspurger/pandas/tree/pandas-array-upstream%2Bfu1%2Binterval)\r\n- [x] PeriodArray: index backed by PerioidArray, store in Series / Frame\r\n- [x] Refactor arithmetic methods from DatetimeIndexOpsMixin/DatetimeIndex/TimedeltaIndex/PeriodIndex into array classes (#19902)\r\n- [x] Refactor comparison methods from DTI/TDI/PI into array classes\r\n- [ ] select_dtypes"},{"labels":["api",null,null],"text":"```python\r\nIn [3]: pd.Categorical(['a', 'b']).fillna(value='a', method='ffill')\r\nOut[3]:\r\n[a, b]\r\nCategories (2, object): [a, b]\r\n\r\nIn [4]: pd.Series(pd.Categorical(['a', 'b'])).fillna(value='a', method='ffill')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-141070886f71> in <module>()\r\n----> 1 pd.Series(pd.Categorical(['a', 'b'])).fillna(value='a', method='ffill')\r\n\r\n~/sandbox/pandas-ip/pandas/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs)\r\n   2659                                           axis=axis, inplace=inplace,\r\n   2660                                           limit=limit, downcast=downcast,\r\n-> 2661                                           **kwargs)\r\n   2662\r\n   2663     @Appender(generic._shared_docs['replace'] % _shared_doc_kwargs)\r\n\r\n~/sandbox/pandas-ip/pandas/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast)\r\n   4745         else:\r\n   4746             if method is not None:\r\n-> 4747                 raise ValueError('cannot specify both a fill method and value')\r\n   4748\r\n   4749             if len(self._get_axis(axis)) == 0:\r\n\r\nValueError: cannot specify both a fill method and value\r\n\r\n\r\n```\r\n\r\nBoth of these should raise. PR coming shortly."},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: s = pd.Series(dtype='object')\r\n\r\nIn [3]: s.loc['myint'] = 1\r\n\r\nIn [4]: s.loc['myfloat'] = 2.\r\n\r\nIn [5]: s\r\nOut[5]: \r\nmyint      1.0\r\nmyfloat    2.0\r\ndtype: float64\r\n```\r\n#### Problem description\r\n\r\nWhen an empty Series is added the first object, it does inference on it and sets its dtype ( https://github.com/pandas-dev/pandas/issues/19576#issuecomment-363875752 ). This can be nice... except if the user had passed a specific dtype on construction.\r\n\r\nEmpty objects should (by default) have no dtype set (or have an [\"Any\" dtype](https://github.com/pandas-dev/pandas/issues/19576#issuecomment-364680239)), and inference should be done only in this case.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-5-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 33.1.1\r\nCython: 0.25.2\r\nnumpy: 1.13.3\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.9\r\npatsy: 0.4.1+dev\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nhttplib2: 0.9.2\r\napiclient: None\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"This would let us provide a default `construct_from_string` method.\r\n\r\n> It seems we could have a default implementation for ExtensionDtype.construct_from_string ? (I now just copy pasted from the decimal example, and I think json example also has the same basic one)\r\n\r\nThe default would have to rely on `ExtensionDtype()` being constructable with no arguments, or an `ExtensionDtype.empty` method. For many that'll be\r\n\r\n```python\r\n@classmethod\r\ndef empty(cls):\r\n    return cls()\r\n```\r\n\r\nwhich is easier than implementing `construct_from_string`. Maybe call it `from_empty`."},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> import pandas as pd; \r\n>>> idx_1d = pd.MultiIndex(levels=[[1.0]], labels=[[0]], names=['x'])\r\n>>> idx_2d = pd.MultiIndex(levels=[[1.], [2.]], labels=[[0], [0]], names=['x', 'y'])\r\n\r\n# DataFrame\r\n>>> print(pd.DataFrame({'data': [1]}, idx_2d).to_csv())\r\nx,y,data\r\n1.0,2.0,1\r\n\r\n>>> print(pd.DataFrame({'data': [1]}, idx_1d).to_csv())\r\nx,data\r\n\"('1.0',)\",1\r\n\r\n# Series\r\n>>> print(pd.Series([1], idx_2d).to_csv())\r\n1.0,2.0,1\r\n\r\n>>> print(pd.Series([1], idx_1d).to_csv())\r\n\"('1.0',)\",1\r\n\r\n```\r\n#### Problem description\r\nThe output for `index.nlevels==1` should be the same as for `index.nlevels==2`, with one column less. That matches the output from other index types and was the output produced by pandas up to 0.20.3.\r\n\r\n#### Expected Output\r\n```python\r\n>>> print(pd.DataFrame({'data': [1]}, idx_1d).to_csv())\r\nx,data\r\n1.0,1\r\n\r\n>>> print(pd.Series([1], idx_1d).to_csv())\r\n1.0,1\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.22.0\r\npytest: 3.2.1\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.11.3\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.4.2\r\nnumexpr: 2.6.1\r\nfeather: None\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: None\r\nsqlalchemy: 1.1.5\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"Split from https://github.com/pandas-dev/pandas/pull/19520\r\n\r\nHow to handle things like `__eq__`, `__add__`, etc.\r\n\r\nNeither the Python 2 or Python 3 object defaults are appropriate, so I think we should make it abstract or provide a default implementation for some / all of these.\r\n\r\nWe should be able to do a default implementation for the simple case of binop(extension_array, extension_array) by\r\n\r\n- masking nulls in either self | other\r\n- cast self / other to ndarrays\r\n- compare and apply null mask\r\n\r\n@jorisvandenbossche raises the point that for some extension arrays, casting to an object ndarray can be expensive. It'd be unfortunate to do the casting if we're just going to raise a `TypeError` when the `binop` is done. In this case the subclass will need to override all those ops (or we could add class attribute like `_comparable` and check that before doing `__eq__` or `__lt__`, etc).\r\n\r\nHow much coercion do we want to attempt? I would vote to stay simple and only attempt a comparison if\r\n\r\n    other is an instance of the same ExtensionArray type\r\n    other is a scalar of self.dtype.type.\r\n\r\nOtherwise we return NotImplemented. Subclasses can of course choose more or less aggressive coercion rules.\r\n\r\nWhen boxed in a `Series` or `Index`, I think the goal should be\r\n\r\n1. dispatch to the underlying `.values` comparsion\r\n2. Reconstruct the appropriate output (box in series / index with name)."},{"labels":["api",null,null],"text":"Follow-up issue on #18577\r\n\r\nIn that PR we added a `result_type='reduce'` argument (partly as replacement for the deprecated `reduce` keyword).\r\n\r\nThe 'reduce' behaviour is the default for cases where the function returns a scalar, list, array, dict, .. (I think basically: everything that is not a Series). And in those cases you can then use `result_type='broadcast'|'expand'` to have other results:\r\n\r\n```\r\nIn [32]: df.apply(lambda x: [0, 1, 2], axis=1)\r\nOut[32]: \r\n0    [0, 1, 2]\r\n1    [0, 1, 2]\r\n2    [0, 1, 2]\r\n3    [0, 1, 2]\r\ndtype: object\r\n\r\nIn [33]: df.apply(lambda x: [0, 1, 2], axis=1, result_type='reduce')\r\nOut[33]: \r\n0    [0, 1, 2]\r\n1    [0, 1, 2]\r\n2    [0, 1, 2]\r\n3    [0, 1, 2]\r\ndtype: object\r\n\r\nIn [34]: df.apply(lambda x: [0, 1, 2], axis=1, result_type='expand')\r\nOut[34]: \r\n   0  1  2\r\n0  0  1  2\r\n1  0  1  2\r\n2  0  1  2\r\n3  0  1  2\r\n```\r\n\r\nBut, for Series, we do not honour that argument when it is passed explicitly:\r\n\r\n```\r\nIn [36]: df = pd.DataFrame(np.tile(np.arange(3), 4).reshape(4, -1) + 1, columns=['A', 'B', 'C'])\r\n\r\nIn [37]: df.apply(lambda x: pd.Series([0, 1, 2]), axis=1)\r\nOut[37]: \r\n   0  1  2\r\n0  0  1  2\r\n1  0  1  2\r\n2  0  1  2\r\n3  0  1  2\r\n\r\nIn [38]: df.apply(lambda x: pd.Series([0, 1, 2]), axis=1, result_type='expand')above\r\nOut[38]: \r\n   0  1  2    # <--- default, so same as output above\r\n0  0  1  2\r\n1  0  1  2\r\n2  0  1  2\r\n3  0  1  2\r\n\r\nIn [39]: df.apply(lambda x: pd.Series([0, 1, 2]), axis=1, result_type='broadcast')\r\nOut[39]: \r\n   A  B  C    # <--- with broadcast we preserve original index\r\n0  0  1  2\r\n1  0  1  2\r\n2  0  1  2\r\n3  0  1  2\r\n\r\nIn [40]: df.apply(lambda x: pd.Series([0, 1, 2]), axis=1, result_type='reduce')\r\nOut[40]: \r\n   0  1  2    # <--- should this be a Series of Series objects ?\r\n0  0  1  2\r\n1  0  1  2\r\n2  0  1  2\r\n3  0  1  2\r\n```\r\n\r\nSo should we follow the `result_type='reduce'` here and return a Series of Series objects?\r\n\r\nI know a Series of Series objects is completely useless (but is it that more useless than Series of lists, or Series of arrays? probably yes, but is that worth the inconsistency?). \r\nI think it would be better to either return it as a Series anyhow, or raise an error that we cannot reduce that. IMO this will be more useful in case somebody tries to do this, as it will educate the user about what `result_type='reduce'` is actually meant for, or it can signal that your function is doing something different than you expected."},{"labels":["api",null,null],"text":"Follow-up issue on https://github.com/pandas-dev/pandas/pull/18577\r\n\r\nIn that PR @jreback cleaned up the `apply(..., axis=1)` result shape inconsistencies, and we added a keyword to control this.\r\n\r\nFor example, when the applied function returns an array or a list, it now defaults to returning a Series of those objects, or expanding it to multiple columns if you pass `result_type` explicitly:\r\n\r\n```\r\nIn [1]: df = pd.DataFrame(np.tile(np.arange(3), 4).reshape(4, -1) + 1, columns=['A', 'B', 'C'], index=pd.date_range(\"2012-01-01\", periods=4))\r\n\r\nIn [2]: df\r\nOut[2]: \r\n            A  B  C\r\n2012-01-01  1  2  3\r\n2012-01-02  1  2  3\r\n2012-01-03  1  2  3\r\n2012-01-04  1  2  3\r\n\r\nIn [3]: df.apply(lambda x: np.array([0, 1, 2]), axis=1)\r\nOut[3]: \r\n2012-01-01    [0, 1, 2]\r\n2012-01-02    [0, 1, 2]\r\n2012-01-03    [0, 1, 2]\r\n2012-01-04    [0, 1, 2]\r\nFreq: D, dtype: object\r\n\r\nIn [4]: df.apply(lambda x: np.array([0, 1, 2]), axis=1, result_type='expand')\r\nOut[4]: \r\n            0  1  2\r\n2012-01-01  0  1  2\r\n2012-01-02  0  1  2\r\n2012-01-03  0  1  2\r\n2012-01-04  0  1  2\r\n\r\nIn [5]: df.apply(lambda x: np.array([0, 1, 2]), axis=1, result_type='broadcast')\r\nOut[5]: \r\n            A  B  C\r\n2012-01-01  0  1  2\r\n2012-01-02  0  1  2\r\n2012-01-03  0  1  2\r\n2012-01-04  0  1  2\r\n```\r\n\r\nHowever, for `axis=0`, the default, we don't yet follow the same rules / the keyword in all cases. Some examples:\r\n    \r\n*  For list, it depends on the length (and if the length matches, it preserves the original index instead of new range index):\r\n    ```\r\n    In [16]: df.apply(lambda x: [0, 1, 2, 3])\r\n    Out[16]: \r\n                A  B  C\r\n    2012-01-01  0  0  0\r\n    2012-01-02  1  1  1\r\n    2012-01-03  2  2  2\r\n    2012-01-04  3  3  3\r\n\r\n    In [17]: df.apply(lambda x: [0, 1, 2, 3, 4])\r\n    Out[17]: \r\n    A    [0, 1, 2, 3, 4]\r\n    B    [0, 1, 2, 3, 4]\r\n    C    [0, 1, 2, 3, 4]\r\n    dtype: object\r\n    ```\r\n\r\n    (`result_type='expand'` and `result_type='broadcast'` do work correctly here)\r\n\r\n*   For an array, it expands when the length does not match (so different as for `axis=1`, and also different as for list):\r\n\r\n    ```\r\n    In [23]: df.apply(lambda x: np.array([0, 1, 2, 3]))\r\n    Out[23]: \r\n                A  B  C\r\n    2012-01-01  0  0  0\r\n    2012-01-02  1  1  1\r\n    2012-01-03  2  2  2\r\n    2012-01-04  3  3  3\r\n\r\n    In [24]: df.apply(lambda x: np.array([0, 1, 2, 3, 4]))\r\n    Out[24]: \r\n       A  B  C\r\n    0  0  0  0\r\n    1  1  1  1\r\n    2  2  2  2\r\n    3  3  3  3\r\n    4  4  4  4\r\n    ```\r\n\r\nSo the question is: should we follow the same rules for `axis=0` as for `axis=1`? \r\nI would say: ideally yes. But doing so might break some behaviour (although it might be possible to do that with warnings).\r\n\r\n\r\n     \r\n     \r\n     \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n "},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(\r\n    {'col1': [1, 2], 'col2': [0.5, 0.75]}, index=[400, 800])\r\n\r\ndf.to_dict('index', header=False)\r\n# {400: [1, 0.5], 800: [2, 0.75]}\r\n```\r\n#### Problem description\r\n\r\nIt would be cool if the `DataFrame` [`.to_dict()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html) functionality would support a new option `header=` just as the [`.to_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html) export.\r\n\r\nIn my specific case, the dataframe as above is a matrix with a time series as index and columns of bins.\r\nI would love to export it to a dict with only the `index` as key but a plain 1D array as values.\r\n\r\nCurrently, the `index` export will put a dict of dict, namely the first index being the `index` and the inner index to the `column names`:\r\n\r\n```python\r\ndf = pd.DataFrame(\r\n    {'col1': [1, 2], 'col2': [0.5, 0.75]}, index=[400, 800])\r\n\r\ndf.to_dict('index')\r\n# {400: {'col1': 1.0, 'col2': 0.5}, 800: {'col1': 2.0, 'col2': 0.75}}\r\n```\r\n\r\n#### Expected Output\r\n\r\nMy current work-around looks like this:\r\n```python\r\niteration=df.index.values\r\ndict(zip(\r\n    iteration,\r\n    df.loc[iteration].as_matrix()\r\n))\r\n# {400: array([1. , 0.5]), 800: array([2.  , 0.75])}\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-5-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.utf8\r\nLOCALE: de_DE.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.2.7\r\nCython: None\r\nnumpy: 1.14.0\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n```"},{"labels":["api",null,null,null],"text":"Series[not-categorical] > CategoricalIndex is inconsistent with the reversed operation.  Which one is canonical?\r\n\r\n```\r\nser = pd.Series([1, 2, 3])\r\nidx = pd.CategoricalIndex(['A', 'B', 'A'])\r\n\r\n>>> ser > idx\r\n0    False\r\n1    False\r\n2    False\r\n\r\n>>> idx < ser\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pandas/core/indexes/category.py\", line 752, in _evaluate_compare\r\n    return getattr(self.values, opname)(other)\r\n  File \"pandas/core/arrays/categorical.py\", line 56, in f\r\n    raise TypeError(\"Unordered Categoricals can only compare \"\r\nTypeError: Unordered Categoricals can only compare equality or not\r\n\r\n>>> ser > idx.values\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pandas/core/ops.py\", line 819, in wrapper\r\n    .format(op=op, typ=self.dtype))\r\nTypeError: Cannot compare a Categorical for op <built-in function gt> with Series of dtype int64.\r\nIf you want to compare values, use 'series <op> np.asarray(other)'.\r\n```\r\n\r\nI'm guessing the right thing to do is to a) have `Series[categorical].__op__` wrap `CategoricalIndex.__op__`, and b) have `Series[non-categorical]` to dispatch to reversed-op for `is_categorical_dtype(other)`, want to confirm before making a PR."},{"labels":["api",null,null],"text":"#### Problem description\r\n\r\nI'd like to suggest a modification to [df.pop(item)](https://github.com/pandas-dev/pandas/blob/a00154dcfe5057cb3fd86653172e74b6893e337d/pandas/core/generic.py#L632-L680). Currently, `pop(item)` deletes the column from the dataframe it's being called on and returns that column as a series. It doesn't accept multiple items.\r\n\r\nIt might be a nice convenience to:\r\n\r\n1. pop multiple columns at once (ex: `pop(['A', 'B'])`\r\n2. specifying an `axis` parameter (default: axis=1) to allow popping rows and columns (ex: `pop(1, axis=0)`)\r\n3. pop slices (ex: `pop([1:3], axis=1)`)\r\n\r\nThought I'd throw it out there to the pandas gods and see if it is interesting. If it's not the best API design decision for `pop`, I completely understand.\r\n\r\n#### Common use-case\r\n\r\n1. you have one or multiple problem rows you want to delete from a dataframe but still keep for later evaluation. You'd just pop the rows and they'd be deleted from your existing dataframe and saved to a new variable.\r\n2. many times people seem to need to pop the last row, or second row. It is easy to pop the last row using `.iloc[:-1]` but popping the second row in one swoop isn't as easy I think. It could be if you just pop it out of there using pop.\r\n3. sometimes people loop through a dataframe. not recommended I understand, but in such a scenario, you could pop a row based on a condition while looping perhaps in a complex manner.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n```python\r\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]},\r\n                  columns=['A', 'B', 'C'])\r\n\r\ndef pop(df, values, axis=1):\r\n    if axis == 0:\r\n        if isinstance(values, (list, tuple)):\r\n            popped_rows = df.loc[values]\r\n            df.drop(values, axis=0, inplace=True)\r\n            return popped_rows\r\n        elif isinstance(values, (int)):\r\n            popped_row = df.loc[values].to_frame().T\r\n            df.drop(values, axis=0, inplace=True)\r\n            return popped_row\r\n        else:\r\n            print('values parameter needs to be a list, tuple or int.')\r\n    elif axis == 1:\r\n        # current df.pop(values) logic here\r\n        return df.pop(values)\r\n```\r\n\r\n#### Example Usage\r\n```python\r\n# example df\r\n>>> df\r\n   A  B  C\r\n0  1  4  7\r\n1  2  5  8\r\n2  3  6  9\r\n\r\n# pop multiple indices, delete from df inplace, return popped rows\r\n# the df param wouldn't exist in the pop method; it'd be self\r\n# df param just shown here to illustrate the idea\r\n>>>pop(df, [0, 2], axis=0)\r\n   A  B  C\r\n0  1  4  7\r\n2  3  6  9\r\n\r\n# pop one index value, delete from df, return row as a dataframe (not series)\r\n>>> pop(df, 1, axis=0)\r\n   A  B  C\r\n1  2  5  8\r\n\r\n\r\n```\r\n\r\n#### Demand for such a feature\r\n[How to pop rows from a dataframe?](https://stackoverflow.com/questions/42285806/how-to-pop-rows-from-a-dataframe)"},{"labels":["api",null,null],"text":"```python\r\nIn [6]: pd.Categorical(['a', 'a', 'b', 'b']).get_values()\r\nOut[6]: array(['a', 'a', 'b', 'b'], dtype=object)\r\n\r\nIn [7]: pd.Categorical(pd.DatetimeIndex(['2017', '2017', '2018', '2018'])).get_values()\r\nOut[7]: DatetimeIndex(['2017-01-01', '2017-01-01', '2018-01-01', '2018-01-01'], dtype='datetime64[ns]', freq=None)\r\n```\r\n\r\nMeanwhile, `Series[Categorical[datetime]].get_values` returns a NumPy array.\r\n\r\n```python\r\nIn [8]: pd.Series(pd.Categorical(pd.DatetimeIndex(['2017', '2017', '2018', '2018']))).get_values()\r\nOut[8]:\r\narray(['2017-01-01T00:00:00.000000000', '2017-01-01T00:00:00.000000000',\r\n       '2018-01-01T00:00:00.000000000', '2018-01-01T00:00:00.000000000'],\r\n      dtype='datetime64[ns]')\r\n```\r\n\r\nThis seemed strange to have the return type depend on the categories' dtype. Going to try to find the original motivation. We could consider deprecating this behavior."},{"labels":["api",null],"text":"This has been mentioned in passing in a few other issues, so making an official issue to document this.  In the same spirit as `PeriodBlock` (#7964).\r\n\r\nI've started working on an interval accessor, so it seems likely that there will be more exposure to `Interval` objects in a `Series`/`DataFrame`, hence this becoming more relevant.\r\n\r\nxref #19272"},{"labels":["api",null,null,null],"text":"Since `Period` objects are inherently intervals of time, it seems like there should be a way to access `Interval` properties and methods.  Inheritance has been suggested in passing in other issues, and seems reasonable.\r\n\r\nAs an example benefit, #9089 suggests implementing `Period.duration`.  If `Period` were to inherit from `Interval`, this would already be implemented via the `Interval.length` property:\r\n\r\n```python\r\nIn [2]: p = pd.Period('2018Q1', freq='Q')\r\n\r\nIn [3]: p_iv = pd.Interval(p.start_time, (p + 1).start_time, closed='left')\r\n\r\nIn [4]: str(p_iv)\r\nOut[4]: '[2018-01-01, 2018-04-01)'\r\n\r\nIn [5]: p_iv.length\r\nOut[5]: Timedelta('90 days 00:00:00')\r\n```\r\n\r\nLikewise for `PeriodIndex` inheriting from `IntervalIndex`.  Would be nice to have cross compatibility between the two, e.g. `IntervalIndex.overlaps(Period)` or `PeriodIndex.get_loc(Interval)`."},{"labels":["api",null],"text":"\r\n```python\r\nimport pandas as pd\r\npd.io.formats.excel.header_style = None\r\n```\r\n\r\nAfter updating, it is no longer possible to override the excel header styles.\r\n\r\nPreviously, this was possible in 0.19.2:\r\n\r\n`pd.formats.format.header_style = None`\r\n\r\nIt seems header_style has been moving around constantly lately... Can we please put it somewhere and leave it there??? Or, if it's going to be moved constantly, it should live on a private module path. I realize python doesn't have private scoping, but there are paths in the pandas module that are privately scoped, by convention, yet don't follow the standard python convention of underscoring the names.\r\n\r\nHow are we supposed to accomplish the removal of styles on the latest build?\r\n\r\nThat module path DOES indeed exist... but something is wonky with the imports, it seems.\r\n\r\nException is raised:\r\n`AttributeError: 'module' object has no attribute 'excel' `\r\n\r\nPossibly related? https://github.com/pandas-dev/pandas/pull/15530"},{"labels":["api",null,null],"text":"This was brought up by @shoyer in a different issue, and it makes sense to me\r\n\r\nAdding `closed` to `IntervalDtype` makes sense to me, as in my mind `closed` is inherently part of the dtype.  Since differing `closed` makes two `IntervalIndex` incompatible, I'd expect their dtypes to not be equal, but they currently are, which seems a little strange:\r\n```python\r\nIn [2]: ii1 = pd.interval_range(0, 3, closed='left')\r\n\r\nIn [3]: ii2 = pd.interval_range(0, 3, closed='right')\r\n\r\nIn [4]: ii1.dtype == ii2.dtype\r\nOut[4]: True\r\n```\r\n\r\nThere's also a larger discussion as to if adding `closed` to `IntervalDtype` would allow us to remove `closed` as a parameter to the `IntervalIndex` constructor.  My preference would be to keep the `closed` parameter for user convenience, similar to how `CategoricalIndex` accepts `categories` and `ordered` parameters despite also accepting `CategoricalDtype` (though maybe this is just for legacy reasons).  Willing to be convinced otherwise though.\r\n\r\nxref https://github.com/pandas-dev/pandas/issues/19263#issuecomment-359986104 (original comment)\r\nxref #19370 (might be rendered moot)\r\n\r\ncc @shoyer"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\nThe `closed` parameter of `IntervalIndex` is currently a bit inconsistent, and it's behavior is input dependent.  By my count, there are 3 different behaviors:\r\n\r\n**1.  `closed` parameter overrides the inferred `closed` when the input data is empty or purely NA**\r\n```python\r\nIn [2]: pd.IntervalIndex([], closed='both')\r\nOut[2]:\r\nIntervalIndex([]\r\n              closed='both',\r\n              dtype='interval[int64]')\r\n\r\nIn [3]: pd.IntervalIndex([np.nan, np.nan], closed='neither')\r\nOut[3]:\r\nIntervalIndex([nan, nan]\r\n              closed='neither',\r\n              dtype='interval[float64]')\r\n```\r\n\r\n**2. `closed` is ignored when the input data is an `IntervalIndex`:**\r\n```python\r\nIn [4]: ii = pd.interval_range(0, 3, closed='both')\r\n\r\nIn [5]: ii\r\nOut[5]:\r\nIntervalIndex([[0, 1], [1, 2], [2, 3]]\r\n              closed='both',\r\n              dtype='interval[int64]')\r\n\r\nIn [6]: pd.IntervalIndex(ii, closed='neither')\r\nOut[6]:\r\nIntervalIndex([[0, 1], [1, 2], [2, 3]]\r\n              closed='both',\r\n              dtype='interval[int64]')\r\n```\r\n\r\n**3. `closed` raises a `ValueError` when a list-like of `Interval` objects with a conflicting value for `closed` is passed as input data:**\r\n```python\r\nIn [7]: ivs = [pd.Interval(0, 1, closed='both'), pd.Interval(1, 2, closed='both')]\r\n\r\nIn [8]: pd.IntervalIndex(ivs, closed='neither')\r\n---------------------------------------------------------------------------\r\nValueError: conflicting values for closed: constructor got 'neither', inferred from data 'both'\r\n```\r\nxref #18421 where I implemented this behavior; I'm no longer sure it's the best approach to take.\r\n\r\n\r\n#### Problem description\r\nThis behavior is inconsistent, and could lead to confusion.  As it's currently implemented, the `closed` parameter is useless in the majority of cases, either doing nothing or raising an error.\r\n\r\n#### Expected Output\r\nI'd expect there to be consistency as to how the `closed` parameter is handled.  My preference would be to have it override the inferred `closed` if explicitly passed by the user.  Note that this is consistent with how the `dtype` parameter is used in constructors throughout the codebase.\r\n\r\nxref #19371 (may render this issue moot)"},{"labels":["api",null,null,null],"text":"This one is a bit complex to explain, but I'll do my best.\r\n\r\nCurrently ``IntervalIndex.get_indexer`` fails if the other index doesn't contain  ``Interval`` only (there's also another bug, but let's keep it simple here).\r\n\r\nThe underlying issue is that ``IntervalIndex.get_indexer`` depends on ``IntervalIndex.get_loc`` which is ambigous for how it treats number inputs:\r\n\r\n```python\r\n>> ii = pd.IntervalIndex.from_breaks([0,1,2,3])\r\n>> ii.get_loc(pd.Interval(1, 2))\r\n1  # ok\r\n>> ii.get_loc(1)  # do we mean exactly 1, or if an interval contains the number 1?\r\n1  # ambigous\r\n```\r\nThe issue is that ``get_loc`` returns the location for both exact matches and inexact matches (i.e. if the number input is in an interval). For the purposes of ``get_indexer`` however, this behavious fails, as ``get_indexer`` needs ``get_loc`` to find exact matches only.\r\n\r\nSee https://github.com/pandas-dev/pandas/pull/19021#issuecomment-359544778 for further discussion.\r\n\r\n## Solution\r\nA solution could be adding a ``'strict'`` option to the ``method`` parameter of ``IntervalIndex.get_loc``.\r\n\r\nThis wasn't so difficult after all, and I've already made a PR on this, see #19353"},{"labels":["api",null,null,null],"text":"I've seen code like\r\n\r\n```python\r\nIn [36]: s = pd.Series(range(10))\r\n\r\nIn [37]: pd.concat([s.diff(i).rename(f\"L{i}\") for i in range(10)], axis=1)\r\nOut[37]:\r\n    L0   L1   L2   L3   L4   L5   L6   L7   L8   L9\r\n0  0.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n1  0.0  1.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n2  0.0  1.0  2.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n3  0.0  1.0  2.0  3.0  NaN  NaN  NaN  NaN  NaN  NaN\r\n4  0.0  1.0  2.0  3.0  4.0  NaN  NaN  NaN  NaN  NaN\r\n5  0.0  1.0  2.0  3.0  4.0  5.0  NaN  NaN  NaN  NaN\r\n6  0.0  1.0  2.0  3.0  4.0  5.0  6.0  NaN  NaN  NaN\r\n7  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  NaN  NaN\r\n8  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  NaN\r\n9  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0\r\n```\r\n\r\nCould we simplify that by allowing `s.diff(range(10), prefix=\"L\")`?\r\n\r\nHow would we handle DataFrames? Prefix with the column name and a separator, like `get_dummies`?"},{"labels":["api",null,null],"text":"- ``pd.Index`` accepts ``name`` - great\r\n- ``pd.MultiIndex`` accepts ``names`` - after all, they are multiple ``name``s\r\n- ``pd.Index`` accepts ``names`` for compatibility with ``pd.MultiIndex`` - well, OK, after all, that constructor _can_ result in a ``MultiIndex``\r\n- ``pd.Index`` should accept ( #19082 ) ``names`` even when it results in a flat index - well, OK, still for compatibility\r\n- ``pd.MultiIndex`` accepts ``name`` for compatibility  - wait, wasn't ``names`` already provided for compatibility?! OK, forget it, go for ``name``.\r\n- ``pd.Index`` accepts ``name`` even for multiple levels' names, for compatibility - with ``MultiIndex``, which accepts it for compatibility with ``pd.Index`` - aaaaalright\r\n- All ``pd.Index`` subclasses (which will _never_ result in multiple levels, and which currently in some cases discard ``names``, in other cases raise an error) [should accept](https://github.com/pandas-dev/pandas/pull/19168/files#r161731210) ``names`` for compatibility - no, wait.\r\n\r\n####  Proposal 1\r\n\r\n1. There is _one_ way to have compatibility across any kind of ``Index`` (sub)class constructor, and it is ``name``. When the constructor results in a ``MultiIndex`` (which can happen with ``pd.Index`` or ``pd.MultiIndex``), then ``name`` should be list-like, and each level will \"receive\" the respective component\r\n\r\n2. in those cases - and only in those cases - in which a constructor _actually_ results in a ``MultiIndex``, ``names`` can be used as an alias for ``name``. In all other cases, it is not accepted.\r\n\r\nor alternatively:\r\n\r\n2. (b) (my preference) ``names`` is deprecated (in all cases in which it is currently supported, and remains unsupported in other constructors/cases)\r\n\r\n3. (c) (tradeoff between mental health and backward compatibility) ``names`` is supported in ``pd.MultiIndex``, still supported but deprecated in ``pd.Index`` when resulting in ``MultiIndex`` (and remains unsupported in other constructors/cases)\r\n\r\nCorollary:\r\n\r\n3. ``names`` will _not_ be supported by any constructor that is not ``pd.Index`` or ``pd.MultiIndex``.\r\n\r\nNotice that a 1-level ``MultiIndex`` is still a ``MultiIndex``. That is,\r\n\r\n- ``pd.Index([('a',), ('b',)], name=('only_one_name',))`` will still work\r\n- ``pd.Index([('a',), ('b',)], names=('only_one_name',))`` will still work (at least as long as we don't deprecate ``names`` entirely)\r\n- ``pd.Index([('a',), ('b',)], name='only_one_name')`` will still _not_ work\r\n\r\n####  Proposal 2\r\n\r\n1. There is _one_ way to have compatibility across any kind of ``Index`` (sub)class constructor, and it is ``names``, which must _always_ be list-like.\r\n\r\n2. In those cases in which the constructor results in a flat index, ``name`` is also accepted, and interpreted as ``names=(name,)``; instead ``name`` is deprecated for ``pd.MultiIndex``, and for ``pd.Index`` when resulting in a ``MultiIndex`` (_even if_ with 1 level)\r\n\r\nCorollary:\r\n\r\n3. implementation-wise, we will want to decorate all ``__new__`` of non-``MultiIndex`` subclasses to convert ``names`` to ``name``"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# create ordinary example data frame\r\nmyDf = pd.DataFrame(data={'myCol': pd.Series(np.random.randn(1000)).cumsum()})\r\n\r\n# rolling + apply example 1: this works as expected because win_type=None returns a core.window.Rolling which provides apply functionality\r\nmyDf2 = myDf.rolling(window=50, center=True, win_type=None).apply(func=lambda x2: np.percentile(a=x2, q=50, interpolation='nearest'))\r\n# rolling + apply example 2: this does not work because as soon as users change win_type to be anything else than None a core.window.Window is returned instead - which does *not* provide apply functionality yet\r\nmyDf3 = myDf.rolling(window=50, center=True, win_type='hamming').apply(func=lambda x2: np.percentile(a=x2, q=50, interpolation='nearest'))\r\n```\r\n#### Problem description\r\n\r\n`core.window.Window` does not yet provide apply functionality like `core.window.Rolling` does, hence functions other than the readily provided ones (like `sum()`, `mean()`, etc) cannot be applied to it.\r\n\r\n(I'm not to deep into the details, but IMHO returning different object types for just changing the window type from rectangular to some other window form seems rather unintuitive altogether. What was the reason for it being designed this way? I couldn't quickly come up with any. It leads to e.g. example 1 from above working fine as long as the `win_type` parameter is untouched - but breaks as soon as `win_type` is changed to a different-than-rectangular window, which does not semantically seem to make sense.)\r\n\r\n#### Expected Output\r\n\r\n`core.window.Window` should provide apply functionality, probably via `.apply()` like `core.window.Rolling`, so that example 2 from above works fine too.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.13.0-26-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: 3.2.1\r\npip: 9.0.1\r\nsetuptools: 36.5.0.post20170921\r\nCython: 0.26.1\r\nnumpy: 1.13.3\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.0\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n"},{"labels":["api",null,null],"text":"We're inconsistent with excluding nuisance columns\r\n\r\n```python\r\nIn [37]: df = pd.DataFrame({\r\n    ...:     \"A\": [1, 2, 3, 4],\r\n    ...:     \"B\": pd.interval_range(0, 4),\r\n    ...: })\r\n\r\nIn [38]: df.dtypes\r\nOut[38]:\r\nA     int64\r\nB    object\r\ndtype: object\r\n\r\nIn [39]: df.sum()\r\nOut[39]:\r\nA    10\r\ndtype: int64\r\n\r\nIn [40]: df.cumsum()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-40-1c578612e53e> in <module>()\r\n----> 1 df.cumsum()\r\n\r\n~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/generic.py in cum_func(self, axis, skipna, *args, **kwargs)\r\n   7781             mask = isna(self)\r\n   7782             np.putmask(y, mask, mask_a)\r\n-> 7783             result = accum_func(y, axis)\r\n   7784             np.putmask(result, mask, mask_b)\r\n   7785         else:\r\n\r\n~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/generic.py in <lambda>(y, axis)\r\n   7400         cls.cumsum = _make_cum_function(\r\n   7401             cls, 'cumsum', name, name2, axis_descr, \"cumulative sum\",\r\n-> 7402             lambda y, axis: y.cumsum(axis), \"sum\", 0., np.nan)\r\n   7403         cls.cumprod = _make_cum_function(\r\n   7404             cls, 'cumprod', name, name2, axis_descr, \"cumulative product\",\r\n\r\nTypeError: unsupported operand type(s) for +: 'pandas._libs.interval.Interval' and 'pandas._libs.interval.Interval'\r\n```\r\n\r\nI'll update this with more dtypes (datetime, period, categorical) and more aggregations. Ideally we'd be consistent between the cumulative and non-cumulative versions."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\nThe `IntervalIndex` constructor takes a `dtype` parameter but doesn't use it (no references to it in the code):\r\n\r\n```python\r\nIn [2]:  pd.IntervalIndex([pd.Interval(0, 1), pd.Interval(2, 3)], dtype='float64')\r\nOut[2]:\r\nIntervalIndex([(0, 1], (2, 3]]\r\n              closed='right',\r\n              dtype='interval[int64]')\r\n```\r\n\r\nThe constructor methods do not support a `dtype` parameter:\r\n```python\r\nIn [3]: pd.IntervalIndex.from_intervals([pd.Interval(0, 1), pd.Interval(2, 3)], dtype='float64')\r\n---------------------------------------------------------------------------\r\nTypeError: from_intervals() got an unexpected keyword argument 'dtype'\r\n\r\nIn [4]: pd.IntervalIndex.from_arrays([0, 2], [1, 3], dtype='float64')\r\n---------------------------------------------------------------------------\r\nTypeError: from_arrays() got an unexpected keyword argument 'dtype'\r\n\r\nIn [5]: pd.IntervalIndex.from_breaks([0, 1, 2, 3], dtype='float64')\r\n---------------------------------------------------------------------------\r\nTypeError: from_breaks() got an unexpected keyword argument 'dtype'\r\n\r\nIn [6]: pd.IntervalIndex.from_tuples([(0, 1), (2, 3)], dtype='float64')\r\n---------------------------------------------------------------------------\r\nTypeError: from_tuples() got an unexpected keyword argument 'dtype'\r\n```\r\n\r\n#### Expected Output\r\n\r\nI'd expect the `IntervalIndex` constructor and constructor methods to support a `dtype` parameter, which can be used to override the inferred dtype.\r\n\r\n"},{"labels":["api",null,null],"text":""},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> ndt = np.dtype(object)\r\n>>> pdt = pd.api.types.CategoricalDtype(categories=['German', 'English', 'French'])\r\n>>> pdt == ndt\r\nFalse  # ok\r\n>>> ndt == pdt\r\nTypeError: data type not understood\r\n```\r\n#### Problem description\r\n\r\nThe dtypes are not always comparable, The same issue is with IntervalDtype and if the numpy types are oher kinds (int, float, dates etc.).\r\n\r\nThe issue may be a numpy issue and not a pandas issue, but I raise it here for discussion first, and can later file an issue at the numpy repository.\r\n\r\n#### Expected Output\r\n\r\nExpected was ``False``.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 78d4e5db4f04086f2006796840bf6f1882de2afb\r\npython: 3.6.3.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.22.0.dev0+561.g78d4e5d\r\npytest: 3.3.1\r\npip: 9.0.1\r\nsetuptools: 38.2.5\r\nCython: 0.26.1\r\nnumpy: 1.13.3\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.4.9\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 1.0b10\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"when I originally added first class timedelta support I allowed ``.astype('m8[unit]')`` to return a float, except if unit==ns. IOW it was de-facto the same a dividing by ``Timedelta(1, unit=unit)``\r\n\r\n```\r\nIn [1]: s = Series(pd.to_timedelta(['1 day', '1 minutes', '1 second']))\r\n\r\nIn [2]: s\r\nOut[2]: \r\n0   1 days 00:00:00\r\n1   0 days 00:01:00\r\n2   0 days 00:00:01\r\ndtype: timedelta64[ns]\r\n\r\nIn [3]: s.astype('timedelta64[s]')\r\nOut[3]: \r\n0    86400.0\r\n1       60.0\r\n2        1.0\r\ndtype: float64\r\n\r\nIn [4]: s / pd.Timedelta('1 s')\r\nOut[4]: \r\n0    86400.0\r\n1       60.0\r\n2        1.0\r\ndtype: float64\r\n\r\nIn [5]: s.astype('timedelta64[ns]')\r\nOut[5]: \r\n0   1 days 00:00:00\r\n1   0 days 00:01:00\r\n2   0 days 00:00:01\r\ndtype: timedelta64[ns]\r\n\r\n```\r\n\r\nnote that for [5], however we just return a timedelta64[ns] (which happens to be the underlying data representation).\r\n\r\n#19224 fixes construction of non-ns (and preserves the astype freq conversions. I think that this is confusing to the user and we should revert this and have a ``m8[unit]`` just work the same as it does for ``M8[unit]`` namely that you get back a timedelta dtype (and not a float), **that is the same**\r\n\r\nSo [3] would be the same as [5]\r\n\r\nWe do this for datetime types now\r\n```\r\nIn [13]: Series(pd.date_range('20170101', periods=3)).astype('M8[s]')\r\nOut[13]: \r\n0   2017-01-01\r\n1   2017-01-02\r\n2   2017-01-03\r\ndtype: datetime64[ns]\r\n```\r\n\r\nsure this is slightly convenient but a bit confusing.\r\n\r\nI would propose that we provide a deprecation warning and change it in the next version."},{"labels":["api",null,null,null,null,null,null],"text":"xref #19187 \r\n\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\nIn [2]: pd.DatetimeIndex(['2000-02-01', '2000-02-02']).is_all_dates\r\nOut[2]: True\r\n\r\nIn [3]: pd.DatetimeIndex(['2000-02-01', '2000-02-02']).append(pd.Index(['not a date']))[:2].is_all_dates\r\nOut[3]: True\r\n\r\nIn [4]: pd.PeriodIndex(['2000', '2001'], freq='A').is_all_dates\r\nOut[4]: True\r\n\r\nIn [5]: pd.PeriodIndex(['2000', '2001'], freq='A').append(pd.Index(['not a date']))[:2].is_all_dates\r\nOut[5]: False\r\n\r\nIn [6]: pd.TimedeltaIndex([2000, 2001]).append(pd.Index(['not a date']))[:2].is_all_dates\r\nOut[6]: False\r\n```\r\n\r\n#### Problem description\r\n\r\n``is_all_dates`` is smart for ``object`` indexes but only for ``DatetimeIndex``, which is inconsistent since the other datetimelike indexes are also considered ``is_all_dates``.\r\n\r\n#### Expected Output\r\n\r\n``Out[5]:`` and ``Out[6]:`` should be the same as ``Out[3]:``.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Problem description\r\n\r\nCurrently `IntervalIndex.astype` doesn't do anything when passed an `IntervalDtype`:\r\n```python\r\nIn [2]: ii = pd.interval_range(0.0, 3.0)\r\n\r\nIn [3]: ii\r\nOut[3]:\r\nIntervalIndex([(0.0, 1.0], (1.0, 2.0], (2.0, 3.0]]\r\n              closed='right',\r\n              dtype='interval[float64]')\r\n\r\nIn [4]: dtype = IntervalDtype('int64')\r\n\r\nIn [5]: ii.astype(dtype)\r\nOut[5]:\r\nIntervalIndex([(0.0, 1.0], (1.0, 2.0], (2.0, 3.0]]\r\n              closed='right',\r\n              dtype='interval[float64]')\r\n```\r\n\r\nThis is because the current implementation of `IntervalIndex.astype` doesn't distinguish between different `IntervalDtype`, and treats them all as equivalent to the existing dtype:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/8acdf801c501a0ce2ac14ddd676cd248f0f32180/pandas/core/indexes/interval.py#L700-L702\r\n\r\n\r\n#### Expected Output\r\nI'd expect the subtype to be converted to `'int64'`:\r\n```python\r\nOut[5]:\r\nIntervalIndex([(0, 1], (1, 2], (2, 3]]\r\n              closed='right',\r\n              dtype='interval[int64]')\r\n```"},{"labels":["api",null,null,null],"text":"xref #16079 (adding lowercase `n/a` to default missing values) - I have a case where I want to treat `'n/a'` as non-missing, but keep the rest of the default behavior.  It seems this is about the best I can do, which involves an internal value.\r\n```python\r\nfrom io import StringIO\r\ndata = '''a,b\r\n1,2\r\n3,n/a'''\r\npd.read_csv(StringIO(data), \r\n    na_values=pd.io.common._NA_VALUES.difference(['n/a']), \r\n    keep_default_na=False)\r\n```\r\nNot suggesting reverting #16079, but wonder if if there might be a better way to expose through some kind of option?  Initial idea was something like `pd.options.na_values` (set to the current `_NA_VALUES`), though might be issues using a mutable value as an option.   Also could expose something like `pd.io.DEFAULT_NA_VALUES`\r\n\r\n"},{"labels":["api",null,null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\ndf=pd.DataFrame({'Year': np.random.randint(2000,2017,10000), 'Month': np.random.randint(1,12,10000), 'Data': np.random.randint(0,100,10000)})\r\ngrouped=df.groupby(['Year','Month', pd.cut(df.Data, range(0,100,10))]).size().unstack()\r\n\r\n# This doesn't work:\r\ngrouped.reset_index() #returns TypeError: unorderable types: int() < str()\r\n\r\n# This works:\r\ngrouped.columns=grouped.columns.astype('str')\r\ngrouped.reset_index()\r\n\r\n```\r\n#### Problem description\r\n\r\nreset_index() should work with dataframes that have any types of columns.\r\n\r\n#### Expected Output\r\n\r\nTwo extra columns with multiindex content, in the example above - Year and Month.\r\nColumn's type changes to string? Or documentation should specify that reset_index() doesn't work with specific types of columns.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\n\r\npandas: 0.22.0\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.4.0\r\nCython: 0.26\r\nnumpy: 1.13.3\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: None\r\nmatplotlib: 2.1.1\r\nopenpyxl: None\r\nxlrd: 1.1.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.9999999\r\nsqlalchemy: 1.2.0\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: df = pd.DataFrame(np.arange(16).reshape(4, 4), index=pd.MultiIndex.from_product([[1, 2], ['a', 'b']]), columns=['a', 'b', 'c', 'd'])\r\n\r\nIn [3]: df.loc[2, 'a'] # select a row: good\r\nOut[3]: \r\na     8\r\nb     9\r\nc    10\r\nd    11\r\nName: (2, a), dtype: int64\r\n\r\nIn [4]: df.loc[2, 'c'] # select a (part of) col: guessing game, but I understand it is a feature\r\nOut[4]: \r\na    10\r\nb    14\r\nName: c, dtype: int64\r\n\r\nIn [5]: df.loc[2, 'e'] = -1 # now there is no column: add a row?\r\n\r\nIn [6]: df # ... nope, still adds a column\r\nOut[6]: \r\n      a   b   c   d    e\r\n1 a   0   1   2   3  NaN\r\n  b   4   5   6   7  NaN\r\n2 a   8   9  10  11 -1.0\r\n  b  12  13  14  15 -1.0\r\n\r\nIn [7]: df.loc[3, 'f'] = -2 # what if the row label is entirely missing?\r\n\r\nIn [8]: df # sitll adds a row _and_ a col\r\nOut[8]: \r\n        a     b     c     d    e    f\r\n1 a   0.0   1.0   2.0   3.0  NaN  NaN\r\n  b   4.0   5.0   6.0   7.0  NaN  NaN\r\n2 a   8.0   9.0  10.0  11.0 -1.0  NaN\r\n  b  12.0  13.0  14.0  15.0 -1.0  NaN\r\n3     NaN   NaN   NaN   NaN  NaN -2.0\r\n```\r\n#### Problem description\r\n\r\nIn general, if ``df.index`` is a ``MultiIndex``, pandas interprets the syntax ``df.loc[a, b]`` as ``df.loc[(a,b),:]``.\r\n\r\n``Out[4]:`` is (debatable, but) understandable: in absence of the desired row, and in presence of a column with the same name, it interprets as ``df.loc[(a,), b]``.\r\n\r\nHowever, there is no reason why ``Out[5]:`` and ``Out[6]:`` should _add_ a column: since priority when labels are present goes to the index, the same should happen when labels are absent.\r\n\r\nSomewhat related to #17024 .\r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [8]: df\r\nOut[8]: \r\n        a     b     c     d\r\n1 a   0.0   1.0   2.0   3.0\r\n  b   4.0   5.0   6.0   7.0\r\n2 a   8.0   9.0  10.0  11.0\r\n  b  12.0  13.0  14.0  15.0\r\n  e  -1.0  -1.0  -1.0  -1.0\r\n3 f  -2.0  -2.0  -2.0  -2.0\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-4-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.23.0.dev0+42.g93033151a\r\npytest: 3.2.3\r\npip: 9.0.1\r\nsetuptools: 36.7.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n\r\n  "},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn : pd.Timedelta('1 day').isoformat()\r\nOut: 'P1DT0H0M0S'\r\n\r\nIn : pd.Timedelta(pd.Timedelta('1 day').isoformat())\r\nOut: ValueError: invalid abbreviation: P\r\n```\r\n#### Problem description\r\n\r\nA ``Timedelta`` can be formatted as an ISO 8601 Duration, but cannot be constructed from one. This makes it difficult to natively parse and convert any ISO 8601 Durations into their respective ``Timedelta`` objects. \r\n\r\nFor a specific example, ``write_json`` takes a ``Timedelta`` and writes it out in the ISO format, but ``read_json`` cannot reasonably parse that format back into a ``Timedelta`` object without such a constructor (see #19039)\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 4319335aadd7355e54236bde2ab6dc130eedf9ad\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.23.0.dev0+31.g4319335aa\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.5\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"Opening a new issue so this isn't lost.\r\n\r\nIn https://github.com/pandas-dev/pandas/pull/18882 banned duplicate names in a MultiIndex. I think this is a good change since allowing duplicates hit a lot of edge cases when you went to actually do something. I want to make sure we understand all the cases that actually produce duplicate names in the MI though, specifically groupby.apply.\r\n\r\n```python\r\nIn [1]: import dask.dataframe as dd\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]:     pdf = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n   ...:                         'b': [4, 5, 6, 3, 2, 1, 0, 0, 0]},\r\n   ...:                        index=[0, 1, 3, 5, 6, 8, 9, 9, 9]).set_index(\"a\")\r\n   ...:\r\n   ...:\r\n\r\nIn [4]: pdf.groupby(pdf.index).apply(lambda x: x.b)\r\n```\r\n\r\nAnother, more realistic example: groupwise drop_duplicates:\r\n\r\n\r\n```python\r\nIn [18]: df = pd.DataFrame({\"B\": [0, 0, 0, 1, 1, 1, 2, 2, 2]}, index=pd.Index([0, 1, 1, 2, 2, 2, 0, 0, 1], name='a'))\r\n\r\nIn [19]: df\r\nOut[19]:\r\n   B\r\na\r\n0  0\r\n1  0\r\n1  0\r\n2  1\r\n2  1\r\n2  1\r\n0  2\r\n0  2\r\n1  2\r\n\r\nIn [20]: df.groupby('a').apply(pd.DataFrame.drop_duplicates)\r\nOut[20]:\r\n     B\r\na a\r\n0 0  0\r\n  0  2\r\n1 1  0\r\n  1  2\r\n2 2  1\r\n```\r\n\r\nIs it possible to throw a warning on this for now, in case duplicate names are more common than we thought?"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\nThis is more of an open question whether this should be implemented or not. If a DataFrame has a SparseSeries inside of it shouldn't 'density' also be defined?\r\n\r\n```python\r\ndf = pd.DataFrame({'a': pd.SparseSeries([1,0,0,1])})\r\ndf.a.density #=> 0.5\r\ndf.density #=> Throws error I would think this should be 0.5\r\n\r\nsdf = pd.SparseDataFrame({'a': pd.SparseSeries([1,0,0,1])})\r\nsdf.density #=> 0.5\r\nsdf.a.density #=> 0.5\r\n```\r\n#### Problem description\r\n\r\nBasically this is a consistency problem between SparseDataFrame and DataFrame. Since DataFrame's can contain SparseSeries it should probably define 'density' as well.\r\n\r\n#### Expected Output\r\n\r\nI would expect a DataFrame to have density defined. If it is dense it would just be 1.0.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.13.16-202.fc26.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: 3.3.1\r\npip: 9.0.1\r\nsetuptools: 28.8.0\r\nCython: 0.27.3\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: 0.10.0\r\nIPython: 6.1.0\r\nsphinx: 1.6.5\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: 1.5.1\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: 0.4.0\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.9\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.1\r\nbs4: 4.6.0\r\nhtml5lib: 1.0b10\r\nsqlalchemy: 1.1.15\r\npymysql: 0.7.11.None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: 0.1.2\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: 2 in pd.MultiIndex.from_product([[1, 2], [3, 4]]).drop(2)\r\nOut[2]: True\r\n```\r\n\r\n#### Problem description\r\n\r\n#2770 was actually more complex than what I understood when I suggested to close it. While ``remove_unused_levels()`` did solve the problem of just resetting the ``.levels``, the above is still buggy. Ironically, the above is [even tested](https://github.com/pandas-dev/pandas/blob/master/pandas/tests/frame/test_mutate_columns.py#L198).\r\n\r\n#### Expected Output\r\n\r\n``False``\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 84335621ad0a0d83302a80b6911d3985c00b5cee\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-4-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.23.0.dev0+10.g84335621a\r\npytest: 3.2.3\r\npip: 9.0.1\r\nsetuptools: 36.7.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"Since Python 3.6 dicts have for practical purposes been ordered by insertion order in CPython, and from Cpython 3.7 dicts will be formally ordered by insertion order in the language specs.\r\n\r\nPandas currently orders series/dataframe comstructed from dict alphabetically. I propose to make the sort order dependent on python version, so Python <3.6 will keep the current behavior, while Python >= 3.6 will use insertion order.\r\n\r\nI have played a bit with this and the code changes seem to be minimal (change a line in ``_init_dict`` in each of ``series.py``/``frame.py``). After making the changes only 36 tests are failing, which is good. Updating the tests will mainly consists of making the columns order fixed in places (i.e. add a ``columns=[...]`` for dataframes and ``index=[...]`` for series), so I predict this would not be a huge PR.\r\n\r\nI could take this on, if there's agreement.\r\n\r\nEDIT: Ok, ``sparse/frame.py`` will also have its ``_init_dict`` changed, but still I think this will be a small code change."},{"labels":["api",null,null,null],"text":"xref #18997 \r\n\r\nprohibit construction of non-numeric sub-types for ``IntervalIndex``, e.g. ``object`` and ``categorical``. Not clear utility and causes a messy implementation."},{"labels":["api",null],"text":"But calling `len(x)` for such an object raises."},{"labels":["api"],"text":"Revisiting some 6 month old class that subclasses `Series` and overrides `__finalize__` as:\r\n\r\n```\r\n    def __finalize__(self, other, method=None, **kwargs):\r\n        pd.Series.__finalize__(self, other, method=method, **kwargs)\r\n        self.metadata = other.metadata.copy()\r\n        self.metadata.parent = self\r\n        return self\r\n```\r\n\r\nWith 0.21.1 these objects can no longer be rendered:\r\n\r\n```\r\n>>> ser\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pandas/core/base.py\", line 80, in __repr__\r\n    return str(self)\r\n  File \"pandas/core/base.py\", line 60, in __str__\r\n    return self.__bytes__()\r\n  File \"pandas/core/base.py\", line 72, in __bytes__\r\n    return self.__unicode__().encode(encoding, 'replace')\r\n  File \"pandas/core/series.py\", line 1066, in __unicode__\r\n    max_rows=max_rows, length=show_dimensions)\r\n  File \"pandas/core/series.py\", line 1109, in to_string\r\n    max_rows=max_rows)\r\n  File \"pandas/io/formats/format.py\", line 176, in __init__\r\n    self._chk_truncate()\r\n  File \"pandas/io/formats/format.py\", line 190, in _chk_truncate\r\n    series.iloc[-row_num:]))\r\n  File \"pandas/core/reshape/concat.py\", line 213, in concat\r\n    return op.get_result()\r\n  File \"pandas/core/reshape/concat.py\", line 377, in get_result\r\n    return cons(mgr, name=name).__finalize__(self, method='concat')\r\n  File \"pdsm/core/series.py\", line 125, in __finalize__\r\n    self.metadata = other.metadata.copy()\r\nAttributeError: '_Concatenator' object has no attribute 'metadata'\r\n```\r\n\r\nI had assumed that `other` would always be of the same class as `self`.  Is `_Concatenator` a recent addition?"},{"labels":["api",null],"text":"### Code demonstrating my issue with the 'inplace' argument of df.set_index().\r\n\r\n```python\r\n\r\nimport pandas as pd\r\n\r\n#start from a 2-dim list\r\nalist = [[1,2], [4,5]]\r\n\r\n#create a dataframe from the list\r\ndf = pd.DataFrame(alist, columns=['time', 'temp'])\r\nprint(df.head())\r\n\r\n#change the index of the dataframe to be the time column\r\ndf.set_index('time')\r\n\r\n#print the \"changed\" dataframe\r\nprint(df.head())\r\n\r\n#....but it's the same\r\n\r\n```\r\n#### Problem description\r\n\r\ndf.set_index() has an 'inplace' argument, with a default value of 'False'.  In the above example, the change made to your dataframe is thrown away.  This choice of convention is at the root of many pandas beginners' problems.  When you call .set_index(), one's mental approach is to modify your existing dataframe, not generate a new one.  Use of this function is outrageously counterintuitive.\r\n\r\n#### Expected Output\r\n\r\n'                temp\r\ntime      \r\n1                2\r\n4               5\r\n'\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.2.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.21.1\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 38.2.4\r\nCython: None\r\nnumpy: 1.13.3\r\nscipy: None\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.1.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.6.0\r\nhtml5lib: 1.0b10\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"There looks like something is wrong with the `compression` argument in `to_csv` for`pd.Series` but `pd.DataFrame`.  This error is also in 0.20.3.\r\n```python\r\nIn [1]: Se_tmp = pd.Series(list(\"ACGT\"))\r\n   ...: Se_tmp.to_csv(\"~/test.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\r\n   ...:\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-de7ce483228f> in <module>()\r\n      1 Se_tmp = pd.Series(list(\"ACGT\"))\r\n----> 2 Se_tmp.to_csv(\"~/test.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\r\n\r\nTypeError: to_csv() got an unexpected keyword argument 'compression'\r\n\r\nIn [2]: Se_tmp.to_frame(\"testing_column\").to_csv(\"~/test.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\r\n\r\nIn [3]: pd.__version__\r\nOut[3]: '0.21.1'\r\n```"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: mi = pd.MultiIndex.from_product([['i'], ['ii'], ['iii']])\r\n\r\nIn [3]: mi.rename([1,5,6]).get_level_values(1) # Interpreted as label\r\nOut[3]: Index(['i'], dtype='object', name=1)\r\n\r\nIn [4]: mi.rename([1,5,1]).get_level_values(1) # Interpreted as index\r\nOut[4]: Index(['ii'], dtype='object', name=5)\r\n\r\nIn [5]: mi.rename(['a',5,'a']).get_level_values('a') # ValueError is OK, KeyError is not\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/nobackup/repo/pandas/pandas/core/indexes/multi.py in _get_level_number(self, level)\r\n    670                 raise ValueError('The name %s occurs multiple times, use a '\r\n--> 671                                  'level number' % level)\r\n    672             level = self.names.index(level)\r\n\r\nValueError: The name a occurs multiple times, use a level number\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-5-e8a616f9610f> in <module>()\r\n----> 1 mi.rename(['a',5,'a']).get_level_values('a')\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexes/multi.py in get_level_values(self, level)\r\n    975         Index(['d', 'e', 'f'], dtype='object', name='level_2')\r\n    976         \"\"\"\r\n--> 977         level = self._get_level_number(level)\r\n    978         values = self._get_level_values(level)\r\n    979         return values\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexes/multi.py in _get_level_number(self, level)\r\n    673         except ValueError:\r\n    674             if not isinstance(level, int):\r\n--> 675                 raise KeyError('Level %s not found' % str(level))\r\n    676             elif level < 0:\r\n    677                 level += self.nlevels\r\n\r\nKeyError: 'Level a not found'\r\n\r\nIn [6]: mi.rename([1, 'a', 'a']).get_level_values(1) # How am I going to access the second level?!\r\nOut[6]: Index(['i'], dtype='object', name=1)\r\n\r\n```\r\n\r\n#### Problem description\r\n\r\nThere are different problems, but the root cause is (I think) the same:\r\n1. the first is trivial: the ``KeyError`` in ``In [5]:`` should not appear\r\n2. the second is that the interpretation of an integer changes when there is a duplicate name (difference between ``Out[3]`` and ``Out[4]``)\r\n3. the third is that in a situation like ``In [6]``, I have no way whatsoever to access the second column, since it is denoted by a duplicated name _and_ its index is also the name of another column (sure, this is a perverse example, but I suspect it can bite in some cases in which users use duplicate labels _and_ pandas internal code adds integer labels)\r\n\r\n\r\n#### Expected Output\r\n\r\nIf we were to design this from scratch, the solution would be simple: prioritize the \"index\" interpretation of an integer over the \"label\" interpretation, so that the former is always unambiguous. Is this a too strong API change?\r\n\r\nIf the answer is \"no\", I will be happy to implement it, possibly with a temporary warning in those cases where the behaviour will change (that is: requested label is integer _and_ is present in the names).\r\n\r\nIf the answer is \"yes\", I would like to at least suppress the ``KeyError`` in ``In [5]:`` and have ``In [4]:`` raise an error rather than return a result inconsistent with ``In [3]:``.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 04db779d4c93d286bb0ab87780a85d50ec490266\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-4-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.22.0.dev0+388.g04db779d4\r\npytest: 3.2.3\r\npip: 9.0.1\r\nsetuptools: 36.7.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"`PeriodIndex` w/ `Series` is allowed, `PeriodIndex` w/ `DatetimeIndex` is not.  Is this intentional?  If not, which should be made to match the other?\r\n\r\n```\r\n>>> dti = pd.date_range('2016-01-01', periods=3)\r\n>>> ser = pd.Series(dti)\r\n>>> pi = dti.to_period()\r\n\r\n>>> ser - pi\r\n0   0 days\r\n1   0 days\r\n2   0 days\r\ndtype: timedelta64[ns]\r\n\r\n>>> dti - pi\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pandas/core/indexes/datetimelike.py\", line 724, in __sub__\r\n    typ2=type(other).__name__))\r\nTypeError: cannot subtract DatetimeIndex and PeriodIndex\r\n\r\n>>> pi - ser\r\n0   0 days\r\n1   0 days\r\n2   0 days\r\ndtype: timedelta64[ns]\r\n\r\n>>> pi - dti\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pandas/core/indexes/datetimelike.py\", line 731, in __rsub__\r\n    return -(self - other)\r\n  File \"pandas/core/indexes/datetimelike.py\", line 724, in __sub__\r\n    typ2=type(other).__name__))\r\nTypeError: cannot subtract DatetimeIndex and PeriodIndex\r\n```"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.Series([1], [0,1])\r\nOut[2]: \r\n0    1\r\n1    1\r\ndtype: int64\r\n```\r\n\r\n#### Problem description\r\n\r\nThis is undocumented, conceptually wrong, works only because of an implementation glitch, and is used in 3 tests only  - I think - by mistake.\r\n\r\nAlready discussed (and potentially fixed) inside #18626 (where @jorisvandenbossche suggested a proper deprecation cycle), but @jreback [suggested](https://github.com/pandas-dev/pandas/pull/18769#discussion_r157476271) to open a specific issue.\r\n\r\n#### Expected Output\r\n\r\n```ValueError: Wrong number of items passed 1, placement implies 2```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: b5f1e716d89487423bb12d6cc4e6da2b39184531\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-4-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.22.0.dev0+371.gb5f1e716d\r\npytest: 3.2.3\r\npip: 9.0.1\r\nsetuptools: 36.7.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"Spending some time on #12534 the only fixes I'm seeing are pretty invasive.  We need to use `checked_add_with_arr` to correctly catch overflows, and for that we need `isna(...)` masks.  But the wrapping/redirection that goes on within `ops._arith_method_SERIES` make that problematic.  In particular the scope in which the masks are defined (for datetime/timedelta dtypes) is different from the scope in which we need to do the overflow checking.  Passing those masks back and forth across scopes gets ugly in a hurry.\r\n\r\nI'd rather just define `__add__`, `__sub__`, `__radd__`, `__rsub__` methods specific to datetimelike-dtypes and have Series dispatch to those when appropriate.  Any thoughts?"},{"labels":["api",null],"text":"#### Problem description\r\nI really like the assign function and it's ability to be applied in pipelines.\r\nHowever, if you pass a dictionary via prefixed by **, the dictionary must only contain columns that already exist in the preceeding dataframe. So in a dataframe, that contains column 'A', and I want to construct column B as f(A) and column C = g(A, B), im forced to do\r\n```python\r\n# Your code here\r\npd.DataFrame({'A':[1, 2]}).assign(**{'B': lambda x:f(x['A'])}).assign(**{'C': lambda x:g(x['A'], x['B])})\r\n\r\n       A    B     C\r\n0      1    f(1)  g(1, f(1))\r\n1      2    f(2)  g(2, f(2))\r\n```\r\nfor some f and g and obtain a result like seen above. In extreme cases, this can lead to a lot of chained assign statements.\r\n\r\nFor convenience we could change the signature slightly to accept also *args, but every element in args should be such that the original assign function could be applied. In particular args could be a list of dictionaries.\r\n\r\nWith this, we could write the previous code as\r\n```python\r\n# Your code here\r\npd.DataFrame({'A':[1, 2]}).assign(*[{'B': lambda x:f(x['A']),{'C': lambda x:g(x['A'], x['B])}])\r\n```\r\nOf course (also as it is right now) the user is responsible to construct a correct \"computational graph\" here. Additionally, the implementation I currently think of would use len(args) (-1 intermediate) copies of the original dataframe. However, using the stacked procedure above, this also happens.\r\n\r\nThus, we obtain a simpler syntactic way of using assign and we don't break the original implementation.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: 3.2.1\r\npip: None\r\nsetuptools: 36.5.0.post20170921\r\nCython: 0.26.1\r\nnumpy: 1.13.3\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.0\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"### Background\r\nFollow-up from this specfic chain of comments: https://github.com/pandas-dev/pandas/pull/18710#discussion_r155950020\r\nAnd these PR's in general: #18677, #18710\r\n\r\n### Issue\r\nFor the context of this discussion, I'm only referring to data that is _already_ categorical; I don't think there was any ambiguity with converting non-categorical to categorical.  This applies using `.astype('category')` on `Categorical`, `CategoricalIndex`, and `Series`.\r\n\r\nThe crux of the issue comes down to whether `.astype('category')` should ever change data that is already categorical.  An argument that it shouldn't is that `.astype('category')` doesn't explicitly specify any changes, so nothing should be changed, and it's the existing behavior.\r\n\r\nThe other argument is that  `.astype('category')` should be equivalent to `.astype(CategoricalDtype())`.  Note that `CategoricalDtype()` is the same as `CategoricalDtype(categories=None, ordered=False)`:\r\n```python\r\nIn [2]: CategoricalDtype()\r\nOut[2]: CategoricalDtype(categories=None, ordered=False)\r\n```\r\n\r\nThis means that if the existing categorical data is ordered, then `.astype(CategoricalDtype())` would change the categorical data from having `ordered=True` to `ordered=False`, and so  `.astype('category')` should do the same.\r\n\r\nI don't think there are any scenarios where the categories themselves would change; the only potential thing that could change is `ordered=True` to `ordered=False`.  See below for a summary of some potential options.  Feel free to modify any of the pro/cons listed below, or suggest any other potential options.\r\n\r\n### Option 1: `.astype('category')` does not change anything\r\n\r\nThis would not require any additional code changes, as it's the current behavior.\r\n\r\n**Pros:**\r\n- Maintains current behavior `.astype('category')`\r\n- Less likely to cause user confusion due to unforeseen changes\r\n    - At least in my mind, but I could be convinced otherwise\r\n    - Forces the user to be explicit when making potentially unintended changes\r\n\r\n**Cons:**\r\n- Inconsistent with `.astype(CategoricalDtype())`\r\n\r\n### Option 2: `.astype('category')` changes `ordered=True` to `ordered=False`\r\n\r\nThis would require some additional code changes, but is relatively minor.\r\n\r\n**Pros:**\r\n- Makes `.astype('category')` consistent with `.astype(CategoricalDtype())`\r\n- A bit cleaner/more maintainable in terms of code\r\n     - No special case checking for the string 'category'\r\n\r\n**Cons:**\r\n- Changes current behavior of `.astype('category')`\r\n\r\n### Option 3: Allow  `ordered=None` in `CategoricalDtype`\r\nBasically, make `CategoricalDtype()` return `CategoricalDtype(categories=None, ordered=None)`.  I should preface this by saying that I have not scoped out the amount of code that would need to be changed for this, nor the potential ramifications.  This may not be a good idea.\r\n\r\n**Pros:**\r\n - Maintains current behavior `.astype('category')`\r\n - Makes  `.astype('category')` consistent with `.astype(CategoricalDtype())`\r\n\r\n**Cons:**\r\n - Changes the default behavior of `CategoricalDtype`\r\n - Could potentially involve a lot of code change and unseen ramifications"},{"labels":["api",null],"text":"What can tick do that Timedelta can’t? I guess normalize? But tick with normalize=true doesn’t make a ton of sense..."},{"labels":["api",null,null],"text":"Hi all, this is a proposal to add a new block and type for representing IP Addresses.\r\nThere are still some details that need ironing out, but I wanted to gauge reactions to\r\nincluding this in pandas before spending too much more time on it.\r\n\r\nHere's a notebook demonstrating the basics: http://nbviewer.jupyter.org/gist/TomAugspurger/3ba2bc273edfec809b61b5030fd278b9\r\n\r\n## Abstract\r\n\r\nProposal to add support for storing and operating on IP Address data.\r\nAdds a new block type for ip address data and an `ip` accessor to\r\n`Series` and `Index`.\r\n\r\n## Rationale\r\n\r\nFor some communities, IP and MAC addresses are a common data format. The data\r\nformat was deemed important enough to add the `ipaddress` module to the standard\r\nlibrary (see `PEP 3144`_). At Anaconda, we hear from customers who would use a\r\nfirst-class IP address array container if it existed in pandas.\r\n\r\nI turned to StackOverflow to gauge interest in this topic. A search for \"IP\" on\r\nthe [pandas stackoverflow\r\ntag](https://stackoverflow.com/search?q=%5Bpandas%5D+IP) turns up 300 results.\r\nUnder the NumPy tag there are another 80. For comparison, I ran a few other\r\nsearches to see what interest there is in other \"specialized\" data types (this\r\nis a very rough, probably incorrect, way of estimating interest):\r\n\r\n| term      | results |\r\n| --------- | ------- |\r\n| financial | 251     |\r\n| geo       | 120     |\r\n| ip        | 300     |\r\n| logs      | 590     |\r\n\r\n\r\nCategorical, which is already in pandas, turned up 1,089 items.\r\n\r\nOverall, I think there's enough interest relative to the implementation /\r\nmaintenance burden to warrant adding the support for IP Addresses. I don't\r\nanticipate this causing any issues for the arrow transition, once ARROW-1587 is\r\nin place. We can be careful which parts of the storage layer are implementation\r\ndetails.\r\n\r\n## Specification\r\n\r\nThe proposal is to add\r\n\r\n1.  A type and container for IPAddress and MACAddress (similar to\r\n    `CategoricalDtype` and `Categorical`).\r\n2.  A block for IPAddress and MACAddress (similar to `CategoricalBlock`).\r\n3.  A new accessor for Series and Indexes, `.ip`, for operating on IP\r\n    addresses and MAC addresses (similar to `.cat`).\r\n\r\nThe type and block should be generic IP address blocks, with no\r\ndistinction between IPv4 and IPv6 addresses. In our experience, it's\r\ncommon to work with data from multiple sources, some of which may be\r\nIPv4, and some of which may be IPv6. This also matches the semantics\r\nof the default `ipaddress.ip_address` factory function, which returns\r\nan `IPv4Address` or `IPv6Address` as needed. Being able to deal with\r\nip addresses in an IPv4 vs. IPv6 agnostic fashion is useful.\r\n\r\n### Data Layout\r\n\r\nSince IPv6 addresses are 128 bits, they do not fit into a standard NumPy uint64\r\nspace. This complicates the implementation (but, gives weight to accepting the\r\nproposal, since doing this on your own can be tricky).\r\n\r\nEach record will be composed of two uint64s. The first element \r\ncontains the first 64 bits, and the second array contains the second 64\r\nbits. As a NumPy structured dtype, that's\r\n\r\n```python\r\nbase = np.dtype([('lo', '>u8'), ('hi', '>u8')])\r\n```\r\n\r\nThis is a common format for handling IPv4 and IPv6 data:\r\n\r\n> Hybrid dual-stack IPv6/IPv4 implementations recognize a special class of\r\n> addresses, the IPv4-mapped IPv6 addresses. These addresses consist of an\r\n> 80-bit prefix of zeros, the next 16 bits are one, and the remaining,\r\n> least-significant 32 bits contain the IPv4 address.\r\n\r\nFrom [here](https://en.wikipedia.org/wiki/IPv6#Software)\r\n\r\n### Missing Data\r\n\r\nUse the lowest possible IP address as a marker. According to RFC2373,\r\n\r\n> The address 0:0:0:0:0:0:0:0 is called the unspecified address. It must\r\n> never be assigned to any node. It indicates the absence of an address.\r\n\r\nSee [here](https://tools.ietf.org/html/rfc2373.html#section-2.5.2).\r\n\r\n### Methods\r\n\r\nThe new user-facing `IPAddress` (analogous to a `Categorical`) will have\r\na few methods for easily constructing arrays of IP addresses.\r\n\r\n```python\r\nIPAddress.from_pyints(cls, values: Sequence[int]) -> 'IPAddress':\r\n    \"\"\"Construct an IPAddress array from a sequence of python integers.\r\n\r\n    >>> IPAddress.from_pyints([10, 18446744073709551616])\r\n    <IPAddress(['0.0.0.10', '::1'])>\r\n    \"\"\"\r\n\r\nIPAddress.from_str(cls, values: Sequence[str]) -> 'IPAddress':\r\n    \"\"\"Construct an IPAddress from a sequence of strings.\"\"\"\r\n```\r\n\r\nThe methods in the new `.ip` namespace should follow the standard\r\nlibrary's design.\r\n\r\n**Properties**\r\n\r\n-   `is_multicast`\r\n-   `is_private`\r\n-   `is_global`\r\n-   `is_unspecificed`\r\n-   `is_reserved`\r\n-   `is_loopback`\r\n-   `is_link_local`\r\n\r\n### Reference Implementation\r\n\r\nAn implementation of the types and block is available at\r\n[pandas-ip](https://github.com/ContinuumIO/pandas-ip/) (at the moment\r\nit's a proof of concept).\r\n\r\n### Alternatives\r\n\r\nAdding a new block type to pandas is a major change. Downstream libraries may\r\nhave special-cased handling for pandas' extension types, so this shouldn't be\r\nadopted without careful consideration.\r\n\r\nSome alternatives to this that exist outside of pandas:\r\n\r\n1.  Store `ipaddress.IPv4Address` or `ipaddress.IPv6Address` objects in\r\n    an `object` dtype array. The `.ip` namespace could still be included\r\n    with an extension decorator. The drawback here is the poor\r\n    performance, as every operation would be done element-wise.\r\n2.  A separate library that provides a container and methods. The\r\n    downside here is that the library would need to subclass `Series`,\r\n    `DataFrame`, and `Index` so that the custom blocks and types are\r\n    interpreted correctly. Users would need to use the custom\r\n    `IPSeries`, `IPDataFrame`, etc., which increases friction when working\r\n    with other libraries that may expect / coerce to pandas objects.\r\n\r\nTo expand a bit on the (current) downside of alternative 2,  when the pandas constructors\r\nsee an \"unknown\" object, they falls back to `object` dtype and stuffs the actual Python object\r\ninto whatever container is being created:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import pandas_ip as ip\r\n\r\nIn [3]: arr = ip.IPAddress.from_pyints([1, 2])\r\n\r\nIn [4]: arr\r\nOut[4]: <IPAddress(['0.0.0.1', '0.0.0.2'])>\r\n\r\nIn [5]: pd.Series(arr)\r\nOut[5]:\r\n0    <IPAddress(['0.0.0.1', '0.0.0.2'])>\r\ndtype: object\r\n```\r\n\r\nI'd rather not have to make a subclass of Series, just to stick an array-like thing into a Series.\r\n\r\nIf pandas could provide an interface such that objects satisfying that interface\r\nare treated as array-like, and not a simple python object, then I'll gladly close\r\nthis issue and develop the IP-address specific functionality in another package.\r\nThat might be the best possible outcome to all this.\r\n\r\n### References\r\n\r\n-   [pandas-ip](https://github.com/ContinuumIO/pandas-ip/)\r\n-   [PEP 3144](https://www.python.org/dev/peps/pep-3144/)\r\n-   [RFC 2373](https://tools.ietf.org/html/rfc2373.html#section-2.5.2)\r\n-   [ipaddress howto](https://docs.python.org/3/howto/ipaddress.html)\r\n-   [ipaddress](https://docs.python.org/3/library/ipaddress.html)\r\n\r\n"},{"labels":["api",null,null,null],"text":"Inspired by #18699 I wrote up a very brief script that when placed in the root directory could help identify functions where ``**kwargs`` are accepted but not used in the method body. While not perfect (may give false positives in cases of nested function definitions) it calls out 128 functions as is (see [extra_kwargs.txt](https://github.com/pandas-dev/pandas/files/1553193/extra_kwargs.txt)). A quick spot check on my end looked good.\r\n\r\nSharing in case anyone would like to look through this and submit PR's to tackle in batches. Here's the script I used in case anyone is interested\r\n\r\n    import re\r\n    import glob\r\n\r\n    if __name__=='__main__':\r\n        pater = re.compile(r\"\"\"\r\n        ^(.+?)       # group to match function name (non-greedy)\r\n        \\((.*?)\\)    # group to match arguments\r\n        (.*)$        # everything else (function body)\r\n        \"\"\", flags=re.DOTALL|re.VERBOSE)\r\n\r\n        fns = glob.glob('pandas/**/*.py', recursive=True)\r\n        for fn in fns:\r\n            with open(fn) as infile:\r\n                source = infile.read()\r\n                funcs = source.split('def ')\r\n                for func in funcs:\r\n                    res = re.search(pater, func)\r\n                    if res:\r\n                        nm, args, body = res.groups()\r\n                        if '**kwargs' in args and not 'kwargs' in body:\r\n                            args = re.sub('\\s+', ' ', args)\r\n                            print(\"{} def {}({})\".format(fn, nm, \" \".join(\r\n                                args.split(\"\\n\"))))"},{"labels":["api",null],"text":"\r\n```\r\n>>> idx = pd.date_range('1994-10-06', freq='D', periods=4)\r\n>>> idx.day.to_series(index=idx)\r\n>>> idx.day.to_series(index=idx)\r\n6    6\r\n7    7\r\n8    8\r\n9    9\r\n```\r\n\r\nExpected:\r\n\r\n```\r\n>>> idx.day.to_series(index=idx)\r\n1994-10-06    6\r\n1994-10-07    7\r\n1994-10-08    8\r\n1994-10-09    9\r\n```\r\n\r\nLooks like Index.to_series takes **kwargs but then ignores them.  Should be an easy fix; would be nice to systematically track down other places this might happen."},{"labels":["api",null],"text":"And the `prod` of those should be 1.\r\n\r\nxref: https://github.com/pandas-dev/pandas/issues/9422, https://github.com/pandas-dev/pandas/pull/17630, https://mail.python.org/pipermail/pandas-dev/2017-November/000657.html\r\n\r\nWe need to implement that and design and implement the alternative (either a new method, or a keyword-argument to sum and prod)."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nvibe_df.plot(x='Time', y='Fz')     # This works just fine.\r\nvibe_df.plot(x='Time', y=['Fz'])     # This raises a UserWarning:\r\n\r\n```\r\n#### Problem description\r\n\r\nI get a UserWarning while plotting a DataFrame.  It only occurs if I pass a list of columns for y.\r\n\r\nUserWarning is:\r\nC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py:1714: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\r\n  series.name = label\r\n\r\n#### Expected Output\r\n<just the graph>\r\n\r\n#### Output of ``pd.show_versions()``\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.4.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.21.0\r\npytest: 2.9.2\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.3\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.4\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n"},{"labels":["api",null],"text":"The implicit index-matching of `pandas` for operations between different `DataFrame`/`Series` is great and most of the times, it just works. It does so consistently enough, that the expectation (for me) is that different Series will be aligned before an operation is performed.\r\n\r\nFor some reason, `str.cat` does not seem to do so.\r\n\r\n```\r\nimport pandas as pd # 0.21.0\r\nimport numpy as np # 1.13.3\r\ncol = pd.Series(['a','b','c','d','e','f','g','h','i','j'])\r\n\r\n# choose random subsets\r\nss1 = [8, 1, 2, 0, 6] # list(col.sample(5).index) \r\nss2 = [4, 0, 9, 2, 6] # list(col.sample(5).index)\r\n\r\n# perform str.cat\r\ncol.loc[ss1].str.cat(col.loc[ss2], sep = '').sort_index()\r\n# 0    ac <-- UNMATCHED!\r\n# 1    ba <-- UNMATCHED!\r\n# 2    cj <-- UNMATCHED!\r\n# 6    gg <-- correct by sheer luck\r\n# 8    ie <-- UNMATCHED!\r\n\r\n# compared for example with Boolean operations on unmatched series\r\n# (matching indices and returning Series with union of both indices),\r\n# this is inconsistent!\r\nb = col.loc[ss1].astype(bool) & col.loc[ss2].astype(bool)\r\nb\r\n# 0     True\r\n# 1    False\r\n# 2     True\r\n# 4    False\r\n# 6     True\r\n# 8    False\r\n# 9    False\r\n\r\n# if we manually align the Series\r\n# (easy here by masking from the Series we just subsampled, hard in practice),\r\n# then the NaNs are handled as expected:\r\nm = col.where(np.isin(col.index, ss1)).str.cat(col.where(np.isin(col.index, ss2)), sep = '')\r\nm\r\n# 0     aa\r\n# 1    NaN\r\n# 2     cc\r\n# 3    NaN\r\n# 4    NaN\r\n# 5    NaN\r\n# 6     gg\r\n# 7    NaN\r\n# 8    NaN\r\n# 9    NaN\r\n\r\n# based on the normal pandas-behaviour for unmatched Series\r\n# (for example as for Boolean \"and\" above), the following would be\r\n# the expected result of col.loc[ss1].str.cat(col.loc[ss2], sep = '').sort_index() !\r\nm.loc[b.index]\r\n# 0     aa <-- MATCHED!\r\n# 1    NaN\r\n# 2     cc <-- MATCHED!\r\n# 4    NaN\r\n# 6     gg <-- MATCHED!\r\n# 8    NaN\r\n# 9    NaN\r\n```\r\n"},{"labels":["api",null],"text":"Series.replace() should potentially raise an exception if an invalid argument is given? I am new to pandas and this is more of a question. **This is a user error on my part, but possible improvement to replace.**\r\n\r\n```python\r\ndf = pd.DataFrame(\r\n{\r\n    'one': ['1','1 ','10'],\r\n    'two': ['1 ', '20 ', '30 ']\r\n})\r\n\r\n# creates\r\n     one  two\r\n0    1    1\r\n1    1    20\r\n2    10    30\r\n\r\n# I intentionally added a space in df.one. So df.one.value_counts() results in\r\n1     1\r\n10    1\r\n1     1\r\n\r\n# I want to strip the spaces around all values in df.one \r\n# so that value_counts() only has 1 and 10 with values 2 and 1 respectively. \r\n# The following does not work.\r\n\r\ndf.one.replace(lambda x: x.strip(), inplace=True)\r\n\r\n```\r\n#### Problem description\r\n\r\nSeries.replace() accepts `\"str, regex, list, dict, Series, numeric, or None\"`. So I understand why the above does not work. But as @toobaz pointed out on gitter, when a lambda is passed, replace should probably raise an error?\r\n\r\n#### Expected Output\r\nRaises TypeError\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.7.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.3.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.8\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.8.0\r\nbs4: None\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.14\r\npymysql: 0.7.11.None\r\npsycopg2: None\r\njinja2: 2.8.1\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\nIn [1]: import pandas\r\n\r\nIn [2]: idx = pandas.Index(['a', 'b', 'c'])\r\n\r\nIn [3]: idx\r\nOut[3]: Index(['a', 'b', 'c'], dtype='object')\r\n\r\nIn [4]: idx.astype('category')\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-b8d97d97d03f> in <module>()\r\n----> 1 idx.astype('category')\r\n\r\nC:\\...\\pandas\\indexes\\base.py in astype(self, dtype, copy)\r\n    889     @Appender(_index_shared_docs['astype'])\r\n    890     def astype(self, dtype, copy=True):\r\n--> 891         return Index(self.values.astype(dtype, copy=copy), name=self.name,\r\n    892                      dtype=dtype)\r\n    893\r\n\r\nTypeError: data type \"category\" not understood\r\n```\r\n\r\n#### Problem description\r\n\r\nThe documentation for this method reads:\r\n\r\n> Create an Index with values cast to dtypes. The class of a new Index is determined by dtype.\r\n\r\nSince there is a CategoricalIndex type, it is reasonable for a user to expect that `.astype('category')` would return a CategoricalIndex object.\r\n\r\nAs a workaround for the issue, users can construct a CategoricalIndex directly:\r\n\r\n```\r\nIn [7]: pandas.CategoricalIndex(idx)\r\nOut[7]: CategoricalIndex(['a', 'b', 'c'], categories=['a', 'b', 'c'], ordered=False, dtype='category')\r\n```\r\n\r\n#### Expected Output\r\n\r\nThe method should return a CategoricalIndex equal to the following:\r\n```\r\nIn [5]: pandas.CategoricalIndex(['a', 'b', 'c'])\r\nOut[5]: CategoricalIndex(['a', 'b', 'c'], categories=['a', 'b', 'c'], ordered=False, dtype='category')\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.5.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 79 Stepping 1, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.8.2\r\nIPython: 5.1.0\r\nsphinx: 1.4.8\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.7\r\nblosc: 1.5.0\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.3\r\nhtml5lib: 0.999\r\nhttplib2: 0.9.2\r\napiclient: None\r\nsqlalchemy: 1.1.3\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: 2.43.0\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nIn [6]: pd._libs.tslibs.frequencies.is_subperiod('D', 'D')\r\nOut[6]: True\r\n\r\nIn [7]: pd._libs.tslibs.frequencies.is_subperiod('M', 'M')\r\nOut[7]: False\r\n```\r\n#### Problem description\r\n\r\nWhen source == target, shouldn't pd._libs.tslibs.frequencies.is_subperiod(source, target) always return True?\r\n\r\n#### Expected Output\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nIn [6]: pd._libs.tslibs.frequencies.is_subperiod('D', 'D')\r\nOut[6]: True\r\n\r\nIn [7]: pd._libs.tslibs.frequencies.is_subperiod('M', 'M')\r\nOut[7]: True\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nIn [12]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-101-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: 3.1.3\r\npip: 9.0.1\r\nsetuptools: 37.0.0\r\nCython: 0.27.3\r\nnumpy: 1.13.1\r\nscipy: 1.0.0\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.9999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"#### Problem description\r\nThe fixes in #18424 allow for and `IntervalIndex` to be constructed with tz aware timestamps, but there are a few attributes/methods that still do not work properly with tz aware:\r\n\r\n-  Disallow an `IntervalIndex` to contain elements with differing tz\r\n- `IntervalIndex.mid` returns tz naive\r\n- Anything using `_get_next_label` or `_get_previous_label` will raise\r\n  - `get_indexer` when passed an `IntervalIndex`\r\n  - `get_loc` for overlapping/non-monotonic `IntervalIndex`\r\n  - `contains` always returns `False` for overlapping/non-monotonic due to `Try`/`Except`\r\n  - etc.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\nSetup:\r\n```python\r\nIn [2]: start = pd.Timestamp('2017-01-01', tz='US/Eastern')\r\n   ...: index = pd.interval_range(start, periods=3)\r\n   ...: index\r\n   ...:\r\nOut[2]:\r\nIntervalIndex([(2017-01-01, 2017-01-02], (2017-01-02, 2017-01-03], (2017-01-03, 2017-01-04]]\r\n              closed='right',\r\n              dtype='interval[datetime64[ns, US/Eastern]]')\r\n```\r\n`IntervalIndex.mid` returns tz naive:\r\n```python\r\nIn [3]: index.mid\r\nOut[3]:\r\nDatetimeIndex(['2017-01-01 17:00:00', '2017-01-02 17:00:00',\r\n               '2017-01-03 17:00:00'],\r\n              dtype='datetime64[ns]', freq=None)\r\n```\r\n\r\n`get_indexer` raises when passed an `IntervalIndex`:\r\n```python\r\nIn [4]: index.get_indexer(index[:-1])\r\n---------------------------------------------------------------------------\r\nTypeError: cannot determine next label for type <class 'pandas.core.indexes.datetimes.DatetimeIndex'>\r\n```\r\n\r\nDiffering tz:\r\n```python\r\nIn [5]: left = pd.date_range('2017-01-01', periods=3, tz='US/Eastern')\r\n   ...: right = pd.date_range('2017-01-02', periods=3, tz='US/Pacific')\r\n   ...: index = pd.IntervalIndex.from_arrays(left, right)\r\n   ...: index\r\n   ...:\r\nOut[5]:\r\nIntervalIndex([(2017-01-01, 2017-01-02], (2017-01-02, 2017-01-03], (2017-01-03, 2017-01-04]]\r\n              closed='right',\r\n              dtype='interval[datetime64[ns, US/Eastern]]')\r\n\r\nIn [6]: index.left.dtype\r\nOut[6]: datetime64[ns, US/Eastern]\r\n\r\nIn [7]: index.right.dtype\r\nOut[7]: datetime64[ns, US/Pacific]\r\n```"},{"labels":["api",null,null],"text":"A Week offset with weekday=None behaves very much like a Tick offset.  The main difference is how they handle `onOffset`.  Assuming `normalize` is False, the `Week` object will always return `week.onOffset(other)` --> `False` while the `Day` object will always return `day.onOffset(other)` --> `True`.\r\n\r\nIs this intentional?"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: d = {(0, 1) : 2, (3, 4) : 5}\r\n\r\nIn [3]: pd.Index(d)\r\nOut[3]: Index([(0, 1), (3, 4)], dtype='object')\r\n\r\nIn [4]: pd.Index(list(d))\r\nOut[4]: \r\nMultiIndex(levels=[[0, 3], [1, 4]],\r\n           labels=[[0, 1], [0, 1]])\r\n\r\nIn [5]: pd.Series(d).index\r\nOut[5]: \r\nMultiIndex(levels=[[0, 3], [1, 4]],\r\n           labels=[[0, 1], [0, 1]])\r\n\r\nIn [6]: pd.Series(index=list(d)).index\r\nOut[6]: Index([(0, 1), (3, 4)], dtype='object')\r\n\r\n```\r\n#### Problem description\r\n\r\nI guess ``Out[3]:`` and ``Out[6]:`` are wrong.\r\n\r\n#### Expected Output\r\n\r\n``Out[4]:``\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-4-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.22.0.dev0+202.g97bd66ea8\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: 33.1.1\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.18.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.2.2\r\nsphinx: None\r\npatsy: 0.4.1+dev\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: None\r\nlxml: 3.7.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null,null],"text":"xref #15081, a couple of issues\r\n\r\n- [ ] ``IntervalIndex.map`` is xfailed\r\n- [x] parametrize [indexex/common.py/test_map](https://github.com/pandas-dev/pandas/blob/master/pandas/tests/indexes/common.py#L1009) similar to how ``indexex/datetimelike.py/test_map`` is done (IOW on dict / Series) (#18491)\r\n- [x] resolve datetimelike with empty map [2] should be all ``NaT`` for the datetimelikes. (#18491)\r\n\r\n```\r\nIn [2]: pd.date_range('20130101', periods=3).map({})\r\nOut[2]: Float64Index([nan, nan, nan], dtype='float64')\r\n\r\nIn [3]: pd.Index([1,2,3]).map({})\r\nOut[3]: Float64Index([nan, nan, nan], dtype='float64')\r\n```\r\n"},{"labels":["api",null,null],"text":"I've got a use case where I have a data frame with a ton of columns and I want to add a level to the column index to help in groupby operations.  My first instinct was to create a dictionary to illustrate the grouping, and use that pretty directly in indexing the dataframe. I.e.:\r\n\r\n```python\r\nhierarchy = {'g0':['c0','c1'], 'g1':['c2', 'c3', 'c4']}\r\nnew_index = MultiIndex.from_dict(hierarchy)\r\n```\r\n\r\nThis just strikes me as a pretty natural way to think about hierarchical structure of data.\r\n\r\nSo something like this, but with support for nested dictionaries, added to the MultiIndex class:\r\n\r\n```python\r\ndef from_dict(cls, data, names=None):\r\n    '''\r\n    Construct a two level MultiIndex from a dictionary-like object.\r\n\r\n    Parameters\r\n    ----------\r\n    data : dict \r\n        {group : array-like}\r\n        Here the array-like represents the group members\r\n    names : optional names for the levels.\r\n\r\n    Returns\r\n    -------\r\n    index : MultiIndex\r\n\t\r\n    Notes\r\n    -----\r\n    Respects grouping order if dict is an OrderedDict\r\n    '''\r\n    lvl1 = []\r\n    lvl2 = []\r\n    for name, cols in data.items():\r\n        lvl1 += [name] * len(cols)\r\n        lvl2 += cols\r\n\r\n    return MultiIndex.from_arrays([lvl1, lvl2], names=names)\r\n\r\n```\r\n"},{"labels":["api",null,null,null],"text":"TLDR: enforcing tzaware vs tznaive compat in DatetimeIndex comparisons (#18162) appears to be inconsistent with current slicing behavior.\r\n\r\nThe following example is based off of tests.series.test_indexing.test_getitem_setitem_datetimeindex:\r\n\r\n```\r\ndti = pd.date_range('1/1/1990', periods=50, freq='H', tz='US/Eastern')\r\nts = pd.Series(np.random.randn(50), index=dti)\r\n\r\nlb = '1990-01-01 04:00:00'\r\nrb = '1990-01-01 07:00:00'\r\n```\r\n\r\nThe behavior that we want to enforce is #18162 requires that `dti < pd.Timestamp(lb)` should raise, as should `dti < lb`.  At the moment they effectively get treated as UTC.  But if we change this so that it does raise, the following from `test_getitem_setitem_datetimeindex` breaks pretty irrevocably:\r\n\r\n```\r\nts[(ts.index >= lb) & (ts.index <= rb)]\r\n```\r\n\r\nThere is also `ts[lb:rb]` which if we're being strict should raise, but at least we _could_ make that still work.  (BTW this implicitly casts lb and rb to US/Eastern, albeit in different places.  So far that appears to be a related but distinct can of worms.)\r\n"},{"labels":["api",null,null],"text":"I find the following counter-intuitive:\r\n\r\n```\r\n>>> per = pd.Period('2016')\r\n>>> per\r\nPeriod('2016', 'A-DEC')\r\n>>> (per.month, per.day, per.hour)\r\n(12, 31, 0)\r\n```\r\n\r\nSince this Period represents the span from Jan1-Dec 31, it seems that month, day, hour, etc are not meaningful, should be NaN.  "},{"labels":["api",null],"text":"This issue is created based on the discussion from #15931 following the deprecation of relabeling dicts in `groupby.agg`. A lot of what is summarized below was already discussed in the previous discussion. I would recommend in particular https://github.com/pandas-dev/pandas/pull/15931#issuecomment-336139085 where the problems are also clearly stated.\r\n\r\nThe motivation behind the deprecation of #15931 was mostly related to bringing a consistent interface for `agg()` between Series and Dataframe (see also #14668 for context).\r\n\r\nThe relabeling functionality with a nested dict has been described by some as being too complex and/or inconsistent and thus deprecated.\r\n\r\nHowever, this comes at a price: the impossibility to aggregate and rename at the same time leads to very annoying issues and some backward incompatibility where no sensible workaround is available:\r\n- _[annoying]_ no more control over the names of the resulting columns\r\n- _[annoying]_ you need to find a way to rename the MultiIndex _after_ performing the aggregation, requiring to keep track of the order of columns at two places in the code.... not practical at all and sometimes downright impossible (cases below).\r\n- ⚠️  _**[breaking]**_ cannot apply more than one callable with the same internal name on the same input column. This results in two sub-cases:\r\n    -  _**[breaking]**_ you can't apply anymore two or more lambda aggregators on the same column\r\n    -  _**[breaking]**_ you can't apply anymore two or more aggregators from partial functions unless you alter their hidden `__name__` attribute\r\n\r\n## Example\r\n\r\n_(please note, this is a crafted example for the purpose of demonstrating the problem in as short a code as possible, but all of the demonstrated issues here did bite me in real life since the change, and in situations not as simple as here)_\r\n\r\n### Input Dataframe\r\n\r\n```python\r\nmydf = pd.DataFrame(\r\n    {\r\n        'cat': ['A', 'A', 'A', 'B', 'B', 'C'],\r\n        'energy': [1.8, 1.95, 2.04, 1.25, 1.6, 1.01],\r\n        'distance': [1.2, 1.5, 1.74, 0.82, 1.01, 0.6]\r\n    },\r\n    index=range(6)\r\n)\r\n```\r\n\r\n```\r\n  cat  distance  energy\r\n0   A      1.20    1.80\r\n1   A      1.50    1.95\r\n2   A      1.74    2.04\r\n3   B      0.82    1.25\r\n4   B      1.01    1.60\r\n5   C      0.60    1.01\r\n```\r\n\r\n### Before:\r\n\r\neasy to write and read, and works as expected\r\n\r\n```python\r\nimport numpy as np\r\nimport statsmodels.robust as smrb\r\nfrom functools import partial\r\n \r\n# median absolute deviation as a partial function\r\n# in order to demonstrate the issue with partial functions as aggregators\r\nmad_c1 = partial(smrb.mad, c=1)\r\n\r\n# renaming and specifying the aggregators at the same time\r\n# note that I want to choose the resulting column names myself\r\n# for example \"total_xxxx\" instead of just \"sum\"\r\nmydf_agg = mydf.groupby('cat').agg({\r\n    'energy': {\r\n        'total_energy': 'sum',\r\n        'energy_p98': lambda x: np.percentile(x, 98),  # lambda\r\n        'energy_p17': lambda x: np.percentile(x, 17),  # lambda\r\n    },\r\n    'distance': {\r\n        'total_distance': 'sum',\r\n        'average_distance': 'mean',\r\n        'distance_mad': smrb.mad,   # original function\r\n        'distance_mad_c1': mad_c1,  # partial function wrapping the original function\r\n    },\r\n})\r\n```\r\n\r\nresults in\r\n\r\n```\r\n          energy                             distance\r\n    total_energy energy_p98 energy_p17 total_distance average_distance distance_mad distance_mad_c1\r\ncat\r\nA           5.79     2.0364     1.8510           4.44            1.480     0.355825           0.240\r\nB           2.85     1.5930     1.3095           1.83            0.915     0.140847           0.095\r\nC           1.01     1.0100     1.0100           0.60            0.600     0.000000           0.000\r\n```\r\n\r\nand all is left is:\r\n\r\n```python\r\n# get rid of the first MultiIndex level in a pretty straightforward way\r\nmydf_agg.columns = mydf_agg.columns.droplevel(level=0)\r\n```\r\n\r\nHappy dance praising pandas 💃 🕺 !\r\n\r\n### After\r\n\r\n```python\r\nimport numpy as np\r\nimport statsmodels.robust as smrb\r\nfrom functools import partial\r\n\r\n# median absolute deviation as a partial function\r\n# in order to demonstrate the issue with partial functions as aggregators\r\nmad_c1 = partial(smrb.mad, c=1)\r\n\r\n# no way of choosing the destination's column names...\r\nmydf_agg = mydf.groupby('cat').agg({\r\n    'energy': [\r\n    \t'sum',\r\n    \tlambda x: np.percentile(x, 98), # lambda\r\n    \tlambda x: np.percentile(x, 17), # lambda\r\n    ],\r\n    'distance': [\r\n    \t'sum',\r\n    \t'mean',\r\n    \tsmrb.mad, # original function\r\n    \tmad_c1,   # partial function wrapping the original function\r\n    ],\r\n})\r\n```\r\n\r\nThe above breaks because the lambda functions will all result in columns named `<lambda>` which results in\r\n\r\n    SpecificationError: Function names must be unique, found multiple named <lambda>\r\n\r\n**Backward incompatible regression: one cannot apply two different lambdas to the same original column anymore.**\r\n\r\nIf one removes the `lambda x: np.percentile(x, 98)` from above, we get the same issue with the partial function which inherits the function name from the original function:\r\n\r\n    SpecificationError: Function names must be unique, found multiple named mad\r\n\r\nFinally, after overwriting the `__name__` attribute of the partial (for example with `mad_c1.__name__ = 'mad_c1'`) we get:\r\n\r\n```\r\n    energy          distance\r\n       sum <lambda>      sum   mean       mad mad_c1\r\ncat\r\nA     5.79   1.8510     4.44  1.480  0.355825  0.240\r\nB     2.85   1.3095     1.83  0.915  0.140847  0.095\r\nC     1.01   1.0100     0.60  0.600  0.000000  0.000\r\n```\r\n\r\nwith still \r\n\r\n- one column missing (98th percentile)\r\n- the handling of the MultiIndex columns \r\n- and the renaming of the columns\r\n\r\nto deal with in separate step.\r\n\r\nThere is no control possible for the column names after aggregation, the best we can get in an automated way is some combination of original column name and the _aggregate function's name_ like this:\r\n\r\n```python\r\nmydf_agg.columns = ['_'.join(col) for col in mydf_agg.columns]\r\n```\r\n\r\nwhich results in:\r\n\r\n```\r\n     energy_sum  energy_<lambda>  distance_sum  distance_mean  distance_mad distance_mad_c1\r\ncat\r\nA          5.79           1.8510          4.44          1.480      0.355825           0.240\r\nB          2.85           1.3095          1.83          0.915      0.140847           0.095\r\nC          1.01           1.0100          0.60          0.600      0.000000           0.000\r\n```\r\n\r\nand if you really need to have different names, you can do it like this:\r\n\r\n```python\r\nmydf_agg.rename({\r\n    \"energy_sum\": \"total_energy\",\r\n    \"energy_<lambda>\": \"energy_p17\",\r\n    \"distance_sum\": \"total_distance\",\r\n    \"distance_mean\": \"average_distance\"\r\n    }, inplace=True)\r\n```\r\n\r\nbut that means that you need to be careful to keep the renaming code (which must now be located at another place in the code) in sync with the code where the aggregation is defined...\r\n\r\nSad pandas user 😢  (which still loves pandas of course)\r\n\r\n---\r\n\r\nI am all in for consistency, and at the same time I deeply regret the deprecation of the _aggregate and rename_ functionality. I hope the examples above make the pain points clear.\r\n\r\n---\r\n\r\n### Possible solutions\r\n\r\n- Un-deprecate the dict-of-dict relabeling functionality\r\n- Provide another API to be able to do it (but why should there be two methods for the same main purpose, namely aggregation?)\r\n- ??? (open to suggestions)\r\n\r\n---\r\n\r\n_Optional read:_\r\n\r\nWith respect to the aforementioned discussion in the pull request which has been going on already for a few months, I only recently realized one of the reasons why I am so bothered by this deprecation: \"aggregate and rename\"  is a natural thing to do with GROUP BY aggregations in SQL since in SQL you usually provide the destination column name directly next to the aggregation expression, e.g. `SELECT col1, avg(col2) AS col2_mean, stddev(col2) AS col2_var FROM mytable GROUP BY col1`.\r\n\r\nI'm **_not_** saying that Pandas should necessarily provide the same functionalities as SQL of course. But the examples provided above demonstrate why the dict-of-dict API was in my opinion a clean and simple solution to many use-cases.\r\n\r\n(* I don't personally agree that the dict-of-dict approach is complex.)"},{"labels":["api",null,null],"text":"Hi everyone,\r\n\r\nIt seems that pandas read_html doesn't process numeric values properly, the detailed issue with code examples on stackoverflow: https://stackoverflow.com/questions/47327966/pandas-converting-numbers-to-strings-unexpected-results\r\n\r\n\r\n**Source table:**\r\n```\r\n<body>\r\n    <table>\r\n        <thead>\r\n            <tr>\r\n                <th class=\"tabHead\" x:autofilter=\"all\">Number</th>\r\n            </tr>\r\n        </thead>\r\n        <tbody>\r\n            <tr>\r\n                <td class=\"tDetail\">1.320,00</td>\r\n            </tr>\r\n            <tr>\r\n                <td class=\"tDetail\">600,00</td>\r\n            </tr>\r\n        </tbody>\r\n    </table>\r\n</body>\r\n```\r\n**Obviously, the expected output is:**\r\n```\r\n     Number\r\n0  1.320,00\r\n1    600,00\r\n```\r\n\r\n### (1) Straightforward reading of the file\r\n**Processing code:**\r\n```\r\nimport pandas\r\n\r\ndf = pandas.read_html('test_file.xls')\r\nprint(df[0])\r\nprint(df[0].dtypes)\r\n```\r\n**Output:**\r\n```\r\n     Number\r\n0      1.32\r\n1  60000.00\r\n\r\nNumber    float64\r\ndtype: object\r\n```\r\n\r\n### (2) Applying str function as a convertor for each dimension\r\n\r\n**Processing code:**\r\n```\r\nconverters = {column_name: str for column_name in df[0].dtypes.index}\r\ndf = pandas.read_html(f, converters = converters)\r\nprint(df[0])\r\nprint(df[0].dtypes)\r\n```\r\n**Output:**\r\n```\r\n    Number\r\n0  1.32000\r\n1    60000\r\n\r\nNumber    object\r\ndtype: object\r\n```\r\n\r\nThere could be cases when one file contains numbers typed in different formats (American / European / etc). This numbers differs with decimal mark, thousand mark, etc. So the logical way to handle such files will be to extract the data \"as it is\" in strings and perform parsing with regexps / other modules separately for each row. Is there a way how to do it in pandas? Thanks guys!\r\n\r\n\r\n**Notes:**\r\n\r\n1. The input value of <td class=\"tDetail\">,,,2,,,,5,,,,,5,,,,0,,,.,,,7,,,7,,,</td> (mind the dote!) also converts to 2550.77\r\n2. Specification of \"decimal\" and \"thousands\" parameters for pandas.read_* doesn't look like a reliable solution because it is appled for all fields. Quick example: it can treat date fields in \"02.2017\" format as numbers and convert it to \"022017\" (even to \"22017\" without leading zero)\r\n3. The workaround if you have 100.000,00 formatted numerical values and 01.12.2017 dates is the following: using decimal = ',', thousands = '.' and passing the convertor dictionary that maps all columns to str: converters = {column_name: str for column_name in df[0].dtypes.index} in read_html call. So the numbers will be correct (according to this format) and dates won't be changed to something like 1122017 (remember that leading zero might be removed!)\r\n\r\nSimilar issues is https://github.com/pandas-dev/pandas/issues/10534.\r\nIt is still opened, but here I also mention a direct unexpected behavior, like:\r\n\r\n1. \"01.12.2017\" -> 1122017\r\n2. \",,,,,42.......42,,.,.,.,.,.,.42........\" -> 424242\r\n\r\nI guess this issue is not about \"how to convert numbers properly\" but \"how to get actual data from html table\". It's clear that pandas provides an analytical way of data processing and management, but at the same time pandas.read_html is **the only reliable way in Python** of obtaining raw data from html tables without parsing tr, th, td, etc... So I think it's really important to think about just \"conversion\" behavior of pandas in these terms."},{"labels":["api",null],"text":"I need a way to apply a custom function on a rolling dataframe where the centered value is not included.  \r\n\r\nThis code below works well, but it does include the center value:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nmad = lambda x: np.mean(np.fabs(x - np.median(x)))\r\n\r\ndf = pd.DataFrame([1,1,1,10,1,1,1,2,2,2,2,2,20,2,2,2,2])\r\n\r\ndf.rolling(3, min_periods=3, center=True).apply(mad)\r\n```\r\n\r\nAnd this code below does not include the center, but only allows me to do mean or sum, no custom function:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame([1,1,1,10,1,1,1,2,2,2,2,2,20,2,2,2,2])\r\n\r\ndf.rolling(window=[1,0,1], win_type='boxcar', center=True, min_periods=3).mean()\r\n```\r\n\r\nIs there a way I am missing that would allow me to compute my custom function (mad) which does not include the center value?\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.Categorical([1,3,2,4], categories=[4,2,3,1,5]).unique()\r\nOut[2]: \r\n[1, 3, 2, 4]\r\nCategories (4, int64): [1, 3, 2, 4]\r\n\r\n```\r\n\r\nsame happens with ``CategoricalIndex`` and ``Series`` of ``dtype=category`` (as long as ``ordered=False``).\r\n\r\n#### Problem description\r\n\r\nThe general pandas approach to categoricals seems to change the ``.categories`` field as seldom as possible. So I see no reason why ``unique()``, which _by definition_ does not affect the set of distinct values, should change the categories (reordering them based on appearance, and dropping unused ones).\r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [2]: pd.Categorical([1,3,2,4], categories=[4,2,3,1,5]).unique()\r\nOut[2]: \r\n[1, 3, 2, 4]\r\nCategories (4, int64): [4, 2, 3, 1, 5]\r\n\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nIn [3]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 63e8527d32aaf6afe1cd4b2a7b3bfadb088c9a72\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.22.0.dev0+131.g63e8527d3\r\npytest: 3.2.3\r\npip: 9.0.1\r\nsetuptools: 36.7.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"There are a bunch of GH issues relating to arithmetic ops with datetimelike objects behaving unexpectedly when boxed in a `(Series|DataFrame)`.  For the issues I have in mind (I'll collect a list later), the operation works as expected when done on a `(DatetimeIndex|TimedeltaIndex|PeriodIndex)`.\r\n\r\nThe relevant `(Series|DataFrame)` operations are defined in core.ops.  The `DatetimeIndex` (et al) ops are defined in `core.indexes.datetimelike.DatetimeIndexOpsMixin`.  With a little gumption the relevant ops can be taken out of `DatetimeIndexOpsMixin` and made into `(Index|Series|DataFrame)`-agnostic classes `TimestampVector`, `TimedeltaVector`, `PeriodVector`.\r\n\r\nWould it be feasible/desirable to have appropriately-dtyped `(Series|DataFrame)` methods dispatch to these implementations?"},{"labels":["api",null,null],"text":"#### Code Sample\r\n\r\n```python\r\nA = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\r\nB = pd.DataFrame({'a': [1, 2, 3], 'c': [7, 8, 9]})\r\n\r\ndef foo(A, B):\r\n    C = A.merge(B, on=['a'])\r\n    C['c']  # Where does c originate from?\r\n\r\n```\r\n#### Problem description\r\n\r\nThere is no way to explicitely specificy whether 'c' originates from A or B. This makes code harder to maintain. Better would be if I could write something like:\r\n\r\n```python\r\nC = A.merge(B, on=['a'], always_suffix=True)\r\nC['c_y]  # Obvious that C comes from B, because suffixes are always created\r\n```\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame.from_dict({'a': [1, 2], 'f':[3, 4]})\r\nprint(df.min(axis=None))\r\nprint(np.min(df, axis=None))\r\n```\r\n#### Problem description\r\nSimilar to numpy I would expect the function to return the total min or max for ```axis=None``` (a scalar).\r\nThe function returns a Series with one value per column instead. \r\n\r\nAs the same happens with np.min, the problem could also be connected to numpy.\r\n\r\nIf this behavior is intended, I would suggest changing the default of axis to 0.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\ncommit: None\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.4.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.9999999\r\nsqlalchemy: 1.1.13\r\npymysql: 0.7.9.None\r\npsycopg2: 2.7.1 (dt dec pq3 ext lo64)\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"These are only to provide some inside into a sparse dtype. These should in theory be encoded in the dtypes (in pandas2), but in reality these are just cluttering the API and actually an implementation detail."},{"labels":["api",null],"text":"```\r\nIn [13]: pd.Series.asobject?\r\nType:        property\r\nString form: <property object at 0x10b674548>\r\nDocstring:\r\nreturn object Series which contains boxed values\r\n\r\n*this is an internal non-public method*\r\n```\r\n\r\nLet's make it private then."},{"labels":["api",null,null,null],"text":"When running pandas in AWS, The following works perfectly fine:\r\n```python\r\npd.read_csv(\"s3://mybucket/data.csv\")\r\n```\r\nBut running the following, does not:\r\n```python\r\npd.read_csv(\"hdfs:///tmp/data.csv\")\r\n```\r\n\r\nIt would be a good user experience to allow for the hdfs:// schema too similar to how http, ftp, s3, and file are valid schemas right now."},{"labels":["api"],"text":"```python\r\n\r\nfrom collections import OrderedDict\r\nimport pandas as pd\r\ndata = OrderedDict([('ele2', OrderedDict([('b', 1), ('a', 2)])),\r\n                    ('ele1', OrderedDict([('b', 2), ('a', 5)]))])             \r\npd.DataFrame(data)\r\n```\r\n![capture](https://user-images.githubusercontent.com/832380/32543581-53bb0d4c-c476-11e7-8b7d-c9c9089663da.PNG)\r\n#### Problem description\r\nThe output shows the rows get sorted. \r\n\r\n#### Expected Output\r\n\r\nWhen constructing a df with OrderedDicts I would expect the order in columns *and* rows to stay the same. \r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nIn [6]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.10.0-514.21.1.el7.x86_64\r\nmachine: x86_64\r\nprocessor:\r\nbyteorder: little\r\nLC_ALL: C\r\nLANG: en_US.UTF-8A\r\nLOCALE: None.None\r\n\r\npandas: 0.21.0\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: 1.6.2\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.3.0\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.8.0\r\nbs4: None\r\nhtml5lib: 0.999\r\nsqlalchemy: 1.1.10\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nind = pd.Series(['first', 'second', 'third'], name='count')\r\ns = pd.Series([1, 2, 3], index=ind, name='i should be a y label')\r\ns.plot()\r\nplt.show()\r\n```\r\n#### Problem description\r\n\r\nSo when you call Series.plot, xlabel is set to the name of the index, but ylabel is not set to the name of the series itself. To me it seems asymmetric and believe it should set ylabel automatically.\r\nAlso, if I plot it with `s.plot(kind='hist')`, I get ylabel \"Frequency\" but no xlabel, which I would expect to be the name of the series as well.\r\n\r\n#### Expected Output\r\n\r\nylabel set to name of the series for usual plot, and xlabel set to name of the series for hist plot\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: AMD64 Family 16 Model 4 Stepping 3, AuthenticAMD\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: 3.2.1\r\npip: 9.0.1\r\nsetuptools: 36.5.0.post20170921\r\nCython: 0.26.1\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.8\r\nlxml: 3.8.0\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"```\r\nIn [2]: pd.Categorical(['a', 'b']).rename_categories(pd.Series([0, 1]))\r\nOut[2]:\r\n[a, b]\r\nCategories (2, object): [a, b]\r\n```\r\n\r\nIn 0.20.3 that was\r\n\r\n```python\r\n[0, 1]\r\nCategories (2, int64): [0, 1]\r\nIn [9]:\r\n```\r\n\r\nDo we want to treat series like an array or a dictionary here?\r\n\r\n(this is causing the perf slowdown in rank I think)."},{"labels":["api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\npropose something like this:\r\n\r\n```python\r\npd.set_option('mode.inplace_operations_default', True)\r\n```\r\n#### ~Problem~ Proposal description\r\n\r\nMy team routinely works with DataFrames of 50GB and up where forgetting `inplace=True` on mutating operations quickly becomes detrimental to shared resources.  We would like to be able to default all operations with an inplace argument to True and be forced to explicitly allow copies if needed.  The proposal is to use a global option to determine the default inplace value vs. making it false in each function/method.  It seems like this should be transparent to the overall Pandas community which expects `inplace=False` semantics (still the default) but gives flexibility to \"advanced\" users.  From what we can tell in the docs, all inplace arguments are currently False, there are no divergent cases which would prevent a single global default.  Even if there were (perhaps in the future), function authors could override the global default with their opposing local default preference (not great for consistency though).\r\n\r\n#### Expected Output\r\n\r\nIf set to True, all functions/methods which can operate inplace will unless explicitly overriden with an `inplace=False` argument.\r\n"},{"labels":["api"],"text":"Posted in https://github.com/pandas-dev/pandas/pull/17800#issuecomment-338989093, but making a new issue to make sure it's not lost and that everone is aware, and to check if we're comfortable with this:\r\n\r\n```python\r\ndf = pd.DataFrame(columns=['A', 'B'])\r\ndf.rename(None, str.lower)\r\n```\r\n\r\nIn previous versions that meant `.rename(index=None, columns=str.lower)`, but now it's `.rename(mapper=None, index=str.lower)`. I think this is fundamentally ambiguous correct? This is breaking dask, which used positional arguments, but I suspect it'll break more code.\r\n\r\ncc @jreback @jorisvandenbossche \r\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nx = datetime.datetime.now()\r\ndf = pd.DataFrame([[x, 1], [x, 2]], columns=['a', 'b'])\r\nprint(df.apply(lambda row: {'result': row['b']}, axis=1))\r\nprint()\r\nx = datetime.datetime.now()\r\ndf = pd.DataFrame([[x, 1], [x, 2]], columns=['a', 'b'], dtype='int64')\r\nprint(df.apply(lambda row: {'result': row['b']}, axis=1))\r\nprint()\r\nx = 101\r\ndf = pd.DataFrame([[x, 1], [x, 2]], columns=['a', 'b'])\r\nprint(df.apply(lambda row: {'result': row['b']}, axis=1))\r\nprint()\r\nx = datetime.date.today()\r\ndf = pd.DataFrame([[x, 1], [x, 2]], columns=['a', 'b'])\r\nprint(df.apply(lambda row: {'result': row['b']}, axis=1))\r\nprint()\r\nx = {'x': 2}\r\ndf = pd.DataFrame([[x, 1], [x, 2]], columns=['a', 'b'])\r\nprint(df.apply(lambda row: {'result': row['b']}, axis=1))\r\n```\r\n#### Problem description\r\n\r\nThe latter 4 cases produce the results that I can expect -- Series containing dicts. The first one is somehow strange as it produces a DataFrame:\r\n    a   b\r\n0 NaN NaN\r\n1 NaN NaN\r\n\r\nAlso, when I change the first case's print line to print(df.apply(lambda row: [10,20], axis=1)) I am also receiving a DataFrame:\r\n    a   b\r\n0  10  20\r\n1  10  20\r\n\r\n\r\n#### Expected Output\r\n0    {'result': 1}\r\n1    {'result': 2}\r\ndtype: object\r\n\r\n0    {'result': 1}\r\n1    {'result': 2}\r\ndtype: object\r\n\r\n0    {'result': 1}\r\n1    {'result': 2}\r\ndtype: object\r\n\r\n0    {'result': 1}\r\n1    {'result': 2}\r\ndtype: object\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-96-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.5.0\r\nCython: None\r\nnumpy: 1.13.3\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 1.0b10\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.7.3.1 (dt dec pq3 ext lo64)\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"The implementation of `Series.get` currenlty simply is:\r\n\r\n```python\r\ndef get(self, key, default=None):\r\n    try:\r\n        return self[key]\r\n    except (KeyError, ValueError, IndexError):\r\n        return default\r\n```\r\n\r\nSo it is simply using `[]` / `__getitem__` under the hood.\r\n\r\nSomehow I think this is the expected thing, but the consequence is that it brings along all complexities of `__getitem__` in pandas (whether to fallback to positional or not ..). \r\n\r\nSo, if we want, we could make `get` more strict by using `loc` under the hood (and it should be possible to do that with a deprecation in a first phase for those cases that `loc` raises but `[]` returns a value).\r\n"},{"labels":["api",null],"text":"I've made a PR(#17871) to get pipe funcionality on GroupBy objects. \r\n\r\n``Resampler`` should IMO have the same ``.pipe`` behaviour as GroupBy objects. Then you could do the below in a single pass:\r\n\r\n```python\r\ndf.resample('3M').pipe(lambda resampled: resampled.Open.first() - resampled.close.last())\r\n```\r\n\r\nThat is, for each resampled period, you could reuse a ``Resampler`` objects multiple time in a pipe. The alternative would be to do it in several lines, which would be less readable, or to use apply, which would be slower.\r\n\r\nCurrently, however, ``Resampler.pipe``/``DatetimeResampler.pipe`` implicitly converts to a dataframe of mean before piping, which to me seems wrong/unintuitive:\r\n\r\n```python\r\n>>> df.resample('3M').pipe(lambda x: x.max() - x.min())\r\nC:\\Users\\TP\\Anaconda3\\envs\\pandasdev\\Scripts\\ipython-script.py:1: FutureWarning:\r\n.resample() is now a deferred operation\r\nYou called pipe(...) on this deferred object which materialized it into a dataframe\r\nby implicitly taking the mean.  Use .resample(...).mean() instead\r\n```\r\n\r\nTo demonstrate:\r\n\r\nFirst set-up:\r\n\r\n```python\r\n>>> d = = pd.date_range('2017-01-01', periods=4)\r\n>>> df = pd.DataFrame(dict(B=[1,2,3, 4]), index=d)\r\n>>> r = df.resample('2D')\r\n```\r\nif we call pipe we get an uexpected result:\r\n```python\r\n>>> r.pipe(lambda x: x.max() - x.min())\r\nB    2.0\r\ndtype: float64\r\n```\r\nThe reason for the unexpected result is that under the hood the above is:\r\n\r\n```python\r\n>>> r.mean().pipe(lambda x: x.max() - x.min())\r\nB    2.0\r\ndtype: float64\r\n```\r\n\r\nExpected would be:\r\n\r\n```python\r\n>>> def diff(r):\r\n...:       return r.max() - r.mean()\r\n>>> diff(r)   # r.pipe(diff) should give the same result as this \r\n              B\r\n2017-01-01  0.5\r\n2017-01-03  0.5\r\n```\r\n\r\nIMO, if the user wants the mean before piping, he should just himself call mean before pipe.\r\n\r\nAdding a pipe was very easy for GroupBy and has very good use cases. I propose adding a (proper)  pipe to Resampler also."},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\nI often find my self doing\r\n```python\r\nIn [2]: df = pd.Series(index=pd.MultiIndex.from_product([['A', 'B'], ['a', 'b']]))\r\n\r\nIn [3]: df.index.get_level_values(0).unique()\r\nOut[3]: Index(['A', 'B'], dtype='object')\r\n```\r\n\r\n#### Problem description\r\n\r\nThe above is very inefficient, because first a ``Series`` is built which includes a copy of the entire level (possibly using way more memory than the index itself), and only then duplicates are stripped. [Other people on SO](https://stackoverflow.com/questions/24495695/pandas-get-unique-multiindex-level-values-by-label#comment44836216_24496435) have faced the same problem, and this is also blocking a fix I wrote for #17845.\r\n\r\nI'm pushing a simple PR in seconds.\r\n\r\n#### Expected Output\r\n\r\nSame as above, but in an efficient way.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.21.0rc1+19.gb15d92d14\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: None\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: df = pd.DataFrame(index=[1,2,3])\r\n\r\nIn [3]: df.loc[1] = 3\r\n\r\nIn [4]: df\r\nOut[4]: \r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [1, 2, 3]\r\n\r\nIn [5]: df.loc[4] = 3\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-fe31ec4a62f3> in <module>()\r\n----> 1 df.loc[4] = 3\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in __setitem__(self, key, value)\r\n    192             key = com._apply_if_callable(key, self.obj)\r\n    193         indexer = self._get_setitem_indexer(key)\r\n--> 194         self._setitem_with_indexer(indexer, value)\r\n    195 \r\n    196     def _has_valid_type(self, k, axis):\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value)\r\n    421                     # no columns and scalar\r\n    422                     if not len(self.obj.columns):\r\n--> 423                         raise ValueError(\"cannot set a frame with no defined \"\r\n    424                                          \"columns\")\r\n    425 \r\n\r\nValueError: cannot set a frame with no defined columns\r\n\r\n\r\n```\r\n#### Problem description\r\n\r\nEither #17894 is accepted, and then ``In [5]:`` should just expand the index, and not raise an error...\r\n\r\n... or it is not accepted, and then ``In [3]:`` should raise.\r\n\r\nIn particular, the error message of ``In [5]:`` is misleading: one _can_ \"set\" a frame with no defined columns, as long as the label is in the index.\r\n\r\n#### Expected Output\r\n\r\nDepends on #17894 .\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 5bf7f9a4ff6968fd444fa7098f8dc95586591994\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.21.0rc1+18.g5bf7f9a4f\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: None\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"Extract of discussion from #16823\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: df = pd.DataFrame()\r\n\r\nIn [3]: df['dummy'] = 1\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-3122daad6dab> in <module>()\r\n----> 1 df['dummy'] = 1\r\n\r\n/home/nobackup/repo/pandas/pandas/core/frame.py in __setitem__(self, key, value)\r\n   2515         else:\r\n   2516             # set column\r\n-> 2517             self._set_item(key, value)\r\n   2518 \r\n   2519     def _setitem_slice(self, key, value):\r\n\r\n/home/nobackup/repo/pandas/pandas/core/frame.py in _set_item(self, key, value)\r\n   2584         \"\"\"\r\n   2585 \r\n-> 2586         self._ensure_valid_index(value)\r\n   2587         value = self._sanitize_column(key, value)\r\n   2588         NDFrame._set_item(self, key, value)\r\n\r\n/home/nobackup/repo/pandas/pandas/core/frame.py in _ensure_valid_index(self, value)\r\n   2561             if not is_list_like(value):\r\n   2562                 # GH16823, Raise an error due to loss of information\r\n-> 2563                 raise ValueError('If using all scalar values, you must pass'\r\n   2564                                  ' an index')\r\n   2565             try:\r\n\r\nValueError: If using all scalar values, you must pass an index\r\n```\r\n#### Problem description\r\n\r\nPreviously, the above would just add a new (obviously empty) column.\r\n\r\n@jreback [objects](https://github.com/pandas-dev/pandas/issues/16823#issuecomment-336867308) that if this is allowed, then we should also allow initialization with only scalars (as in ``pd.Dataframe({'a' : 1, 'b' : 2})``\r\n\r\nI'm not 100% sure of what @jorisvandenbossche [suggests](https://github.com/pandas-dev/pandas/issues/16823#issuecomment-336825391), but he agrees with me that the current state is inconsistent.\r\n\r\n[My view](https://github.com/pandas-dev/pandas/issues/16823#issuecomment-336878719) is that previously things were just (almost) fine:\r\n- at initialization, a ``DataFrame`` _needs_ to have an index. You can avoid providing one expliclty only if it can be automatically built for the values you pass (i.e. 1-dimensional objects of the same length, or a single 2-dimensional block of data). Scalars clearly do not satisfy this requirement, so the constructor will raise if passed only scalars (but ``pd.DataFrame({'A'  : range(3), 'B' : 23})`` works, which is cool).\r\n- at assignment, there is already an index, and in particular, when assigning a(n entire) column you know you'll _never_ alter the index. More specifically, when you assign a scalar to a column, you know it will alter all _existing_ rows, which means \"none\" if the index is empty. And if the column does not exist, it will just be added, clearly empty as well.\r\n\r\nIn both cases, scalars/empty indexes represent no exception to the general behavior.\r\n\r\nFor consistency, we might want to fix the following too:\r\n\r\n``` python\r\nIn [2]: pd.DataFrame().loc[1] = 0\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-20bba0809d0b> in <module>()\r\n----> 1 pd.DataFrame().loc[1] = 0\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in __setitem__(self, key, value)\r\n    192             key = com._apply_if_callable(key, self.obj)\r\n    193         indexer = self._get_setitem_indexer(key)\r\n--> 194         self._setitem_with_indexer(indexer, value)\r\n    195 \r\n    196     def _has_valid_type(self, k, axis):\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value)\r\n    421                     # no columns and scalar\r\n    422                     if not len(self.obj.columns):\r\n--> 423                         raise ValueError(\"cannot set a frame with no defined \"\r\n    424                                          \"columns\")\r\n    425 \r\n\r\nValueError: cannot set a frame with no defined columns\r\n\r\n```\r\n\r\nbut I will detail this in a separate issue.\r\n\r\n#### Expected Output\r\n\r\nNone, but a new column ``\"dummy\"`` is added to ``df``.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 5bf7f9a4ff6968fd444fa7098f8dc95586591994\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.21.0rc1+18.g5bf7f9a4f\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: None\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"I propose adding a pipe method to GroupBy objects, having the same interface as ``DataFrame.pipe``/``Series.pipe``. \r\n\r\nThe use case is reusing a GroupBy object when doing calculations, see use case example below.\r\n\r\n## Use case\r\n\r\nA pipe is useful for succintly reusing Groupby objects in calculations, for example calculating prices given a column of revenue and a columns of quantity sold:\r\n\r\n```python\r\n>>> from numpy.random import choice, random\r\n>>> n = 100_000\r\n>>> df = pd.DataFrame({'Store': choice(['Store_1', 'Store_2'], n),\r\n                       'Year': choice(['Year_1', 'Year_2', 'Year_3', 'Year_4'], n),\r\n                       'Revenue': (np.random.random(n)*50+10).round(2),\r\n                       'Quantity': np.random.randint(1, 10, size=n)})\r\n>>> df.head(2)\r\n   Quantity  Revenue    Store    Year\r\n0         2    14.69  Store_1  Year_1\r\n1         9    25.89  Store_2  Year_4\r\n```\r\n\r\nThen having ``.pipe``, we could for example get prices per store/year like so\r\n```python\r\n>>> (df.groupby(['Store', 'Year'])\r\n...    .pipe(lambda grp: grp.Revenue.sum()/grp.Quantity.sum())\r\n...    .unstack().round(2))\r\nYear     Year_1  Year_2  Year_3  Year_4\r\nStore\r\nStore_1    6.99    6.99    7.01    6.92\r\nStore_2    6.95    6.98    6.97    6.96\r\n```\r\n\r\nNote that the above is vectorized and piping makes the code succint and clear.\r\n\r\n## Alternatives to .pipe\r\nThe alternatives would be:\r\n1. use ``.apply``, \r\n2. create a function and call that with a GroupBy object as its argument\r\n3. Create a price column\r\n\r\nOption 1 is not good because of slowness. \r\n\r\nA pipe is just syntactic sugar for option 2, but would piping be more readable, especially it you're piping other stuff already.\r\n\r\nCreating a concrete calculated column is in some instances the right approach, but in other cases it is better to calculate stuff."},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\na = pd.Series(data=[0],index=['A'])\r\nprint(a.get(1))#expected behaviour: returns default, here None\r\na['B']=2\r\nprint(a.get(1))#expected behaviour: returns 2\r\na.loc['C']=3\r\nprint(a.get(1))#expected behaviour: returns 2\r\na.loc[1]=5\r\nprint(a.get(1))#What is it? 2 or 5? (answer: 5=> Integer location based)\r\nprint(a.get(0))#What is it? default or 0? (answer:0=> index-name location based)\r\n```\r\n#### Problem description\r\nSometimes it is necessary to return a value from a DataFrame or a Series without throwing an error if the index is not present. The \"get\" function does just that.\r\nUnfortunately, the \"get\" method doesn't allow to use an integer-location based index except for Series. \r\n\r\nHowever, as shown in the example, the \"get\" method can return unexpected values.\r\n\r\nI would suggest to add an \"iget\" method for both Series and DataFrames similar to the \"get\" method:\r\n```python\r\ndef iget(self, key, default=None):\r\n            \"\"\"\r\n            Get item from object for given index. Returns default value if not found.\r\n            Parameters\r\n            ----------\r\n            key : object\r\n            Returns\r\n            -------\r\n            value : type of items contained in object\r\n            \"\"\"\r\n            try:\r\n                return self.iloc[key]\r\n            except (KeyError, ValueError, IndexError):\r\n                return default\r\n```\r\nWhile it doesn't solve the unexpected behavior of \"get\" with Series, it allows for an integer-based get methods with expected behavior for both pandas classes.\r\n\r\n#### Expected Output\r\n```python\r\na = pd.Series(data=[0],index=['A'])\r\nprint(a.get(1))#returns default, here None\r\na['B']=2\r\nprint(a.get(1))#returns default, here None\r\na.loc['C']=3\r\nprint(a.get(1))#returns default, here None\r\na.loc[1]=5\r\nprint(a.get(1))#returns 5\r\nprint(a.iget(1))#returns 2\r\nprint(a.iget(0))#returns 0\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en\r\nLOCALE: None.None\r\n\r\npandas: 0.20.1\r\npytest: 3.0.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 6.0.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.2.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.3\r\nbs4: 4.6.0\r\nhtml5lib: 0.999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api"],"text":"I often want to remove certain elements from an index while retaining the original ordering of the index. For this reason a sort keyword in the ``index.difference`` method would be required:\r\n\r\n```\r\nIn: index = pd.Index([0, 1, 2, 4, 3])\r\nIn: index.difference({1, 2})\r\nOut: Int64Index([0, 3, 4], dtype='int64')\r\n# It would be cool to have instead\r\nIn: index.difference({1, 2}, sort=False)\r\nOut: Int64Index([0, 4, 3], dtype='int64')\r\n```\r\n\r\nI think that this usecase appears frequently and setting the default to ``sort=True`` doesn't affect existing code."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\nimport pandas as pd\r\n\r\nA = pd.DataFrame({'colname': [1, 2]})\r\n\r\nB = pd.DataFrame({'colname': [1, 2]})\r\n\r\nC = pd.DataFrame({'colname': [1, 2]})\r\n\r\nD = pd.merge(A, B, right_index=True, left_index=True, suffixes=('_A', '_B'))\r\n\r\nprint(pd.merge(D, B, right_index=True, left_index=True, suffixes=('', '_C'))\r\n\r\n>   colname_A  colname_B  colname\r\n> 0          1          1        1\r\n> 1          2          2        2\r\n\r\n```\r\n#### Problem description\r\nWhen using pandas.merge() suffixes are only added to the column-names if a column-name appears in both data frames. With more than two data frames that can lead to the above situation. The first merge adds suffixes. When the resulting table is merged with a third table the column-name does not appear twice and the suffix is not added. The suffix then needs to be added manually. This is overhead, especially when larger numbers of data-frames are merged. An option \"force_suffixes\" would be appreciated that ensures the suffix is added. \r\n\r\n#### Expected Output\r\n```\r\n>   colname_A  colname_B  colname_C\r\n> 0          1          1        1\r\n> 1          2          2        2\r\n```\r\n"},{"labels":["api",null],"text":"Given we are adding the \"labels, axis=0/1\" idiom to `reindex` and `rename` itself (https://github.com/pandas-dev/pandas/issues/12392, PR https://github.com/pandas-dev/pandas/pull/17800), I think the specific `reindex_axis` and `rename_axis` will be superfluous.\r\n\r\nShould be deprecate them?"},{"labels":["api",null,null],"text":"xref https://github.com/pandas-dev/pandas/pull/17826\r\n\r\n`DatetimeIndex.to_series` has a `keep_tz` keyword with a default of False (= dropping the timezone information). This stems from a time we could not store datetime tz data inside a series (with `keep_tz=True`, you would get an object dtyped Series of Timestamps). Nowadays this just gives a series with `datetime64[ns, tz]` dtype, so it makes sense to make this the default and deprecate the keyword.\r\n\r\nSince it is an API change, ideally we first raise a warning that the behaviour will be changed (the default will change from False to True). You could then suppress this warning by passing `keep_tz=True`. \r\nHowever, that means that we cannot directly deprecate the keyword itself, as we have to keep it for suppressing the warning / getting the future behaviour.\r\n\r\n"},{"labels":["api",null,null],"text":"Possible API breakage for dask in https://github.com/pandas-dev/pandas/pull/16821\r\n\r\n```python\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\n\r\nfrom dask.dataframe.utils import assert_eq\r\n\r\n\r\ns = dd.core.Scalar({('s', 0): 10}, 's', 'i8')\r\npdf = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7],\r\n                    'b': [7, 6, 5, 4, 3, 2, 1]})\r\nresult = (pdf + s).dtypes  # This now casts to object, used to retain int64\r\nexpected = pdf.dtypes\r\nassert_eq(result, expected)\r\n```\r\n\r\nIn master, `result` is now `(object, object)`. Before it was `(int64, int64)`.\r\n\r\nI'm looking into #16821 to see if this was unintentional, and can be avoided."},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [3]: df = pd.DataFrame(index=range(3), columns=pd.MultiIndex.from_product([['I', 'II'], ['a', 'b']]))\r\n\r\nIn [4]: df.dropna(subset=['I'])\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-4-2ec2c1f3958a> in <module>()\r\n----> 1 df.dropna(subset=['I'])\r\n\r\n/home/pietro/nobackup/repo/pandas/pandas/core/frame.py in dropna(self, axis, how, thresh, subset, inplace)\r\n   3339                 check = indices == -1\r\n   3340                 if check.any():\r\n-> 3341                     raise KeyError(list(np.compress(check, subset)))\r\n   3342                 agg_obj = self.take(indices, axis=agg_axis)\r\n   3343 \r\n\r\nKeyError: ['I']\r\n```\r\n\r\n... which is particularly unexpected when doing:\r\n``` python\r\nIn [5]: df['other'] = (1, 3, np.nan)\r\n\r\nIn [6]: df.dropna(subset=['other'])\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-6-5d8972ce7a1c> in <module>()\r\n----> 1 df.dropna(subset=['other'])\r\n\r\n/home/pietro/nobackup/repo/pandas/pandas/core/frame.py in dropna(self, axis, how, thresh, subset, inplace)\r\n   3339                 check = indices == -1\r\n   3340                 if check.any():\r\n-> 3341                     raise KeyError(list(np.compress(check, subset)))\r\n   3342                 agg_obj = self.take(indices, axis=agg_axis)\r\n   3343 \r\n\r\nKeyError: ['other']\r\n\r\n```\r\n\r\n\r\n#### Problem description\r\n\r\n``dropna(subset=...)`` should mimic the behaviour of indexing with incomplete ``MultiIndex`` keys.\r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [7]: df.dropna(subset=[('other', '')])\r\nOut[7]: \r\n     I        II      other\r\n     a    b    a    b      \r\n0  NaN  NaN  NaN  NaN   1.0\r\n1  NaN  NaN  NaN  NaN   3.0\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.21.0.dev+546.gad7d051bd\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: None\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"I am importing data that comes from other tools (SPSS, Excel) that handles categories, and these categories are represented by dictionary-like structures. Say, a column `Rate` would accept the following categories:\r\n\r\n    valuelabels = {\r\n        1 :  'High',\r\n        2 :  'Medium',\r\n        3 : 'Low',\r\n        99999 : 'No idea',\r\n    }\r\n\r\nI would like to create a pandas `category` column that matches not only the `labels` (i.e. `'High'`, `'Good'`, ...), but also the `ids`, so that I could easily export back to the source format without reassigning ids (to be sure, I can do this by keeping external structures and such, but I think that this could be an improvement to pandas).\r\n\r\n A current possibility would be doing the following:\r\n\r\n    values_of_column = [1,99999,3,1] # would correspond to 'High', 'No Idea', 'Low', 'High'\r\n    labels = ['dummy0']  + ['Hight','Medium','Low'] + [ f'dummy{i}' for i in range(4,99999)] + ['No idea']\r\n    s = pd.Series(pd.Categorical.from_codes( values_of_column , categories= labels ))\r\n\r\nThe above  is simply ugly, and unfortunately not enough, as I have already seen value labels with negative numbers:\r\n\r\n    value_labels = {\r\n        -999: 'n/a',\r\n        -99: 'No answer',\r\n        1 :  'High',\r\n        2 :  'Medium',\r\n        3 : 'Low',\r\n        99999 : 'No idea',\r\n    }\r\n\r\nI can image other cases (tools different from SPSS) in which there is strings in both sides of the dictionary, say something like\r\n\r\n    value_labels = {\r\n        'a' : 'answer a',\r\n        'b' : 'answer b',\r\n        'no_answer': 'Doesn't know/Doesn't answer'\r\n    } \r\n\r\nIdeally, using the last example, I would like to have the possibility of using the dictionaries directly:\r\n\r\n   s = pd.Series(pd.Categorical.from_codes( ['a','b','no_answer','b'], categories=value_labels))\r\n\r\nGiven the 1:1 nature of this relation, *this could also be refined in order to have some dual representation of values, specially when one is dealing with similar data from different sources*. For example, one database use the 2 letter ISO country representation (e.g. 'US') and another 3 letter ISO country representation (e.g. 'USA').\r\n\r\n"},{"labels":["api",null],"text":"There are a bunch of issues outstanding that relate to `Period` ops and comparison.  AFAICT making a decision about the comparison issue will snowball into resolving (some of) the ops issues.\r\n\r\n#5202 ENH: Period ops NaT & timedelta ops\r\n#10798 Date / Datetime in Period Index\r\n#6779 Adding Period and Offset not implemented\r\n#13077 ENH/API: Decide what to return Period - Period subtraction\r\n#17112 MultiIndex - Comparison with Mixed Frequencies (and other FUBAR)\r\n\r\nRight now two Period objects are comparable iff they equal `freq` attributes.  AFAICT this is to avoid guessing in cases where the \"correct\" answer is ambiguous.  But there are some other cases with an obviously correct answer.  Namely, if the `per1.end_time < per2.start_time`, then it should be the case that `per1 < per2` unambiguously.  This intuition also extends to `datetime` and `Timestamp` objects that do not lie between `per.start_time` and `per.end_time`. \r\n\r\nFor cases with overlap there are a couple of reasonable approaches.  My preferred approach is lexicographic: first compare `per1.start_time` with `per2.start_time`.  If they are equal, then compare the `end_time`s.  Then we treat `datetime` and `Timestamp` objects as analogous to zero-duration periods.\r\n\r\nThoughts?"},{"labels":["api",null],"text":"### Melt Enhancement\r\n\r\n**Summary**: This is a proposal with a pull request to enhance `melt` to simultaneously melt multiple groups of columns and to add functionality from `wide_to_long` along with better MultiIndexing capabilities. See [this notebook]( https://nbviewer.jupyter.org/github/tdpetrou/Machine-Learning-Books-With-Python/blob/master/melt%20enhancemenet.ipynb) for more examples.\r\n \r\n- Melts different groups of columns by passing a list of lists into `value_vars`. Each group gets melted into its own column. This feature replaces the need for lreshape.\r\n- When melting different groups of columns, groups do not have to be the same length. The shorter groups are filled with missing values.\r\n- Adds parameters `stubnames`(boolean), `prefix` and `sep` from function `wide_to_long`. It keeps the suffixes in separate columns and does not align them in the same way.\r\n- Can select any number of MultiIndex levels and greatly increase MultiIndex functionality\r\n- Works with repeated column names, which normally show up when selecting a subset of MultiIndex levels\r\n- Performance is ~30-40% faster than original `melt`, slightly slower than `lreshape` and much faster than `wide_to_long`\r\n\r\n\r\n\r\n```python\r\n>>> df = pd.DataFrame({'City': ['Houston', 'Austin', 'Hoover'],\r\n                   'State': ['Texas', 'Texas', 'Alabama'],\r\n                   'Name':['Aria', 'Penelope', 'Niko'],\r\n                   'Mango':[4, 10, 90],\r\n                   'Orange': [10, 8, 14], \r\n                   'Watermelon':[40, 99, 43],\r\n                   'Gin':[16, 200, 34],\r\n                   'Vodka':[20, 33, 18]},\r\n                 columns=['City', 'State', 'Name', 'Mango', 'Orange', 'Watermelon', 'Gin', 'Vodka'])\r\n\r\n      City    State      Name  Mango  Orange  Watermelon  Gin  Vodka\r\n0  Houston    Texas      Aria      4      10          40   16     20\r\n1   Austin    Texas  Penelope     10       8          99  200     33\r\n2   Hoover  Alabama      Niko     90      14          43   34     18\r\n```\r\nUse a list of lists in `value_vars` to melt the fruit and drinks\r\n```\r\n>>> df.melt(id_vars=['City', 'State'], value_vars=[['Mango', 'Orange', 'Watermelon'], ['Gin', 'Vodka']], \r\n                    var_name=['Fruit', 'Drink'], value_name=['Pounds', 'Ounces'])\r\n\r\n      City    State       Fruit  Pounds  Drink  Ounces\r\n0  Houston    Texas       Mango       4    Gin    16.0\r\n1   Austin    Texas       Mango      10    Gin   200.0\r\n2   Hoover  Alabama       Mango      90    Gin    34.0\r\n3  Houston    Texas      Orange      10  Vodka    20.0\r\n4   Austin    Texas      Orange       8  Vodka    33.0\r\n5   Hoover  Alabama      Orange      14  Vodka    18.0\r\n6  Houston    Texas  Watermelon      40    nan     NaN\r\n7   Austin    Texas  Watermelon      99    nan     NaN\r\n8   Hoover  Alabama  Watermelon      43    nan     NaN\r\n```\r\n`wide_to_long` functionality. Added parameters `stubnames`(boolean), `sep` and `suffix`.\r\n```\r\n>>> df1 = pd.DataFrame({'group': ['a', 'b', 'c'],\r\n                   'exp_1':[4, 10, -9],\r\n                   'exp_2': [10, 8, 14], \r\n                   'res_1':[8, 5, 4],\r\n                   'res_3':[11, 0, 7]}, columns=['group', 'exp_1', 'exp_2', 'res_1', 'res_3'])\r\n\r\n  group  exp_1  exp_2  res_1  res_3\r\n0     a      4     10      8     11\r\n1     b     10      8      5      0\r\n2     c     -9     14      4      7\r\n\r\n>>> df1.melt(id_vars='group', value_vars=['exp','res'], stubnames=True, sep='_')\r\n\r\n  group  variable_exp  exp  variable_res  res\r\n0     a             1    4             1    8\r\n1     b             1   10             1    5\r\n2     c             1   -9             1    4\r\n3     a             2   10             3   11\r\n4     b             2    8             3    0\r\n5     c             2   14             3    7\r\n```\r\nAlso adds support for all kinds of multiindexing\r\n```\r\n>>> df2 = df.copy()\r\n>>> df2.columns = pd.MultiIndex.from_arrays([list('aabbcccd'), list('ffffgggg'), df.columns], \r\n                                       names=[None, None, 'some vars'])\r\n\r\n                 a                  b            c                     d\r\n                 f                  f            g                     g\r\nsome vars     City    State      Name Mango Orange Watermelon  Gin Vodka\r\n0          Houston    Texas      Aria     4     10         40   16    20\r\n1           Austin    Texas  Penelope    10      8         99  200    33\r\n2           Hoover  Alabama      Niko    90     14         43   34    18\r\n\r\n>>> df2.melt(id_vars=[('a', 'f', 'State')], \r\n           value_vars=[[('b', 'f', 'Name'), ('c', 'g', 'Watermelon')],\r\n                       [('b','f','Mango'), ('c','g', 'Orange'), ('d', 'g', 'Vodka')]],\r\n           var_name=[['myvar1', 'myvar2', 'myvar3'],\r\n                     ['next_myvar1', 'next_myvar2', 'next_myvar3']],\r\n           value_name=['some values', 'more_values'])\r\n\r\n (a, f, State) myvar1 myvar2      myvar3 some values next_myvar1 next_myvar2  \\\r\n0         Texas      b      f        Name        Aria           b           f   \r\n1         Texas      b      f        Name    Penelope           b           f   \r\n2       Alabama      b      f        Name        Niko           b           f   \r\n3         Texas      c      g  Watermelon          40           c           g   \r\n4         Texas      c      g  Watermelon          99           c           g   \r\n5       Alabama      c      g  Watermelon          43           c           g   \r\n6         Texas    nan    nan         nan         NaN           d           g   \r\n7         Texas    nan    nan         nan         NaN           d           g   \r\n8       Alabama    nan    nan         nan         NaN           d           g   \r\n\r\n  next_myvar3  more_values  \r\n0       Mango            4  \r\n1       Mango           10  \r\n2       Mango           90  \r\n3      Orange           10  \r\n4      Orange            8  \r\n5      Orange           14  \r\n6       Vodka           20  \r\n7       Vodka           33  \r\n8       Vodka           18 \r\n```\r\n\r\n#### Problem description\r\n\r\nCurrently, there is poor support for simultaneous melting of multiple groups of columns. `lreshape` is old and undocumented. `wide_to_long` api does not match `melt` and it's slow.\r\n"},{"labels":["api",null,null],"text":"- use ``display.max_categories`` in ``CagetegoricalDtype.__unicode__`` [here](https://github.com/pandas-dev/pandas/pull/16015/files#r140619601)\r\n- use ``display.max_categories`` in ``Categorical.__unicode__``\r\n- deprecate usage of ``fastpath`` in ``Categorical`` constructor (#17562)\r\n- deprecate ``.astype(..., categories=)`` in favor of ``CategoricalDtype`` (https://github.com/pandas-dev/pandas/issues/17636)\r\n- move hash type routines to ``hashing.py`` [here](https://github.com/pandas-dev/pandas/pull/16015/files#r137931284)\r\n- validate that pyarrow is compat with changes"},{"labels":["api",null],"text":"I found this while working on [this pull request](https://github.com/pandas-dev/pandas/pull/17168). I intend to fix it in the same PR, but for thoroughness wanted to open it as its own issue in case there's debate about whether the current behavior is more desirable.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nj = \"\"\"\r\n{\"A\":1,\"B\":4}\r\n\r\n{\"A\":2,\"B\":5}\r\n\"\"\"\r\npd.read_json(j, lines=True)\r\n```\r\n#### Problem description\r\nThis raises `ValueError: Expected object or value` in a call to `json.loads` on line 639 of `pandas/io/json/json.py`. This happens in 0.20.2 and on the latest dev version.\r\n\r\n#### Expected Output\r\nI would expect this to return a DataFrame as though there were no blank lines in the input."},{"labels":["api",null,null],"text":"Using get_dummies is moving the columns to the end. What @jreback and @TomAugspurger have commented is right. But there are situations in which we require to preserve the order. For example, in scipy model creation algorithms,  the functions give preference for the columns based on the order of the columns. We are having to reorder the columns explicitly. I get it that it is not pandas' concern. But it will be better if we have an option to preserve.\r\n\r\nSo, I guess we must atleast have an option to preserve the order. Let the default be as the new columns to be at the end."},{"labels":["api",null],"text":"Following up on some discussion from #17554. \r\n\r\nThe main point of discussion was regarding merging the functionality of `cdate_range` into `bdate_range`.  This would involve adding the two additional keywords supported by `cdate_range` to `bdate_range` (`weekmask` and `holidays`), which would only be used (if provided) when a custom frequency is passed to `bdate_range`.   \r\n\r\nThis wouldn't be terribly invasive, as `cdate_range` is only top-level on master (not 0.20.3), and there is no documentation mentioning it.  Performing the merge would help keep the top-level API clean, and reduce the number of `date_range` related functions. On the user end, the change would amount to `cdate_range(...)` -> `bdate_range(..., freq='C')`.\r\n\r\nAn alternative would be to merge both `bdate_range` and `cdate_range` into `date_range`.  The only difference between `bdate_range` and `date_range` is the default value of `freq`, so from a technical standpoint it wouldn't add complexity to implement compared to just merging `cdate_range` -> `bdate_range`.\r\n\r\nThis would be a more invasive change though, as `bdate_range` currently is currently top-level, and there is some documentation mentioning it, so there'd be some additional work in terms of deprecation/doc modification.  It would provide the advantage of having only one `date_range` related function.  On the user end, the change would amount to `cdate_range(...)` -> `date_range(..., freq='C')`, and `bdate_range(...)` -> `date_range(..., freq='B')`.\r\n\r\nThe current plan is to do the `cdate_range` -> `bdate_range` merge, but we're interested in community input.\r\n\r\ncc @jorisvandenbossche \r\n"},{"labels":["api"],"text":"For my own purposes, I am considering writing classes to represent PMFs and CDFs using Pandas Series as the implementation, and an API similar to what I did in the thinkstats2 library (but made more stylistically consistent with Pandas).\r\n\r\nHas there been any discussion of adding something like this to Pandas?  If I develop it, would you be interested in seeing a proposal to include it?  (I ask now because it might influence some design decisions if I am targeting inclusion in Pandas).\r\n\r\n\r\n"},{"labels":["api",null,null,null],"text":"After https://github.com/pandas-dev/pandas/pull/16015 the `fastpath` argument to the Categorical constructor is unnecessary since you can skip the categorical validation by passing in a `CategoricalDtype`.l"},{"labels":["api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\nCurrent:\r\n\r\n```python\r\ndf = pd.DataFrame({'A': [1, 2, np.nan, np.nan, np.nan]})\r\ndf.mode()\r\n```\r\n>          A\r\n>     0  1.0\r\n>     1  2.0\r\n\r\nProposed:\r\n\r\n```python\r\ndf = pd.DataFrame({'A': [1, 2, np.nan, np.nan, np.nan]})\r\ndf.mode(dropna=False)\r\n```\r\n>          A\r\n>     0   NA\r\n\r\n#### Problem description\r\n\r\nCurrent behavior is inconsistent with `value_counts()` because that allows `value_counts(dropna=False)`.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.7.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 32.2.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Works fine\r\n``` python\r\n(\r\n    Series([1,2,3], name='A')\r\n    .astype('category')\r\n    .name\r\n)\r\n```\r\nI get 'A'\r\n\r\n#### Code in trouble\r\n``` python\r\n(\r\n    Series([1,2,3], name='A')\r\n    .astype('category')\r\n    .cat.set_categories([1,2,3])\r\n    .name\r\n)\r\n```\r\nI get NoneType\r\n\r\nI think its better to keep original name after set_categories()\r\n"},{"labels":["api"],"text":"It would be nice to have a function like DataFrame.from_product with a interface similar to MultiIndex.from_product. For example, the following code:\r\n```python\r\ndf = pd.DataFrame.from_product([np.linspace(1, 5, 2), np.linspace(1, 10, 3)], columns=['a', 'b'])\r\n```\r\n\r\nconstructs the table:\r\n```\r\n     a    b\r\n0  1.0  0.0\r\n1  1.0  4.0\r\n2  1.0  8.0\r\n3  5.0  0.0\r\n4  5.0  4.0\r\n5  5.0  8.0\r\n```\r\n\r\nCurrently to generate such table one can use:\r\n```python\r\ndf = pd.DataFrame(list(itertools.product(np.linspace(1, 5, 2), np.linspace(0, 8, 3))), columns=['a', 'b'])\r\n```\r\nor rather hackish:\r\n```python\r\ndf = pd.MultiIndex.from_product([np.linspace(1, 5, 2), np.linspace(1, 10, 3)], names=['a', 'b']).to_frame()\r\ndf.reset_index(drop=True, inplace=True)\r\n```\r\n\r\nBoth are inefficient as they constructs a temporary to hold the Cartesian product before it is copied into the DataFrame.\r\n\r\n\r\n"},{"labels":["api",null,null],"text":"Currently there are some inconsistencies in behavior between the various `*_range` functions:   `date_range`,  `period_range`, `timedelta_range`, `interval_range`.\r\n\r\nNote: `bdate_range` and `cdate_range` largely use the same code as `date_range`, so I'm lumping the behavior all three together in the examples below.\r\n\r\n### End Inclusion\r\nThe `end` parameter of `interval_range` is not included in the resulting `IntervalIndex`:\r\n```python\r\nIn [2]: pd.interval_range(start=0, end=4)\r\nOut[2]:\r\nIntervalIndex([(0, 1], (1, 2], (2, 3]]\r\n              closed='right',\r\n              dtype='interval[int64]')\r\n```\r\nHowever, `end` is included in the output of the other `*_range` functions:\r\n```python\r\nIn [3]: pd.period_range(start='2017Q1', end='2017Q4', freq='Q')\r\nOut[3]: PeriodIndex(['2017Q1', '2017Q2', '2017Q3', '2017Q4'], dtype='period[Q-DEC]', freq='Q-DEC')\r\n\r\nIn [4]: pd.date_range(start='2017-01-01', end='2017-01-04')\r\nOut[4]: DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'], dtype='datetime64[ns]', freq='D')\r\n\r\nIn [5]: pd.timedelta_range(start='1 day', end='4 days')\r\nOut[5]: TimedeltaIndex(['1 days', '2 days', '3 days', '4 days'], dtype='timedelta64[ns]', freq='D')\r\n```\r\nProposal: `interval_range` should include `end` as the endpoint of the last interval in the resulting `IntervalIndex`.  \r\n\r\n\r\n### Behavior when too many parameters are passed\r\nWhen the `start`, `end`, and `periods` parameters are all passed, `interval_range` ignores the `periods` parameter:\r\n```python\r\nIn [6]: pd.interval_range(start=0, end=4, periods=6)\r\nOut[6]:\r\nIntervalIndex([(0, 1], (1, 2], (2, 3]]\r\n              closed='right',\r\n              dtype='interval[int64]')\r\n```\r\n\r\nHowever, `period_range` ignores the `end` parameter:\r\n```python\r\nIn [7]: pd.period_range(start='2017Q1', end='2017Q4', periods=6, freq='Q')\r\nOut[7]: PeriodIndex(['2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1', '2018Q2'], dtype='period[Q-DEC]', freq='Q-DEC')\r\n```\r\nBoth `date_range` and `timedelta_range` raise:\r\n```python\r\nIn [8]: pd.date_range(start='2017-01-01', end='2017-01-04', periods=6)\r\nValueError: Must specify two of start, end, or periods\r\n\r\nIn [9]: pd.timedelta_range(start='1 day', end='4 days', periods=6)\r\nValueError: Must specify two of start, end, or periods\r\n```\r\nProposal:  `interval_range` and `period_range` should raise.  Add the word \"exactly\" to the error message of all for additional clarity: \"Must specify _exactly_ two of start, end, or periods\".\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: 3.1.2\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.26\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.2.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.3\r\nbs4: 4.6.0\r\nhtml5lib: 0.999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Index gets lost when DataFrame melt method is used\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame({\"Numbers_1\":range(0,3),\r\n                   \"Numbers_2\":range(3,6),\r\n                   \"Letters\":[\"A\",\"B\",\"C\"]})\r\ndf.set_index(\"Letters\",inplace=True)\r\nprint(df)\r\n```\r\n         \r\nLetters | Numbers_1 |  Numbers_2             \r\n| ------| ------| ------| \r\nA        |        0      |    3\r\nB        |        1      |    4\r\nC        |        2      |    5\r\n```python\r\n\r\ndf_melted = df.melt()\r\nprint(df_melted)\r\n```\r\n .     |variable | value\r\n| ------| ------| ------| \r\n0 | Numbers_1   |   0\r\n1 | Numbers_1   |   1\r\n2 | Numbers_1   |   2\r\n3 | Numbers_2   |   3\r\n4 | Numbers_2   |   4\r\n5 | Numbers_2   |   5\r\n\r\n#### Problem description\r\nWhen melting a dataframe, I expected the original index to be reused. However, the original index is lost in the melt method. This is probably meant by wesm's comment (# TODO: what about the existing index?)\r\nhttps://github.com/pandas-dev/pandas/blob/133a2087d038da035a57ab90aad557a328b3d60b/pandas/core/reshape/reshape.py#L715 \r\n\r\n#### Expected Output\r\nI would expect something like\r\n```python\r\nn_row,n_col = df.shape\r\nindex_melted = list(df.index.get_values())*n_col\r\nmelt_id = list(np.arange(n_col).repeat(n_row))\r\ntemp = list(zip(*[index_melted,melt_id]))\r\n\r\nindex_melted_uniq = pd.MultiIndex.from_tuples(temp,names=[df.index.names[0], 'melt_id'])\r\nindex_numbers = list(range(df.shape[1]))*n_row\r\n\r\ndata = {'variable':df.columns.repeat(n_row),\r\n        \"value\":df.values.ravel('F')}\r\n\r\ndf_expected = pd.DataFrame(data,columns = [\"variable\",\"value\"], index=index_melted_uniq)\r\nprint(df_expected)\r\n```     \r\nLetters | melt_id | variable | value\r\n|------|------|------|------|\r\nA    |   0    |    Numbers_1   |   0\r\nB    |   0     |   Numbers_1   |   1\r\nC   |    0    |    Numbers_1   |   2\r\nA   |    1    |    Numbers_2   |   3\r\nB   |    1    |    Numbers_2   |   4\r\nC   |    1   |     Numbers_2   |   5\r\n\r\nWhere Letters and melt_id are two multiindex levels and variable and value are actual columns.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: d0f62c2816ada96a991f5a624a52c9a4f09617f7\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en\r\nLOCALE: None.None\r\n\r\npandas: 0.21.0.dev+420.gd0f62c2\r\npytest: 3.2.1\r\npip: 9.0.1\r\nsetuptools: 36.2.2.post20170724\r\nCython: 0.26\r\nnumpy: 1.13.1\r\nscipy: None\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.9999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"```python\r\ntest_frame = pd.DataFrame({\r\n    'a': [1, 2, 3],\r\n    'b': [1, 2, float('nan')],\r\n})\r\n```\r\n\r\nWhen we use the `.sum(axis=1)` method to add these up, nans are excluded or treated as zeros:\r\n\r\n```\r\n> test_frame.sum(axis=1)\r\n2.0\r\n4.0\r\n3.0\r\n```\r\n\r\nHowever, if we use `.add()` (or equivalently `+`), nans are propagated:\r\n```\r\n> test_frame.a.add(test_frame.b)\r\n> test_frame.a + test_frame.b\r\n2.0\r\n4.0\r\nNaN\r\n```\r\n\r\nI don't see a clear reason why these two methods should have different nan policys and the principle of least surprise suggests they should perform identically. \r\n\r\nFor what it's worth, I think a default policy of propagating nans would be best since the result is more likely to draw attention to the problem and evoke a reasoned response from the user, rather than assuming a solution on the user's behalf. \r\n"},{"labels":["api",null],"text":"Looks like the pandas deepcopy is no longer fully recursive.  For example if we have nested lists as shown below, the most inner lists are actually exactly the same.  \r\n\r\n```python\r\n>>> x = [[1, 2, 3, 4]]\r\n>>> y = [[2, 3, 4, 5]]\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame({'x':x, 'y':y})\r\n>>> import copy\r\n>>> df2 = copy.deepcopy(df)\r\n>>> id(df2['x'][0])\r\n4572099912\r\n>>> id(df['x'][0])\r\n4572099912\r\n```\r\n\r\nWe first noticed this issue on [skbio](https://github.com/biocore/scikit-bio/pull/1530) when the copy unittests were failing.  This hasn't been a problem with the previous pandas release.  Looking at the commits in the previous release, this [one](https://github.com/pandas-dev/pandas/commit/0b4fdf988e3125f7c55aaf6e08a2dfa7d9e2e8a0) looks suspicious.\r\n\r\nCC @gregcaporaso @ebolyen @jairideout \r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\r\ndf.to_json(orient='split')\r\n# '{\"columns\":[\"a\",\"b\",\"c\"],\"index\":[0,1],\"data\":[[1,2,3],[4,5,6]]}'\r\n```\r\n\r\n#### Problem description\r\n\r\nUseless index data increases dump size. Affects only orients `'split'` and `'table'`.\r\n\r\n#### Expected Output\r\n\r\n```python\r\n>>>  df.to_json(orient='split', index=False)\r\n'{\"columns\":[\"a\",\"b\",\"c\"],\"data\":[[1,2,3],[4,5,6]]}'\r\n``` \r\nThis resembles a bit how `.to_csv(index=False)` works.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.10.0-28-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: 2.6.4\r\npip: 9.0.1\r\nsetuptools: 36.2.5\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.16.0\r\nxarray: None\r\nIPython: 5.4.1\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.6 (dt dec pq3 ext lo64)\r\njinja2: 2.7.3\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n#Does not work, raises: \"TypeError: 'DataFrame' objects are mutable, thus they cannot be hashed \"\r\ndf.query(' ITEM in @label.keys() and \"H2O\" in @label[ITEM]')\r\n\r\n#Does not work, raises: \" PandasExprVisitor' object has no attribute 'visit_ListComp' \"\r\ndb.query(' ITEMID in [k for k,v in label.items() if \"H2O\" in v]')\r\n\r\n# Works\r\ndf[df['item'].isin([k for k,v in label.items() if 'H2O' in v])]\r\n\r\n# df header:\r\n\r\n   SUBJECT_ID  ITEM\r\n0           1     3\r\n1           2     5\r\n2           1     5\r\n3           1     2\r\n4           1     2\r\n\r\nlabel:\r\n{\r\n 1: 'Coffee',\r\n 2: 'Apple Juice',\r\n 3: 'Soda',\r\n 4: 'Tea',\r\n 5: 'Sparkling H2O'\r\n}\r\n```\r\n#### Problem description\r\n\r\nAs the code shows, I have a dataframe with item numbers whose labels are saved in another dictionary to save memory. I am trying to get all the rows that contain items who have a certain substring in their labels. Currently using query() to accomplish this will raise one of the two errors shown in the code. \r\n\r\n#### Expected Output\r\n```\r\n   SUBJECT_ID  ITEM\r\n1           2     5\r\n2           1     5\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.1\r\npytest: 3.0.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.2.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.3\r\nbs4: 4.6.0\r\nhtml5lib: 0.9999999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"xref https://github.com/pandas-dev/pandas/pull/14432\r\n\r\nWhich of these should raise `FutureWarning`s? I think the idea was to use `pd.Grouper(name)` to disambiguate? In that case `In [21]` should not raise a warning?\r\n\r\n```python\r\nIn [17]: df = pd.DataFrame({\"A\": [1] * 5 + [2] * 5, \"B\": ['a', 'b'] * 5, 'C': range(10)}, index=pd.Index(range(10), name='A'))\r\n\r\nIn [19]: _ = df.groupby('A').mean()\r\n/Users/taugspurger/.virtualenvs/pandas-dev/bin/ipython:1: FutureWarning: 'A' is both a column name and an index level.\r\nDefaulting to column but this will raise an ambiguity error in a future version\r\n  #!/Users/taugspurger/Envs/pandas-dev/bin/python3.6\r\n\r\nIn [20]: _ = df.groupby(['A']).mean()\r\n/Users/taugspurger/.virtualenvs/pandas-dev/bin/ipython:1: FutureWarning: 'A' is both a column name and an index level.\r\nDefaulting to column but this will raise an ambiguity error in a future version\r\n  #!/Users/taugspurger/Envs/pandas-dev/bin/python3.6\r\n\r\nIn [21]: _ = df.groupby(pd.Grouper('A')).mean()\r\n/Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/groupby.py:1699: FutureWarning: 'A' is both a column name and an index level.\r\nDefaulting to column but this will raise an ambiguity error in a future version\r\n  return klass(obj, by, **kwds)\r\n\r\nIn [22]: _ = df.groupby([pd.Grouper('A')]).mean()\r\n```\r\n\r\ncc @jmmease"},{"labels":["api",null,null],"text":"xref #4916, #17312\r\n\r\nexample of where it is needed\r\n```python\r\nitems = [\r\n ('a', [1,2,3]),\r\n ('b', ['j', 'k', 'l'])\r\n]\r\n\r\ndf = pd.DataFrame.from_items(items)\r\ndf\r\nOut[3]: \r\n   a  b\r\n0  1  j\r\n1  2  k\r\n2  3  l\r\n```\r\n\r\nWe possibly could deprecate this to shrink the construction api - I find `DataFrame(dict(items))` easier to think about / remember (item is a pretty generic term), only downside I see is it discards order on <3.6\r\n"},{"labels":["api",null],"text":"These shouldn't be public methods. Deprecate and make private (lead with ``_``) for now."},{"labels":["api"],"text":"#### Code Sample\r\n\r\n```python\r\n>> x = pd.Series([True,False,True])\r\n>> x.diff()\r\n0     NaN\r\n1    True\r\n2    True\r\ndtype: object\r\n>> x - x.shift()\r\n0    NaN\r\n1     -1\r\n2      1\r\ndtype: object\r\n```\r\n#### Problem description\r\n\r\nIt's counter-intuitive that the results of above are different.\r\n\r\nThe current implementation of pd.Series.diff uses algorithms.diff that subtracts 2 numpy arrays in the following way\r\n\r\n```python\r\nout_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\r\n```\r\n\r\nAs pointed [here](https://github.com/numpy/numpy/issues/9251) such behaviour is deprecated in favor to np.diff. But np.diff also treats booleans in binary operations in its own numpy way, that is different from native python (replace False with 0, replace True with 1).\r\n\r\n```python\r\n>> np.array([True, False]) - np.array([False, True])\r\n/home/deoxys/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\r\narray([ True,  True], dtype=bool)\r\n>> np.diff([False, True, False])\r\narray([ True,  True], dtype=bool)\r\n>> True - False\r\n1\r\n>> False - True\r\n-1\r\n```\r\n\r\n#### Expected Output\r\n\r\nI believe there is no correct way of subtracting booleans. But, it's definitely strange that operations like `x - x.shift()` and `x.diff()` provide different results.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\n\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.11.9-1-ARCH\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.1\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 6.0.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.6.0\r\nhtml5lib: 0.999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.7.1 (dt dec pq3 ext lo64)\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api"],"text":"See https://github.com/pandas-dev/pandas/pull/16951#issuecomment-320971946. Opening this issue to not loose track of this"},{"labels":["api",null,null],"text":"Currently an empty `Series` has a float dtype:\r\n\r\n```python\r\nIn [15]: pd.Series().dtype\r\nOut[15]: dtype('float64')\r\n```\r\n\r\nThis is consistent with NumPy, but inconsistent with the rest of pandas:\r\n\r\n```python\r\nIn [18]: pd.Index([]).dtype\r\nOut[18]: dtype('O')\r\n\r\nIn [19]: pd.DataFrame(columns=['A']).dtypes\r\nOut[19]:\r\nA    object\r\ndtype: object\r\n```\r\n\r\nI think it's best to just make all our containers treat empty as object dtype, unless explicitly told otherwise (excluding specialized containers like `Int64Index`)."},{"labels":["api",null,null,null],"text":"Two proposals:\r\n\r\n## Consolidate all inference to the `Index` constructor\r\n\r\n- Retain `Index(...)` inferring the best container for the data passed\r\n- Remove `MultiIndex(data)` returning an `Index` when data is a list of length-1 tuples (xref https://github.com/pandas-dev/pandas/pull/17236)\r\n\r\n## Passing `dtype=object` disables inference\r\n\r\n`Index(..., dtype=object)` disable all inference. So `Index([1, 2], dtype=object)` will give you an `Index` instead of `Int64Index`, and `Index([(1, 'a'), (2, 'b')], dtype=object)` an `Index` instead of `MultiIndex`, etc.\r\n\r\n(original post follows)\r\n\r\n---\r\n\r\n\r\nOr how much magic should we have in the Index constructors? Currently we infer the index type from the data, which is often convenient, but sometime difficult to reason able behavior. e.g. `hash_tuples` currently doesn't work if your tuples all happen to be length 1, since it uses a MultiIndex internally.\r\n\r\nDo we want to make our `Index` constructors more predictable? For reference, here are some examples:\r\n\r\n```python\r\n>>> import pandas as pd\r\n# 1.) Index -> MultiIndex\r\n>>> pd.Index([(1, 2), (3, 4)])\r\nMultiIndex(levels=[[1, 3], [2, 4]],\r\n           labels=[[0, 1], [0, 1]])\r\n\r\n>>> pd.Index([(1, 2), (3, 4)], tupleize_cols=False)\r\nIndex([(1, 2), (3, 4)], dtype='object')\r\n\r\n# 2.) Index -> Int64Index\r\n>>> pd.Index([1, 2, 3, 4, 5])\r\nInt64Index([1, 2, 3, 4, 5], dtype='int64')\r\n\r\n# 3.) Index -> RangeIndex\r\n>>> pd.Index(range(1, 5))\r\nRangeIndex(start=1, stop=5, step=1)\r\n\r\n# 4.) Index -> DatetimeIndex\r\n>>> pd.Index([pd.Timestamp('2017'), pd.Timestamp('2018')])\r\nDatetimeIndex(['2017-01-01', '2018-01-01'], dtype='datetime64[ns]', freq=None)\r\n\r\n# 5.) Index -> IntervalIndex\r\n>>> pd.Index([pd.Interval(3, 4), pd.Interval(4, 5)])\r\nIntervalIndex([(3, 4], (4, 5]]\r\n              closed='right',\r\n              dtype='interval[int64]')\r\n\r\n# 5.) MultiIndex -> Index\r\n>>> pd.MultiIndex.from_tuples([(1,), (2,), (3,)])\r\nInt64Index([1, 2, 3], dtype='int64')\r\n```\r\n\r\nOf these, I think the first (`Index -> MultiIndex` if you have tuples) and the last (`MultiIndex -> Index` if you're tuples are all length 1) are undesirable. The `Index -> MultiIndex` one has the `tupleize_cols` keyword to control this behavior. In https://github.com/pandas-dev/pandas/pull/17236 I add an analogous keyword to the MI constructor. The rest are probably fine, but I don't have any real reason for saying that `[1, 2, 3]` magically returning an Int64Index is ok, but `[(1, 2), (3, 4)]` returning a `MI` isn't (maybe the difference between a MI and Index is larger than the difference between an Int64Index and Index?). I believe that in either the `RangeIndex` or `IntervalIndex` someone (@shoyer?) had objections to overloading the `Index` constructor to return the specialized type.\r\n\r\nSo, what should we do about these? Leave them as is? Deprecate the type inference? My vote is for merging #17236 and leaving everything else as is. To me, it's not worth breaking API over.\r\n\r\ncc @jreback, @jorisvandenbossche, @shoyer "},{"labels":[null,"api"],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nfor k, c in df.items():\r\n    ...\r\n```\r\n\r\n#### Problem description\r\n\r\nThis works on python3 and fails on python2.  This seems like a natural thing to do to have easy duck-typing between dataframes and dicts of iterables.\r\n\r\nEither the docs should be updated to flag this as a python3 only feature or `items` should be shimmed onto DataFrames for all versions of python.   In python2 this would have different semantics than `dict.item` given DataFrame has not had `item` before you are not changing behavior and the 'iterator-not-a-list' semantics are better.\r\n\r\nIt looks like the initial shim came in via 16dbeaba9ec2cc6d87636981644a08cb9bb9b2cd .\r\n\r\nAs a side-note, `six.iteritems` works as expected on DataFrames.\r\n\r\n\r\n#### Expected Output\r\n\r\nit does not raise.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.11.9-1-ARCH\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: 3.2.0\r\npip: 9.0.1\r\nsetuptools: 36.2.2.post20170724\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.5\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"I propose adding a string formatting possibility to ``.astype`` when converting to ``str`` dtype: I think it's reasonable to expect that you can choose the string format when converting to a string dtype, as you're basically freezing a representation of your series, and just using ``.astype(str)`` for this is often too crude.\r\n\r\nThis possibility should take shape of a ``format`` parameter to ``.astype``, that can take a string and can only be used when converting to string dtype. This would lessen the reliance on ``.apply`` for converting non-strings to more complex strings and make such conversions more readable (IMO) and maybe faster (as we're avoiding ``.apply`` which is slow, though Im not too knowledgable on such optimizations). \r\n\r\nThe current procedure for converting to a complex string goes like this:\r\n\r\n```python\r\nIn [1] ser = pd.Series([-1, 1.234])\r\nIn [2] ser.apply(\"{:+.1f} $\".format)\r\n0    -1.0 $\r\n1    +1.2 $\r\ndtype: object\r\n```\r\n\r\nI propose to make this possible:\r\n\r\n```python\r\nIn [3] ser.astype(str, format=\"{:+.1f} $\")\r\n0    -1.0 $\r\n1    +1.2 $\r\ndtype: object\r\n```\r\n\r\nIf the ``dtype`` parameter is not ``str``, setting of the ``format`` parameter should raise an exception. If ``format`` is not set, the current behaviour will be used. The proposed change is therefore backward compatible.\r\n\r\n## Also to consider:\r\n\r\n### Allowing a placeholder name\r\nShould a placeholder name be available? Then you could do:\r\n```python\r\nIn [4] ser = pd.Series(pd.date_range('2017-03', periods=2, freq='M'))\r\nIn [x] ser.astype(str, format=\"Y{value.dt.year}-Q{value.dt.quarter}\")\r\n0    Y2017-Q1\r\n1    Y2017-Q2\r\ndtype: object\r\n```\r\n\r\n(Note that we above have an implicit parameter on ``.astype`` with a default value \"value\", so adding a placeholder name is transparent. Note also the above behaviour is present in ser.dt.strftime, but please look at the principle rather than the concrete example).\r\n\r\nA downside to allowing a placeholder name could be the potential for abuse (stuffing too much into the format string) and possibly losing the option to vectorize (though this is not my expertize).\r\n\r\n### Adding a ``.format`` method\r\nIt could also be considered adding a ``.str.format`` or ``.format`` method to DataFrame/Series.\r\n\r\nIf ``.format`` is added to the ``.str`` namespace it would only be usable for string dataframes/series (which I'd be quite ok with, if the ``format`` parameter is also available on ``.astype`` for other data types). \r\n\r\nAlternatively, such a method could be available directly on all DataFrames/Series. Then you'd do ``ser.format('{:+.1f}')`` rather than ``ser.astype(str, format='{:+.1f}')``. IMO though, it would be inconsistent to have such a string conversion method directly on pandas objects, but not for other types. Why have ``.format`` but not ``.to_numeric`` as a dataframes/series method?\r\n\r\nIMO therefore, ``astype(str, format=...)`` combined with a ``.str.format`` method is better than adding a new ``.format`` method for this. So:\r\n*  ``.astype(str, format=...)`` makes it very obvious that we're now changing to string datatype, and \r\n* ``.str.format(...)`` makes it clear that we're doing a string manipulation."},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\nThe following example will trigger an exception:\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime\r\n\r\ndata = []\r\nnow = datetime.datetime.now(tz=datetime.timezone.utc)\r\nfor i in range(10):\r\n    data.append(dict(date=now + datetime.timedelta(days=i), data=i))\r\n\r\ndf = pd.DataFrame(data, index=[x['date'] for x in data])\r\n\r\ndf.plot()\r\n```\r\n\r\nThis should fail with the error:\r\n\r\n```python\r\npandas/_libs/tslib.pyx in pandas._libs.tslib.dates_normalized (pandas/_libs/tslib.c:88136)()\r\n\r\nAttributeError: 'datetime.timezone' object has no attribute '_transition_info'\r\n```\r\n\r\n#### Problem description\r\n\r\nThe \"else\" branch in this code will fail on instances of datetime.timezone which don't have a `_transition_info` internal field, which is true with objects like `datetime.timezone.utc`.\r\n\r\nThe code on master looks like this:\r\n\r\n```python\r\n    else:\r\n        trans, deltas, typ = _get_dst_info(tz)\r\n\r\n        for i in range(n):\r\n            # Adjust datetime64 timestamp, recompute datetimestruct\r\n            pos = trans.searchsorted(stamps[i]) - 1\r\n            inf = tz._transition_info[pos]\r\n\r\n            pandas_datetime_to_datetimestruct(stamps[i] + deltas[pos],\r\n                                              PANDAS_FR_ns, &dts)\r\n            if (dts.hour + dts.min + dts.sec + dts.us) > 0:\r\n                return False\r\n```\r\n\r\nThe problem is the setting of `inf` and its reference to `_transition_info`, but if I'm reading this code correctly, this is dead code anyway.  With that line removed, this code should work with things like `datetime.timezone.utc` again.\r\n\r\n#### Expected Output\r\n\r\nI'd expect to be able to plot off timestamps which use datetime.timezone.utc as their timezone.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.10.0-27-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.2\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.13.0\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"From [this SO post](https://stackoverflow.com/q/45470991/2336654)\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ns = pd.Series(pd.to_datetime(['2010-08-05']))\r\nsp = s.dt.to_period('M')\r\nsp.iloc[0].end_time == sp.dt.end_time.iloc[0]\r\n```\r\n#### Problem description\r\nIn the above code, I compare two ways in which I expected to get the same exact value.  It seems that when I use `sp.dt.end_time` I get the final date of the period but with time set to zeros.  If I use `sp.iloc[0].end_time` I get what I expected, which is the final date and time set to `'23:59:59.999999999'`\r\n\r\n#### Expected Output\r\n```python\r\nTrue\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: 3.0.5\r\npip: 9.0.1\r\nsetuptools: 36.2.0\r\nCython: 0.25.2\r\nnumpy: 1.13.1\r\nscipy: 0.18.1\r\nxarray: 0.9.5\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.4.0\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.2\r\nbs4: 4.5.3\r\nhtml5lib: 0.9999999\r\nsqlalchemy: 1.1.5\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.4.0\r\n</details>\r\n"},{"labels":["api",null],"text":"I am opening this issue because I want (to try) to pursue this in GeoPandas to add a custom `GeometryBlock` (work together with Matthew Rocklin in https://github.com/geopandas/geopandas/pull/467, ultra short motivation: we want to store integers (pointers to C objects) in a column but box it to shapely python objects when the user interacts with the column (repr, accessing element, ..))\r\n\r\nI am of course free to try this :-), but I wanted to raise this because it has some consequences. With the \"allow external libraries\" in the issue title, I mean the following:\r\n\r\n- agree that this is 'OK' which means that we try to not break the Block API (to a certain extent of course, or try to change it backwards compatible)\r\n- accept some changes to pandas to make this possible where needed (as long as they are only some internal clean-ups)\r\n\r\nI don't think we plan many internal refactorings for pandas 0.x / 1.x, so on that regard the Block API should/could remain rather stable (of course for 2.0 this is a whole other issue).\r\n\r\nSo this issue can serve as general discussion for this (or if people have input or feedback) and as a reference for when changes in pandas are made for this.\r\n\r\ncc @pandas-dev/pandas-core "},{"labels":[null,"api",null],"text":"Broken off of #17117 for discussion, xref #8162, #17061, recent mailing list [thread](https://mail.python.org/pipermail/pandas-dev/2017-June/000608.html).\r\n\r\nWith a datetime-like column we can access year, hour, ... with `self.dt.foo`.  With a DatetimeIndex (PeriodIndex, ...) we access these attributes directly `self.foo` without the `.dt`.  I'd like to add a `.dt` property to the appropriate Index subclasses so that these attributes can be accessed symmetrically.  i.e. instead of:\r\n```\r\nif isinstance(obj, pd.Index):\r\n    year = obj.year\r\nelif isinstance(obj, pd.Series):\r\n    year = obj.dt.year\r\n```\r\nwe can just use `year = obj.dt.year` regardless.\r\n\r\nThe implementation is three lines in core.indexes.datetimelike.DatetimeIndexOpsMixin`:\r\n```\r\n    @property\r\n    def dt(self):\r\n        return self\r\n```\r\n\r\nThoughts?"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnp.random.seed(42)\r\n\r\nnums = np.random.choice(range(0, 999), 1000)\r\ncut = pd.cut(nums, 10)\r\n\r\ndiscretization = cut.value_counts()\r\ndiscretization.sort_index(inplace=True)\r\n\r\nintervals = list(discretization.index)\r\nmids = [i.mid for i in intervals]\r\n\r\nprint(discretization.reindex(index=mids))\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n(-0.998, 99.8]    100\r\n(99.8, 199.6]     103\r\n(199.6, 299.4]     97\r\n(299.4, 399.2]     94\r\n(399.2, 499.0]     97\r\n(499.0, 598.8]     85\r\n(598.8, 698.6]    118\r\n(698.6, 798.4]    103\r\n(798.4, 898.2]    108\r\n(898.2, 998.0]     95\r\ndtype: int64\r\n```\r\n\r\n#### Problem description\r\n\r\n`Series.reindex()` returns the original `Series` even though the index is changed.\r\n\r\n#### Expected Output\r\n\r\n`print(pd.Series(discretization.values, index=mids))` produces:\r\n\r\n```\r\n49.401     100\r\n149.700    103\r\n249.500     97\r\n349.300     94\r\n449.100     97\r\n548.900     85\r\n648.700    118\r\n748.500    103\r\n848.300    108\r\n948.100     95\r\ndtype: int64\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------------\r\ncommit: None\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.39-1-MANJARO\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: 3.1.3\r\npip: 9.0.1\r\nsetuptools: 36.2.2\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: None\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.4.0\r\n</details>\r\n"},{"labels":["api",null],"text":"I think it would be useful if the `pd.DataFrame`'s `drop` method could support a `lambda` or `function` that one can use as a filter such as below: \r\nAlso, on a side note I noticed that masking requires an extra step in the newest pandas in that it requires the mask to be converted to an array and then to a boolean. Not sure if that warrants an issue in itself but just wanted to mention that.  It is also shown below:\r\n\r\n![image](https://user-images.githubusercontent.com/9061708/28790400-bd7c1bf0-75dd-11e7-8af8-1a2231442081.png)\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [19]: index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\r\n    ...: ...                                    ('two', 'a'), ('two', 'b'\r\n    ...: )])\r\n\r\nIn [20]: s = pd.Series(np.arange(1.0, 5.0), index=index)\r\n\r\nIn [21]: s\r\nOut[21]:\r\none  a    1.0\r\n     b    2.0\r\ntwo  a    3.0\r\n     b    4.0\r\ndtype: float64\r\n\r\nIn [22]: s.unstack(0)\r\nOut[22]:\r\n   one  two\r\na  1.0  3.0\r\nb  2.0  4.0\r\n\r\nIn [24]: s.index.names= [1,0]\r\n\r\nIn [25]: s.unstack(0)\r\nOut[25]:\r\n0      a    b\r\n1\r\none  1.0  2.0\r\ntwo  3.0  4.0\r\n```\r\n#### Problem description\r\n\r\nRelated #14969\r\n\r\nWhen calling `unstack(level=1)` on a `MultiIndex` with integer names,  it is ambiguous whether to unstack by level name or the level position since both are valid level identifiers in unstack() (but will prioritize level name)  \r\n\r\nWhile possibly an uncommon case, a keyword identifier in `unstack()` may help clarify whether the user wants to unstack by the level position or level name.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 465c59f964c8d71d8bedd16fcaa00e4328177cb1\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-45-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.21.0.dev+313.g465c59f\r\npytest: 3.0.6\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.2\r\nscipy: None\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.8\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n```\r\n</details>\r\n"},{"labels":["api",null,null,null,null],"text":"My impression (discussion somewhere on gitter) is that `pd.Period` is supposed to be immutable.  That would certainly make sense for objects going into a `PeriodIndex`.\r\n\r\n```\r\nper = pd.Period('2014Q1')\r\n>>> per\r\nPeriod('2014Q1', 'Q-DEC')\r\n\r\nfreq = per.freq.copy()\r\nfreq.n *= 2\r\nper.freq = freq\r\n>>> per\r\nPeriod('2014Q1', '2Q-DEC')\r\n```\r\n\r\n`per.freq` is a `DateOffset` object*, which is defined in `tseries.offsets`.  There is a comment in `tseries.offsets.BusinessHourMixin.apply` saying \"# calculate here because offset is not immutable\".\r\n\r\nSo a couple of questions:\r\n\r\n1) Are we *sure* `pd.Period` should be immutable?  If so, is there a preferred way of imposing that?  Note that the class is defined in a .pyx file and some methods explicitly call `__new__`.\r\n2) Does the comment about offset not being immutable mean that it can't or shouldn't be?  Outside of `__init__` and `__setstate__`, the only place in `tseries.offsets` where I see any `self.foo` attributes being set is `self.daytime = False` in `BusinessHourMixin._get_business_hours_by_sec`.  grepping for \"daytime\", this attribute does not appear to be referenced anywhere else.\r\n\r\n\\* BTW, I think `DateOffset` is one of a small number of commonly-used classes for which there is not a `ABCDateOffset` equivalent.  Is that by design?  If not, I'll be happy to make one."},{"labels":["api",null],"text":"There has been some discussion in #8162 and [pandas2#17](https://github.com/pandas-dev/pandas2/issues/17) about making `Index` behave more like a regular Series or DataFrame column(s).  Is there a consensus towards moving in that direction?  In particular, I'm thinking about:\r\n\r\n- `DatetimeIndex`, `PeriodIndex`, `TimedeltaIndex` could have a property `dt` that returns `self`.  Then users can access `foo.dt` without having to check whether `foo` is a `Series` or `Index`.\r\n\r\n- If `StringAccessorMixin` is deprecated (which can be done with a tiny subset of #17042), `Index` can have `_accessors`, `_dir_deletions`, and `_dir_additions` copy/pasted from `Series`.\r\n\r\n- Less obvious so more of an implied question: `Index.map` seems to behave a lot like `Series.apply` and less like `Series.map`.  Any reason not to alias `Index.apply = Index.map`?\r\n\r\n\r\nRelated Index/MultiIndex compat, a non-multi Index could have `self.levels = [self]`, so users can iterate over levels without pre-checking."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\np = pd.Period('2000-01-01 00:00:00.123456789', freq='N')\r\nprint(p.freq)\r\nprint(p)\r\nprint(p + pd.Timedelta(789, 'ns'))\r\n```\r\n\r\nThis results in:\r\n\r\n```\r\n<Nano>\r\n2000-01-01 00:00:00.123456000\r\n2000-01-01 00:00:00.123456789\r\n```\r\n#### Problem description\r\n\r\nFrom my reading of the docs, it appears that Period objects should accept nanosecond input, and they appear to support it, above, using `Timedelta`s.\r\n\r\nThe problem is that it appears to truncate the input strings.\r\n\r\n#### Expected Output\r\n\r\n```\r\n<Nano>\r\n2000-01-01 00:00:00.123456789\r\n2000-01-01 00:00:00.123457578\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 21.0.0\r\nCython: 0.24\r\nnumpy: 1.13.1\r\nscipy: 0.18.0.dev0+46034f7\r\nxarray: None\r\nIPython: 5.0.0b4\r\nsphinx: 1.4.4\r\npatsy: 0.2.1\r\ndateutil: 2.2\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.4\r\nfeather: None\r\nmatplotlib: 2.0.0rc2\r\nopenpyxl: 1.8.6\r\nxlrd: 0.9.2\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.4.0\r\nbs4: 4.3.2\r\nhtml5lib: None\r\nsqlalchemy: 1.0.14\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: df = pd.DataFrame([[1], [2], [3.3]])\r\n\r\nIn [3]: df.groupby([1,1,1]).agg(len)\r\nOut[3]: \r\n     0\r\n1  3.0\r\n```\r\n#### Problem description\r\n\r\nThe result of ``len`` should be ``int``, regardless of the input. This is not specific to ``len``: ``lambda x : 3`` results in the same.\r\n\r\n#### Expected Output\r\n\r\nCompare to\r\n\r\n```python\r\nIn [4]: df.apply(len)\r\nOut[4]: \r\n0    3\r\ndtype: int64\r\n\r\nIn [5]: df.groupby([1,1,1]).apply(len)\r\nOut[5]: \r\n1    3\r\ndtype: int64\r\n\r\nIn [6]: df.astype(int).groupby([1,1,1]).agg(len)\r\nOut[6]: \r\n   0\r\n1  3\r\n\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 9e7666dae3b3b10d987ce154a51c78bcee6e0728\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.21.0.dev+265.g9e7666dae\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: None\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: df = pd.DataFrame([[1,2], [3,4]], index=pd.MultiIndex.from_tuples([['a', 'b'], ['c', '']]))\r\n\r\nIn [3]: df.loc['c'].shape\r\nOut[3]: (1, 2)\r\n\r\nIn [4]: df.transpose().loc[:, 'c'].shape\r\nOut[4]: (2,)\r\n```\r\n#### Problem description\r\n\r\nMaybe the \"fill an incomplete key with empty string(s)\" rule is not implemented at all for rows? (also in light of #17024 ) If this the case, then I think it should be.\r\n\r\n#### Expected Output\r\n\r\nThe same as ``Out[4]`` but reversed.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 9e7666dae3b3b10d987ce154a51c78bcee6e0728\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.21.0.dev+265.g9e7666dae\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: None\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nprint pd.__version__\r\n\r\n#different result example\r\nd = pd.DataFrame({'a':[200, 300, '', 'NaN', 10000000000000000000]})\r\n\r\n#returns dtype object\r\na = pd.to_numeric(d['a'], errors='coerce')\r\nprint a.dtype\r\n\r\n#return dtype float64\r\nb = d['a'].apply(pd.to_numeric, errors='coerce')\r\nprint b.dtype\r\n\r\n#why not float64?\r\nd = pd.DataFrame({'a':[200, 300, '', 'NaN', 30000000000000000000]})\r\n\r\n#returns dtype object\r\na = pd.to_numeric(d['a'], errors='coerce')\r\nprint a.dtype\r\n\r\n#returns OverflowError\r\nb = d['a'].apply(pd.to_numeric, errors='coerce')\r\nprint b.dtype\r\n```\r\n#### Problem description\r\n\r\nHi guys, I realized that result of to_numeric changes depending on the way you pass a Series to that function. Please see example above. When I call to_numeric with series passed as parameter, it returns \"object\", but when I apply to_numeric to that series, it returns \"float64\". Moreover, I'm a bit confused what's the correct behavior of to_numeric, why it doesn't convert looooong int-like number to float64? It throws an exception from which I can't even deduce which number (position, index) caused that exception... \r\n\r\nI'm pretty sure my issue is being discussed somewhere already, I tried to search the proper issue but rather found bits and pieces about to_numeric and convertions in general. Please feel free to put my issue in more appropriate thread. \r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: None\r\nxarray: None\r\nIPython: 5.4.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"This was suggested by @MaximilianR [here](https://github.com/pandas-dev/pandas/issues/12392#issuecomment-186308548), but @jorisvandenbossche (rightly) [proposed](https://github.com/pandas-dev/pandas/issues/12392#issuecomment-186451899) to discuss separately.\r\n\r\nThis is related to the discussion on ``.rename`` and ``.rename_axis``'s signatures (see above), but touches a different issue. Currently, these two functions are confusing not just because they do the same thing, but also because they (each) do two _different_ things (renaming axis/object name, and relabeling), and because they do not do the most obvious thing (relabeling by passing a list-like - since a list-like is assumed to refer to multiple names for a ``MultiIndex``' levels).\r\n\r\nMy proposal is the following:\r\n- add a new method ``.relabel``, which changes the _content_ of the axis, as in\r\n``` python\r\ndf.relabel(['l1', 'l2', 'l3']) # changes df.index labels - assuming df.index is flat\r\ndf.relabel([['l1a', 'l1b', 'l1c'], [...], [...]]) # changes df.index labels - assuming df.index has 3 levels\r\ndf.relabel(a_dict) # again, changes the index labels, analogously to what df.rename(a_dict) currently does\r\n```\r\n- deprecate the use of ``.rename`` and ``.rename_index`` for doing the same operation, that is when the ``index`` or ``mapper`` argument (respectively) is a callable or dict-like; keep them (or, even better, keep one and deprecate the other) for changing only index names\r\n\r\nWhether the signature of ``.relabel`` should follow the ``axis=`` or ``index=``, ``column=`` standard will depend from the outcome of the discussion about ``rename`` and ``rename_index`` (#12392)."},{"labels":["api",null,null,null],"text":"Given a timezone-aware Timestamp:\r\n\r\n    foo = pd.Timestamp('2016-10-30 00:00:00', tz=pytz.timezone('Europe/Helsinki'))\r\n\r\n(Please note that `2016-10-30` is a 25-hour day, due to a DST change. This day the hour changes from `+0300` to `+0200`)\r\n\r\nI'm trying to get the next day. If I understand correctly the DateOffset behaviour, all these lines should be equivalent:\r\n\r\n    foo + pd.tseries.frequencies.to_offset('D')\r\n    foo + pd.tseries.offsets.Day()\r\n    foo + pd.DateOffset()\r\n    foo + pd.DateOffset(1)\r\n    foo + pd.DateOffset(days=1)\r\n\r\nBut the first four return a wrong date, presumably because they're not adding a day, but 24 hours:\r\n\r\n    Timestamp('2016-10-30 23:00:00+0200', tz='Europe/Helsinki')\r\n\r\nAnd for some reason, the last one returns the correct date:\r\n\r\n    Timestamp('2016-10-31 00:00:00+0200', tz='Europe/Helsinki')\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.6.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: es_ES.UTF-8\r\nLOCALE: es_ES.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 28.8.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>"},{"labels":["api"],"text":"```\r\nIn [164]: df = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\r\n\r\nIn [165]: df.select_dtypes(include='object')\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-165-04044faa1a5a> in <module>()\r\n----> 1 df.select_dtypes(include='object')\r\n\r\n~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in select_dtypes(self, include, exclude)\r\n   2355         include, exclude = include or (), exclude or ()\r\n   2356         if not (is_list_like(include) and is_list_like(exclude)):\r\n-> 2357             raise TypeError('include and exclude must both be non-string'\r\n   2358                             ' sequences')\r\n   2359         selection = tuple(map(frozenset, (include, exclude)))\r\n\r\nTypeError: include and exclude must both be non-string sequences\r\n\r\nIn [166]: df.select_dtypes(include=['object'])\r\nOut[166]: \r\n   b\r\n0  a\r\n1  b\r\n2  c\r\n```\r\n#### Problem description\r\n\r\nOnly a convenience thing, but basically anywhere else we take list-likes, we accept a single string and I think should do the same here.\r\n\r\n`pandas 0.20.2`\r\n"},{"labels":["api",null,null],"text":"Rather often, especially when exploring the data or creating reports, one is interested both in `pd.Series.value_counts()` and `pd.Series.value_counts(normalize=True). I prepared a branch which implements a wrapper which takes care of this ([link](https://github.com/drorata/pandas/tree/dual_value_counts))\r\n\r\n- Does it make sense?\r\n- Is this implementation meeting pandas' standards?\r\n- Should testing of this simple wrapper be extended?\r\n\r\n"},{"labels":["api",null,null],"text":"In https://github.com/pandas-dev/pandas/issues/6214 it was reported that `argmax` changed (I think since Series didn't subclass `ndarray` anymore). It's sometimes useful to get the index position of the max, so it'd be nice if `.argmax` did that, leaving `idxmax` to always be label-based.\r\n\r\n```python\r\n\r\nIn [1]: import pandas as pd\r\npd.Ser\r\nIn [2]: s = pd.Series([1, 2, 3, 0], index=['a', 'b', 'c', 'd'])\r\n\r\nIn [3]: s.idxmax()\r\nOut[3]: 'c'\r\n\r\nIn [4]: s.argmax()\r\nOut[4]: 'c'\r\n```\r\n\r\nI would expect `Out[4]` to be `2`. The current workaround is `np.argmax(x)`, which is OK, but doesn't generalize to DataFrames well.\r\n\r\nWe could deprecate `.argmax` in 0.21, pointing users to `.idxmax`.  Then later we can change `.argmax` to be always be positional.\r\n\r\n---\r\n\r\nUpdate: We've deprecated `.argmax/min`. Need to change the behavior next."},{"labels":["api",null],"text":"#### Problem description\r\n\r\nI find it inconistent that ``_get_dtype`` accept a ``CategoricalDtype``, but will thow TypeError on ``Categorical``s. \r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> cats = pd.Categorical([1,2,3])\r\n>>> pd.core.dtypes.common._get_dtype(cats.dtype)\r\ncategory\r\n>>> pd.core.dtypes.common._get_dtype(cats)\r\nTypeError: data type not understood\r\n```\r\n\r\n#### Expected Output\r\n\r\n```python\r\n>>> cats = pd.Categorical([1,2,3])\r\n>>> pd.core.dtypes.common._get_dtype(cats.dtype)\r\ncategory\r\n>>> pd.core.dtypes.common._get_dtype(cats)\r\ncategory\r\n```\r\n\r\n#### Proposed solution\r\n\r\nA solution could be to check for extension type (maybe just categorical dtype, not sure) + that ``arr_or_dtype`` has a dtype attribute:\r\n\r\n```python\r\ndef _get_dtypes(arr_or_dtype):\r\n   ... \r\n    elif isinstance(arr_or_dtype, IntervalDtype):\r\n        return arr_or_dtype   # old code\r\n   elif is_extension_type(arr_or_dtype) and hasattr(arr_or_dtype, 'dtype'):   # new code\r\n        return arr_or_dtype.dtype   # new code\r\n    elif isinstance(arr_or_dtype, string_types):   # old code\r\n      ...\r\n```\r\n"},{"labels":["api",null],"text":"#### Code Sample\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: series = pd.Series(['a', '(b)'])\r\n\r\nIn [3]: series.str.replace('a', '[a]')\r\nOut[3]: \r\n0    [a]\r\n1    (b)\r\ndtype: object\r\n\r\nIn [4]: series.str.replace('(b)', '[b]')  # unexpected behavior\r\nOut[4]: \r\n0        a\r\n1    ([b])\r\ndtype: object\r\n\r\nIn [5]: series.str.replace('\\(b\\)', '[b]')  # need to escape\r\nOut[5]: \r\n0      a\r\n1    [b]\r\ndtype: object\r\n\r\nIn [6]: '(b)'.replace('(b)', '[b]')   # Python str.replace is different, uses literal string\r\nOut[6]: '[b]'\r\n\r\n```\r\n#### Problem description\r\n\r\nThe documentation for ``Series.str.replace`` says that it takes a \"string or compiled regex\" ... \"String can be a character sequence or regular expression.\" ... \"When _repl_ is a string, every _pat_ is replaced as with str.replace()\"\r\n\r\nHowever, that's not what is happening - it appears it's interpreting a string as a regex, so you need to escape characters like parentheses.\r\n\r\n#### Expected Output\r\n\r\nI would expect that for vanilla strings, it works like regular Python str.replace() - using literal strings instead of regexes.\r\n\r\nAlternatively the documentation could be updated, but I think the Python str.replace() behavior is what most users would expect.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-83-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.20.2\r\npytest: 3.1.2\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.13.0\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.3.0\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\npath1 = '/some.xls'\r\ndf1 = pd.read_excel(path1)\r\ncolumns_values_map={\r\n    'positive': {\r\n        '正面':1,\r\n        '中立': 1,\r\n        '负面':0\r\n    }\r\n}\r\n\r\ndf1.replace(columns_values_map)\r\n```\r\n\r\n#### Problem description\r\n\r\ngot error: `TypeError: Cannot compare types 'ndarray(dtype=int64)' and 'unicode'`\r\n\r\nActually `df1['positive']` only has value in (0, 1) , but I think it should not throw exception here.\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nfrom io import StringIO\r\nimport pandas as pd\r\n\r\ndata = u\"\"\" a bbb\r\n ccdd \"\"\"\r\n\r\ndf = pd.read_fwf(StringIO(data), widths=[3, 3], header=None)\r\n```\r\n\r\nThe output is\r\n```python\r\n>>> df.iloc[0,0]\r\nu'a'\r\n```\r\n\r\n#### Expected Output\r\n```python\r\nu' a '\r\n```\r\n\r\n#### Problem description\r\n\r\nApparently, leading and trailing whitespaces are removed but I want to keep them. Adding dtype options, converters does not solve the problem. Is this expected behaviour?\r\n\r\nI do not think this is intended because if we implement the same example with ``pd.read_csv()``, whitespaces are preserved.\r\n\r\n```python\r\nfrom io import StringIO\r\nimport pandas as pd\r\n\r\ndata = u\"\"\" a ,bbb\r\n cc,dd \"\"\"\r\n\r\ndf = pd.read_csv(StringIO(data), header=None)\r\n```\r\n```python\r\n>>> df.iloc[0, 0]\r\n' a '\r\n```\r\n\r\nFor consistency, behaviour should be identical.\r\n\r\nThe problem is also mentioned on Stackoverflow (https://stackoverflow.com/questions/41558138/pandas-read-fwf-removing-leading-and-trailing-whitespace). \r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.1\r\npytest: 3.0.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.2.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.3\r\nbs4: 4.6.0\r\nhtml5lib: 0.999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"I propose a more generic type checking function for pandas, based on subclass checking of dtype.types.\r\n\r\nQuite often (most of the time?) when checking dtypes, you simply want to check if a dtype's type is a subclass of some base numpy/pandas type, but today you can't take such a subclass approach.\r\n\r\nMost type checking is possible today using functions in ``pd.api.types`` using various functions, but I propose a single, more generic ``is_dtype_subclass`` function (name mirrors ``is_dtype_equal``), that checks if a ``Series.dtype.type``, a ``dtype.type`` a numpy/pandas type etc.  is a subclass of another.\r\n\r\nI find this approach very pythonic and practically useful. The downside is that it will have overlaps with other type checking functions, but as this function allows making very specific type subclass checks, I feel it would add value to pandas. Should I proceed with this (write tests, docs etc) or do you feel that this function wouldn't add value?\r\n\r\nSee code example below.\r\n\r\n```python\r\n    import numpy as np\r\n    import pandas as pd\r\n\r\n    def is_dtype_subclass(source, target):\r\n        \"\"\"\r\n        Check if a dtype.type is a subclass of another dtype.type.\r\n        \r\n        For any given args, the relevant dtype.type will be found,\r\n        so you can give this function different numpy/pandas objects to compare,\r\n        e.g. compare a Series to a numpy type. Strings will be coerced into the relevant types\r\n        per the usual rules of pandas/numpy.\r\n\r\n        Parameters\r\n        ----------\r\n        source : The first object to compare\r\n        target : The second object to compare\r\n\r\n        Returns\r\n        ----------\r\n        boolean : Whether or not ``source`` dtype.type is a subclass of ``target`` dtype.type.\r\n\r\n        Examples\r\n        --------\r\n        >>> import pandas as pd, numpy as np\r\n        >>> is_dtype_subclass(pd.Series([1, 2, 3]), np.integer)\r\n        True\r\n        >>> is_dtype_subclass(np.dtype('float32'), np.number)\r\n        True\r\n        >>> is_dtype_subclass('int64', np.integer)\r\n        True\r\n        >>> is_dtype_subclass('int64', 'int32')\r\n        False\r\n        >>> is_dtype_subclass(int, float)\r\n        False\r\n        >>> is_dtype_subclass(object, \"category\")\r\n        False\r\n        >>> is_dtype_subclass(pd.Categorical([]), \"category\")\r\n        True\r\n        >>> is_dtype_subclass(np.datetime64, \"datetime64\")\r\n        True\r\n        >>> is_dtype_subclass(pd.date_range(start='2017-06-23', periods=5), 'datetime64[ns]')\r\n        True\r\n        \"\"\"\r\n\r\n        def get_type(arg):\r\n            \"\"\"Get the dtype.type of ``arg``.\r\n            \"\"\"\r\n            if isinstance(arg, pd.DataFrame):\r\n                raise ValueError(\"Arg is a dataframe.\"\r\n                                 \" Check each column individually\")\r\n            elif isinstance(arg, (pd.Series, pd.Index,\r\n                                  pd.Categorical, pd.Interval,\r\n                                  pd.DatetimeIndex)):\r\n                return arg.dtype.type\r\n            elif isinstance(arg, np.dtype):\r\n                return arg.type\r\n            elif np.issubclass_(arg, np.generic):\r\n                return arg\r\n            dt = pd.core.dtypes.dtypes\r\n            dtypes = (dt.CategoricalDtype, dt.DatetimeTZDtype,\r\n                      dt.IntervalDtype, dt.PeriodDtype)\r\n            for dtype in dtypes:\r\n                if arg in (dtype, dtype.name, dtype.str, dtype.type):\r\n                    return dtype.type\r\n            return np.dtype(arg).type\r\n\r\n        return issubclass(get_type(source), get_type(target))\r\n```\r\n"},{"labels":["api",null,null,null],"text":"xref #16015\r\n\r\nThis _might_ be something nice to provide.  Specific usecase I was thinking of is interacting with `numba` - it understands enums in nopython mode, so it would be a way to work with Categoricals there.\r\n\r\nPartially working implementation below, doing something like [`namedtuple`](https://github.com/python/cpython/blob/3.6/Lib/collections/__init__.py#L303) with exec\r\n\r\n```python\r\ndef categorical_to_enum(categorical, name):\r\n    cats = categorical.categories\r\n\r\n    template = \"\"\"from enum import IntEnum\r\nclass {name}(IntEnum):\r\n    NA = -1\r\n{fields}\r\n\"\"\"\r\n    assert all(isinstance(x, str) and x.isidentifier()\r\n               for x in cats)\r\n\r\n    fields = '\\n'.join('    {cat}={code}'.format(cat=cat, code=i)\r\n                       for i, cat in enumerate(cats))\r\n    ns = {}\r\n    exec(template.format(name=name, fields=fields), ns)\r\n    return ns[name]\r\n\r\n# example usage\r\nIn [81]: c = pd.Categorical(['a', 'b', 'a', 'c'])\r\n\r\nIn [82]: MyEnum = categorical_to_enum(c, 'MyEnum')\r\n\r\nIn [83]: MyEnum.c\r\nOut[83]: <MyEnum.c: 2>\r\n```"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nreader = pd.read_hdf(filename, 'foo', chunksize=1000)\r\nnext(reader)\r\n```\r\n```python\r\nTypeError: 'TableIterator' object is not an iterator\r\n```\r\n#### Problem description\r\n\r\nUnless I'm missing something, I would expect something called `TableIterator` to be (itself) an iterator. Since this is an analog of `df.read_csv()`, I would expect it to behave along the same lines (which is `TextFileReader` and works with `next()`). \r\n\r\n`reader.__iter__()` is defined, so `for chunk in reader` works. It's just slightly surprising that `TextFileReader` defines `__next__()`, but TableIterator does not. \r\n\r\n#### Expected Output\r\n`next(reader)` should give the dataframe corresponding to that chunk.\r\n\r\n(yes I know Iterators and Iterables are slightly different, but in this case I would expect both to be supported)\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-79-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 36.0.1\r\nCython: None\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"```python\r\nIn [3]: df1 = pd.DataFrame(np.random.randint(0,10,(4,3)), columns=['a','b','c'])\r\n\r\nIn [4]: df1\r\nOut[4]: \r\n   a  b  c\r\n0  9  3  2\r\n1  9  0  2\r\n2  7  7  6\r\n3  3  4  2\r\n\r\nIn [5]: df2 = df1.reindex(columns=['c','a','d'])\r\n\r\nIn [6]: df2\r\nOut[6]: \r\n   c  a   d\r\n0  2  9 NaN\r\n1  2  9 NaN\r\n2  6  7 NaN\r\n3  2  3 NaN\r\n\r\nIn [7]: df2.columns\r\nOut[7]: Index([u'c', u'a', u'd'], dtype='object')\r\n\r\nIn [8]: ci = pd.MultiIndex.from_product([['x','y'],['a','b','c']])\r\n\r\nIn [10]: df3 = pd.DataFrame(np.random.randint(0,10,(4,6)), columns=ci)\r\n\r\nIn [11]: df3\r\nOut[11]: \r\n   x        y      \r\n   a  b  c  a  b  c\r\n0  3  1  5  3  8  7\r\n1  2  7  6  8  7  4\r\n2  8  3  5  7  1  1\r\n3  7  5  8  7  8  7\r\n\r\nIn [12]: df4 = df3.reindex(columns=['c','a','d'], level=1)\r\n\r\nIn [13]: df4\r\nOut[13]: \r\n   x     y   \r\n   c  a  c  a\r\n0  5  3  7  3\r\n1  6  2  4  8\r\n2  5  8  1  7\r\n3  8  7  7  7\r\n\r\nIn [14]: df4.columns\r\nOut[14]: \r\nMultiIndex(levels=[[u'x', u'y'], [u'c', u'a', u'd']],\r\n           labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\r\n```\r\n\r\nWhen passing a _nonexistent_ column name to `reindex` on a dataframe without multiindex columns, the result is:\r\n- a `NaN` column with the \"new\" column name\r\n- the `columns` attribute matches the columns in the dataframe\r\n\r\nThe same action on a multiindex dataframe produces different results:\r\n- there are no `NaN` columns (this may not be a problem)\r\n- the `columns` attribute of the resulting dataframe does not match the dateframe column names  (this appears to be a bug)\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.8\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.4.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: 2.43.0\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"```python\r\n>>> import pandas as pd\r\n>>> pd.CategoricalIndex(['a', 'b'], categories=['a', 'b']).equals(\r\n...     pd.CategoricalIndex(['a', 'b'], categories=['b', 'a']))\r\nFalse\r\n```\r\n\r\nI fixed this for regular Categoricals in https://github.com/pandas-dev/pandas/pull/16339, but that didn't affect CategoricalIndex.equals. Do we want `.equals` to ignore order when `ordered=False` on both (I think we do)?\r\n\r\nDiscovered during my CategoricalDtype refactor, which does fix it so that order is ignored when `order=False`."},{"labels":["api",null,null],"text":"The parameter `inplace=False` should be deprecated across the board in preparation for `pandas` 2, which will not support that input (we will always return a copy).  That would give people time to stop using it.\r\n\r\nThoughts?\r\n\r\n**Methods using `inplace`:**\r\n\r\nDeprecation non controvertial (a copy will be made anyway, and `inplace=True` does not add value):\r\n- (Series/DataFrame).drop\r\n- (Series/DataFrame).drop_duplicates\r\n- (Series/DataFrame).dropna\r\n- DataFrame.set_index (with `drop=False` wouldn't change the data, but that doesn't seem the main use case)\r\n- DataFrame.query\r\n- DataFrame.eval\r\n\r\nNot sure:\r\n- (Series/DataFrame).sort_values\r\n- (Series/DataFrame).sort_index\r\n\r\nShould be able to not copy memory (under discussion on what to do):\r\n- (Series/DataFrame).clip\r\n- (Series/DataFrame).where\r\n- (Series/DataFrame).fillna\r\n- (Series/DataFrame).rename_axis\r\n- (Series/DataFrame).reset_index\r\n- (Series/DataFrame).replace\r\n- (Series/DataFrame).set_axis\r\n- (Series/DataFrame).mask\r\n- (Series/DataFrame).interpolate\r\n- DataFrame.rename\r\n- Index.rename\r\n- Index.set_names\r\n- MultiIndex.set_levels\r\n- MultiIndex.set_labels\r\n- pandas.core.resample.Resampler.interpolate\r\n\r\nSpecial cases:\r\n- pandas.eval (with `inplace=False` the value is not returned but set to an argument `target`)\r\n\r\nWill be fully deprecated, so doesn't matter:\r\n- (Series/DataFrame/Panel).clip_lower\r\n- (Series/DataFrame/Panel).clip_upper\r\n- pandas.Panel.clip\r\n- pandas.Panel.drop\r\n- pandas.Panel.dropna\r\n- pandas.Panel.sort_index"},{"labels":["api",null,null],"text":"xref https://github.com/pandas-dev/pandas/commit/e8d9e79fc7d0a31e8c37c82f1e48d51cce59e9e0\r\n\r\n```\r\nIn [1]: left = pd.DataFrame({'num': np.int32([1, 2, 1, 2, 1])})\r\n  right = pd.DataFrame({'num': np.int64([1, 2, 3]), 'value': list('abc')})\r\n\r\nIn [4]: pd.merge(left, right, on='num', how='left').dtypes\r\nOut[4]: \r\nnum       int32\r\nvalue    object\r\ndtype: object\r\n\r\nIn [5]: pd.merge(left, right, on='num', how='inner').dtypes\r\nOut[5]: \r\nnum       int32\r\nvalue    object\r\ndtype: object\r\n\r\nIn [6]: pd.merge(left, right, on='num', how='outer').dtypes\r\nOut[6]: \r\nnum       int64\r\nvalue    object\r\ndtype: object\r\n\r\nIn [7]: pd.merge(left, right, on='num', how='right').dtypes\r\nOut[7]: \r\nnum       int64\r\nvalue    object\r\ndtype: object\r\n```\r\n\r\nI guess this is a bit inconsistent as with mixed types we should use the common one. This is an explicit decision though, we are *choosing* to cast to the left/right as appropriate (it is a safe cast though)."},{"labels":["api",null,null,null],"text":"xref #14145  (reviving this PR and superceding)\r\nxref #15533\r\n \r\nSo when we fill (either via fillna, where, or using setitem) we are inconsistent in handling what we do when we have a non-compat type (note have not exhaustively tested other methods, I suspect these pretty much coerce to ``object``, except for ``.where``). The classic example is trying to fill with a tz-aware in tz-naive.\r\n\r\nFor current pandas (ex-pandas2), we generally upcast to ``object``. This is the case for setitem and fillna, but NOT for ``.where``, where we raise. We DO have a parameter to control this ``raise_on_error=True``, and it works for ``.where`` (though not for ``.fillna``).\r\n\r\n```\r\nIn [1]: s = Series([Timestamp('20130101'), pd.NaT])\r\n\r\nIn [2]: s\r\nOut[2]: \r\n0   2013-01-01\r\n1          NaT\r\ndtype: datetime64[ns]\r\n\r\n# this is wrong\r\nIn [3]: s.fillna(Timestamp('20130101', tz='US/Eastern'))\r\nOut[3]: \r\n0   2012-12-31 19:00:00-05:00\r\n1   2013-01-01 00:00:00-05:00\r\ndtype: datetime64[ns, US/Eastern]\r\n\r\n# current behavior\r\nIn [4]: s.fillna('foo')\r\nOut[4]: \r\n0    2013-01-01 00:00:00\r\n1                    foo\r\ndtype: object\r\n\r\nIn [5]: s[1] = 'bar'\r\n\r\nIn [6]: s\r\nOut[6]: \r\n0    2013-01-01 00:00:00\r\n1                    bar\r\ndtype: object\r\n\r\nIn [8]: s = Series([Timestamp('20130101'), pd.NaT])\r\n\r\n# this is inconsistent with fillna\r\nIn [9]: s.where([True, False], Timestamp('20130101',tz='US/Eastern'), raise_on_error=False)\r\nOut[9]: \r\n0          2013-01-01 00:00:00\r\n1    2013-01-01 00:00:00-05:00\r\ndtype: object\r\n\r\nIn [10]: s.where([True, False], Timestamp('20130101',tz='US/Eastern'), raise_on_error=True)\r\nTypeError: Could not operate [Timestamp('2013-01-01 00:00:00-0500', tz='US/Eastern')] with block values [cannot coerce a Timestamp with a tz on a naive Block]\r\n```\r\n\r\nSo I think its reasonable to make these *all* raise, though that would be a pretty big API change. We are going to do this for pandas2 anyhow. Would be beneficial to not be so strict like this.\r\n\r\nAlternatively we can fix ``.where`` to by default upcast to ``object``.\r\n\r\nthoughts?\r\n\r\ncc @wesm @shoyer @TomAugspurger @jorisvandenbossche @sinhrks @cpcloud \r\n@ResidentMario "},{"labels":["api",null,null],"text":"Sorry in advance if this is already discussed/reported - I searched in the archive, but didn't know exactly what to search.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: tt = pd.DataFrame([[1, 2, 'v1', 'v2'], [3, 4, 'v3','v4']],\r\n   ...:                   columns=['idx1', 'idx2', 2, 6]).set_index(['idx1', 'idx2'])\r\n\r\nIn [3]: tt\r\nOut[3]: \r\n            2   6\r\nidx1 idx2        \r\n1    2     v1  v2\r\n3    4     v3  v4\r\n\r\nIn [4]: tt.loc[1,2]\r\nOut[4]: \r\n2    v1\r\n6    v2\r\nName: (1, 2), dtype: object\r\n\r\nIn [5]: tt.loc[:1,2]\r\nOut[5]: \r\nidx1  idx2\r\n1     2       v1\r\nName: 2, dtype: object\r\n\r\nIn [6]: tt.loc[:,2]\r\nOut[6]: \r\nidx1  idx2\r\n1     2       v1\r\n3     4       v3\r\nName: 2, dtype: object\r\n\r\n```\r\n#### Problem description\r\n\r\n``.loc[l1, l2]`` called on a ``MultiIndex``ed ``DataFrame`` is ambiguous: ``l2`` could refer to the second level of the ``index``, or to the ``columns``. Apparently, the decision has been taken to follow the first interpretation, and it is fine. But then, the same must happen when ``l1`` and ``l2`` are slices.\r\n\r\nI can understand that ``In [6]`` might \"look different\" from ``In [4]``: but ``In [4]`` and ``In [5]`` should really give the same result (and hence ``In [6]`` too).\r\n\r\n#### Expected Output\r\n\r\n``Out [4]`` in all three cases (or ``Out [6]`` if we prefer to favour the second interpretation - which however would probably be more disruptive).\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.7.0-1-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.utf8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.20.1\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: None\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: 0.9.2\r\nIPython: 5.1.0.dev\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndfMain = pd.DataFrame({\r\n    'a': [0, 1, np.NAN, 3, 4],\r\n    'b': [np.NaN, np.NaN, np.NaN, 3, 4],\r\n    'c': [0 , 1, 2, 3, np.NaN]})\r\n\r\nfor col in dfMain:\r\n    start = dfMain[col].first_valid_index()\r\n    end = dfMain[col].last_valid_index()\r\n    dfMain.loc[start:end, col] = dfMain.loc[start:end, col].interpolate()\r\n\r\nprint(dfMain)\r\n```\r\n\r\n#### Problem description\r\nIt would be very nice to have a limit_direction='inside' that would make interpolate only fill values that are surrounded (both in front and behind) with valid values.\r\n\r\nThis would allow an interpolate to only fill missing values in a series and **not extend** the series beyond its original limits.  The key here is that it is sometimes important to maintain the original range of a series, but still fill in the gaps.\r\n\r\nThe example shows a simple DataFrame with an 'inside' interpolation.\r\n\r\n#### Expected Output\r\n\r\n```python\r\n     a    b    c\r\n0  0.0  NaN  0.0\r\n1  1.0  NaN  1.0\r\n2  2.0  NaN  2.0\r\n3  3.0  3.0  3.0\r\n4  4.0  4.0  NaN\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-75-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 34.4.1\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.2\r\nbs4: 4.5.3\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.1.5\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: 2.45.0\r\npandas_datareader: 0.2.1\r\nNone\r\n\r\n\r\n</details>\r\n"},{"labels":["api"],"text":"PR #15998 removed the `pandas.types` module which AFAICT wasn't deprecated.\r\n\r\nThis breaks 3rd party libraries such as `dask` without giving them any warning or time to adjust to the new api.\r\n\r\nIMHO a shim should be put in place with a deprecation warning to give 3rd party libraries a chance to adjust to the new api"},{"labels":["api"],"text":"I frequently use the `.to_dict()` method and was wondering how difficult it would be to implement a `dict_obj` argument to specify what type of dictionary will be used.  For example, if one was interested in preserving the order: \r\n\r\n```\r\nfrom collections import OrderedDict\r\n\r\npd.Series(list(\"abcd\")).to_dict(dict_obj=OrderedDict)\r\n```"},{"labels":["api",null,null],"text":"```\r\nIn [1]: pd.api.types.infer_dtype([2*63, -1])\r\nOut[1]: 'integer'\r\n\r\nIn [3]: pd.api.types.infer_dtype([2*63])\r\nOut[3]: 'integer'\r\n```\r\n[1] should be 'mixed' (inference which means object)\r\n[2] should be 'uinteger'\r\n\r\nnote this will probably break some things in other parts of the suite as we sometimes infer to figure out the dtype and act on it, so 'uinteger' would then be unhandled.\r\n\r\ne.g. in Index creation and https://github.com/pandas-dev/pandas/pull/16108/files\r\nxref https://github.com/pandas-dev/pandas/pull/16295/files"},{"labels":["api",null,null,null],"text":"\r\n```python\r\nSeries(['a', 'b', 'c']).str.cat(['A', np.NaN, 'C'], sep=',')\r\n\r\n0    a,A\r\n1    NaN\r\n2    c,C\r\ndtype: object\r\n\r\n```\r\n#### Problem description\r\n\r\nI was expecting NaN to be skipped and just get \"b\" in the second line. \r\n\r\nWhen using na_rep it does something like this.\r\n\r\n```python\r\nSeries(['a', 'b', 'c']).str.cat(['A', np.NaN, 'C'], sep=',', na_rep='X')\r\n\r\n0    a,A\r\n1    b,X\r\n2    c,C\r\ndtype: object\r\n```\r\n\r\nI feel this behavior is inconsistent. \r\n\r\nI can see that if you are expecting to get output that could be used like a record/csv current behavior would help. Perhaps this could work as a parameter.\r\n\r\n#### Expected Output\r\n\r\n```python\r\n0    a,A\r\n1    b\r\n2    c,C\r\ndtype: object\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-59-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 8.1.2\r\nsetuptools: 28.2.0\r\nCython: None\r\nnumpy: 1.11.1\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.2.0\r\nsphinx: 1.3.1\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.1\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999999999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\r\njinja2: 2.9.6\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api"],"text":"Suppose I have two DataFrames:\r\n\r\n```python\r\nleft = pd.DataFrame({'key': list(\"abcd\"), 'value': [1, 2, 3, 4]})\r\nright = pd.DataFrame({'key': list(\"aceb\"), 'value': [5, 6, 7, 8]})\r\n```\r\n\r\nColumns with with the same name are given suffixes in a `merge()`:\r\n\r\n```python\r\nIn [8]: pd.merge(left, right, on='key')\r\nOut[8]:\r\n  key  value_x  value_y\r\n0   a        1        5\r\n1   b        2        8\r\n2   c        3        6\r\n```\r\n\r\nI would like to invoke a function instead:\r\n\r\n```python\r\nIn [9]: pd.merge(left, right, on='key', combine=lambda x, y: x + y)\r\nOut[9]:\r\n  key  value\r\n0   a      6\r\n1   b     10\r\n2   c      9\r\n```\r\n\r\nIn a left/right/outer join, missing keys imply no function call (just take the single value):\r\n\r\n```python\r\nIn [10]: pd.merge(left, right, on='key', how='outer', combine=lambda x, y: x + y)\r\nOut[10]:\r\n  key  value\r\n0   a      6\r\n1   b     10\r\n2   c      9\r\n3   d      4\r\n4   e      7\r\n```\r\n\r\nOnly one of `combine` or `suffixes` parameter can be specified. Of course, the user can simply request an *override* by taking only the right's values if both keys are present:\r\n\r\n```python\r\nIn [11]: pd.merge(left, right, on='key', how='left', combine=lambda x, y: y)\r\nOut[11]:\r\n  key  value\r\n0   a      5\r\n1   b      8\r\n2   c      6\r\n3   d      4\r\n```"},{"labels":["api"],"text":"Currently, the public `pandas.api.types` modules holds the following functions:\r\n\r\n```\r\nIn [11]: [f for f in dir(pd.api.types) if not f.startswith('_')]\r\nOut[11]: \r\n['infer_dtype',\r\n 'is_any_int_dtype',\r\n 'is_bool',\r\n 'is_bool_dtype',\r\n 'is_categorical',\r\n 'is_categorical_dtype',\r\n 'is_complex',\r\n 'is_complex_dtype',\r\n 'is_datetime64_any_dtype',\r\n 'is_datetime64_dtype',\r\n 'is_datetime64_ns_dtype',\r\n 'is_datetime64tz_dtype',\r\n 'is_datetimetz',\r\n 'is_dict_like',\r\n 'is_dtype_equal',\r\n 'is_extension_type',\r\n 'is_file_like',\r\n 'is_float',\r\n 'is_float_dtype',\r\n 'is_floating_dtype',\r\n 'is_hashable',\r\n 'is_int64_dtype',\r\n 'is_integer',\r\n 'is_integer_dtype',\r\n 'is_interval',\r\n 'is_interval_dtype',\r\n 'is_iterator',\r\n 'is_list_like',\r\n 'is_named_tuple',\r\n 'is_number',\r\n 'is_numeric_dtype',\r\n 'is_object_dtype',\r\n 'is_period',\r\n 'is_period_dtype',\r\n 'is_re',\r\n 'is_re_compilable',\r\n 'is_scalar',\r\n 'is_sequence',\r\n 'is_signed_integer_dtype',\r\n 'is_sparse',\r\n 'is_string_dtype',\r\n 'is_timedelta64_dtype',\r\n 'is_timedelta64_ns_dtype',\r\n 'is_unsigned_integer_dtype',\r\n 'pandas_dtype',\r\n 'union_categoricals']\r\n```\r\n\r\nTwo questions I would like to discuss a bit:\r\n\r\n- **Do we need to expose all of them?**\r\n  Putting the functions here, means keeping them stable. So we could also limit it to the more pandas specific ones. For example, `is_re`, `is_dict_like`, `is_iterator`, etc are general utility functions not actually related to pandas specifics. Of course it can be handy for other projects that they can use them instead of implementing it themselves, but it increases the number of functions we have to keep stable.\r\n\r\n\r\n~~- **Do we need to expose more of the pandas extension types API?** (xref #16099)~~\r\n\r\n  From comment of @wesm (https://github.com/pandas-dev/pandas/pull/15541#issuecomment-286008496):\r\n\r\n  > I've been running into these internal API / external API issues lately (e.g. needing to use `DatetimeTZDtype` in an external library), so it might be worth documenting the pandas < 0.19 way to get some of these APIs\r\n\r\n  It is how the dtypes will be implemented in pandas 1.x, so shouldn't we just expose this officially and put it in `types`? `pyarrow` uses the datetime tz one, and in https://github.com/pandas-dev/pandas/pull/16015 we will probably also like to add `CategoricalDtype` \r\n  Not sure if the others (interval, period) are also needed.\r\n  "},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\nI was surprised to find that `Categorical` accepts scalars for `categories`.\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\nIn [2]: pd.Categorical([1, 2], categories=2)\r\nOut[2]:\r\n[NaN, 2.0]\r\nCategories (1, int64): [2]\r\n```\r\n\r\nAFAICT, this isn't documented, though we explicitly test it [here](https://github.com/pandas-dev/pandas/blob/8daf9a7e45344dc5a247410e037dccb41b97a3db/pandas/tests/test_categorical.py#L252). Am I missing a use-case where this is desirable, or should we just depreciate it and simplify the constructor?\r\n\r\nAnyone want to guess what the categories are on `pd.Categorical(['a', 'b'], categories='abcd')` :)\r\n\r\ncc @janschulz "},{"labels":["api"],"text":"It would be great to be able to exclude/slice columns like you can with np arrays.\r\n\r\n```\r\ndf = pd.DataFrame(np.random.rand(4,4), columns = list('abcd'))\r\ndf[-('b')] #neat!\r\n```\r\ninstead of\r\n```\r\ndf.drop('b', axis=1)\r\ndf.ix[:, df.columns != 'b']\r\n```\r\netc\r\n\r\nsee \r\nhttp://stackoverflow.com/questions/29763620/how-to-select-all-columns-except-one-column-in-pandas-using-ix\r\nhttp://stackoverflow.com/questions/14940743/selecting-excluding-sets-of-columns-in-pandas\r\n"},{"labels":["api",null],"text":"`pandas.pivot_table` already provides the parameter `margins_name ` . `pandas.crosstab` currently does not."},{"labels":["api",null,null,null],"text":"Working on https://github.com/dask/dask/issues/2190 (df.rolling('5s') for dask), and I think these should all be equivalent.\r\n\r\n```python\r\nIn [26]: import pandas as pd\r\nIn [27]: import numpy as np\r\nIn [31]: from pandas.tseries.frequencies import to_offset\r\nIn [28]: s = pd.Series(range(10), index=pd.date_range('2017', freq='s', periods=10))\r\n```\r\n\r\n```python\r\n>>> s.rolling('2s')  # Case 1: correct\r\n>>> s.rolling(window=2000000000, min_periods=1, win_type='freq')  # Case 2\r\n>>> s.rolling(window=to_offset('2s'), min_periods=1, win_type='freq')  # Case 3\r\n>>> s.rolling(window=pd.Timedelta('2s'), min_periods=1, win_type='freq')  # Same as 3\r\n```\r\n\r\nI don't *think* there are any parsing ambiguities.\r\n\r\nCurrently we have\r\n\r\n```python\r\n# Case 1\r\nIn [33]: s.rolling('2s')\r\nOut[33]: Rolling [window=2000000000,min_periods=1,center=False,win_type=freq,axis=0]\r\n```\r\n\r\n```python\r\n# Case 2\r\nIn [35]: s.rolling(window=2000000000, min_periods=1, win_type='freq')\r\n```\r\n```pytb\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-a24a29dff0ab> in <module>()\r\n----> 1 s.rolling(window=2000000000, min_periods=1, win_type='freq')\r\n\r\n/Users/taugspurger/.virtualenvs/dask-dev/lib/python3.6/site-packages/pandas/core/generic.py in rolling(self, window, min_periods, freq, center, win_type, on, axis)\r\n   5502                                    min_periods=min_periods, freq=freq,\r\n   5503                                    center=center, win_type=win_type,\r\n-> 5504                                    on=on, axis=axis)\r\n   5505\r\n   5506         cls.rolling = rolling\r\n\r\n/Users/taugspurger/.virtualenvs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in rolling(obj, win_type, **kwds)\r\n   1795\r\n   1796     if win_type is not None:\r\n-> 1797         return Window(obj, win_type=win_type, **kwds)\r\n   1798\r\n   1799     return Rolling(obj, **kwds)\r\n\r\n/Users/taugspurger/.virtualenvs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in __init__(self, obj, window, min_periods, freq, center, win_type, axis, on, **kwargs)\r\n     76         self.win_type = win_type\r\n     77         self.axis = obj._get_axis_number(axis) if axis is not None else None\r\n---> 78         self.validate()\r\n     79\r\n     80     @property\r\n\r\n/Users/taugspurger/.virtualenvs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in validate(self)\r\n    505                 raise ValueError('Invalid win_type {0}'.format(self.win_type))\r\n    506             if getattr(sig, self.win_type, None) is None:\r\n--> 507                 raise ValueError('Invalid win_type {0}'.format(self.win_type))\r\n    508         else:\r\n    509             raise ValueError('Invalid window {0}'.format(window))\r\n\r\nValueError: Invalid win_type freq\r\n```\r\n\r\n```python\r\n# Case 3\r\nIn [36]: s.rolling(window=to_offset('2s'), min_periods=1, win_type='freq')\r\n```\r\n```pytb\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-36-93e14a8d05f6> in <module>()\r\n----> 1 s.rolling(window=to_offset('2s'), min_periods=1, win_type='freq')\r\n\r\n/Users/taugspurger/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/generic.py in rolling(self, window, min_periods, freq, center, win_type, on, axis)\r\n   5502                                    min_periods=min_periods, freq=freq,\r\n   5503                                    center=center, win_type=win_type,\r\n-> 5504                                    on=on, axis=axis)\r\n   5505\r\n   5506         cls.rolling = rolling\r\n\r\n/Users/taugspurger/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in rolling(obj, win_type, **kwds)\r\n   1795\r\n   1796     if win_type is not None:\r\n-> 1797         return Window(obj, win_type=win_type, **kwds)\r\n   1798\r\n   1799     return Rolling(obj, **kwds)\r\n\r\n/Users/taugspurger/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in __init__(self, obj, window, min_periods, freq, center, win_type, axis, on, **kwargs)\r\n     76         self.win_type = win_type\r\n     77         self.axis = obj._get_axis_number(axis) if axis is not None else None\r\n---> 78         self.validate()\r\n     79\r\n     80     @property\r\n\r\n/Users/taugspurger/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in validate(self)\r\n    507                 raise ValueError('Invalid win_type {0}'.format(self.win_type))\r\n    508         else:\r\n--> 509             raise ValueError('Invalid window {0}'.format(window))\r\n    510\r\n    511     def _prep_window(self, **kwargs):\r\n\r\nValueError: Invalid window <2 * Seconds>\r\n```"},{"labels":["api",null,null,null],"text":"might be useful to expose this as a top-level groupby method for intra-group sorting. Anyone have a good usecase for this? We could make this *much* faster that the current impl (not that this is an issue, though potentially it could be).\r\n```\r\nIn [10]: df = DataFrame({'A': [1, 1, 2, 2, 2], 'B': [2, 1, 3, 3, 1], 'C':[1, 1, 2, 1, 1]})\r\n\r\nIn [11]: df\r\nOut[11]: \r\n   A  B  C\r\n0  1  2  1\r\n1  1  1  1\r\n2  2  3  2\r\n3  2  3  1\r\n4  2  1  1\r\n\r\nIn [12]: df.groupby('A').apply(lambda x: x.sort_values(['B', 'C']))\r\nOut[12]: \r\n     A  B  C\r\nA           \r\n1 1  1  1  1\r\n  0  1  2  1\r\n2 4  2  1  1\r\n  3  2  3  1\r\n  2  2  3  2\r\n\r\n```"},{"labels":["api",null],"text":"xref #15888 \r\nxref #15751 \r\n\r\nWe generally *don't* actually keep these around very much (instead we use a numpy boolean array), but would be nice for compat. (e.g. #15589; were we could return an actual Index rather than a boolean array).\r\n\r\n```\r\nIn [31]: Index([True, False])\r\nOut[31]: Index([True, False], dtype='object')\r\n```\r\n"},{"labels":["api",null],"text":"this may seem counter intutive, but this would allow us to store nulls as well (efficiently, rather than as ``object`` dtype), or casting to floats.\r\n\r\nI am sure if this would really be possible w/o some API breaks, so will have a look."},{"labels":["api",null,null],"text":"https://github.com/pandas-dev/pandas/pull/15838#discussion_r109296615\r\n\r\nwe have fairly uniform IO routines of the form\r\n\r\n``.to_format(path, df, **kwargs)`` (takes DataFrame)\r\nand \r\n``pd.read_format(path, **kwargs)`` (returns DataFrame)\r\n\r\nso should document various aspects of this:\r\n\r\n- contract on input path strings\r\n- file-like objects  & ``is_file_like`` (https://github.com/pandas-dev/pandas/pull/15894)\r\n- do we do any encoding / compression (only on csv/json ATM), compression\r\n- various guarantees on what we are sending in (e.g. no Index, string columns which are non-duplicated), no non-string objects (see feather/parquet impl).\r\n- make these more pluggable\r\n- perhaps allow a specification for block access / chunking.\r\n- additional args to accept/use: ``mode`` (for writing)"},{"labels":["api",null,null,null,null],"text":"xref #15828\r\n\r\nCurrently, if ordinals are passed to `to_datetime` without a unit, a `ns` resolution is assumed.  Given relative rarity of ns ordinals in the wild, I've been tripped up by this, e.g., below.  I wonder if it would be better to raise in the case of no unit, and force ordinal parsing to always be explicit?\r\n\r\n```python\r\nordinals = pd.Series([1483228800000, 1483315200000, 1483401600000])\r\n\r\npd.to_datetime(ordinals)  # wrong\r\nOut[49]: \r\n0   1970-01-01 00:24:43.228800\r\n1   1970-01-01 00:24:43.315200\r\n2   1970-01-01 00:24:43.401600\r\ndtype: datetime64[ns]\r\n\r\npd.to_datetime(ordinals, unit='ms')\r\nOut[51]: \r\n0   2017-01-01\r\n1   2017-01-02\r\n2   2017-01-03\r\ndtype: datetime64[ns]\r\n```"},{"labels":["api",null,null],"text":"```\r\nIn [4]: df = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,3,4]})\r\n\r\nIn [5]: df.groupby('A').transform('max')\r\nOut[5]: \r\n   B\r\n0  2\r\n1  2\r\n2  4\r\n3  4\r\n```\r\n\r\nMight be nice to have a deferred ``.transform()`` object (notice it doesn't take any args).\r\n```\r\nIn [7]: df.groupby('A').transform().max()\r\n```\r\n\r\nPretty sure this could be back-compat."},{"labels":["api",null,null],"text":"Is there a reason why `read_csv` has a `usecols` and `skiprows` as arguments, but not `skipcols` and `userows`? Is this to avoid parameter checks or something more fundamental than that?\r\n\r\nIt would be nice to have all four options to avoid clunky inversions of the type `usecols = columns.remove(unwanted_col)`."},{"labels":["api",null],"text":"From discussion in https://github.com/pandas-dev/pandas/pull/15589, the handling of NaT is different for different datetime fields.\r\n\r\nOn a scalar value, `is_leap_year` returns False, the others are not defined:\r\n\r\n```\r\nIn [1]: pd.Timestamp('NaT').is_month_start\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-654cc4e613aa> in <module>()\r\n----> 1 pd.Timestamp('NaT').is_month_start\r\n\r\nAttributeError: 'NaTType' object has no attribute 'is_month_start'\r\n\r\nIn [2]: pd.Timestamp('NaT').is_leap_year\r\nOut[2]: False\r\n```\r\n\r\nOn an index (or series `.dt` accessor), `is_leap_year` returns False, the others propagate NaN (with the consequence it is no longer a boolean array/series):\r\n\r\n```\r\nIn [3]: pd.DatetimeIndex(['2012-01-01', 'NaT']).is_month_start\r\nOut[3]: array([  1.,  nan])\r\n\r\nIn [4]: pd.DatetimeIndex(['2012-01-01', 'NaT']).is_leap_year\r\nOut[4]: array([ True, False], dtype=bool)\r\n```\r\n\r\nSo when `is_leap_year` was introduced in #13739, this was done on purpose, citing @sinhrks \"pd.NaT.is_leap_year results in False, as I think users want bool array.\".\r\n\r\nThis seems indeed the more logical thing to return (certainly for arrays/series), so you can eg use it for indexing. For scalars it is a bit less clear what is the preferable option (return False or NaN), but probably best to be consistent."},{"labels":["api",null,null],"text":"From #15755 (e.g. [this comment](https://github.com/pandas-dev/pandas/pull/15756#issuecomment-288108170)).\r\n\r\nCurrent status:\r\n- there is a check on ``nrows``, which ensure floats are converted to int with no loss of precision, but not that the resulting/passed value is non-negative\r\n- there is no check on ``chunksize``, so that a ``pd.concat`` based on ``chunksize=0`` will enter an infinite loop\r\n\r\nI guess we want to ensure ``chunksize > 0`` and ``nrows >= 0``."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.read_csv('/tmp/boh.csv', nrows=10, chunksize=2)\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-2-c652c48282ad> in <module>()\r\n----> 1 pd.read_csv('/tmp/boh.csv', nrows=10, chunksize=2)\r\n\r\n/home/nobackup/repo/pandas/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\r\n    651                     skip_blank_lines=skip_blank_lines)\r\n    652 \r\n--> 653         return _read(filepath_or_buffer, kwds)\r\n    654 \r\n    655     parser_f.__name__ = name\r\n\r\n/home/nobackup/repo/pandas/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)\r\n    391 \r\n    392     if (nrows is not None) and (chunksize is not None):\r\n--> 393         raise NotImplementedError(\"'nrows' and 'chunksize' cannot be used\"\r\n    394                                   \" together yet.\")\r\n    395     elif nrows is not None:\r\n\r\nNotImplementedError: 'nrows' and 'chunksize' cannot be used together yet.\r\n```\r\n\r\n(as introduced by #6774 )\r\n\r\n#### Problem description\r\n\r\nSupporting them together is not complicated - PR on its way."},{"labels":["api",null],"text":"```python\r\nIn [2]: pd.Series([1, 2, 3]).loc[[2,3]]\r\nOut[2]: \r\n2    3.0\r\n3    NaN\r\ndtype: float64\r\n\r\nIn [3]: pd.Series([1, 2, 3]).loc[[3]]\r\n[...]\r\nKeyError: 'None of [[3]] are in the [index]'\r\n```\r\n#### Problem description\r\n\r\nAlthough coherent (except for some unfortunate side-effects - some of them below) with the [docs](http://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-label) where they say \"At least 1 of the labels for which you ask, must be in the index or a ``KeyError`` will be raised!\", the current behavior is - I claim - a terrible choice for both developers and users.\r\n\r\nThere are (at least) three ways to behave with missing labels:\r\n1. you raise an error if requested at least one missing label\r\n2. you raise an error if requested _only_ missing labels\r\n2a ... while  if at least one label is present, missing labels become ``NaN`` __(current)__\r\n2b. ... while if at least one label is present, missing labels are silently dropped\r\n3. you never raise an error for missing labels\r\n3a ... and they become ``NaN``\r\n3b. ... and they are silently dropped\r\n\r\n#### For developers\r\n\r\nOptions 1. and 3. are both much easier to implement, because in both cases you can reduce the question \"am I going to get an error?\" in smaller pieces - e.g. when indexing a ``MultiIndex``, you will get an error if you get an error on any of its levels. Option 2. is instead more complicated (and computationally expensive), because you need to first aggregate in some way across levels/axes, and only then can you decide whether to raise an error or not. Several incoherences came out as a consequence of this choice, some of them still unsolved, such as #15452, [this](https://github.com/pandas-dev/pandas/pull/15615#issuecomment-287826997), the fact that ``pd.Series(range(2)).loc[[]]`` does not raise, and the fact that ``pd.DataFrame.ix[[missing_label]]`` doesn't either.\r\n\r\n#### Other consequences of 2.\r\n\r\nAdditionally, it was decided that the behavior with missing labels would be to introduce ``NaN``s (rather than to drop them), and I think this was also not a good choice (and indeed partial indexing ``MultiIndex``es does not behave this way - it couldn't). I think it is also undocumented.\r\n\r\nAnd finally, since the above wouldn't always tell you what to do when there are missing labels in a ``MultiIndex``, it was decided that ``.loc`` would rather behave as ``.reindex`` when there are missing _and incomplete_ labels, which is totally unexpected and, I think, undocumented.\r\n\r\nNotice that these further issues (and more in general, the question \"what to do when some labels are missing and you are not raising an error\") would partially still hold with 3, but could be dealt with, I think, more elegantly.\r\n\r\n#### For users\r\n\r\nI think the current behavior is annoying to users not just because of those \"__Other consequences__\", but also because it is more complicated to describe in terms of set operation on labels/indices. For instance, with options 1. and 3.\r\n\r\n``pd.concat([chunk.loc[something] for chunk in chunks])``\r\n\r\nand\r\n\r\n``pd.concat(chunks).loc[something]``\r\n\r\nboth return the same result (or raise). Instead with 2. it actually depends on how missing labels are distributed across chunks.\r\n\r\n#### (Why this?)\r\n\r\nIt is worth understanding why 2. was picked in the first place, and I think the answer is \"to be coherent with intervals\". But I think it's not worth the damage - after all, an iterable and an interval are different objects. And moreover, introducing ``NaN``s for missing labels is anyway incoherent with intervals.\r\n\r\n#### Backward incompatibility\r\n\r\nOption 1. is, I think, the best, because it is also coherent with numpy's behavior with out-of-bounds indices (e.g. ``np.array([[1,2], [3,4]])[0,[1,3]]`` raises an ``IndexError``).\r\n\r\nBut while clearly both 1. and 3. could break some existing code, 3. would be better from this point of view, in the sense that it would break only code _assuming_ that an error is raised. Although one might even claim that 1., by breaking code which looks for missing labels, can help discover bugs in user code (not a great argument, I know).\r\n\r\nSo overall I am not sure about what we should pick between 1. and 3. But I really think we should leave 2., and that the later it is done, the worse. @jreback , @jorisvandenbossche if you want to tell me your thoughts about this, I can elaborate on what we could do with the \"__Other consequences__\" in the desired option.\r\n\r\nThen if you approve the change, I'm willing to help in implementing it."},{"labels":["api",null,null],"text":"we have quite a lot of code to support .mode (internally we use pretty much the same algos as ``.value_counts()``)\r\n\r\nbut unless I am not understanding isn't.\r\n\r\n``x.mode() == x.value_counts(sort=True).index[0]`` ?\r\n\r\nif ties, then return a slice of the result\r\n\r\ne.g.\r\n```\r\nIn [17]: Series([1, 1, 1, 2, 2, 2, 3]).value_counts()\r\nOut[17]: \r\n2    3\r\n1    3\r\n3    1\r\ndtype: int64\r\n\r\nIn [18]: Series([1, 1, 1, 2, 2, 2, 3]).mode()\r\nOut[18]: \r\n0    1\r\n1    2\r\ndtype: int64\r\n```\r\ncc @TomAugspurger "},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame({'a':np.random.random(10),'b':np.random.random(10)})\r\ndf\r\ndf.rename_axis('numbers', inplace=True)\r\ndf\r\n```\r\n#### Problem description\r\n\r\ndf.rename_axis with the inplace=True should return None and make the named axis change the index name in the original object.  However, up listing df again, no index name is present.\r\n\r\n#### Expected Output\r\ndf.index.name should return 'numbers'\r\n#### Output of ``pd.show_versions()``\r\n\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-66-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: 0.999999999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: 0.3.0.post\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"The astropy package is excellent for calculations with units!\r\nIs there a method to pass the Units (from astropy import units as u) to a pandas Series or DataFrame?\r\n\r\nOtherwise, would it be nice to implement this approach somehow?"},{"labels":["api",null],"text":"https://github.com/pandas-dev/pandas/issues/15674\r\n\r\n```\r\nIn [19]: import pandas as pd\r\n    ...: import numpy as np\r\n    ...: d = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\r\n    ...:      'r': ['right', 'left', 'right', 'left', 'right', 'left'],\r\n    ...:      'v': [-1, 1, -1, 1, -1, np.nan]}\r\n    ...: df = pd.DataFrame(d)\r\n    ...: \r\n\r\nIn [20]: df.groupby('l').v.sum()\r\nOut[20]: \r\nl\r\nleft    -3.0\r\nright    2.0\r\nName: v, dtype: float64\r\n\r\nIn [21]: df.groupby('l').v.apply(lambda x: x.sum(skipna=False))\r\nOut[21]: \r\nl\r\nleft    -3.0\r\nright    NaN\r\nName: v, dtype: float64\r\n\r\n```\r\n\r\nideally write [21] as\r\n``df.groupby('l').v.sum(skipna=False)``"},{"labels":["api",null,null,null],"text":"From https://github.com/pandas-dev/pandas/issues/10647#issuecomment-123715600\r\n\r\nIn context of discussion on the default parsing of 'NA' in a csv file as NaN, I don't think we can change the default of parsing `NA`, but we *may consider* parsing of `\"NA\"` as a string instead of NaN:\r\n\r\n> Another change that we would maybe be more likely to consider, is the parsing of quoted values of NA (so only changing \"NA\" not be converted automatically, but leaving NA and alike converted to NaN as it is now).\r\nPersonally, I would even this consider as a bug that it treats \"NA\" and NA the same, as I would expect that quoted values should be left untouched. But I don't know how long this behaviour has been this way.\r\n\r\nAlthough I am not sure how invasive such a change would be, and difficult to assess, so maybe this is not worth risking many breakages?"},{"labels":["api",null,null],"text":"supporting ``in`` for a ``Timestamp`` in a ``Period`` is straightforward and useful. Note that we current consider both endpoints inclusive.\r\n\r\n```\r\nIn [28]: p = Period('2016',freq='M')\r\n\r\nIn [29]: p\r\nOut[29]: Period('2016-01', 'M')\r\n\r\nIn [30]: p.start_time\r\nOut[30]: Timestamp('2016-01-01 00:00:00')\r\n\r\nIn [31]: p.end_time\r\nOut[31]: Timestamp('2016-01-31 23:59:59.999999999')\r\n\r\nIn [32]: p.start_time in p\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-32-b4a9c4196274> in <module>()\r\n----> 1 p.start_time in p\r\n\r\nTypeError: argument of type 'Period' is not iterable\r\n```"},{"labels":["api",null],"text":"The sort_index method does not seem to work properly if the dataframe was created with concat. See this example:\r\n\r\n```python\r\n>>> df = pd.DataFrame(index=['a', 'b'])\r\n>>> pd.concat([df, df], keys=[0.8, 0.5]).sort_index()\r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [(0.8, a), (0.8, b), (0.5, a), (0.5, b)]\r\n```\r\n\r\nThe 0.5 tuples should come before the 0.8 ones. Everything works fine if I create the multi-index from a product:\r\n\r\n```python\r\n>>> pd.DataFrame(index=pd.MultiIndex.from_product([[0.8, 0.5], ['a', 'b']]))\r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [(0.8, a), (0.8, b), (0.5, a), (0.5, b)]\r\n>>> pd.DataFrame(index=pd.MultiIndex.from_product([[0.8, 0.5], ['a', 'b']])).sort_index()\r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [(0.5, a), (0.5, b), (0.8, a), (0.8, b)]\r\n```\r\n\r\nI'm on pandas version 0.18.1."},{"labels":["api",null,null],"text":"\r\n```python\r\n# Your code here\r\n\r\ntdf.rolling( '10D' ).mean()\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-49-707b0ff61efc> in <module>()\r\n     10 tdf = df.groupby(level=1).get_group( 64413 )\r\n     11 \r\n---> 12 tdf.rolling( '10D' ).mean()\r\n     13 \r\n     14 tdf.reset_index().set_index('time').rolling( '10D' ).mean()\r\n\r\n/home/firdaus/.conda/envs/tsconda/lib/python3.5/site-packages/pandas/core/generic.py in rolling(self, window, min_periods, freq, center, win_type, on, axis)\r\n   5502                                    min_periods=min_periods, freq=freq,\r\n   5503                                    center=center, win_type=win_type,\r\n-> 5504                                    on=on, axis=axis)\r\n   5505 \r\n   5506         cls.rolling = rolling\r\n\r\n/home/firdaus/.conda/envs/tsconda/lib/python3.5/site-packages/pandas/core/window.py in rolling(obj, win_type, **kwds)\r\n   1797         return Window(obj, win_type=win_type, **kwds)\r\n   1798 \r\n-> 1799     return Rolling(obj, **kwds)\r\n   1800 \r\n   1801 \r\n\r\n/home/firdaus/.conda/envs/tsconda/lib/python3.5/site-packages/pandas/core/window.py in __init__(self, obj, window, min_periods, freq, center, win_type, axis, on, **kwargs)\r\n     76         self.win_type = win_type\r\n     77         self.axis = obj._get_axis_number(axis) if axis is not None else None\r\n---> 78         self.validate()\r\n     79 \r\n     80     @property\r\n\r\n/home/firdaus/.conda/envs/tsconda/lib/python3.5/site-packages/pandas/core/window.py in validate(self)\r\n   1055 \r\n   1056         elif not is_integer(self.window):\r\n-> 1057             raise ValueError(\"window must be an integer\")\r\n   1058         elif self.window < 0:\r\n   1059             raise ValueError(\"window must be non-negative\")\r\n\r\nValueError: window must be an integer\r\n```\r\n\r\n\r\n#### Problem description\r\n\r\nThe offset feature of specifying timelike windows in 'rolling' doesn't work if the dataframe has multindex with level_0 = 'time' and level_1 = something else.\r\n\r\n\r\n#### Expected Output\r\n``` python\r\ntdf.reset_index().set_index('time').rolling( '10D' ).mean()\r\n```\r\nWorks correctly.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-0.bpo.4-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2.post+ts3\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.4.0\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n# Your code here\r\ndf1 = pd.DataFrame({'a': [0, 10, 20]})\r\ndf2 = pd.DataFrame({'b': [200, 100]}, index=[2,1])\r\n\r\nprint(df1.join(df2, how='inner'))\r\nprint(df2.join(df1, how='inner'))\r\n\r\nprint(df1.join(df2, how='inner', sort=True))\r\n```\r\n#### Problem description\r\n\r\nContrary to what is stated in the documentation of DataFrame.join(), when using the default sort=False, the return DataFrame preserves the index order of the other (right) DataFrame, instead of the index order of the calling (left) DataFrame.\r\n\r\nBesides, the sort=True argument does not work.\r\n\r\n#### Expected Output\r\nThe expected output is that the return DataFrame should preserve the index order of the calling (left) DataFrame.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-108-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: ca_ES.UTF-8\r\nLOCALE: ca_ES.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 27.2.0.post20161106\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: 0.2.1\r\n</details>\r\n"},{"labels":["api",null],"text":"Subclassed Series and DataFrames with custom `_constructor`, `_constructor_sliced`, and `_constructor_expanddim` methods return pandas Series and DataFrame objects on reshape operations:\r\n\r\n```python\r\n>>> import numpy as np, pandas as pd\r\n>>> import pandas.util.testing as tm\r\n>>> df = tm.SubclassedDataFrame(np.random.random((4,5)))\r\n>>> df\r\n          0         1         2         3         4\r\n0  0.151944  0.948561  0.639122  0.718071  0.296193\r\n1  0.004606  0.978378  0.827614  0.479320  0.495945\r\n2  0.158874  0.535476  0.173194  0.065292  0.365851\r\n3  0.772392  0.105038  0.671064  0.164165  0.795803\r\n>>> type(df)\r\n<class 'pandas.util.testing.SubclassedDataFrame'>\r\n```\r\n\r\nSlicing operations use `_constructor_sliced`:\r\n```python\r\n>>> type(df[0])\r\n<class 'pandas.util.testing.SubclassedSeries'>\r\n```\r\n\r\nReshape operations return regular pandas objects:\r\n```python\r\n>>> type(df.stack())\r\n<class 'pandas.core.series.Series'>\r\n```\r\n\r\nIt would be great if this returned a `SublcassedSeries`. Same goes for unstack and pivot."},{"labels":["api",null],"text":"if we have a string Series, e.g. ``s = Series(['2013-01-02', '2013-1-03'])``\r\n\r\nwe normally just run this thru ``pd.to_datetime(s)``\r\n\r\nit might be convenient to do these operations (``.to_numeric,.to_datetime,.to_timedelta``)\r\ndirectly as methods.\r\n\r\n- ``s.str.to_numeric(....)`` , IOW in the ``.str`` namespace\r\n- ``s.to_numeric(...)``, in main Series namespace\r\n- ``s.astype(.., dtype=...)`` but actually pass thru to using the ``.to_numeric/.to_datetime`` functions directly, so you could have something like\r\n  ``s.astype('datetime', format='%Y-%m-%d')`` and it would do the right thing. This overloads things a bit though\r\n\r\nright now we have a bit of confusion because we support ``.astype`` which is pretty much a pass thru to numpy, and the more dtype-aware (and functional) ``.to_*`` ones.\r\n\r\nFurthermore I think we could update the ``.astype`` docs (and the ``.to_*``) ones to cross reference."},{"labels":["api",null],"text":"I have a suggestion for how to use Enums to codify less transparent parameter values. Since 3.4, Python comes with built-in support for Enums, which could be useful in certain instances for Pandas.\r\n\r\nOne example is allowing enums in place of integers for axis values, which frankly to this day I cannot seem to remember which is which. These could for instance be stored in an attribute in each function. This could potentially work like this:\r\n\r\n    import pandas as pd\r\n\r\n    df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4]})\r\n    df.sum(axis=df.sum.axis.COLUMNWISE)\r\n\r\n    a    6\r\n    b    9\r\n    dtype: int64\r\n\r\n\r\nAnother example is having support for enums in place of string values. This would be useful as a way to codify all supported string values outside of documentation, and also enabling autocomplete support in REPLs. Another example:\r\n\r\n    import pandas as pd\r\n    from pandas import axis\r\n\r\n    df = pd.DataFrame({'a': [1,2,3,5], 'b': [2,3,4,None]})\r\n\r\n    df.sort_values('b', na_position=df.sort_values.na_position.FIRST)\r\n\r\n       a    b\r\n    3  5  NaN\r\n    0  1  2.0\r\n    1  2  3.0\r\n    2  3  4.0\r\n"},{"labels":["api",null,null,null],"text":"Inspired by \r\n\r\n> http://stackoverflow.com/questions/25606478/grouping-by-everything-except-for-one-index-column-in-pandas\r\n\r\n I was thinking if it would be useful to add a parameter to the groupby function (e.g. allbut=True) so that pandas will group all levels except the specified ones) .\r\n\r\nI suggest so because in a multilevel index with several levels, eliminating just one level requires a quite verbose approach "},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: s = pd.Series(range(8), index=pd.MultiIndex.from_product([[1,2], [3,4], [3,4]],\r\n                                                                 names=['a', 'b', 'c']))\r\n\r\nIn [3]: s.loc[s.index] # Works as expected\r\nOut[3]: \r\na  b  c\r\n1  3  3    0\r\n      4    1\r\n   4  3    2\r\n      4    3\r\n2  3  3    4\r\n      4    5\r\n   4  3    6\r\n      4    7\r\ndtype: int64\r\n\r\nIn [4]: s.loc[s.iloc[2:-1].index] # Works as expected\r\nOut[4]: \r\na  b  c\r\n1  4  3    2\r\n      4    3\r\n2  3  3    4\r\n      4    5\r\n   4  3    6\r\ndtype: int64\r\n\r\nIn [5]: s.loc[s.index.droplevel('c')] # Just reindexes... weird\r\nOut[5]: \r\n1  3   NaN\r\n   3   NaN\r\n   4   NaN\r\n   4   NaN\r\n2  3   NaN\r\n   3   NaN\r\n   4   NaN\r\n   4   NaN\r\ndtype: float64\r\n\r\nIn [6]: s.loc[s.index.droplevel(['b', 'c']), :] # Works (flat index)\r\nOut[6]: \r\na  b  c\r\n1  3  3    0\r\n      4    1\r\n   4  3    2\r\n      4    3\r\n2  3  3    4\r\n      4    5\r\n   4  3    6\r\n      4    7\r\ndtype: int64\r\n\r\nIn [7]: s.loc[s.index.droplevel(['b', 'c'])] #... but fails if I use the shortened notation!\r\n[...]\r\nTypeError: unhashable type: 'Int64Index'\r\n\r\nIn [8]: s.loc[s.swaplevel('b', 'c')] # Works\r\nOut[8]: \r\na  b  c\r\n1  3  3    0\r\n      4    1\r\n   4  3    2\r\n      4    3\r\n2  3  3    4\r\n      4    5\r\n   4  3    6\r\n      4    7\r\ndtype: int64\r\n\r\nIn [9]: s.loc[s.index.swaplevel('b', 'c')]  # Different result! (reindexes)\r\nOut[9]: \r\na  c  b\r\n1  3  3    0\r\n   4  3    2\r\n   3  4    1\r\n   4  4    3\r\n2  3  3    4\r\n   4  3    6\r\n   3  4    5\r\n   4  4    7\r\ndtype: int64\r\n\r\nIn [10]: s.loc[pd.MultiIndex.from_product([[1,2], [3], [4]],\r\n                                          names=['a', 'c', 'b'])] # Does not respect column names!\r\nOut[10]: \r\na  c  b\r\n1  3  4    1\r\n2  3  4    5\r\ndtype: int64\r\n\r\n\r\n```\r\n#### Problem description\r\n\r\nThis clearly needs a unified approach (and I can try).\r\n\r\n#### Expected Output\r\n\r\nI guess most expected outputs above are obvious, except for ``In [10]:`` (and maybe ``In [5]:``, which however is already discussed [elsewhere](15452)). That is: it is not obvious whether level names in the indexer should be matched to level names in the indexed, when both are set ([see this comment](https://github.com/pandas-dev/pandas/pull/15425#issuecomment-280500783)). It would probably be more ``pandas``-ish if they were.\r\n\r\nIn other terms, while there is no doubt that\r\n\r\n``` python\r\nOut[10]: \r\na  c  b\r\n1  3  4    1\r\n2  3  4    5\r\ndtype: int64\r\n```\r\nis wrong, we must decide whether we want\r\n``` python\r\nOut[10]: \r\na  b  c\r\n1  3  4    1\r\n2  3  4    5\r\ndtype: int64\r\n```\r\nor\r\n``` python\r\nOut[10]: \r\na  b  c\r\n1  4  3    2\r\n2  4  3    6\r\ndtype: int64\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.7.0-1-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.utf8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.19.0+478.g12f2c6a\r\npytest: 3.0.6\r\npip: 8.1.2\r\nsetuptools: 28.0.0\r\nCython: 0.23.4\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.4.8\r\npatsy: 0.3.0-dev\r\ndateutil: 2.5.3\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nfeather: None\r\nmatplotlib: 2.0.0rc2\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: 0.999\r\nhttplib2: 0.9.1\r\napiclient: 1.5.2\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandas_datareader: 0.2.1\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> s = pd.Series([2, 1, 0], index=pd.CategoricalIndex([2, 1, 0]))\r\n>>> s.index\r\nCategoricalIndex([2, 1, 0], categories=[0, 1, 2], ordered=False, dtype='category')\r\n>>> s[[0, 1, 2]].equals(s.loc[[0, 1, 2]])  # Should be True\r\nFalse\r\n>>> s[[0, 1, 2]].equals(s.iloc[[0, 1, 2]])  # Should be False\r\nTrue\r\n```\r\n#### Problem description\r\n\r\nFor Series with CategoricalIndex, `__getitem__` indexing behaves differently than `.loc`. On other-indexed series, these accessors return the same result:\r\n```py\r\n>>> s = pd.Series([2, 1, 0], index=[2, 1, 0])\r\n>>> s[[0, 1, 2]].equals(s.loc[[0, 1, 2]])  # Should be True\r\nTrue\r\n>>> s[[0, 1, 2]].equals(s.iloc[[0, 1, 2]])  # Should be True\r\nFalse\r\n```\r\n#### Expected Output\r\n\r\n```python\r\n>>> s = pd.Series([2, 1, 0], index=pd.CategoricalIndex([2, 1, 0]))\r\n>>> s[[0, 1, 2]].equals(s.loc[[0, 1, 2]])\r\nTrue\r\n```\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n0.19.0+479.git"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: s = pd.Series(range(4), index=pd.MultiIndex.from_product([[1,2], ['a', 'b']]))\r\n\r\nIn [3]: s.loc[['not', 'found']]\r\nOut[3]: Series([], dtype: int64)\r\n```\r\n#### Problem description\r\n\r\nWith regular ``Index``es, looking for a list of labels of which none is present will raise ``KeyError`` (see below). We should be coherent (and while I would prefer the empty result to the exception, this was already discussed in #7999).\r\n\r\n#### Expected Output\r\n\r\nCompare to\r\n\r\n``` python\r\nIn [4]: s.reset_index().loc[['not', 'found']]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-4-e13655430320> in <module>()\r\n----> 1 s.reset_index().loc[['not', 'found']]\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)\r\n   1339         else:\r\n   1340             key = com._apply_if_callable(key, self.obj)\r\n-> 1341             return self._getitem_axis(key, axis=0)\r\n   1342 \r\n   1343     def _is_scalar_access(self, key):\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)\r\n   1539                     raise ValueError('Cannot index with multidimensional key')\r\n   1540 \r\n-> 1541                 return self._getitem_iterable(key, axis=axis)\r\n   1542 \r\n   1543             # nested tuple slicing\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_iterable(self, key, axis)\r\n   1049     def _getitem_iterable(self, key, axis=0):\r\n   1050         if self._should_validate_iterable(axis):\r\n-> 1051             self._has_valid_type(key, axis)\r\n   1052 \r\n   1053         labels = self.obj._get_axis(axis)\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _has_valid_type(self, key, axis)\r\n   1429 \r\n   1430                 raise KeyError(\"None of [%s] are in the [%s]\" %\r\n-> 1431                                (key, self.obj._get_axis_name(axis)))\r\n   1432 \r\n   1433             return True\r\n\r\nKeyError: \"None of [['not', 'found']] are in the [index]\"\r\n\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: f65a6415f15d438432cc6954ead61b052c5d4d60\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.7.0-1-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.utf8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.19.0+473.gf65a641\r\npytest: 3.0.6\r\npip: 8.1.2\r\nsetuptools: 28.0.0\r\nCython: 0.23.4\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.4.8\r\npatsy: 0.3.0-dev\r\ndateutil: 2.5.3\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nfeather: None\r\nmatplotlib: 2.0.0rc2\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: 0.999\r\nhttplib2: 0.9.1\r\napiclient: 1.5.2\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"The current solution is to call `str.replace(<compiled_re>.pattern, flags=<compiled_re>.flags)` which is relatively ugly and verbose in my opnion.\r\n\r\nHere's a contrived example of removing stopwords and normalizing whitespace afterwards:\r\n\r\n```python\r\nimport pandas as pd\r\nimport re\r\n\r\nsome_names = pd.Series([\"three weddings and a funeral\", \"the big lebowski\", \"florence and the machine\"])\r\n\r\nstopwords = [\"the\", \"a\", \"and\"]\r\nstopwords_re = re.compile(r\"(\\s+)?\\b({})\\b(\\s+)?\".format(\"|\".join(stopwords), re.IGNORECASE)\r\nwhitespace_re = re.compile(r\"\\s+\")\r\n\r\n# desired code:\r\n# some_names.str.replace(stopwords_re, \" \").str.strip().str.replace(whitespace_re, \" \")\r\n\r\n# actual code:\r\nsome_names.\\\r\n    str.replace(stopwords_re.pattern, \" \", flags=stopwords_re.flags).\\\r\n    str.strip().str.replace(whitespace_re.pattern, \" \", flags=whitespace_re.flags)\r\n```\r\n\r\nWhy do I think this is better?\r\n\r\n1. It's [nice](http://stackoverflow.com/a/453568/2954547) to have commonly used regular expressions compiled and to carry their flags around with them (and also allows the use of [\"verbose\" regular expressions](http://stackoverflow.com/a/29159014/2954547))\r\n2. It's not that compiled regular expressions should quack like strings... it's that in this case we're making strings quack like compiled regular expressions, but at the same time not letting those compiled regular expressions quack their own quack.\r\n\r\nIs there a good reason _not_ to implement this?"},{"labels":["api",null],"text":"#### Problem description\r\nThis is a feature request for usecols to accept integer value and string.  If integer/string is provided, flag `squeeze` is to be turned on, thus producing a Series.\r\n\r\n#### Rationale\r\n+ consistency with `index_col` which accepts both integer/string and lists\r\n+ shortcut\r\n\r\n#### Expected Output\r\n\r\n```python\r\n# test\r\ntestfile = \"pandas/tests/io/data/tips.csv\"\r\ndf = pd.read_table(testfile, sep=\",\", usecols=\"tip\")\r\nassert isinstance(df, pd.Series)\r\n\r\ndf = pd.read_table(testfile, sep=\",\", usecols=2)\r\nassert isinstance(df, pd.Series)\r\n\r\ndf = pd.read_table(testfile, sep=\",\", usecols=[2])\r\nassert isinstance(df, pd.DataFrame)\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.3.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-642.13.1.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.0\r\nnose: 1.3.0\r\npip: 8.1.2\r\nsetuptools: 20.9.0\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.0.0.dev\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.3.1-dev0\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: 0.9.2\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"(From #15425 )\r\n\r\nCurrently, (non-``Multi``)``Index``es can be indexed with ``Series`` indexers. And this actually also applies to ``MultiIndex``es, of which you would be selecting from the first level. Hence, it seems a natural consequence for ``MultiIndex``es to be indexed with ``DataFrame`` indexers.\r\n\r\nMoreover, once #15434 is fixed, we will have a bi-dimensional object (``MultiIndex``) which can be indexed with ``np.array``s... but only one-dimensional ones! This is also strange.\r\n\r\nThe feature per se is certainly useful. As a simple real world example, I am currently working with a ``subjects`` ``DataFrame`` to which I must attribute two columns from ``design``, another ``DataFrame``, depending on a ``group`` and ``time`` columns of ``subjects``, which are also levels of the ``MultiIndex`` of ``design``. I would like to just do\r\n\r\n``` python\r\nsubjects[design.columns] = design.loc[subjects[[\"group\", \"time\"]]]\r\n```\r\n\r\nNow, I know this could be solved by ``.join``ing the two ``DataFrames``... but this is conceptually more complicated (I even currently ignore whether I can join one ``DataFrame`` on columns and the other on index levels... but this is OT), to the point that I'm rather doing:\r\n\r\n```python\r\nto_mi = lambda df : df.set_index(list(df.columns)).index\r\nsubjects[design.columns] = design.loc[to_mi(subjects[[\"group\", \"time\"]])]\r\n```\r\n\r\n@jorisvandenbossche [suggests](https://github.com/pandas-dev/pandas/pull/15425#issuecomment-280500783) this feature would add complexity to indexing, _\"eg, should the column names align on the level names?\"_. I'm personally fine with both answers:\r\n\r\n- **Yes**: then we just use something like ``to_mi`` above (transforming a ``DataFrame`` in ``MultiIndex``, and then using it to actually index)\r\n- **No**: then it's really really simple (we just transform the ``DataFrame`` into tuples - I had actually already done this in #15425 before rolling back)\r\n\r\n\"**Yes**\" is probably the cleanest answer (possibly together with allowing indexing with bi-dimensional ``np.array``s, to obtain the equivalent of the \"**No**\" answer). In any case, once we decide, I can take care of this."},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n# It's pretty simple but it doesn't work.\r\n# Here stocks is inside a class which type is pd.Series.\r\n# I tried to print money / price * (1-r) and found it was not zero.\r\n# However, self.stocks[ticker] is never changed which confuses me\r\nself.stocks[ticker] += money / price * (1 - r)\r\n\r\n```\r\n#### Problem description\r\nSee the comments above. It only happens when I tried to use a certain set of data but works for other situations. I've tried to change the value of self.stocks[name] and something more strange happened.\r\n``` python\r\n# this works\r\nself.stocks[ticker] = 6\r\nself.stocks[ticker] += money / price * (1 - r)\r\n# I was able to see the different value printed after the line\r\n# but if I change it to\r\nif self.stocks[ticker] == 0:\r\n    self.stocks[ticker] = 0\r\nself.stocks[ticker] += money / price* (1 - r)\r\n# it repeated the unchanged result which drove me mad\r\n```\r\n\r\n#### Expected Output\r\nself.stocks[ticker] should be changed\r\n#### Output of ``pd.show_versions()``\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: zh_CN.UTF-8\r\nLOCALE: zh_CN.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 32.2.0\r\nCython: None\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.6.2\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.2\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.7.2\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.5\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null],"text":"Is it possible to support clip in place, just like numpy does? I assume that would speed up working with large dataframe/series as no extra copies will be needed.\r\n\r\nIf not, is that a workaround to achieve the same effect? I tried `np.clip(df.values,-clip_bounds.values, clip_bounds.values, out=df.values)`, but realized that df.values is actually an expensive call as it is merging  blocks underneath."},{"labels":["api",null,null],"text":"Hello there,\r\n\r\nHave I said that Pandas is awesome? yes, many times ;-)\r\n\r\nI have a question, I am working with a very large dataframe of trades, timestamped at the millisecond precision. Latest Pandas 19.2 here.\r\n\r\nI need to resample the dataframe every `200 ms`, but given that my data spans several years and I am only interested in resampling data between `10:00 am` and `12:00 am` every day (handled by `between_time()`), using a plain `resample` will crash and burn my machine. \r\n\r\nInstead, I tried the `sparse resampling` shown in the http://pandas.pydata.org/pandas-docs/stable/timeseries.html#sparse-resampling, but it **fails** when i provide it with a dictionary of columns. \r\n\r\nIs that expected? Is it a bug?\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nrng = pd.date_range('2014-1-1', periods=100, freq='D') + pd.Timedelta('1s')\r\nts = pd.DataFrame({'value' : range(100)}, index=rng)\r\n\r\n\r\nfrom functools import partial\r\nfrom pandas.tseries.frequencies import to_offset\r\n\r\ndef round(t, freq):\r\n freq = to_offset(freq)\r\n return pd.Timestamp((t.value // freq.delta.value) * freq.delta.value)\r\n\r\n# works\r\nts.groupby(partial(round, freq='3T')).value.sum()\r\n\r\n# does not work\r\nts.groupby(partial(round, freq='3T')).apply({'value' : 'sum'})\r\n\r\nts.groupby(partial(round, freq='3T')).apply({'value' : 'sum'})\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-104-6004b307a469>\", line 1, in <module>\r\n    ts.groupby(partial(round, freq='3T')).apply({'value' : 'sum'})\r\n\r\n  File \"C:\\Users\\m1hxb02\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\pandas\\core\\groupby.py\", line 674, in apply\r\n    func = self._is_builtin_func(func)\r\n\r\n  File \"C:\\Users\\m1hxb02\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\pandas\\core\\base.py\", line 644, in _is_builtin_func\r\n    return self._builtin_table.get(arg, arg)\r\n\r\nTypeError: unhashable type: 'dict'\r\n\r\n```\r\n\r\n\r\nProblem is: I need to resample several columns at once in my dataframe, eventually using different functions (`sum`, `mean`, `max`). Is anything wrong here?\r\n\r\nThanks~"},{"labels":["api",null,null],"text":"Currently there is some inconsistency around the scalar type returned from a `Series` aggregation, both in terms of whether it is a numpy or python type, as well as different behavior for an empty `Series` - see table below.\r\n\r\nNormally this isn't a big deal as the numpy types mostly behave like the python type, but can be an issue with serialization, which is where I ran into this.\r\n\r\nIs the desired behavior to make all of these python types?\r\n\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th>function</th>\r\n      <th>type</th>\r\n      <th>type_empty</th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <td>sum</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n      <td>&lt;class 'int'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>mean</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>median</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>count</td>\r\n      <td>&lt;class 'numpy.int32'&gt;</td>\r\n      <td>&lt;class 'numpy.int32'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>var</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>std</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>sem</td>\r\n      <td>&lt;class 'numpy.float64'&gt;</td>\r\n      <td>&lt;class 'numpy.float64'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>nunique</td>\r\n      <td>&lt;class 'int'&gt;</td>\r\n      <td>&lt;class 'int'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>prod</td>\r\n      <td>&lt;class 'numpy.float64'&gt;</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>min</td>\r\n      <td>&lt;class 'numpy.float64'&gt;</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n    </tr>\r\n    <tr>\r\n      <td>max</td>\r\n      <td>&lt;class 'numpy.float64'&gt;</td>\r\n      <td>&lt;class 'float'&gt;</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\nCode for table\r\n```python\r\nfxs = ['sum', 'mean', 'median', 'count', 'var', 'std', 'sem',  'nunique', 'prod', 'min', 'max']\r\ns = pd.Series([1., 2., 3.])\r\ns_empty = pd.Series([], dtype='f8')\r\ndata = []\r\n\r\nfor f in fxs:\r\n    row = dict(function=f)\r\n    res = getattr(s, f)()\r\n    row['type'] = type(res)\r\n    res = getattr(s_empty, f)()\r\n    row['type_empty'] = type(res)\r\n    data.append(row)\r\n\r\npd.DataFrame(data).to_html(index=False)\r\n```"},{"labels":["api",null,null],"text":"#### Code Sample\r\n\r\n```pycon\r\n>>> import pandas as pd\r\n\r\n>>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4], 'C': [5, 6], 'D': [7, 8]})\r\n>>> pd.melt(df, id_vars=('A', 'B'), value_vars=('C', 'D'))\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-14-ef146f73874a> in <module>()\r\n----> 1 pd.melt(df, id_vars=('A', 'B'), value_vars=('C', 'D'))\r\n\r\n/Users/josh/env/analysis/lib/python3.5/site-packages/pandas/core/reshape.py in melt(frame, id_vars, value_vars, var_name, value_name, col_level)\r\n    766         if not isinstance(value_vars, (tuple, list, np.ndarray)):\r\n    767             value_vars = [value_vars]\r\n--> 768         frame = frame.ix[:, id_vars + value_vars]\r\n    769     else:\r\n    770         frame = frame.copy()\r\n\r\nTypeError: can only concatenate list (not \"tuple\") to list\r\n\r\n```\r\n#### Problem description\r\n\r\nThe [documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html) says that `id_vars` and `value_vars` can be tuples, lists, or ndarrays, but it doesn't work if `value_vars` is a tuple. The function works as expected if `value_vars` is given as a list.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 26.1.1\r\nCython: 0.24.1\r\nnumpy: 1.12.0\r\nscipy: 0.18.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> df = pd.DataFrame([1], columns=['A'])\r\n\r\n>>> df \r\n   A\r\n0  1\r\n\r\n>>> df.squeeze()\r\n1\r\n\r\n>>> type(df.squeeze())\r\nnumpy.int64\r\n\r\n>>> df.squeeze(axis=1)  # Only want to squeeze a single dimension, i.e. into a series\r\n...\r\nValueError: the 'axis' parameter is not supported in the pandas implementation of squeeze()\r\n```\r\n#### Problem description\r\n\r\n`np.squeeze` supports `axis` parameter and [this comment in the source](https://github.com/pandas-dev/pandas/blob/542c9166a6ceff4a4889caae3843c3a82a2301cd/pandas/compat/numpy/function.py#L217-L219) implies it should eventually be implemented.\r\n\r\n#### Expected Output\r\n```py\r\n>>> df.squeeze(axis=1)  # Squeeze a single dimension, i.e. into a series\r\n0    1\r\nName: A, dtype: int64\r\n\r\n>>> df.squeeze(axis=0)\r\nA    1\r\nName: 0, dtype: int64\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\npandas: 0.19.0+416.ge1390cd"},{"labels":["api",null,null,null,null,null],"text":"Currently any type conversions on merge  are silent, e.g.\r\n\r\n```\r\nIn [24]: a = pd.DataFrame({'cat_key': pd.Categorical(['a', 'b', 'c']), 'int_key': [1, 2, 3]})\r\n\r\nIn [25]: b = pd.DataFrame({'cat_key': pd.Categorical(['b', 'a', 'c']), 'values': [1, 2, 3]})\r\n\r\nIn [26]: a.merge(b).dtypes\r\nOut[26]: \r\ncat_key    object\r\nint_key     int64\r\nvalues      int64\r\ndtype: object\r\n\r\nIn [29]: b2 = pd.DataFrame({'int_key': [2.0, 1.0, 3.0], 'values': [1, 2, 3]})\r\n\r\nIn [30]: a.merge(b2)\r\nOut[30]: \r\n  cat_key  int_key  values\r\n0       a        1       2\r\n1       b        2       1\r\n2       c        3       3\r\n\r\nIn [31]: a.merge(b2).dtypes\r\nOut[31]: \r\ncat_key    object\r\nint_key     int64\r\nvalues      int64\r\ndtype: object\r\n\r\n```\r\n\r\n#15321 will make `[26]` preserve a categorical dtype, but if the categories don't overlap, it will be converted to object.\r\n\r\nSo, should there be a something like a `conversions='ignore'|'warn'|'error'` option?\r\n\r\n"},{"labels":["api",null],"text":"xref https://github.com/pandas-dev/pandas/pull/15326#issuecomment-278008177\r\n\r\n- when ``pd.Grouper`` was introduced, we used ``key`` to indicate the column which to group on\r\n- ``.resample`` added the same idea as ``on``\r\n- we use ``on`` in ``pd.merge`` (existing for a long time)\r\n- ``.groupby`` uses ``by`` (though to be fair this can take lots of things and is not strictly a string column name).\r\n\r\nSo the proposal is to deprecate ``pd.Grouper(key=...)`` in favor of ``on=``.\r\n\r\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nfor i, k in df.resample('W-MON'):\r\n    print(i, k)\r\n```\r\n```sh\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-60-5ca9218261db> in <module>()\r\n----> 1 for i, k in df.resample('W-MON'):\r\n      2     print(i, k)\r\n      3     break\r\n\r\n/xxx/lib/python3.5/site-packages/pandas/core/groupby.py in __iter__(self)\r\n    629         for each group\r\n    630         \"\"\"\r\n--> 631         return self.grouper.get_iterator(self.obj, axis=self.axis)\r\n    632\r\n    633     @Substitution(name='groupby')\r\n\r\nAttributeError: 'NoneType' object has no attribute 'get_iterator'\r\n```\r\n#### Problem description\r\ncan't iterate the resampler's key and grouped dataframe\r\n\r\n#### Expected Output\r\niterate the resampler's key and the grouped dataframe, now workaround\r\n```python\r\nfor key in df.resample('W-MON').groups.keys():\r\n  try:\r\n    print(key, df.resample('W-MON').get_group(key))\r\n  except KeyError:\r\n    continue\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_GB.UTF-8\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 30.2.0\r\nCython: 0.25.2\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.4.1\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.9999999\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n\r\n----\r\nupdate the workaround a bit"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\npd.Timestamp('2017-01-02 00:00:00') - pd.tseries.offsets.MonthBegin(n=0)\r\n\r\n```\r\n#### Problem description\r\n\r\nCurrently the above produces Timestamp('2017-02-01 00:00:00') which is the next MonthBegin point instead of the previous\r\nI think this is a usability issue - I don't see who can benefit from this behavior.\r\nMy expectation is that when I use parameter n=0,  it makes sense for it to snap to the next anchor point IN THE DIRECTION of the offset and stay there INCLUSIVE of the snap point. This is really useful because it allows you to stay within the same month no matter your starting point.\r\nThis is exactly how it already works if you move forward like in\r\n\r\npd.Timestamp('2017-01-02 00:00:00') + pd.tseries.offsets.MonthBegin(n=0)\r\nwhich gives you Timestamp('2017-02-01 00:00:00')\r\n\r\nBasically you want to be able to take a random date in a month INCLUSIVE of start and end day of the month and use (-) MonthBegin(n=0) offset in a manner where you always moved to the beginning of the same month \r\n\r\nI understand that there's a work-around to use \r\n```python\r\npd.Timestamp('2017-01-02 00:00:00')+pd.tseries.offsets.MonthEnd(n=0)-pd.tseries.offsets.MonthBegin(n=1)\r\n```\r\nThis will get me into Timestamp('2017-01-01 00:00:00') no matter where I start in January (inclusive of the ends) but I just think that this expression is really what (-) MonthBegin(n=0) is supposed to do\r\n\r\nPlease note the same usability issue exists for (-) MonthEnd\r\n\r\n#### Expected Output\r\nso my desired output for the above would be \r\nTimestamp('2017-01-01 00:00:00')\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\n\r\n</details>\r\n"},{"labels":["api",null],"text":"#### Problem description\r\nThis is an enhancement request to allow handling and saving comment strings with DataFrame text file IO. Two related stackoverflow questions about such feature: \r\nhttp://stackoverflow.com/questions/39724298/pandas-extract-comment-lines\r\nhttp://stackoverflow.com/questions/29233496/write-comments-in-csv-file-with-pandas\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n```python\r\ndf = pd.read_csv(\"mydata.csv\", comment=\"#\")\r\ndf.comment\r\n\"this is a comment from mydata.csv file\"\r\ndf.to_csv(\"output.csv\", comment=\"#\") # saves the `comment` string by pasting \"#\" before its each line and putting it before the table. \r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.0\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 29.0.1\r\nCython: 0.24\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.0.0\r\nsphinx: 1.4.5\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: 0.9.2\r\napiclient: 1.5.1\r\nsqlalchemy: 1.0.14\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api"],"text":"@spencerkclark is working on a custom `pandas.Index` subclass for xarray (see https://github.com/pydata/xarray/issues/1084) like `pandas.DatetimeIndex` to handle arrays of `netcdftime.datetime` objects. This index is primarily intended for use with xarray, but ideally we'd like it to work in pandas `Series` and `DataFrame` objects, too.\r\n\r\nThe subclass will include implementations of at least `get_loc`, `get_slice_bound` and `get_value` (this one should probably be unnecessary, but it's needed for `pandas.Series`). To minimize fragility, it will not subclass `DatetimeIndex` but will instead copy some of the relevant code (thank you open source!).\r\n\r\nTwo questions for other pandas devs:\r\n- Is there any fundamental reason why a custom `pandas.Index` subclass won't work on a `Series` or `DataFrame`?\r\n- Does this seem like a reasonable thing to do, or we are setting ourselves up for suffering in the future? I'll update this issue when we have a concrete PR to look at.\r\n\r\nAt a bare minimum, we should probably add some tests to pandas to ensure that a basic subclass works."},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```ipython\r\nIn [1]: import pandas as pd\r\nIn [2]: pd.Timestamp(\"2016-01-01\", tz=\"Europe/Berlin\") - pd.Timestamp(\"now\", tz=\"UTC\")\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-691a8df26ecd> in <module>()\r\n----> 1 pd.Timestamp(\"2016-01-01\", tz=\"Europe/Berlin\") - pd.Timestamp(\"now\", tz=\"UTC\")\r\n\r\npandas\\tslib.pyx in pandas.tslib._Timestamp.__sub__ (pandas\\tslib.c:23697)()\r\n\r\nTypeError: Timestamp subtraction must have the same timezones or no timezones\r\n\r\n```\r\n#### Problem description\r\n\r\nIf *both* timestamps have a timezone specified, the result of this operation is perfectly well-defined. It's quite surprising that I have to riddle my code with `lhs.tz_convert(\"UTC\") - rhs.tz_convert(\"UTC\")` lines to get the difference of timestamps.\r\n\r\n#### Expected Output\r\n\r\n    Timedelta('-393 days +06:29:07.057926')\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: 2.4.0\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8.1\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"In the currently online pandas 0.19.2 documentation, I see the following inconsistency:\r\n\r\n* There is [pandas.DataFrame.pivot_table](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot_table.html), but also [pandas.pivot_table](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html).\r\n* In the same way, there is [pandas.DataFrame.pivot](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html) and also [pandas.pivot](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot.html). (BTW., the latter has the dataframe argument missing.)\r\n* There is [pandas.melt](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html), but no pandas.DataFrame.melt.\r\n\r\nThe global versions of these methods (and probably others) are identical to the local ones except for an additional DataFrame argument, I think that all these should only be DataFrame methods. This would e.g. consistently enable chaining.\r\n\r\nSo let me suggest that the global versions should be removed, of course after a deprecation period.\r\n"},{"labels":["api",null,null],"text":"xref https://github.com/pandas-dev/pandas/pull/15216#issuecomment-275240736\r\n\r\n``Index.to_series()``\r\n``MultiIndex.to_frame()``\r\n\r\nshould we harmonize? (e.g. make both available to both? for compat), leave it like this?\r\n\r\nshould also prob add the ``index=True`` kw to ``.to_series()`` for compat."},{"labels":["api",null,null,null,null],"text":"In the PR implementing ``.str/.dt`` on Categoricals, https://github.com/pandas-dev/pandas/pull/11582.\r\n\r\nThis is perfectly reasonable. We perform the string op on the uniques. This routine is a boolean result, so we return a boolean result.\r\n```\r\nIn [2]: s = pd.Series(list('aabb')).astype('category')\r\n\r\nIn [3]: s\r\nOut[3]: \r\n0    a\r\n1    a\r\n2    b\r\n3    b\r\ndtype: category\r\nCategories (2, object): [a, b]\r\n\r\nIn [4]:  s.str.contains(\"a\")\r\nOut[4]: \r\n0     True\r\n1     True\r\n2    False\r\n3    False\r\ndtype: bool\r\n```\r\n\r\nHowever, I don't recall the rationale for: performing the op on the uniques (as its a categorical), but then returning an ``object`` dtype.\r\n```\r\nIn [5]: s.str.upper()\r\nOut[5]: \r\n0    A\r\n1    A\r\n2    B\r\n3    B\r\ndtype: object\r\n```\r\n\r\nThese are by-definition pure transforms, and so a new categorical makes sense. e.g. in this case\r\n\r\n```\r\nIn [6]: pd.Series(pd.Categorical.from_codes(s.cat.codes, s.cat.categories.str.upper()))\r\nOut[6]: \r\n0    A\r\n1    A\r\n2    B\r\n3    B\r\ndtype: category\r\nCategories (2, object): [A, B]\r\n```\r\n\r\nThis will be way more efficient than actually converting to object."},{"labels":["api",null,null],"text":"Currently `read_csv` has some ways to deal with \"bad lines\" (bad in the sense of too many or too few fields compared to the determined number of columns):\r\n\r\n- by default, it will error for too many fields, and fill with NaNs for too few fields\r\n- with `error_bad_lines=false` rows with too many fields will be dropped instead of raising an error (and in that case, `warn_bad_lines` controls to get a warning or not) \r\n- with `usecols` you can select certain columns, and in this way deal with rows with too many fields.\r\n\r\nSome possibilities are missing in this scheme:\r\n\r\n- \"process\" bad lines with too many fields, i.e. drop the excessive fields instead of either raising an error or dropping the full row (discussed in #9549)\r\n- getting a warning or error with too few fields instead of automatically filling with NaNs (asked for in #9729), or dropping those rows\r\n\r\nApart from that, https://github.com/pandas-dev/pandas/issues/5686 makes the request to be able to specify a custom function to process a bad line, to have even more control.\r\n\r\nIn https://github.com/pandas-dev/pandas/issues/9549#issuecomment-76498787 (and surrounding comments) there was some discussion about how to integrate this, and some idea from there from @jreback and @selasley:\r\n\r\nProvide more fine grained control in a new keyword (and deprecate `error_bad_lines`):\r\n\r\n```\r\nbad_lines='error'|'warn'|'skip'|'process'\r\n```\r\n\r\nor leave out `'warn'` and keep `warn_bad_lines` to be able to combine a warning with both 'skip' and 'process'.\r\n\r\nWe should further think about whether we can integrate this with the case of too few fields and not only too many.\r\n\r\nI think it would be nice to have some better control here, but we should think a bit about the best API for this.\r\n "},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n# Your code here\r\nIn [135]: df = pd.DataFrame({'a': range(10), 'b': range(10)}, \r\n                            index=pd.date_range('2014-01-01', periods=10))\r\n\r\nIn [136]: df.asof('2014-01-15')\r\nOut[136]: \r\na    9\r\nb    9\r\nName: 2014-01-10 00:00:00, dtype: int32\r\n\r\nIn [137]: df.asof('2013-01-15')\r\nOut[137]: nan\r\n```\r\n#### Problem description\r\n\r\nI got tripped up by `[137]` returning a scalar - it makes sense for a `Series`, but not sure it does with a  `DataFrame` lookup?  So maybe it should instead return an empty Series, e.g.\r\n```\r\nIn [138]: df.asof('2013-01-15')\r\nOut[138]: \r\na  NaN\r\nb  NaN\r\ndtype: float64\r\n```\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.0\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 23.0.0\r\nCython: 0.24.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.8.2\r\nIPython: 5.1.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.2\r\nlxml: 3.6.0\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\nhttplib2: 0.9.2\r\napiclient: 1.5.3\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.40.0\r\npandas_datareader: 0.2.1\r\n</details>\r\n"},{"labels":["api",null,null],"text":"Create a new function to remove outliers.\r\n\r\nhttp://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-dataframe\r\n\r\nI find myself using the code from SO quite often to remove outliers in a particular column when preprocessing data and it seems this is a common issue. It would be nice to have a function that operates on a Series to do this automatically.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\ndf = pd.DataFrame(np.random.randn(100, 3))\r\n\r\n# from SO answer by tanemaki\r\nfrom scipy import stats\r\ndf[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\r\n\r\n# instead, would prefer\r\ndf.drop_outliers(3)\r\n\r\n```\r\n\r\nAlternatively, instead of a new function, we could modify .clip(), though I think a new function makes more sense.\r\n\r\nThe implementation could be similar to the SO implementation.\r\n\r\nIf there is agreement that this would be useful and the implementation makes sense, happy to do the PR.\r\n\r\n"},{"labels":["api",null,null,null],"text":"It would be really nice if there was a `sort=False` option on stack/unstack and pivot. (Preferably the default)\r\n\r\nIt is reasonably common to have data in non-standard order that actually provides information (in my case, I have model names, and the order of the names denotes  complexity of the models). Stacking or unstacking currently loses all of this information, with no way to retrieve it. That does not seem like a sensible default to me.\r\n\r\nIt would be relatively easy to work around a non-sorted stack/unstack method (using .sort_index). To go the other way is less trivial, requiring the user to store a list of the values in the necessary order.\r\n\r\nI actually find it hard to think of a situation where a sort on unstack would be more useful..."},{"labels":["api"],"text":"In https://github.com/pandas-dev/pandas/pull/11603#issuecomment-162113949 (the main PR implementing the deferred API for rolling / expanding / ewm), we discussed how to specify table-wise `apply`s. `Groupby.apply(f)` feeds the entire group (all columns) to `f`. For backwards-compatibility, `.rolling(n).apply(f)` needed to be column-wise.\r\n\r\nhttps://github.com/pandas-dev/pandas/pull/11603#issuecomment-162116556 mentions a possible API like what I added for `.style`\r\n\r\n- `axis=0`: apply to each column independently\r\n- `axis=1`: apply to each row independently\r\n- `axis=None`: apply the supplied function to the entire table\r\n\r\nSo it'd be `df.rolling(n).apply(f, axis=None)`.\r\nDo people like the axis=0 / 1 / None idiom? Is it obvious enough?\r\n\r\nThis is prompted by @josef-pkt's [post on the mailinglist](https://groups.google.com/forum/#!topic/pydata/FcAT8LBPmlg). Needing a rolling OLS.\r\n\r\nAn example:\r\n\r\n```python\r\nIn [2]: import numpy as np\r\n   ...: import pandas as pd\r\n   ...:\r\n   ...: np.random.seed(0)\r\n   ...: df = pd.DataFrame(np.random.randint(0, 10, size=(10, 2)), columns=[\"A\", \"B\"])\r\n   ...: df\r\n   ...:\r\nOut[2]:\r\n   A  B\r\n0  5  0\r\n1  3  3\r\n2  7  9\r\n3  3  5\r\n4  2  4\r\n5  7  6\r\n6  8  8\r\n7  1  6\r\n8  7  7\r\n9  8  1\r\n```\r\n\r\nFor a concrete example, get the table-wise max (this is equivalent to `df.rolling(4).max().max(1)`)\r\n\r\n```python\r\nIn [10]: df.rolling(4).apply(np.max, axis=None)\r\nOut[10]:\r\n0    NaN\r\n1    NaN\r\n2    NaN\r\n3    9.0\r\n4    9.0\r\n5    9.0\r\n6    8.0\r\n7    8.0\r\n8    8.0\r\n9    8.0\r\ndtype: float64\r\n```\r\n\r\nA real example is something like a rolling OLS:\r\n\r\n```python\r\nimport statsmodels.api as sm\r\nf = lambda x: sm.OLS.from_formula('A ~ B', data=x).fit()  # wrong, but w/e\r\n\r\ndf.rolling(5).apply(f, axis=None)\r\n```"},{"labels":["api",null,null],"text":"Inspired by the timedelta plotting issue, I thought to look again at our timeseries plotting machinery. We know it is quite complex, and due to that several bugs, inconsistencies or unexpected behaviours exist (eg different results depending on order of plotting several serieses, wrong results when combining different types of time series, among others https://github.com/pandas-dev/pandas/issues/9053, https://github.com/pandas-dev/pandas/issues/6608, https://github.com/pandas-dev/pandas/issues/14322, ..). \r\nThere has been some discussion related to this on the tsplot refactor PR of @sinhrks https://github.com/pandas-dev/pandas/pull/7670 (not merged).\r\n\r\nOne of the reasons of the complexities is the distinction between 'irregular' and 'regular' time series (see eg https://github.com/pandas-dev/pandas/pull/7670#issuecomment-149235874):\r\n\r\n- 'regular' time series plotting is based on Periods, and is used for timeseries with a freq or inferred_freq (and also for periods)\r\n- 'irregular' time series plotting is based on the default matplotlib's handling of dates, i.e. converting to 'numerical values' (floats representing time in days since 0001-01-01, http://matplotlib.org/api/dates_api.html). You can always get this also for regular timeseries by passing `x_compat=True`.\r\n\r\nSo part of the problems and confusions comes from the differences between both (eg different label formatting) and from combining those two. Leading to the question:\r\n\r\n### Do we need both types of timeseries plotting?\r\n\r\nThe question is what the reason is that we convert DatetimeIndex to periods for plotting. The reasons I can think of:\r\n\r\n- **Performance**. Currently, the regular plotting is faster (so for a regular series `ts.plot()`is faster as `ts.plot(x_compat=True)`). However, I think this could be solved as most of the time is spent in converting the datetimes to floats (which should be vectorizable).\r\n- **Nicer tick label locations and formatting**. This is a clear plus, our (convoluted) ticklocators and formatters give much nicer results as the default matplotlib (IMO)\r\n\r\nOthers reasons that I am missing?\r\n\r\nBut, there are also clear drawbacks. Apart from the things mentioned above, you sometimes get clearly wrong behaviour: see eg the plot in https://github.com/pandas-dev/pandas/pull/7670#issuecomment-57410361. In this case, the dates somewhere within a month, are snapped to the month edges when first a regular series is plotted with monthy frequency. \r\nAnother example of 'wrong' plotting is a yearly series (bug with freq 'A-dec', so end of year) plotted in the beginning of a year. See http://nbviewer.jupyter.org/gist/jorisvandenbossche/c0c68dce2fa02f1dfc4a8c343ec88cb6. But of course, in many cases, this behaviour is can also be the desired behaviour.\r\n\r\nBut do we need both? Would we want, if possible, to unify into one approach?\r\n\r\n### Can we unify both approaches?\r\n\r\nCan we just use the matplotlib floats for timeseries plotting? Or always use the period-based machinery?\r\n\r\n* **Using matplotlib's float-based plotting**\r\n  * Do we want this? It will give slightly different behaviour for certain 'regular' cases.\r\n  * Assuming we can implement a similar tick locator/formatter comparable to period-based one. But, this may be impossible and the reason we have the current situation?\r\n  * But we could keep the PeriodConverter for purely plotting actual Periods\r\n  * Problem: float64 representing days can only give a precision of \\~5µs, not up to 1ns (note: the period-based plotting can also not handle ns, but can handle 1µs precision).\r\n* **Using period-based plotting for all timeseries**\r\n  * Do we want this? (deviates more from matplotlib -> larger difference in plotting dates with and without importing pandas)\r\n  * What prevents us from converting an irregular timeseries to Periods? I would think we can find some common freq in almost all cases? (just a high-precision freq if needed)\r\n* **Or create a new converter based on datetime64[ns] (so int64)**?\r\n  * Instead of using matplotlibs floats, and instead of varying freq Periods (at least for DatetimeIndex)\r\n  * Again, assuming we can have nice tick label locator/formatting for this\r\n\r\ncc @pandas-dev/pandas-core (especially @TomAugspurger and @sinhrks, I think you haven been most involved in plotting code recently, or @wesm for historical viewpoint)\r\nI know it's a long issue, but if you could give it a read and give your thoughts on this, very welcome!\r\n"},{"labels":["api",null,null],"text":"If no ``suffixes`` are specified, you get '_x', '_y' behavior.\r\n\r\n```\r\nIn [76]: a = pd.DataFrame(dict(a=np.arange(5), b=np.arange(5), c=np.arange(5)))\r\n\r\nIn [77]: b = pd.DataFrame(dict(a=np.arange(6), b=[0,1,2,3,5,6]))\r\n\r\nIn [78]: a\r\nOut[78]:\r\n   a  b  c\r\n0  0  0  0\r\n1  1  1  1\r\n2  2  2  2\r\n3  3  3  3\r\n4  4  4  4\r\n\r\nIn [79]: b\r\nOut[79]:\r\n   a  b\r\n0  0  0\r\n1  1  1\r\n2  2  2\r\n3  3  3\r\n4  4  5\r\n5  5  6\r\n\r\nIn [80]: res = pd.merge(a, b, left_index=True, right_index=True)\r\n\r\nIn [81]: res\r\nOut[81]:\r\n   a_x  b_x  c  a_y  b_y\r\n0    0    0  0    0    0\r\n1    1    1  1    1    1\r\n2    2    2  2    2    2\r\n3    3    3  3    3    3\r\n4    4    4  4    4    5\r\n```\r\n\r\nIf can be useful to allow ``suffix='expand'`` (or something like this). to enable an automatic\r\ncreation of a column multiindex (this is analagous to ``keys=`` arg in ``pd.concat``)\r\n\r\n```\r\nIn [82]: res.columns = res.columns.str.split('_', expand=True)\r\n\r\nIn [83]: res = res.sort_index(axis=1)\r\n\r\nIn [84]: res\r\nOut[84]:\r\n   a     b      c\r\n   x  y  x  y NaN\r\n0  0  0  0  0   0\r\n1  1  1  1  1   1\r\n2  2  2  2  2   2\r\n3  3  3  3  3   3\r\n4  4  4  4  5   4\r\n```\r\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'a': [np.nan, np.nan, np.nan, np.nan, np.nan, 1., np.nan],\r\n                   'b': [1., np.nan, np.nan, 1., np.nan, np.nan, np.nan]}).to_sparse()\r\nprint(type(df))\r\nprint(type(df.stack()))\r\n```\r\n\r\n> <class 'pandas.sparse.frame.SparseDataFrame'>\r\n> <class 'pandas.core.series.Series'>\r\n\r\n#### Problem description\r\n\r\nI'm trying to convert a SparseDataFrame (obtained it from pd.get_dummies()) into a scipy sparse matrix, by using the experimental .to_coo(). As this method accepts a MultiIndex Series, instead of a DataFrame, i call the .stack() method of this SparseDataFrame.\r\n\r\nThe problem is that it looks like the .stack() method doesn't process the SparseDataFrame as sparse, and instead stacks it as dense, consuming too much memory, and returning a (dense) Series.\r\n\r\nReturning a dense Series could be all right, as np.nan values are drop by default with the dropna parameters, but the memory consumption is a problem.\r\n\r\nI'm aware the whole sparse functionality is not yet mature. And I saw the function pd.sparse.frame.stack_sparse_frame which I guess it's a step to fix this problem (which doesn't work for me). But as I couldn't find a specific issue for this problem, I thought it was worth opening it.\r\n\r\n#### Expected Output\r\n\r\n> <class 'pandas.sparse.frame.SparseDataFrame'>\r\n> <class 'pandas.sparse.series.SparseSeries'>\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n> commit: None\r\n> python: 3.5.2.final.0\r\n> python-bits: 64\r\n> OS: Linux\r\n> OS-release: 4.7.5-100.fc23.x86_64\r\n> machine: x86_64\r\n> processor: x86_64\r\n> byteorder: little\r\n> LC_ALL: None\r\n> LANG: en_US.utf8\r\n> LOCALE: en_US.UTF-8\r\n> \r\n> pandas: 0.19.2+0.g825876c.dirty\r\n> nose: 1.3.7\r\n> pip: 9.0.1\r\n> setuptools: 23.0.0\r\n> Cython: 0.24\r\n> numpy: 1.11.1\r\n> scipy: 0.17.1\r\n> statsmodels: 0.6.1\r\n> xarray: None\r\n> IPython: 4.2.0\r\n> sphinx: 1.4.1\r\n> patsy: 0.4.1\r\n> dateutil: 2.5.3\r\n> pytz: 2016.4\r\n> blosc: None\r\n> bottleneck: 1.1.0\r\n> tables: 3.2.3.1\r\n> numexpr: 2.6.0\r\n> matplotlib: 1.5.1\r\n> openpyxl: 2.3.2\r\n> xlrd: 1.0.0\r\n> xlwt: 1.1.2\r\n> xlsxwriter: 0.9.2\r\n> lxml: 3.6.0\r\n> bs4: 4.4.1\r\n> html5lib: None\r\n> httplib2: None\r\n> apiclient: None\r\n> sqlalchemy: 1.0.13\r\n> pymysql: None\r\n> psycopg2: 2.6.1 (dt dec pq3 ext)\r\n> jinja2: 2.8\r\n> boto: 2.40.0\r\n> pandas_datareader: None\r\n> \r\n\r\n</details>\r\n"},{"labels":["api",null,null],"text":"#### Problem description\r\n\r\nThe `quantile` method is currently not supported on `resample`. While `resample(...).agg(lambda x: x.quantile(..))` works fine, it would be nice to add the `resample(..).quantile(..)` shortcut.\r\n\r\n#### Code Sample\r\n\r\n```\r\nIn [19]: ts = pd.Series(range(20), index=pd.date_range('2016-01-01', periods=20))\r\n\r\nIn [21]: ts.resample('W').agg(lambda x: x.quantile(0.75))\r\nOut[21]: \r\n2016-01-03     1.5\r\n2016-01-10     7.5\r\n2016-01-17    14.5\r\n2016-01-24    18.5\r\nFreq: W-SUN, dtype: float64\r\n```\r\n\r\nSo it would be nice the following would give the same result:\r\n```\r\nIn [22]: ts.resample('W').quantile(0.75)\r\n/home/joris/miniconda3/envs/dev/bin/ipython:1: FutureWarning: \r\n.resample() is now a deferred operation\r\nYou called quantile(...) on this deferred object which materialized it into a series\r\nby implicitly taking the mean.  Use .resample(...).mean() instead\r\n  #!/home/joris/miniconda3/envs/dev/bin/python\r\nOut[22]: 14.25\r\n```\r\n\r\nThe fact that this currently implicitly takes the mean before calculating the quantile (`ts.resample('W').mean().quantile(0.75)`) would make this change slightly API breaking.\r\n\r\n\r\nUsing pandas master, 0.19.0+289.g1bf94c8\r\n"},{"labels":["api",null],"text":"#### Problem description\r\n\r\nCurrently the date and time components on a DatetimeIndex return a numpy array. I would propose to let them return a new Index object (and in this way retaining all the interesting added functionality of an Index object).\r\n\r\nRelated to https://github.com/pandas-dev/pandas/pull/14506 that changed `Index.map` to return an Index instead of an array.\r\n\r\n#### Code Sample\r\n\r\n```\r\nIn [14]: ts = pd.Series(range(10), index=pd.date_range('2016-01-01', periods=10))\r\n\r\nIn [15]: ts\r\nOut[15]: \r\n2016-01-01    0\r\n2016-01-02    1\r\n2016-01-03    2\r\n2016-01-04    3\r\n2016-01-05    4\r\n2016-01-06    5\r\n2016-01-07    6\r\n2016-01-08    7\r\n2016-01-09    8\r\n2016-01-10    9\r\nFreq: D, dtype: int64\r\n\r\nIn [16]: ts.index.dayofweek\r\nOut[16]: array([4, 5, 6, 0, 1, 2, 3, 4, 5, 6], dtype=int32)\r\n```\r\n\r\nWhen the above would return an index, something that would become possible is this:\r\n\r\n```\r\nts[ts.index.dayofweek.isin([5,6])]\r\n```\r\n\r\nwhich is now not possible as an array has no isin method.\r\n\r\n\r\nUsing pandas master, 0.19.0+289.g1bf94c8"},{"labels":["api",null,null],"text":"followup to #14668 \r\n\r\nWhen you have an exception in *some* aggregation columns, need to be more friendly. We exclude them in .groupby aggs so need to do that here.\r\n\r\n```\r\nIn [1]: df = tm.makeMixedDataFrame()\r\n\r\nIn [2]: df\r\nOut[2]: \r\n     A    B     C          D\r\n0  0.0  0.0  foo1 2009-01-01\r\n1  1.0  1.0  foo2 2009-01-02\r\n2  2.0  0.0  foo3 2009-01-05\r\n3  3.0  1.0  foo4 2009-01-06\r\n4  4.0  0.0  foo5 2009-01-07\r\n\r\nIn [3]: df.agg('sum')\r\nOut[3]: \r\nA    10.0\r\nB     2.0\r\ndtype: float64\r\n\r\nIn [4]: df.agg(['sum'])\r\nValueError: no results\r\n\r\nIn [6]: df.agg(['min'])\r\nOut[6]: \r\n       A    B     C          D\r\nmin  0.0  0.0  foo1 2009-01-01\r\n```\r\n\r\nso [4] should prob not happen"},{"labels":["api",null,null],"text":"There are at least three things that many of the IO methods must deal with: reading from URL, reading/writing to a compressed format, and different text encodings. It would be great if all io functions where these factors were relevant could use the same code (consolidated codebase) and expose the same options (uniform API).\r\n\r\nIn https://github.com/pandas-dev/pandas/pull/14576, we consolidated the codebase but more consolidation is possible. In [`io.common.py`](https://github.com/pandas-dev/pandas/blob/dc4b0708f36b971f71890bfdf830d9a5dc019c7b/pandas/io/common.py), there are three functions that must be sequentially called to get a file-like object: `get_filepath_or_buffer`, `_infer_compression`,  and `_get_handle`. This should be consolidated into a single function, which can then delegate to sub functions.\r\n\r\nCurrently, pandas supports the [following io methods](http://pandas-docs.github.io/pandas-docs-travis/io.html). First for reading:\r\n\r\n- [ ] read_csv\r\n- [ ] read_excel\r\n- [ ] read_hdf\r\n- [ ] read_feather\r\n- [ ] read_sql\r\n- [ ] read_json\r\n- [ ] read_msgpack (experimental)\r\n- [ ] read_html\r\n- [ ] read_gbq (experimental)\r\n- [ ] read_stata\r\n- [ ] read_sas\r\n- [ ] read_clipboard\r\n- [ ] read_pickle\r\n\r\nAnd then for writing:\r\n\r\n- [ ] to_csv\r\n- [ ] to_excel\r\n- [ ] to_hdf\r\n- [ ] to_feather\r\n- [ ] to_sql\r\n- [ ] to_json\r\n- [ ] to_msgpack (experimental)\r\n- [ ] to_html\r\n- [ ] to_gbq (experimental)\r\n- [ ] to_stata\r\n- [ ] to_clipboard\r\n- [ ] to_pickle\r\n\r\nSome of these should definitely use the consilidated/uniform API, such as `read_csv`, `read_html`, `read_pickle`, `read_excel`.\r\n\r\nSome functions perhaps should be kept separate, such as `read_feather` or `read_clipboard`."},{"labels":["api",null],"text":"xref https://github.com/pandas-dev/pandas/issues/2567\r\n\r\n```\r\nIn [27]: data = pd.DataFrame({'hr1': [514, 573], 'hr2': [545, 526],\r\n    ...:                       'team': ['Red Sox', 'Yankees'],\r\n    ...:                       'year1': [2007, 2008], 'year2': [2008, 2008]})\r\n    ...: \r\n\r\nIn [28]: data\r\nOut[28]: \r\n   hr1  hr2     team  year1  year2\r\n0  514  545  Red Sox   2007   2008\r\n1  573  526  Yankees   2008   2008\r\n\r\nIn [29]: pd.lreshape(data, {'year': ['year1', 'year2'], 'hr': ['hr1', 'hr2']})\r\nOut[29]: \r\n      team  year   hr\r\n0  Red Sox  2007  514\r\n1  Yankees  2008  573\r\n2  Red Sox  2008  545\r\n3  Yankees  2008  526\r\n\r\nIn [30]: pd.wide_to_long(data, ['hr', 'year'], 'team', 'index')\r\nOut[30]: \r\n                hr  year\r\nteam    index           \r\nRed Sox 1      514  2007\r\nYankees 1      573  2008\r\nRed Sox 2      545  2008\r\nYankees 2      526  2008\r\n```\r\n\r\nSo we should drop one of these."},{"labels":["api",null,null],"text":"In 2.0, these functions could be named more consistently, either all `*na` or `*null`. Or, to preserve backwards compatibility, `isna`, `notna`, `fillnull` and `dropnull` could be added as aliases. Either way, it would be helpful to take the guess work out of this.\r\n\r\nLet me know if this is not the correct forum for such proposals."},{"labels":["api"],"text":"First of all, if I missed a point, please feel free to comment.\r\n\r\nUsing arithmetic operations on pd.DataFrames is sometimes a mouthful. Take the following example, where columns `a` and `b` should be multiplied by the column `c`:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnp.random.seed(0)\r\n\r\ndf = pd.DataFrame(np.random.randn(3, 3), columns=list('abc'))\r\n\r\ndf[['a', 'b']] * df['c']\r\n```\r\n\r\nApparently this doesn't work as expected. Instead one has to use either `pd.Dataframe.mul()`, which brings up poor legibility, or `pd.Dataframe.values`, which yields long lines and therefore also results in poor legibility:\r\n\r\n```python\r\n# using pd.DataFrame.mul()\r\ndf[['a', 'b']].mul(df['c'], axis='index')\r\n\r\n# This is quite short, but does not work...\r\ndf[['a', 'b']] * df[['c']].values\r\n\r\n# .. you have to use numpy arrays instead\r\ndf[['a', 'b']].values * df[['c']].values\r\n```\r\n\r\nSurely, the last call in this example returns a numpy array, but in my case thats the only thing I'm interested in, since I'm rewrapping my data at a later stage.\r\n\r\nI'm proposing a new short indexer for operating on values, sth like:\r\n\r\n```python\r\ndf.v[['a', 'b']] * df.v[['c']]\r\n\r\n# which returns the same as\r\ndf[['a', 'b']].values * df[['c']].values\r\n```\r\n\r\nOr even more sophisticated:\r\n```python\r\ndf[['a', 'b']] * df.v[['c']]\r\n\r\n# which returns the same as\r\ndf[['a', 'b']].mul(df['c'], axis='index')\r\n```\r\n\r\nBtw the same goes for all other arithmetic operators."},{"labels":["api",null],"text":"xref #14878\r\n\r\n#14878 deprecated the `raise_on_error` kwarg in favour of the `errors` kwarg \r\nfor `DataFrame.astype()`.\r\n\r\nThis issue addresses the same deprecation of the `raise_on_error` kwarg for \r\n`DataFrame.where()`. Again this should be replaced with the `errors` kwarg.\r\n\r\nThe `errors` kwarg can have the value `raise` or `ignore` with a default \r\nvalue of `raise`.\r\n\r\nThis issue can be assigned to me - @m-charlton"},{"labels":["api",null],"text":"#### Problem description\r\n\r\nCorrect me if I'm wrong but [TextFileReader](https://github.com/pandas-dev/pandas/blob/master/pandas/io/parsers.py#L691-L700) overwrites the `delimiter`, `quoting` etc kwds if the `dialect` kwarg is passed. But this isn't highlighted in the `pandas.read_table` documentation.\r\n\r\nI don't know if this was implicit. Sorry for the noise if this is a known issue or if it isn't actually one."},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas\r\ndf = pandas.DataFrame({'a': [1,2], 'b': [3,4]})\r\ndf.rename({'C':'D'})\r\n```\r\n#### Problem description\r\n\r\nI expected the method to throw an error, complaining that the key passed via\r\nthe dictionary isnt a valid column name in the DataFrame. Instead, nothing happens.\r\n\r\n#### Expected Output\r\nsome sort of error.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.1.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.0\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 23.1.0\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 4.1.2\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.2\r\npytz: 2016.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"xref https://github.com/pandas-dev/pandas/pull/14779\r\n\r\nSo I think it might be nice to have these as convenience methods on DataFrame().\r\nwe can deprecate (``pd.melt`` & ``pd.wide_to_long``), though we *didn't* deprecate ``pd.pivot_table``, even though it exists on ``DataFrame``, so this can be a discussion point.\r\n\r\nbut if we do this, then maybe have a shorter / better name than ``wide_to_long``? (even though it is strictly speaking pretty informative).\r\n\r\n"},{"labels":["api",null],"text":"This is closely related to #12392, but I think separate issue.  Proposal would be to be able to pass a list-like to `DataFrame.rename` to make method-chaining easier.  I think it would also be consistent with #11980 (Series rename)\r\n\r\n```python\r\ndf = pd.DataFrame({'a': [1,2], 'b': [3, 4]})\r\n\r\n# make this\r\ndf = df.rename(columns=['j', 'k'])\r\n\r\n# equivalent to \r\ndf.columns = ['j', 'k']\r\n```\r\n\r\n\r\n\r\n"},{"labels":["api",null,null],"text":"#### Problem description\r\n\r\nCurrently to append to a DataFrame, the following is the approach:\r\n```\r\ndf = pd.DataFrame(np.random.rand(5,3), columns=list('abc'))\r\ndf = df.append(pd.DataFrame(np.random.rand(5,3), columns=list('abc')))\r\n```\r\n\r\n`append` is a DataFrame or Series method, and as such should be able to modify the DataFrame or Series in place. If in place modification is not required, one may use `concat` or set `inplace` kwag to `False`. It will avoid an explicit assignment operation which is quite slow in Python, as we all know. Further, it will make the expected behavior similar to Python lists, and avoid questions such as these: [1](http://stackoverflow.com/questions/16597265/appending-to-an-empty-data-frame-in-pandas), [2](http://stackoverflow.com/questions/37009287/using-pandas-append-within-for-loop)...\r\n\r\nAdditionally at present, `append` is full subset of `concat`, and as such it need not exist at all. Given the vast number of functions to append a DataFrame or Series to another in Pandas, it makes sense that each has it's merits and demerits. Gaining an `inplace` kwag will clearly distinguish `append` from `concat`, and simplify code.\r\n\r\nI understand that this issue was raised in #2801 a long time ago. However, the conversation in that deviated from the simplification offered by the `inplace` kwag to performance enhancement. I (and many like me) are looking for ease of use, and not so much at performance. Also, we expect the data to fit in memory (which is a limitation even with current version of `append`). \r\n\r\n#### Expected Code\r\n```\r\ndf = pd.DataFrame(np.random.rand(5,3), columns=list('abc'))\r\ndf.append(pd.DataFrame(np.random.rand(5,3), columns=list('abc')), inplace=True)\r\n```\r\n"},{"labels":["api",null],"text":"`xarray` provides a decorator to register a custom accessor on a `Dataset` - see docs [here](http://xarray.pydata.org/en/stable/internals.html#extending-xarray).\r\n\r\nIt's a bit of a blunt tool, in that it applies to all instances, but outside of that, I found it to be a nice alternative to subclassing or a composition class.\r\n\r\nI haven't looked at the implementation, so not sure how hard it would be retrofit onto `DataFrame`\r\n\r\n@shoyer - any thoughts or feedback from people using it?"},{"labels":["api",null,null],"text":"add a ``.to_epoch(unit='s')`` method to ``Timestamp`` and ``DatetimeIndex`` that returns the epoch for that unit. I think would default this to ``s`` as that seems pretty common, but allow any of our units.\r\n```\r\nIn [19]: s = Series(pd.date_range('20160101',periods=3))\r\n\r\nIn [20]: s\r\nOut[20]: \r\n0   2016-01-01\r\n1   2016-01-02\r\n2   2016-01-03\r\ndtype: datetime64[ns]\r\n\r\nIn [21]: ((s-Timestamp(0)) / Timedelta('1s')).astype('i8')\r\nOut[21]: \r\n0    1451606400\r\n1    1451692800\r\n2    1451779200\r\ndtype: int64\r\n\r\n```"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nidx = pd.MultiIndex.from_product([['foo', 'bar'], ['a', 'b']],\r\n                                 names=['first', 'second'])\r\ndf = pd.DataFrame(np.random.randn(4, 2), index=idx).sort_index()\r\n\r\nbool_idx = pd.Series([True, False], index=['foo', 'bar'])\r\n\r\nprint ('dataframe')\r\nprint(df)\r\n\r\nprint('\\nbool idx')\r\nprint(bool_idx)\r\n\r\ntry:\r\n    df.loc[(bool_idx, slice(None)), :]\r\nexcept ValueError as e:\r\n    import traceback\r\n    print('\\nbool index in MultiIndex failed')\r\n    traceback.print_exc()\r\n\r\nno_multi_idx = df.reset_index(level=1)\r\nprint('\\nno multi index')\r\nprint(no_multi_idx)\r\n\r\nprint('successful indexing')\r\nprint(no_multi_idx.loc[bool_idx])\r\n```\r\n\r\n#### Problem description\r\n\r\nA level of a MultiIndex cannot be sliced by a boolean array shorter than the level. A single level of a MultiIndex can contain repeated values (as one level isn't the entire index), so slicing by a smaller array can be necessary.\r\nA single Index with repeated values can currently be sliced by a boolean array without repeated Index values. It would be a smoother user experience if the same feature existed for MultiIndex, especially since this is less of a corner case in with MultiIndex (because it doesn't require repeating index values). I would guess that the same or very similar code to the single Index case could be used to handle the MultiIndex case.\r\n\r\n#### Expected Output\r\n```\r\n                     0         1\r\nfirst second                    \r\nfoo   a      -0.569935 -1.146863\r\n      b      -2.087799 -0.506962\r\n```\r\n\r\nThis can currently be computed by calling ``reset_index``, slicing, and then calling ``set_index`` with append=True.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.19.0-15-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.4\r\npip: 9.0.1\r\nsetuptools: 18.0.1\r\nCython: 0.21.1\r\nnumpy: 1.11.2\r\nscipy: 0.14.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 3.2.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.4.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\nhttplib2: 0.9.2\r\napiclient: None\r\nsqlalchemy: 1.0.8\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n"},{"labels":["api",null,null],"text":"This is to discuss pushing the `Categorical.categories` and\r\n`Categorical.ordered` information into the extension type `CategoricalDtype`.\r\n\r\n```python\r\npd.CategoricalDtype(categories, ordered=False)\r\n```\r\n\r\nNote that there is no `values` argument. This is a type constructor, that\r\nisn't attached to any specific `Categorical` instance.\r\n\r\n## Why?\r\n\r\nSeveral times now (`read_csv(..., dtype=...)`, `.astype(...)`, `Series([], dtype=...)`)\r\nwe have places where we accept `dtype='category'` which takes the values\r\nin the method (the series, or column from the CSV, etc.)\r\nand hands it off to the *value* constructor, with no control over the\r\n`categories` and `ordered` arguments.\r\n\r\n```python\r\nCategorical(values, categories=None, ordered=False)\r\n```\r\n\r\nThe proposal here would add the `categories` and `ordered`\r\nattributes / arguments to `CategoricalDtype` and provide a common API\r\nfor specifying non-default parameters for the `Categorical` constructor\r\nin methods like `read_csv`, `astype`, etc.\r\n\r\n\r\n```python\r\nt = pd.CategoricalDtype(['low', 'med', 'high'], ordered=True)\r\npd.read_csv('foo.csv', dtype={'A': int, 'B': t)\r\npd.Series(['high', 'low', 'high'], dtype=t)\r\n\r\ns = pd.Series(['high', 'low', 'high'])\r\ns.astype(t)\r\n```\r\n\r\nWe would continue to accept `dtype='category'`.\r\n\r\nThis becomes even more import when doing operations on larger than memory datasets with something like `dask` or even (`read_csv(..., chunksize=N)`). Right now you don't have an easy way to specify the `categories` or `ordered` for columns (assuming you know them ahead of time).\r\n\r\n## Issues\r\n\r\n1. `CategoricalDtype` currently isn't part of the public API. Which methods\r\non it do we make public?\r\n2. Equality semantics: For backwards compat, I think all instances\r\nof `CategoricalDtype` should compare equal with all others. You can use\r\nidentity to check if two types are the same\r\n\r\n```python\r\nt1 = pd.CategoricalDtype(['a', 'b'], ordered=True)\r\nt2 = pd.CategoricalDtype(['a', 'b'], ordered=False)\r\n\r\nt1 == t2  # True\r\nt1 is t2  # False\r\nt1 is t1  # True\r\n```\r\n\r\n3. Should the `categories` argument be required? Currently `dtype='category'`\r\nsays 1.) infer the categories based on the *values*, and 2.) it's unordered.\r\nWould `CategoricalDtype(None, ordered=False)` be allowed?\r\n4. Strictness? If I say\r\n\r\n```python\r\npd.Series(['a', 'b', 'c'], dtype=pd.CategoricalDtype(['a', 'b']))\r\n```\r\n\r\nWhat happens? I would probably expect a `TypeError` or `ValueError` as `c`\r\nisn't \"supposed\" to be there. Or do we replace `'c'` with `NA`? Should\r\n`strict` be another parameter to `CategoricalDtype` (I don't think so).\r\n\r\nI'm willing to work on this over the next couple weeks.\r\n\r\nxref https://github.com/pandas-dev/pandas/issues/14676 (astype)\r\nxref https://github.com/pandas-dev/pandas/issues/14503 (read_csv)"},{"labels":["api",null],"text":"Lets say we have data with both column and row headers. Lets say this dataset also simply outputs a ```0``` in that csv file for the cell at the intersection of the column and row headers (A1 in excel notation, or cell (0,0) in the csv file). Additionally, both ```\"0\"``` and ```\"0.1\"``` are valid column names:\r\n\r\nRH\\\\CH|-0.1|0|0.1\r\n---|---|---|---\r\n10.0|123|456|789\r\n20.0|012|345|678\r\n\r\nThus, \"RH\\\\CH\" is replaced by ```\"0\"``` on export (of which I have no control). \r\n\r\n```python\r\nimport pandas as pd\r\nfrom cStringIO import StringIO\r\n\r\ndata = '0,-0.1,0,0.1\\n10.0,123,456,789\\n20.0,012,345,678'\r\ndf = pd.read_csv(StringIO(data),index_col=0,header=0)\r\nprint(df)\r\n\r\n      -0.1  0.1  0.1\r\n0                   \r\n10.0   123  789  789\r\n20.0    12  678  678\r\n\r\n```\r\n\r\nThe name mangling will change the duplicate ```\"0\"``` column containing data to ```\"0.1\"```, thus if a real data column has the name ```\"0.1\"```, this data will be copied (?) back to the mangled duplicate ```\"0\"``` column. Additionally, if the true ```\"0.1\"``` data name is missing, then this name mangling is frustrating as there is no obvious way to determine if the now present ```\"0.1\"``` column is a duplicate ```\"0\"``` that has been mangled or is a real ```\"0.1\"``` data series.\r\n\r\nI propose the addition of a ```mangle_dupe_cols_str``` keyword option that defaults to ```'.'``` to preserve the current behavior. However, it can be passed as a kwarg to ```read_csv``` in cases where the period name mangling could result in further duplicate columns.\r\n\r\nIn https://github.com/pandas-dev/pandas/blob/v0.19.1/pandas/io/parsers.py#L2109-L2111:\r\n```python\r\n                        if cur_count > 0:\r\n                            this_columns[i] = '%s.%d' % (col, cur_count)\r\n                        counts[col] = cur_count + 1\r\n```\r\nshould be adapted to \r\n```python\r\n                        if cur_count > 0:\r\n                            this_columns[i] = '%s%s%d' % (col, self.mangle_dupe_cols_str, cur_count)\r\n                        counts[col] = cur_count + 1\r\n```\r\nand lines 1042-1043 in https://github.com/pandas-dev/pandas/blob/v0.19.1/pandas/io/parsers.py#L1042-L1043 changed to:\r\n```python\r\n        self.mangle_dupe_cols = kwds.get('mangle_dupe_cols', True)\r\n        self.mangle_dupe_cols_str = kwds.get('mangle_dupe_cols_str', '.')        \r\n        self.infer_datetime_format = kwds.pop('infer_datetime_format', False)\r\n```\r\n\r\n#### Expected Output\r\nThen the corrected output would be:\r\n```python\r\n>>> data = '0,-0.1,0,0.1\\n10.0,123,456,789\\n20.0,012,345,678'\r\n>>> df = pd.read_csv(StringIO(data),index_col=0,header=0,mangle_dupe_cols_str='__')\r\n>>> print(df)\r\n\r\n      -0.1  0__1  0.1\r\n0                   \r\n10.0   123  456  789\r\n20.0    12  345  678\r\n\r\n```\r\nwhere subsequent operations could identify any columns with the mangle_dupe_cols_str, if needed. The current behavior silently causes data loss for certain column names.\r\n\r\n<details>\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: None\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: 1.1.3\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: None\r\n\r\n</details>\r\n"},{"labels":["api",null,null,null],"text":"#### A small, complete example of the issue\r\n\r\nThis is a proposal to allow something like\r\n\r\n```python\r\ndf.astype({'A': pd.CategoricalDtype(['a', 'b', 'c', 'd'], ordered=True})\r\n```\r\n\r\nCurrently, it can be awkward to convert many columns in a DataFrame to a Categorical *with control over the categories and orderedness*. If you just want to use the defaults, it's not so bad with `.astype`:\r\n\r\n```python\r\nIn [5]: df = pd.DataFrame({\"A\": list('abc'), 'B': list('def')})\r\n\r\nIn [6]: df\r\nOut[6]:\r\n   A  B\r\n0  a  d\r\n1  b  e\r\n2  c  f\r\n\r\nIn [8]: df.astype({\"A\": 'category', 'B': 'category'}).dtypes\r\nOut[8]:\r\nA    category\r\nB    category\r\ndtype: object\r\n```\r\n\r\nIf you need to control `categories` or `ordered`, your best off with\r\n\r\n```python\r\nIn [20]: mapping = {'A': lambda x: x.A.astype('category').cat.set_categories(['a', 'b'], ordered=True),\r\n    ...:            'B': lambda x: x.B.astype('category').cat.set_categories(['d', 'f', 'e'], ordered=False)}\r\n\r\nIn [21]: df.assign(**mapping)\r\nOut[21]:\r\n     A  B\r\n0    a  d\r\n1    b  e\r\n2  NaN  f\r\n```\r\n\r\nBy expanding astype to accept instances of `Categorical`, you remove the need for the `lambda`s and you can do conversions of other types at the same time.\r\n\r\nThis would mirror the semantics in https://github.com/pandas-dev/pandas/issues/14503\r\n\r\nUpdated to change `pd.Categorical(...)` to a new/modified `pd.CategoricalDtype(...)` based on the discussion below."},{"labels":["api",null],"text":"As of <a href=\"https://github.com/pandas-dev/pandas/tree/1d6dbb41b26a39121ec8c4f19f5da78bb0ab4af7/pandas\">1d6dbb</a>, the behaviour, documentation, and signatures for `to_dense` are widely inconsistent:\r\n\r\n<a href=\"https://github.com/pandas-dev/pandas/blob/1d6dbb41b26a39121ec8c4f19f5da78bb0ab4af7/pandas/sparse/array.py#L382\">SparseArray</a>: documentation is incorrect (we are not converting a `SparseSeries`), and the `fill` parameter is not even respected if you trace the code.  Not sure why it was ever there in the first place.\r\n\r\n<a href=\"https://github.com/pandas-dev/pandas/blob/1d6dbb41b26a39121ec8c4f19f5da78bb0ab4af7/pandas/sparse/series.py#L529\">SparseSeries</a>: What is the purpose of `sparse_only`?  Not only is it not documented, but it also is inconsistent with `SparseArray` behaviour.\r\n\r\n<a href=\"https://github.com/pandas-dev/pandas/blob/1d6dbb41b26a39121ec8c4f19f5da78bb0ab4af7/pandas/sparse/frame.py#L231\">SparseDataFrame</a>: Perhaps the most intuitive of them all.  Documentation is correct, and it has no unnecessary parameters like the other two do.\r\n\r\nI propose that we change the `to_dense` behaviour to match that of `SparseDataFrame`.  Take no arguments and just convert to a dense object with no questions asked."},{"labels":["api",null],"text":"Resampling timeseries with Pandas resample and ohlc method and Pandas DataReader should output DataFrame with same columns name to provide a consistent columns naming.\r\n\r\nCurrently this is not consistent.\r\n\r\n```python\r\nIn [1]: import pandas_datareader as pdr\r\n\r\nIn [2]: df = pdr.DataReader(\"IBM\", \"google\")\r\nIn [2]: df\r\nOut[2]:\r\n              Open    High     Low   Close   Volume\r\nDate\r\n2010-01-04  131.18  132.97  130.85  132.45  6155846\r\n2010-01-05  131.68  131.85  130.10  130.85  6842471\r\n2010-01-06  130.68  131.49  129.81  130.00  5605290\r\n2010-01-07  129.87  130.25  128.91  129.55  5840569\r\n2010-01-08  129.07  130.92  129.05  130.85  4197105\r\n...            ...     ...     ...     ...      ...\r\n2016-11-02  152.48  153.34  151.67  151.95  3074438\r\n2016-11-03  152.51  153.74  151.80  152.37  2878803\r\n2016-11-04  152.40  153.64  151.87  152.43  2470368\r\n2016-11-07  153.99  156.11  153.84  155.72  3706214\r\n2016-11-08  154.56     NaN     NaN  155.17  3921944\r\n```\r\n\r\nPandas DataReader uses `Open`, `High`, `Low`, `Close` column name for candlesticks\r\n\r\n```python\r\nIn [3]: price = df[\"Close\"]\r\nIn [4]: price.resample(\"W\").ohlc()\r\nOut[4]:\r\n                  open        high         low       close\r\nDate\r\n2010-01-10  132.449997  132.449997  129.550003  130.850006\r\n2010-01-17  129.479996  132.309998  129.479996  131.779999\r\n2010-01-24  134.139999  134.139999  125.500000  125.500000\r\n2010-01-31  126.120003  126.330002  122.389999  122.389999\r\n2010-02-07  124.669998  125.660004  123.000000  123.519997\r\n...                ...         ...         ...         ...\r\n2016-10-16  157.020004  157.020004  153.720001  154.449997\r\n2016-10-23  154.770004  154.770004  149.630005  149.630005\r\n2016-10-30  150.570007  153.350006  150.570007  152.610001\r\n2016-11-06  153.690002  153.690002  151.949997  152.429993\r\n2016-11-13  155.720001  155.720001  155.720001  155.720001\r\n\r\n[358 rows x 4 columns]\r\n```\r\n\r\nPandas uses `open`, `high`, `low`, `close` column name for candlesticks\r\n\r\nWe should change this on pandas side or on pandas-datareader side.\r\n\r\nPS: on pandas_datareader side\r\nhttps://github.com/pydata/pandas-datareader/issues/260"},{"labels":["api",null,null,null,null],"text":"Follow-up of https://github.com/pandas-dev/pandas/pull/14545. \r\n\r\nWe had a long discussion on what the behaviour of `concat` should be when you have categorical data: https://github.com/pandas-dev/pandas/pull/13767. In the end, for 0.19.0, we changed the behaviour of raising an error when categories didn't match to returning object dtyped data (only data with identical categories and ordered attributed gives a categorical as result). The table below is a summary of the changes between 0.18.1 and 0.19.0:\r\n\r\n**For categorical Series:**\r\n\r\n| left         | right        | append/concat 0.18 | append/concat 0.19.0    |\r\n|---------|-------------------------------------------|---------|------------------------------|\r\n| category | category (identical categories) | category | category |\r\n| category | category (different categories) | error | object |\r\n| category | not category | category | object  |\r\n| category | not category (different categories) | category with NaNs | object |\r\n\r\nHowever, we didn't change behaviour of `append` for Indexes (the above append is for series):\r\n\r\n**For `CategoricalIndex`:**\r\n\r\n| left         | right        | append 0.18 | append 0.19.0    | append 0.19.1 |\r\n|---------|-------------------------------------------|---------|------------------------------|----|\r\n| category | category (identical categories) | category | category | category |\r\n| category | category (different categories) | error | error  | error  |\r\n| category | not category | category | category |  category | \r\n| category | not category (with other values) | error | error  | error  |\r\n| not category | category (with other values) | object | error | object\r\n\r\nThe last line, i.e. the case where the calling Index is not a CategoricalIndex, changed by accident in 0.19.0, and it is this that I corrected for in PR #14545 for 0.19.1.\r\n\r\nQuestions:\r\n\r\n* Do we want the same behaviour for `Index.append` as we now have for `Series.append` with categorical data? This means that the column in the table above becomes 'object' apart from the first row.\r\n* Do we want to make an exception for the case where the values in the 'right' correspond with the categories? (so that `pd.CategoricalIndex(['a', 'b', 'c']).append(pd.Index(['a']))`keeps working)\r\n\r\nChanging this to always return object dtype unless for categoricals with indentical categories is easy, but gives a few failures in our test suite. Namely, in some indexing tests (indexing a DataFrame with a CategoricalIndex) there are changes in behaviour because indexing with a non-existing value in the index was performed using `CategoricalIndex.append()`. But this we can workaround in the indexing code of course.\r\n\r\ncc @janschulz @sinhrks "},{"labels":["api",null,null],"text":"Currently the group-by-aggregation in pandas will create MultiIndex columns if there are multiple operation on the same column. However, this introduces some friction to reset the column names for fast filter and join. (If all operations could be chained together, analytics would be smoother)\r\n\r\n```python\r\ndf = pd.DataFrame([\r\n        ['A', 1],\r\n        ['A', 2],\r\n        ['A', 3],\r\n        ['B', 4],\r\n        ['B', 5],\r\n        ['B', 6]\r\n    ], columns=['Key', 'Value'])\r\n\r\ndf1 = df.groupby('Key') \\\r\n    .agg({\r\n            'Value': {'V1': 'sum', 'V2': 'count'}\r\n        })\r\n\r\ndf1.columns = df1.columns.droplevel() # This line introduce friction\r\ndf1.query('V1 > V2')\r\n```\r\n\r\n#### Expected Output\r\nIt would be great if there is a simple alias function for columns (like the pyspark's implementation), such as\r\n```python\r\n\r\n# Just one approach, there may be others more appropriate\r\ndf.groupby('Key') \\\r\n    .agg(\r\n            pd.Series.sum('Value').alias('V1'),\r\n            pd.Series.count('Value').alias('V2')\r\n        ) \\\r\n    .query('V1 > V2')\r\n```"},{"labels":["api",null,null,null],"text":"In https://github.com/pandas-dev/pandas/pull/13406 Chris added support for `read_csv(..., dtype={'col': 'category'})` (thanks!). This issue is for expanding that syntax to allow a more complete specification of the resulting categorical.\n\n``` python\n# Your code here\ndf = pd.read_csv(path, dtype={'col': pd.Categorical(['a', 'b', 'c'], ordered=True})\ndf = pd.read_csv(path, dtype={'col': ['a', 'b', 'c']})  # shorthand, but unordered only\n# we would still accept `dtype={'col': 'category'}` as well, to infer categories\n```\n\nImplementation-wise, I think we can keep all the parsing logic as is, and simply loop over `dtype` and call `set_categories` (and maybe `as_ordered`) on all the categoricals just before returning to the user.\n\nThis would help a bit in dask, where their category type inference can fail if the first partition doesn't contain all the categories (see https://github.com/dask/dask/issues/1705). This is why it'd be preferable to do it as an option to `read_csv`, rather than putting in on the user to followup with a `set_categories`.\n"},{"labels":["api",null,null],"text":"Sometimes pickle files saved in Python v3.x are needed to read in Python v2.x. It would be nice if one can easily set pickle protocol version in `to_pickle()`.\n\nIt can be done with the following little changes:\n\nIn file `/pandas/io/pickle.py`:\n\n```\ndef to_pickle(obj, path, protocol=pkl.HIGHEST_PROTOCOL):\n    \"\"\"\n    Pickle (serialize) object to input file path\n\n    Parameters\n    ----------\n    obj : any object\n    path : string\n        File path\n    \"\"\"\n    with open(path, 'wb') as f:\n        pkl.dump(obj, f, protocol=protocol)\n```\n\nIn file `/pandas/core/generic.py`:\n\n```\n    def to_pickle(self, path, protocol):\n        \"\"\"\n        Pickle (serialize) object to input file path.\n\n        Parameters\n        ----------\n        path : string\n            File path\n        \"\"\"\n        from pandas.io.pickle import to_pickle\n        return to_pickle(self, path, protocol)\n```\n"},{"labels":["api",null,null],"text":"Copying from https://github.com/pandas-dev/pandas/issues/14448#issuecomment-255068330 (as the issue is closed now):\n\n```\nIn [1]: pd.to_datetime('13000101', errors='ignore')\nOut[1]: '13000101'\n\nIn [2]: pd.to_datetime('13000101', errors='ignore', format='%Y%m%d')\nOut[2]: datetime.datetime(1300, 1, 1, 0, 0)\n```\n\nThe above inconsistency (which is also not documented at all I think), is that something we want to fix?\n"},{"labels":["api",null,null,null],"text":"Assume we have a simple DF like this\n\n```\nx = pd.DataFrame({'a':[1,1,2,2],'b':['a','a','a','b'],'c':[[1,2],[3,4],[5,6],[7,8]]})\n\n   a  b       c\n0  1  a  [1, 2]\n1  1  a  [3, 4]\n2  2  a  [5, 6]\n3  2  b  [7, 8]\n```\n\nand we want to do a groupby operation that concatenates the lists in column `c` together.  This works fine when grouping on one column:\n\n```\nx.groupby('b')['c'].sum()\n\nb\na    [1, 2, 3, 4, 5, 6]\nb                [7, 8]\ndtype: object\n\nx.groupby('a')['c'].sum()\n\na\n1    [1, 2, 3, 4]\n2    [5, 6, 7, 8]\ndtype: object\n```\n\nBut fails when grouping on multiple columns (i.e. `x.groupby(['a','b'])['c'].sum()`) with `ValueError: Function does not reduce`.\n\nThis is based on this SO question: https://stackoverflow.com/questions/40224793/pandas-concatenate-list-columns-with-groupby-when-grouping-on-multiple-columns/40226389#40226389\n#### Output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.12.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 17.1.1\nCython: 0.22.1\nnumpy: 1.11.2\nscipy: 0.18.1\nstatsmodels: 0.6.1\nxarray: 0.8.2\nIPython: 5.1.0\nsphinx: 1.3.1\npatsy: 0.3.0\ndateutil: 2.5.3\npytz: 2015.4\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.0\nnumexpr: 2.6.1\nmatplotlib: 1.4.3\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 1.0.0\nxlsxwriter: 0.7.3\nlxml: 3.4.4\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.5\npymysql: None\npsycopg2: None\njinja2: 2.7.3\nboto: 2.33.0\npandas_datareader: None\n"},{"labels":["api",null],"text":"When using DataFrame.apply(), it would be helpful to be able to apply different functions to different columns. There's a nice workaround on [StackOverflow](http://stackoverflow.com/questions/26434123/pandas-apply-different-functions-to-different-columns):\n\n``` python\nimport numpy as np\nimport pandas as pd\n\ndef multi_func(functions):\n    def f(col):\n        return functions[col.name](col)\n    return f\n\ndf = pd.DataFrame(np.random.random((10, 2)), columns=['A', 'B'])\nresults = df.apply(multi_func({'A': np.mean, 'B': np.sum}))\n\nresults\nA    0.401456\nB    6.845529\ndtype: float64\n```\n\nMy guess is that the changes would be made in [pandas.core.frame.DataFrame](https://github.com/pandas-dev/pandas/blob/master/pandas/core/frame.py), potentially adding a new `_apply_X` method. Any thoughts/advice?\n"},{"labels":["api",null,null,null,null],"text":"Pandas DataFrames containing text columns are expensive to serialize.  This affects dask.dataframe performance in multiprocessing or distributed settings.\n### Pickle is expensive\n\nIn particular the current solution of using `pickle.dumps` for object dtype columns can be needlessly expensive when all of the values in the column are text.  In this case fairly naive solutions, like `msgpack` can be much much faster.  \n\nHere is an old blogpost on the topic: http://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization\n\nAnd an image\n\n![](https://mrocklin.github.com/blog/images/serialize.png)\n### Alternatives\n\nThere are naive solutions like msgpack (already in pandas) or encoding the text directly.\n\nThere are more sophisticated solutions as well that would provide efficient packing, including with repeated elements.\n### But sometimes objects are objects\n\nOne concern here is that sometimes the Python objects aren't text.  I propose that Pandas does a check each time or asks for forgiveness on an exception.\n\nAnyway, I would find this valuable.  It would help to reduce bottlenecks in dask.dataframe in some situations.\n"},{"labels":["api",null],"text":"#### Create a dataframe with a variable of dtype category and then convert to a panel\n\n``` python\n# A dataframe with a category variable\ndf = pd.DataFrame({ 'day' : [1,2,3], 'month' : ['a','b','c'], 'dow' : ['mon','mon','thur' ]})\ndf = df.set_index(['day','month'])\ndf.dow = df.dow.astype('category')\n\n# Transform to panel and check the dtype\ndf.to_panel().dtypes\n#### Expected Output\ndow    category\ndtype: object\n```\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.24\nnumpy: 1.11.1\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: 1.4.1\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.2\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.40.0\npandas_datareader: None\n# The  variable was converted from a category to an object when put into a panel\n\ndow    object\ndtype: object\n</details>\n"},{"labels":["api",null,null,null],"text":"For even numbered periods the centered moving average is calculated incorrectly. Suppose the period length is 5. Then the center of 5 periods is 3. However if the period length is 4 then the center of the period is 2.5. The value at index 3 should be the average of the values at 2.5 and 3.5. Pandas is showing the 2.5 value at 3 which is incorrect.\n\nEXAMPLE:\na=pd.Series(range(1,6), index=range(1,6))\na.rolling(4, center=True).mean()\n\n1    NaN\n2    NaN\n3    2.5\n4    3.5\n5    NaN\n"},{"labels":["api",null],"text":"#### A small, complete example of the issue\n\n``` python\nimport pandas as pd\n\naxis_length = 1\ndf = pd.Series([0])\ndf.sample(frac=2**(-axis_length))\n\nSeries([], dtype: int64)\n\n```\n#### Expected Output\n\nSeries([1], dtype: int64)\n#### Why?\n\nI tried to sample fractions of groups, some groups containted only one entry and were not returned in the sampled frame ( I used `groupby(key).apply(lambda x:x.sample)`).\nA potential fix may be easy: in generic.py line 2634 might be changed from\n\n``` python\nn = int(round(frac * axis_length))\n```\n\nto\n\n``` python\nn = max(int(round(frac * axis_length)),1)\n```\n\nIf this behaviour is not desired, maybe one could add to the API some keyword like, min_sample_size\n#### Further Remarks\n\nThe example can be generalized using np.arange(axis_length). The bound $2^{-axis_length}$ seems to be hard. Increasing the value would return at least one sample entry.\n\nEdit: Depending on the discussion, I'd love to provide a pull request by myself. I think, it's a good start to support pandas and it fits my skill level\n#### Output of `pd.show_versions()`\n\nSeries([], dtype: int64)\n<details>\n# Paste the output here\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\nLOCALE: None.None\n\npandas: 0.19.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 27.2.0\nCython: 0.24.1\nnumpy: 1.11.2\nscipy: 0.18.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 5.1.0\nsphinx: 1.3.1\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.7\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.2\nnumexpr: 2.6.1\nmatplotlib: 1.5.1\nopenpyxl: 2.4.0\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.3\nlxml: 3.6.4\nbs4: 4.5.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.1.0\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.42.0\npandas_datareader: None\n</details>\n"},{"labels":["api",null],"text":"#### A small, complete example of the issue\n\nAs called out in #14381, if `dtype` is not specified, values of `None` should be coerced to `np.nan`.  However, when a list of only `None` is passed to `data`, the `None` remains.\n\n``` python\n>>> pd.DataFrame([None])\n      0\n0  None\n>>> pd.DataFrame([None, None])\n      0\n0  None\n1  None\n```\n#### Expected Output\n\n``` python\n>>> pd.DataFrame([None])\n      0\n0  NaN\n>>> pd.DataFrame([None, None])\n      0\n0  NaN\n1  NaN\n```\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.16.0-77-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: None.None\n\npandas: 0.19.0\nnose: 1.3.4\npip: 8.1.2\nsetuptools: 5.8\nCython: 0.21\nnumpy: 1.11.2\nscipy: 0.16.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.0.0\nsphinx: 1.2.3\npatsy: 0.3.0\ndateutil: 2.5.3\npytz: 2016.7\nblosc: None\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.5.0\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.7\nlxml: 3.4.0\nbs4: 4.3.2\nhtml5lib: None\nhttplib2: 0.9.1\napiclient: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.7.3\nboto: 2.32.1\npandas_datareader: None\n</details>\n"},{"labels":["api",null,null,null],"text":"- [x] `.groupby` #5677\r\n- [ ] `.sort_values` #14353, though this is directly in combat with `.sort_index` and non-explict\r\n- [x] `.merge` (this issue)\r\n#### Overview\r\n\r\n@jorisvandenbossche \r\nAs a part of the [Pandas 1.0](https://docs.google.com/document/d/151ct8jcZWwh7XStptjbLsda6h2b3C0IuiH_hfZnUA58/edit#) goal to \"Make the index/column distinction less painful (#5677, #8162)\" I propose that the `df.merge` method support merging DataFrames on a combination of columns and index levels.\r\n\r\nThis could be accomplished in the API by allowing the `on`, `left_on`, and `right_on` keywords to accept a combination of column names and index level names.  Any index levels that are joined on would be preserved as index levels in the resulting merged DataFrame, while all other index levels would be removed.\r\n\r\nThis proposal is in the spirit of #5677 for `df.groupby`  and #14353 for `df.sort_values`.\r\n"},{"labels":["api",null],"text":"#### Background\n\nDuring the review of @jreback's PR last year to cleanup the sorting API (#10726) there was some discussion of how the DataFrame API could eventually support sorting by a combination of columns and index levels.  I'm interested in working on implementing this soon and would like to continue the discussion of where this should fit into the DataFrame sorting API.\n\nIn https://github.com/pydata/pandas/pull/10726#issuecomment-128066486 @jorisvandenbossche made the following suggestion\n\n> If we want to add this enhancement to simultaneously specify to sort on index levels and columns (the 5d option of above), then the question is: where do we add this functionality and how? In sorted, sort_index or both? I would then lean towards saying: only add it in sorted, where the by keyword can also denote a index level name.\n\nThis approach makes good sense to me. Each object passed to the `by` keyword of `sort_values` (referred to as `sorted` in the quote above) could refer to either a column or an index level.  For backwards compatibility, column references would take precedence. And my assumption is that we would want to preserve the index when sorting by a combination of columns and index levels this way.\n\nThis proposal is the sorting analog of the groupby proposal in #5677 (which I will be working on soon)\n"},{"labels":["api",null,null],"text":"from the timeseries realease example: [http://pandas.pydata.org/pandas-docs/version/0.19.0/whatsnew.html#merge-asof-for-asof-style-time-series-joining](http://pandas.pydata.org/pandas-docs/version/0.19.0/whatsnew.html#merge-asof-for-asof-style-time-series-joining)\nmaking ticker as category:\n\n``` python\ntrades.ticker = trades.ticker.astype('category') \nquotes.ticker = quotes.ticker.astype('category') \n\nmerge = pd.merge_asof(trades, quotes,\n                  on='time',\n                  by='ticker')\n```\n#### Expected Output\n\nmerge.dtypes\nOut[31]:\ntime        datetime64[ns]\nticker            category\nprice              float64\nquantity             int64\nbid                float64\nask                float64\n\ngeting :\ntime        datetime64[ns]\nticker              object\nprice              float64\nquantity             int64\nbid                float64\nask                float64\n"},{"labels":["api",null],"text":"I could not find any public documentation for `pandas.json.dump`, so I guess this makes it private? Or is it public and I can depend on it?\n\nWould you consider making this function public, or at least as public as `pandas.json.dumps` which are at least imported into/as `pandas.io.json.dumps`. \n\nI find the performance of `pandas.json.dump` to be an improvement on the normal json module.\n"},{"labels":["api",null],"text":"#### A small, complete example of the issue\n\n``` python\nfrom sqlalchemy import create_engine\nimport pandas as pd\n\nengine = create_engine('sqlite://')\nconn = engine.connect()\nconn.execute(\"create table test (a float)\")\nfor _ in range(5):\n    conn.execute(\"insert into test values (NULL)\")\n\ndf = pd.read_sql_query(\"select * from test\", engine, coerce_float=True)\nprint(df.a)\n```\n#### Expected Output\n\nIn pandas 0.18.1 this will result in a column of type `object` with `None` values, whereas I needed float(\"nan\"). The coerce_float=True option made no difference. This is most needed, when reading in a float column _chunk-wise_, since there may be sequences of NULLs.\n\n(also http://stackoverflow.com/questions/30652457/adjust-pandas-read-sql-query-null-value-treatment/)\n"},{"labels":["api",null,null,null],"text":"#### A small, complete example of the issue\n\n``` python\na = pd.Series([1.,3.,4.,3.,5.,6.,7.,8.], ['2016-05-25 00:00:35','2016-05-25 00:00:50','2016-05-25 00:01:05','2016-05-25 00:01:35','2016-05-25 00:02:05','2016-05-25 00:03:00','2016-05-25 00:04:00','2016-05-25 00:06:00'])                                   \n\nIn [79]: a\nOut[79]: \n2016-05-25 00:00:35    1.0\n2016-05-25 00:00:50    3.0\n2016-05-25 00:01:05    4.0\n2016-05-25 00:01:35    3.0\n2016-05-25 00:02:05    5.0\n2016-05-25 00:03:00    6.0\n2016-05-25 00:04:00    7.0\n2016-05-25 00:06:00    8.0\ndtype: float64\n\nIn [80]: a.index = pd.to_datetime(a.index)\n\nIn [81]: a.resample('15S', base=5).interpolate()\n\n\n```\n#### Expected Output\n\nI expect that I would get valid values, based on the input at 2:05 and later.  It appears that the data data after 2:05 is ignored.  \n#### Output of `pd.show_versions()`\n\nIn [146]: pd.show_versions()\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.12.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 27.2.0\nCython: 0.24.1\nnumpy: 1.11.1\nscipy: 0.18.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 5.1.0\nsphinx: 1.4.6\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.3.1\nnumexpr: 2.6.1\nmatplotlib: 1.5.2\nopenpyxl: 2.4.0\nxlrd: 1.0.0\nxlwt: None\nxlsxwriter: None\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: 1.0b3\nhttplib2: 0.9.2\napiclient: 1.5.0\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n<details>\n# Paste the output here\n\nOut[81]: \n2016-05-25 00:00:35    1.0\n2016-05-25 00:00:50    3.0\n2016-05-25 00:01:05    4.0\n2016-05-25 00:01:20    3.5\n2016-05-25 00:01:35    3.0\n2016-05-25 00:01:50    4.0\n2016-05-25 00:02:05    5.0\n2016-05-25 00:02:20    5.0\n2016-05-25 00:02:35    5.0\n2016-05-25 00:02:50    5.0\n2016-05-25 00:03:05    5.0\n2016-05-25 00:03:20    5.0\n2016-05-25 00:03:35    5.0\n2016-05-25 00:03:50    5.0\n2016-05-25 00:04:05    5.0\n2016-05-25 00:04:20    5.0\n2016-05-25 00:04:35    5.0\n2016-05-25 00:04:50    5.0\n2016-05-25 00:05:05    5.0\n2016-05-25 00:05:20    5.0\n2016-05-25 00:05:35    5.0\n2016-05-25 00:05:50    5.0\nFreq: 15S, dtype: float64\n\nI was told that this error does not show up in 0.18.0, but I have not confirmed that.\n\nThis comes from my attempts to interpolate some irregular data as shown in this question:  https://stackoverflow.com/questions/39599192/fill-in-time-data-in-pandas\n\n</details>\n"},{"labels":["api",null,null],"text":"The current `merge_asof()` has only one parameter for `by`, assuming that both DataFrames have the same column name. It does have `left_on` and `right_on`, so perhaps something similar for `by` would be helpful.\n\nAlso, there is no `left_index` or `right_index` parameter in `merge_asof()`. Adding these might be helpful too.\n\nFor example:\n\n```\npd.merge_asof(df1, df2,\n              left_index=True, right_on='timestamp',\n              left_by='Ticker', right_by='ticker')\n```\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\n\n```\ndf = pd.DataFrame({'foo': [1, 2, 3, 4]},\n                   index=pd.Index(['a', 'b', 'c', 'd'], name='names'))\ndf.reset_index(inplace=True)\n```\n\nreturns None\n#### Expected Output\n\nSame as output of\n\n```\ndf.reset_index()\n```\n#### output of `pd.show_versions()`\n\nThis is on trunk. Caused by the lines in DataFrame.reset_index\n\n```\n        if not inplace:\n            return new_obj\n```\n"},{"labels":["api",null],"text":"The python operator 'in' acting on a pandas.Series object behaves in two different ways.\nWhen iterating over the series it acts on the list of values. But for membership testing it acts on the index. Is this the desired behavior?\n\n```\nimport pandas as pd\ns = pd.Series([1, 2, 3])\nfor v in s: print v\n    # prints 1 2 3, i.e. 'in' is acting on the values\nfor v in [1, 2, 3]: print v in s\n    # prints True True False, i.e. the latter 'in' is acting on the index\n```\n\nThis behavior also leads to peculiar behavior when working with DataFrames.\n\n```\ndf = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\ndf\n\n#      a  b\n#  0  1  4\n#  1  2  5\n#  2  3  6\n\nfor value in df['a']:\n    print value,\n    if value in df['b']:\n        print 'also in column b'\n\n#  1 also in column b\n#  2 also in column b\n#  3\n```\n\nThis is not what one would naively expect from that code snippet and it can potentially lead to errors that might be difficult to spot.\nThis is the behavior that is also mentioned in the docs (in a pretty hidden way -- one has to actively search for it), but I suggest that this should not be the desired behavior. Instead, I'd find it much more natural, if 'in' worked always on the values and never on the index. If needed one can always use 'in s.index', but the standard way should be acting on the values always.\n\npandas version 0.18.1\n"},{"labels":["api",null,null],"text":"There is an inconsistency between how Series and DataFrames export types to dicts. The issue only manifests if the dataframe has multiple columns with different types. I think the issue is probably pretty far under the surface with differences between Series and DataFrame dtypes and just happened to be uncovered by export to dict.\n#### Code Sample, a copy-pastable example if possible\n\n```\nimport pandas as pd\n\norig_dict = [{0: 'a', 1: 1}]\n\n# does not happen with\n# orig_dict =[{0: 0, 1: 1}]\n\nprint 'Original dict'\nfor k in orig_dict[0]:\n    print type(orig_dict[0][k])\nprint\n\ndf = pd.DataFrame(orig_dict)\ndict_exported_from_dataframe = [df.loc[[x]].to_dict(orient='records')[0] for x in df.index]\ndict_exported_from_series = [df.loc[x].to_dict() for x in df.index]\n\nprint 'Dict exported from DataFrame'\nfor k in dict_exported_from_dataframe[0]:\n    print type(dict_exported_from_dataframe[0][k])\nprint\n\nprint 'Dict exported from Series'\nfor k in dict_exported_from_series[0]:\n    print type(dict_exported_from_series[0][k])\nprint\n```\n#### Expected Output\n\n```\nOriginal dict\n<type 'str'>\n<type 'int'>\n\nDict exported from DataFrame\n<type 'str'>\n<type 'int'>\n\nDict exported from Series\n<type 'str'>\n<type 'int'>\n```\n\nOR\n\n```\nOriginal dict\n<type 'str'>\n<type 'int'>\n\nDict exported from DataFrame\n<type 'str'>\n<type 'numpy.int64'>\n\nDict exported from Series\n<type 'str'>\n<type 'numpy.int64'>\n```\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 25.2.0\nCython: None\nnumpy: 1.11.1\nscipy: 0.13.0b1\nstatsmodels: None\nxarray: None\nIPython: 5.1.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: None\npandas_datareader: None\n"},{"labels":["api",null],"text":"Add a flag (or extend `na_action`) to raise an exception in `Series.map` if a value is not successfully mapped (ie in the case where `arg` is a dict.\n"},{"labels":["api",null],"text":"Since kwarg order will be [guaranteed](https://docs.python.org/3.6/whatsnew/3.6.html#pep-468-preserving-keyword-argument-order) in python 3.6, we could allow this...though would only want to try with a 3.6+ version check, otherwise code could more or less randomly work/not work, depending on dict order.\n\n``` python\nIn [44]: df = pd.DataFrame({'a': [1,2,3]})\n\nIn [45]: df.assign(b=1, c=lambda x: x['b'] * 2)\n```\n"},{"labels":["api",null],"text":"``` python\r\n>>> from pandas.compat import StringIO\r\n>>> from pandas import read_csv\r\n>>> data = 'a\\nfoo\\n1'\r\n>>>\r\n>>> read_csv(StringIO(data), na_values={0: 'foo'}, engine='c')\r\n...\r\nTypeError: Expected list, got set\r\n>>> read_csv(StringIO(data), na_values={0: 'foo'}, engine='python')\r\n     a\r\n0  foo  # Should be NaN\r\n1    1\r\n```\r\n\r\nThis behaviour is slightly inconsistent with what we do with `usecols` for example, so it would be nice to be able to process column indices with `na_values` too.\r\n\r\nxref #7119.\r\n"},{"labels":["api",null],"text":"Any Serie or DataFrame method with arguments inplace != [0, False, None, [], {}] process inplace operation. \nIs not explicit True or False like in the documentation.\nOther methods tested: where, mask, sort_index\nAny bool(arg) == True is accepted\nLike: inplace='Fail' , inplace=3, inplace=[0]\n\nMaybe is not a bug, but can generate problems if someone is not careful using positional arguments, like me.\n#### Code Sample, a copy-pastable example if possible\n\n```\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\ndf.where(lambda x: x > 4, lambda x: x + 10, 'fail')\n\n```\n#### Expected Output\n\n```\nOut[72]: \n    A   B  C\n0  11  14  7\n1  12   5  8\n2  13   6  9\n```\n#### output of `pd.show_versions()`\n\n```\npd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: AMD64 Family 18 Model 1 Stepping 0, AuthenticAMD\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 25.1.6\nCython: 0.23.4\nnumpy: 1.11.1\nscipy: 0.18.0\nstatsmodels: 0.6.1\nxarray: 0.8.1\nIPython: 5.1.0\nsphinx: 1.3.5\npatsy: 0.4.1\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.2\nnumexpr: 2.6.1\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: 0.2.0\n\n```\n"},{"labels":["api",null,null,null],"text":"Follow up of https://github.com/pydata/pandas/pull/13767 (this part was left out of the final merged version). Summary copied from there:\n\n**Discussion: addition of new keyword to `concat` to union categoricals with different categories**\n\nPR #13767 proposed to add a `union_categoricals` keyword to `concat`, which lets you concatenate categoricals with different categories (like the `union_categoricals` function does). Default would be False.\n\nThere was some discussion about the name to use. @wesm suggested to use a dict to pass type specific options. But that is maybe something we should first explore further to see if there are other use cases for such a construct?\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\n```\nimport pandas as pd\n\ndf = pd.DataFrame({'a': [1,2,3,4], 'b':[None, None, None, None]})\n\nchunks = df.groupby('a')\nassert len([g for g, chunk in chunks]) == 4\n\nchunks = df.groupby(['a', 'b'])\n\n# failed here, it's emtpy\nassert len([g for g, chunk in chunks]) == 4\n# however, chunks.groups shows that there are 4 groups\n#{(1, nan): [0], (2, nan): [1], (3, nan): [2], (4, nan): [3]}\n\n\n# but if we fillna('') first then it could pass\nchunks = df.fillna('').groupby(['a', 'b'])\nassert len([g for g, chunk in chunks]) == 4\n\n\n# In[11]:\n\n```\n#### Expected Output\n\nI don't see why have NaN in the grouped by key should fail the groupby. Especially if you use `chunks.groups`, you do have them grouped like this `{(1, nan): [0], (2, nan): [1], (3, nan): [2], (4, nan): [3]}\nIn [18]:`\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.4.0-36-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: \nLANG: en_US.UTF-8\n\npandas: 0.17.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 24.0.0\nCython: 0.23.4\nnumpy: 1.11.1\nscipy: 0.17.0\nstatsmodels: 0.6.1\nIPython: 4.0.3\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: 4.4.1\nhtml5lib: 1.0b8\nhttplib2: 0.9.2\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\nJinja2: None\n```\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\n\n``` python\n>>> np.array(pd.SparseSeries([0, 1], fill_value=0))\narray([1])\n```\n#### Expected Output\n\n```\narray([0, 1])\n```\n\nthis should really be consistent with `Series` rather than just returning the non-fill values (i.e. rather than being equivalent to `np.array(ps.SparseArray([np.nan, 1]))`.\n#### output of `pd.show_versions()`\n\nPandas 0.18.1\n\nshould alone be relevant.\n\nApologies I've not checked if this is fixed in master. Just passing on issues from \nscikit-learn/scikit-learn#7352.\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\n\nI have not found an easy way to access a column of a multi index by name. Consider the following code:\n\n```\nimport pandas as pd\ndf = pd.DataFrame([[1,1,10,200],\n                   [1,2,11,201],\n                   [1,3,12,202],\n                   [2,1,13,210],\n                   [2,2,14,230]],\n                  columns=list('ABCD')).set_index(['A','B'])\nii = df.index\n\n```\n\nTo access the values of column B i can e.g. do:\n\n```\ndf.reset_index('B').B.tolist()\n```\n\nor \n\n```\n[v[1] for v in ii.tolist()]\n```\n\nBoth of there are somewhat cumbersome What I'm proposing is to add similar shorcut access to a multiindex just like a dataframe. E.g.\n\n```\nii.B\n```\n\nor\n\n```\nii['B']\n```\n\nthat would return the same list as in the two examples above.\n#### output of `pd.show_versions()`\n\n0.18.1\n"},{"labels":["api",null,null],"text":"While using a `groupby`, applying an `expanding` function duplicates the grouped dimensions.  I would expect the result of this to work like `groupby.cumsum` where the `expanding` function is applied over the groups and then the result has the same index as the original data frame.\n#### Code Sample, a copy-pastable example if possible\n\nMy Data Frame sample\n\n```\n>>> dataframe\n           one\ncont cat1     \n0    a       0\n     b       1\n1    a       2\n     b       3\n2    a       4\n     b       5\n3    a       6\n     b       7\n4    a       8\n     b       9\n5    a      10\n     b      11\n6    a      12\n     b      13\n7    a      14\n     b      15\n```\n\nExample of the function call and incorrect output.\n\n```\n>>> dataframe.groupby(level=1).expanding(min_periods=1).mean()\n\n                one\ncat1 cont cat1     \na    0    a     0.0\n     1    a     1.0\n     2    a     2.0\n     3    a     3.0\n     4    a     4.0\n     5    a     5.0\n     6    a     6.0\n     7    a     7.0\nb    0    b     1.0\n     1    b     2.0\n     2    b     3.0\n     3    b     4.0\n     4    b     5.0\n     5    b     6.0\n     6    b     7.0\n     7    b     8.0\n```\n#### Expected Output\n\n```\n>>> dataframe.groupby(level=1).cumsum()\n\n           one\ncont cat1     \n0    a       0.0\n     b       1.0\n1    a       1.0\n     b       2.0\n2    a       2.0\n     b       3.0\n3    a      3.0\n     b      4.0\n4    a      4.0\n     b      5.0\n5    a      5.0\n     b      6.0\n6    a      6.0\n     b      7.0\n7    a      7.0\n     b      8.0\n```\n#### output of `pd.show_versions()`\n\n```\n>>> pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.0.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 18.1\nCython: None\nnumpy: 1.11.1\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.2\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: None\npandas_datareader: None\n```\n"},{"labels":["api",null,null],"text":"We have had some conversations in the past regarding an `.plot(....., engine=)` kw. in #8018 \nThis would allow pandas to redirect the plotting to a user selectable back-end, keeping `matplotlib` as the default.\n\nsee [chartpy here](https://github.com/cuemacro/chartpy), for a way to selectively enable `matplotlib`, `bokeh` and `plotly`.\n\nand generically via [altair](https://github.com/ellisonbg/altair).\n\nMissing from here is when to re-direct to `seaborn`.\n\nSo this issue is for discussion:\n\n1) should we do this\n2) implementation method and minimalist dependencies to actually do this (ideally pandas would add _NO_ dependences itself, just import for a particular engine, raising if its not available).\n3) maybe should spin off much of the current pandas code into a separate repo (`pandas-plot`)?\nand this actually should be the default (rather than `matplotlib`), which is of course the dependency. This might allow simply removing the vast majority of the custom plotting code.\n"},{"labels":["api",null,null],"text":"Would return `Period` or `PeriodIndex`, analogous to functions like `to_datetime` and `to_timedelta`.\n\nxref #5886\nxref #14048\n\ncc @sinhrks \n"},{"labels":["api",null],"text":"When calling `my_groupby.hist(bins=8, histtype='stepfilled')` I would expect the same behavior as when calling `my_series.hist(bins=8, histtype='stepfilled')` separately for each of the groups.\nHowever, while the 'bins' parameter is taken into account, the 'histtype' parameter is ignored and not passed along to matplotlib's implementation.\n\nLooking into the code, it appears that `pandas.core.groupby._whitelist_method_generator` only passes along named arguments of the wrapped method (hist in our example). While the signature was fixed to include any `*args` or `**kwargs` arguments (done to fix #8733), they are not passed on.\n\nNote: the problem applies to all 'wrappers' generated by _whitelist_method_generator (in addition to 'hist'), and also to any use of `*args` (in addition to `**kwargs`).\n\nWe are working on a PR and will send it soon.\n#### output of `pd.show_versions()`\n\ncommit: None\npython: 2.7.12.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 24.0.0\nCython: 0.23.4\nnumpy: 1.11.1\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.0.1\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.1\nopenpyxl: 2.2.6\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.7.7\nlxml: 3.4.4\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.9\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.38.0\npandas_datareader: None\n"},{"labels":["api",null,null,null],"text":"from [comment](https://github.com/pydata/pandas/pull/13981#issuecomment-239846027) on #13981 \n\n```\nIn [1]: pd.Index([datetime.datetime(2012,1,1), datetime.datetime(2012,1, 2)], tz=\"Europe/Brussels\")\nOut[1]: DatetimeIndex(['2012-01-01 00:00:00+01:00', '2012-01-02 00:00:00+01:00'], dtype='datetime64[ns, Europe/Brussels]', freq=None)\n\nIn [2]: pd.Index([1, 2], tz=\"Europe/Brussels\")\nOut[2]: DatetimeIndex(['1970-01-01 00:00:00+01:00', '1970-01-01 00:00:00+01:00'], dtype='datetime64[ns, Europe/Brussels]', freq=None)\n\nIn [3]: pd.Index(['20120102', '20120103'], tz=\"Europe/Brussels\")\nOut[3]: DatetimeIndex(['2012-01-02 00:00:00+01:00', '2012-01-03 00:00:00+01:00'], dtype='datetime64[ns, Europe/Brussels]', freq=None)\n```\n\n[1] is fine/expected\n[2] should be disallowed (e.g. integer construction)\n[3] is ok, maybe should disallow to be consistent.\n\nI am not sure what this would break. But we don't necessarily want to force datetime construction on a generic `Index` (as opposed to using `.to_datetime` or a constructor, e.g. `pd.date_range`). This is a bit too much inference.\n\nSo the rule would be that it has to be actual datetimes/Timestamps, if tz is provided (which is a form of dtype).\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\n``` python\nimport numpy\nimport pandas as pd\n\nidx = pd.date_range('1/1/2016', periods=100, freq='d')\n\nz = pd.DataFrame({\"z\": numpy.zeros(len(idx))}, index=idx)\nobj = pd.DataFrame({\"obj\": numpy.zeros(len(idx), dtype=numpy.dtype('O'))}, index=idx)\nnan = pd.DataFrame({\"nan\": numpy.zeros(len(idx)) + float('nan')}, index=idx)\nobjnan = pd.DataFrame([], columns=['objnan'], index=idx[0:0])  # dtype('O') nan after join\n\ndf_list = [z, obj, nan, objnan]\n\ndef r1w(x):\n    return x.resample('1w').sum()\n\nresample_join_result = r1w(pd.DataFrame().join(df_list, how='outer'))\nprint(resample_join_result.shape)  # (15, 2) --- I thought this should be (15, 4)\n\njoin_resample_result = pd.DataFrame().join([r1w(x) for x in df_list], how='outer')\nprint(join_resample_result.shape)  # (15, 4)\n\nfor columnname in join_resample_result.columns:\n    if columnname not in resample_join_result.columns:\n        print(\"DataFrame.resample missing column: \" + str(columnname) + \" (\" + str(join_result[columnname].dtype) + \")\")\n```\n#### Expected Output\n\nI would have expected `resample_join_result` to have all four columns and be the same as `join_resample_result`, but they are not, because it seems `pandas.DataFrame.resample` drops dtype('O') (object) columns; while `pandas.Series.resample` converts those columns into numeric dtypes.\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.10.0-327.22.2.el7.x86_64\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: 0.2.0\n```\n"},{"labels":["api",null,null,null],"text":"When getting the values of a dataframe with multiple columns with a single type datetime64[ns,UTC], it returns a numpy array with dtype=object instead of dtype=datetime64[ns,UTC].\nWhen returning a single column with dtype=datetime64[ns,UTC] or returning a multiple columns of dtype=datetime64[ns] (ie without tz), it works.\nLooking at the code, it appears that SingleBlockManager.is_mixed_type returns False in the first case (datetime64[ns,UTC]) and True in the two other cases (single datetime64[ns,UTC] or multiple datetime64[ns]).\n#### Code Sample, a copy-pastable example if possible\n\n```\nimport pandas\n\nidx = pandas.date_range(\"2010\", \"2011\", tz=\"UTC\")\ndf = pandas.DataFrame(index=idx)\ndf[\"A\"] = idx\ndf[\"B\"] = idx\nprint(df[[\"B\"]].values.dtype)\nprint(df[[\"A\", \"B\"]].values.dtype)\n```\n#### Actual Output\n\n```\ndatetime64[ns, UTC]\nobject\n```\n#### Expected Output\n\n```\ndatetime64[ns, UTC]\ndatetime64[ns, UTC]\n```\n#### output of `pd.show_versions()`\n\nINSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 32\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 23.0.0\nCython: None\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.6.1\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 1.0.0\nxlwt: None\nxlsxwriter: 0.9.2\nlxml: 3.6.0\nbs4: None\nhtml5lib: None\nhttplib2: 0.9.2\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: 2.6.2 (dt dec pq3 ext)\njinja2: 2.8\nboto: None\npandas_datareader: None\n"},{"labels":["api",null,null],"text":"When we pass a list of numbers, it parsed as date as it is. The same values is regarded as `ordinal` if it is passed as an array.\n#### Code Sample, a copy-pastable example if possible\n\n```\nraw = [2005, 2007, 2009]\n\npd.PeriodIndex(raw, freq='A')\n# PeriodIndex(['2005', '2007', '2009'], dtype='period[A-DEC]', freq='A-DEC')\n\npd.PeriodIndex(np.array(raw), freq='A')\n# PeriodIndex(['3975', '3977', '3979'], dtype='period[A-DEC]', freq='A-DEC')\n```\n#### Expected Output\n\nBecause `PeriodIndex` has a `data` and `ordinal` arg, it should be regarded as:\n- date if it is passed via `data` kwd\n- `ordinal` if it is passed via `ordinal` kwd\n#### output of `pd.show_versions()`\n\non current master.\n"},{"labels":["api",null,null,null],"text":"When converting a fractional year to a `Timedelta` object the float is truncated:\n\n```\nprint pd.to_timedelta(1., unit='Y')\n365 days 05:49:12\n\nprint pd.to_timedelta(1.5, unit='Y')\n365 days 05:49:12\n\nprint pd.to_timedelta(0.5, unit='Y')\n0 days 00:00:00\n\nprint pd.to_timedelta(0.9, unit='Y')\n0 days 00:00:00\n```\n\nUsing a float with the hour unit is OK:\n\n```\nprint pd.to_timedelta(0.9, unit='H')\n0 days 00:54:00\n\nprint pd.to_timedelta(1., unit='H')\n0 days 01:00:00\n```\n\nPandas version:\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 21.2.1\nCython: 0.20.1\nnumpy: 1.11.0\nscipy: 0.16.0\nstatsmodels: 0.7.0.dev-51faa1a\nxarray: None\nIPython: 4.2.0\nsphinx: 1.2.2\npatsy: 0.3.0\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.4.4\nmatplotlib: 1.5.1\nopenpyxl: 2.0.2\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.5\nlxml: 3.3.5\nbs4: 4.3.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.28.0\npandas_datareader: None\n```\n"},{"labels":["api",null,null,null],"text":"I found another oddity while digging through #13966.\n\nBegin with the initial DataFrame in that issue:\n\n```\ndf = pd.DataFrame({'A': [1] * 20 + [2] * 12 + [3] * 8,\n                   'B': np.arange(40)})\n```\n\nSave the grouping:\n\n```\nIn [215]: g = df.groupby('A')\n```\n\nCompute the rolling sum:\n\n```\nIn [216]: r = g.rolling(4)\n\nIn [217]: r.sum()\nOut[217]:\n         A      B\nA\n1 0    NaN    NaN\n  1    NaN    NaN\n  2    NaN    NaN\n  3    4.0    6.0\n  4    4.0   10.0\n  5    4.0   14.0\n  6    4.0   18.0\n  7    4.0   22.0\n  8    4.0   26.0\n  9    4.0   30.0\n...    ...    ...\n2 30   8.0  114.0\n  31   8.0  118.0\n3 32   NaN    NaN\n  33   NaN    NaN\n  34   NaN    NaN\n  35  12.0  134.0\n  36  12.0  138.0\n  37  12.0  142.0\n  38  12.0  146.0\n  39  12.0  150.0\n\n[40 rows x 2 columns]\n```\n\nIt maintains the `by` column (`A`)! That column should not be in the resulting DataFrame.\n\nIt gets weirder if I compute the sum over the entire grouping and then re-do the rolling calculation. Now `by` column is gone as expected:\n\n```\nIn [218]: g.sum()\nOut[218]:\n     B\nA\n1  190\n2  306\n3  284\n\nIn [219]: r.sum()\nOut[219]:\n          B\nA\n1 0     NaN\n  1     NaN\n  2     NaN\n  3     6.0\n  4    10.0\n  5    14.0\n  6    18.0\n  7    22.0\n  8    26.0\n  9    30.0\n...     ...\n2 30  114.0\n  31  118.0\n3 32    NaN\n  33    NaN\n  34    NaN\n  35  134.0\n  36  138.0\n  37  142.0\n  38  146.0\n  39  150.0\n\n[40 rows x 1 columns]\n```\n\nSo the grouping summation has some sort of side effect.\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\nAssuming we have int `DataFrame` and assign float values.\n- assigning 2-d float array keeps `int` columns if possible.\n\n```\ndf = pd.DataFrame(np.zeros((3, 2), dtype=np.int64))\ndf.iloc[0:2, 0:2] = np.array([[1, 2.1], [2, 2.2]], dtype=np.float64)\ndf\n#    0    1\n#0  1  2.1\n#1  2  2.2\n#2  0  0.0\n```\n- but assigning 1-d array coerces to `float`, not keeps `int` .\n\n```\ndf = pd.DataFrame(np.zeros((3, 2), dtype=np.int64))\ndf.iloc[0:2, 0] = np.array([1, 2], dtype=np.float64)\ndf\n#      0  1\n#0  1.0  0\n#1  2.0  0\n#2  0.0  0\n```\n#### Expected Output\n\nI think 1st one should be all float, as we're assigning float dtype.\n\n```\ndf = pd.DataFrame(np.zeros((3, 2), dtype=np.int64))\ndf.iloc[0:2, 0:2] = np.array([[1, 2.1], [2, 2.2]], dtype=np.float64)\ndf\n#      0    1\n#0  1.0  2.1\n#1  2.0  2.2\n#2  0.0  0.0\n```\n\nThe inference occurs around [here](https://github.com/pydata/pandas/blob/master/pandas/core/internals.py#L744), and it should be something like:\n\n```\nif hasattr(value, 'dtype'):\n    if is_dtype_equal(values.dtype, value.dtype):\n        dtype = value.dtype\n    else:\n        dtype = _find_common_type(values.dtpye, value.dtype)\nelif is_scalar(value):\n    dtype, _ = _infer_dtype_from_scalar(value)\nelse:\n    # not sure can reach here...\n    dtype = 'infer'\n...\n```\n#### output of `pd.show_versions()`\n\n0.18.1\n"},{"labels":["api",null,null,null],"text":"```\nimport pandas as pd\n\ns = pd.Series(list('ABCDEF'))\ngrouper = pd.Series([0]*3+[1]*3)\n```\n\nThe obvious attempt to obtain\n\n```\n0      A\n1     AB\n2    ABC\n3      D\n4     DE\n5    DEF\ndtype: object\n```\n\nusing\n\n```\ns.groupby(grouper).cumsum()\n```\n\nraises a `DataError: No numeric types to aggregate`. A workaround is available via\n\n```\ns.groupby(grouper).apply(pd.Series.cumsum)\n```\n\n`SeriesGroupby.cumsum` should follow the behavior of `SeriesGroupby.sum`, where both `s.groupby(grouper).apply(pd.Series.sum)` and `s.groupby(grouper).sum()` produce the correct output:\n\n```\n0    ABC\n1    DEF\ndtype: object\n```\n\nThis was introduced some time between 0.15.2 and 0.18.1, as observed [here](http://stackoverflow.com/a/38935108/509824).\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: None\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: 1.0.0\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n```\n"},{"labels":["api",null,null],"text":"`Series/Index.nunique` has `dropna` kw to exclude `NaN`. Add the same kw to `.unique` also.\n\nAs #13979 changes `.unique` to always return `Index`, we can replace `Index._get_unique_index(dropna)` if this option is added.\n"},{"labels":["api",null],"text":"We often find ourselves checking for column existence before assigning a value to it, e.g.:\n\n``` python\ndf = pd.DataFrame()\ncol = 'ColumnA'\nval = 'Value1'\nif col not in df.columns:\n    df[col] = val\n```\n\nAnd we were thinking it would be nice if there was a way to tell pandas: add this column with that value(s) but only if it does not exist already - somewhat similar to dict.setdefault() so we could say, for instance:\n\n``` python\ndf.setdefault(col, val)\n```\n\nDoesn't necessarily have to be that syntax obviously - just a suggestion. What do you guys think?\n"},{"labels":["api",null,null,null],"text":"I would like a time-based aggregating function that also acts as a join:\n\nhttp://code.kx.com/wiki/Reference/wj\n\nIn pandas, this might look something like:\n\n```\nms = pd.Timedelta(1, 'ms')\nmerge_window(df1, df2, on='time', by='ticker', time_range=(-100*ms, 100*ms), how=(np.sum, 'volume'))\n```\n\nThis would compute the total volume from df2 whose timestamps are within (-100, 100) ms of df1’s timestamps and whose tickers match.\n\nI imagine I could specify different columns:\n\n```\nmerge_window(df1, df2, left_on='trade_time', right_on='exchange_time', time_range=(0*ms, 500*ms),\n             how={'total_volume':(np.sum, 'volume'), 'mean_volume':(np.mean, 'volume')})\n```\n\nBy the way, this is a more general form of the `.rolling()` functions @jreback [wrote recently](https://github.com/pydata/pandas/pull/13513) since I can just use one DataFrame as both parameters:\n\n```\nmerge_window(df, df, on='time', time_range=(-5*ms, 0*ms), how=[(np.median, 'price'), (np.sum, 'volume')])\n```\n\nMy firm has custom business calendars for things like market hours as well as exchange holidays. The kdb version takes arrays of timestamps directly for the begin and end, which handles the general-purpose case of custom business calendars. So I imagine could get the five-day average of volume with:\n\n```\n# internal functions\ncal = get_our_internal_business_calendar()\nbegin = adjust_our_time(df.time, -5, cal)\nend = adjust_our_time(df.time, 0, cal)\n\n# use values directly\nmerge_window(df1, df2, on='time', begin_times=begin, end_times=end, how=(np.mean, 'volume'))\n```\n\nI can get started on this right away if my proposal makes sense.\n"},{"labels":["api",null,null],"text":"It's not an issue in py3.5, but in older versions this is annoying - because a decorator is used in the definition, the function signature on `to_datetime` is lost.  This is the big one I've run into, but there may be some other places in the API it comes up.\n![image](https://cloud.githubusercontent.com/assets/1924092/17310153/49ca6d18-5808-11e6-91aa-483ab3458efd.png)\n\nI saw this blog today - could use one of the strategies there.  It may make sense to vendor rather than adding another dependency?\nhttps://hynek.me/articles/decorators/\n"},{"labels":["api",null],"text":"It seems that future development in Pandas creates a distinction between \"production\" and \"interactive\" use. I frequently use Pandas to do interactive data analyses. As such, things like indexing shortcuts are very handy. However, I also can see that some of this magic can create very subtle bugs in production systems.\n\nOne potential solution is to have a global configuration switch. When an analysis project starts, many of these \"shortcut\" functions could be made available. Once the code is ready, the user can change this setting to \"production\" which would issue warnings for functions deemed unsafe.\n\nAny thoughts on this?\n\nThis issue came up while talking to @jreback on #13548 .\n"},{"labels":["api",null],"text":"I looked briefly at your new custom styling feature from version 0.17.1 (http://pandas.pydata.org/pandas-docs/stable/style.html). I'm excited about the possibilities of custom styling and all the benefits it would allow researchers, but I feel that the current interface is somewhat non-intuitive and limited.\nI would like to suggest a single generic feature that has the potential to include almost all other existing features, and allow a lot more flexibility. The idea is to allow overriding the default function that \"draws\" the elements in the dataframe as HTML table cells.\n\nLet's say that this drawing function is `df.style.draw_element` and it receives the following arguments:\n- **value** (any object/dtype): the value of the element to display\n- **column** (str): the name of the value's column\n- **row** (pandas.Series): the value of the entire element's row (if one wants to make a context-dependent drawing)\n\nAdditionally you can also add **index** and **df** as arguments, if you want to allow even more flexible context-dependent drawing.\n\nThe default built-in implementation of this function is just:\n`def draw_element(value, column, row): return str(value)`\n(i.e. every element is just displayed as it is)\n\nBut now, it will allow users to override it and add their own cool functionality. For example, one could do the following:\n`df.style.draw_element = lambda: value, column, row: str(value) if value >= 0 else '<div style = \"background-color: red;\">%s</div>' % str(value)`\nThis will highlight in red all the negative values in the table.\n\nThis trick is already achievable with the current interface, but here's another example for some cool application that I don't think can be achieved with the current interface. Suppose that a bioinformatician working with DNA sequences (made of the four letters A, C, G, T) is interested in drawing each DNA letter with a different color (although all the letters are part of a single sequence within the same element in the dataframe). Using the new interface she will be able to simply do the following:\n`df.style.draw_element = lambda: value, column, row: ''.join(['<span style = \"color: %s;\">%s</span>' % (get_dna_letter_color(letter), letter) for letter in value]) if 'dna' in column.lower() else  str(value)`\nHow cool is that? :)\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\nCurrently the from_records does not take a chunksize. \n\n```\nimport pandas as pd\ndf = pd.DataFrame.from_records(data, index=None, exclude=None, \n                                     columns=None, \n                                     coerce_float=False, \n                                     nrows=None)\n```\n#### Enhancement\n\nI would like to see the chunksize options like in read_csv()\n\n```\nimport pandas as pd\ndfs = pd.DataFrame.from_records(data, index=None, exclude=None, \n                                     columns=None, \n                                     coerce_float=False, \n                                     nrows=None, chunksize=10000)\nres = []\nfor df in dfs:\n    df['col'] = 'blah'\n   res.append(df)\n\ndf = pd.concat(res)\n```\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 32\nOS: Windows\nOS-release: 8\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.16.1\nnose: 1.3.7\nCython: None\nnumpy: 1.9.2\nscipy: 0.15.1\nstatsmodels: None\nIPython: 4.2.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: 1.0.0\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: 0.9.2\napiclient: 1.5.1\nsqlalchemy: None\npymysql: None\npsycopg2: 2.6.2 (dt dec pq3 ext)\n"},{"labels":["api",null,null],"text":"xref #13749 \n\nthis is not consistent with other Indexes where we don't allow them to be constructed with no data passed (it can be `None`), but that's the idea. `start` must be not-None.\n\nIOW this should raise `ValueError`\n\n```\nIn [3]: pd.RangeIndex()\nOut[3]: RangeIndex(start=0, stop=0, step=1)\n```\n"},{"labels":["api",null],"text":"Triggered by #13785, I looked at the top level namespace, and I was wondering if the following functions are needed:\r\n- [ ] `pd.Expr`\r\n- [ ] `pd.pnow()`: returns the current date as a Period with specified freq (https://github.com/pydata/pandas/issues/12641)\r\n- [ ] `pd.info()`\r\n- [ ] `pd.groupby()`\r\n- [ ] `match`: \"Compute locations of to_match into values\" (imported from `core/api.py` from `\\core\\algorithms.py` together with `factorize`, `unique` and `value_counts`, but I don't think `match` is used anywhere in the docs)\r\n- [ ] `pd.Term`\r\n\r\nQuite a lot are already deprecated: `TimeSeries`, `SparseTimeSeries`, all `rolling_` and `expanding_` methods\r\n\r\nI also learned the existence of `pd.lreshape` and `pd.plot_params` .. :-) (but the latter is mentioned in the docs, `lreshape` not).\r\n"},{"labels":["api",null,null,null],"text":"xref #13351\n\n[Here's the test](https://github.com/pydata/pandas/commit/964b7bba7f5878c79130479f75461c58dd0c4b3e), currently skipped\n"},{"labels":["api",null,null],"text":"I'm dissatisfied with the current way `DataFrame.to_hdf(...format='table')` works. Take this simple example:\n\n``` python\nimport numpy as np\nimport pandas as pd\nimport tables\n\ndata = {\n    'foo': np.zeros(5, dtype=float),\n    'bar': np.ones(5, dtype=float),\n    'baz': 2 * np.ones(5, dtype=float),\n}\ndf = pd.DataFrame.from_dict(data)\ndf.to_hdf('bla.h5', '/mu', mode='a', format='table',\n          complevel=5, complib='zlib', fletcher32=True)\n```\n\nNow if we inspect the file with pytables, there are some issues with this:\n\n```\nIn [1]: h5 = tb.open_file('bla.h5')\nIn [2]: h5.root.mu\nOut[2]:\n/mu (Group) ''\n  children := ['table' (Table)]\n\nIn [3]: h5.root.mu.table\nOut[3]:\n/mu/table (Table(5,), shuffle, zlib(5)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": Float64Col(shape=(3,), dflt=0.0, pos=1)}\n  byteorder := 'little'\n  chunkshape := (2048,)\n  autoindex := True\n  colindexes := {\n    \"index\": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n\nIn [4]: h5.root.mu.table.dtype\nOut[4]: dtype([('index', '<i8'), ('values_block_0', '<f8', (3,))])\n```\n\nI've been using pytables + numpy so far to write Dataframes / structured arrays to HDF5 Tables. Compared to that, the pandas way of writing tables has 2 issues:\n- What I'd have expected from the above example is that it'd create a table (not a group) called `mu` sitting right at the root node. Instead, it uses the `mu` as _Groupname_, and names the table itself just `table`, which is not exactly informing.\n- The Table datatype does not hold the full information. Since all the Dataframe columns I passed are float, it just creates a _single table column_ holding 3-tuples. But I'd like it to create 3 separate Float columns, with the column names reflecting `DataFrame.columns`.\n#### Workaround\n\nI've been circumventing this issue so far by using `Dataframe.to_records()` to convert it to a structured numpy array & then use pytables directly to write the table to disc. This works without issues.\nI'll whip up a Pull Request later I suppose.\n#### output of `pd.show_versions()`\n\n```\n\u001b[?1034h\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.8.final.0\npython-bits: 64\nOS: Linux\nOS-release: 2.6.32-573.12.1.el6.x86_64\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 21.0.0\nCython: 0.24\nnumpy: 1.11.1\nscipy: 0.17.1\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: 3.2.3.1\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n\n```\n"},{"labels":["api",null],"text":"xref #1134. There is another inconsistencies related to op. `Index/Series/DataFrame` op can accept 1-d list-like as input and coercing to `Index/Series`. However, supported 1-d list-likes differ depending on the class and kind of op.\n#### Code Sample, a copy-pastable example if possible\n\n```\n# Series + equal length list, OK\npd.Series([1, 2, 3]) + [2, 2, 2]\n#0    3\n#1    4\n#2    5\n# dtype: int64\n\n# Series + equal length Index, OK\npd.Series([1, 2, 3]) + pd.Index([2, 2, 2])\n#0    3\n#1    4\n#2    5\n# dtype: int64\n\n# DataFrame + equal length list, OK\npd.DataFrame([[1, 2, 3]]) + [2, 2, 2]\n#    0  1  2\n#0  3  4  5\n\n# DataFrame + equal length Index, NG\npd.DataFrame([[1, 2, 3]]) + pd.Index([2, 2, 2])\n# ValueError: cannot evaluate a numeric op with unequal lengths\n```\n\nI've organize the result once as the below table. I think all these must be supported and consistent.\n\n| Class | op | list | tuple | ndarray(1dim) | Index |\n| --- | --- | --- | --- | --- | --- |\n| Index | Arithmetic | x | x | o | o |\n| Index | Comparison | o | o | o | o |\n| Index | Boolean | o | o | o | o |\n| Series | Arithmetic | o | o | o | o |\n| Series | Comparison | o | o | o | o |\n| Series | Boolean | o | o | o | x |\n| Frame | Arithmetic | o | o | o | x |\n| Frame | Comparison | o | o | o | o |\n| Frame | Boolean | o | o | o | x |\n\nNOTE: Index result may depends on its type.\n#### output of `pd.show_versions()`\n\n0.18.1\n"},{"labels":["api",null,null],"text":"xref #13147\n\nideally add doc-strings and add to API.rst? for the introspectors\nthese are pretty intuitve (e.g. `is_integer`), but have some nuances that\ndoc-strings (and an example) would be useful.\n"},{"labels":["api",null],"text":"In xref #13147\n\nestablished a bit of a public api in `pandas.api`; ATM this only contains the type introspection routines.\n## 1st question\n\nSome disagreement on whether we should not do this, and rather just expose `pandas.types` directly.\n\nI think `pandas.api` is actually very important because:\n\n1) limits scope of what we choose to expose in the future; its not simply 'everything' that isnt nailed down (the case now). This does change pandas to make everything by default private, EXCEPT what is explicitly public.\n\n2) a single entry point for other packages to use the pandas public API that is known / stable / maintained (will with c-API as well)\n\n3) provide consistent naming externally (and allow us to fix / hide internally as needed)\n\n4) namespaced. I only import _what_ I need as a user / other package, rather than _everything_\n## ~~2nd question~~\n\n as discussed [here](https://github.com/pydata/pandas/pull/13147#discussion_r70594911), should these deprecated API functions should be `DeprecationWrarning` rather than `FutureWarning`?\n-> done in https://github.com/pydata/pandas/pull/13990\n\nIdeally we should resolve this before the end of 0.19.0 (or remove it).\n"},{"labels":["api",null,null,null],"text":"xref #7795 #13221. Series/Index concat-like op which triggers object-coercion is not well tested. Followings are needed: \n- Refactor `concat` internal to make it consistent / stabled. \n- Add comprehensive tests to cover both Index/Series object-coercion cases \n#### Code Sample, a copy-pastable example if possible\n\nFound some problems below:\n\n```\n# NG. though it looks OK, the objects are python built-in (must be Timestamp and Timedelta)\ns = pd.Series([pd.Timestamp('2011-01-01')]).append(pd.Series([pd.Timedelta('1 days')]))\ns\n#0    2011-01-01 00:00:00\n#0         1 day, 0:00:00\n# dtype: object\n\ntype(s.iloc[0]), type(s.iloc[-1])\n# (datetime.datetime, datetime.timedelta)\n```\n\n```\n# NG, the result must be object dtype contaions Timestamp and Timedelta\nidx = pd.DatetimeIndex(['2011-01-01']).append(pd.TimedeltaIndex(['1 days']))\nidx\n# Index([2011-01-01 00:00:00, 86400000000000], dtype='object')\n\ntype(idx[0]), type(idx[-1])\n# (pandas.tslib.Timestamp, int)\n```\n#### Expected Output\n- result must be object dtype containing `Timestamp` and `Timedelta` (not `datetime` and `timedelta`)\n#### output of `pd.show_versions()`\n\n0.18.1\n"},{"labels":["api",null,null],"text":"This is a topic that has come up recently (https://github.com/pydata/pandas/issues/10000, https://github.com/pydata/pandas/issues/8906, pandas-dev mailing list discussion), let's make this an issue to track the discussion about it.\n- https://github.com/pydata/pandas/issues/8906#issuecomment-152613473 experience of the 'experimental' status of Panels by @MaximilianR and following discussion on pointing users to xarray\n- Issue for docs on using xarray instead of Panels: https://github.com/pydata/pandas/issues/12283\n- \"WIP for transitioning from Panel docs\" PR https://github.com/pydata/xarray/pull/832\n\nDeprecating Panels would be a rather large change, so:\n- Do we need to further discuss if we actually want to do this?\n- Are there people who make intensive use of Panels to ask feedback?\n- How do we go about such a deprecation? First making a note in the whatsnew / pinging mailing list or other fora before actually deprecating?\n\ncc @pydata/pandas @MaximilianR \n"},{"labels":["api",null],"text":"#### Copy-pastable Code Sample\n\n``` python\nimport pandas as pd\nimport pandas.util.testing as tm\n\nclass myClass1:\n    def __call__(self):\n        return \"Result 1\"\n    def __str__(self):\n        return \"Class 1\"\n\nclass myClass2:\n    def __call__(self):\n        return \"Result 2\"\n    def __str__(self):\n        return \"Class 2\"\n\nobj1 = myClass1()\nobj2 = myClass2()\ndf = pd.DataFrame([], index=[0], columns=[obj1, obj2])\n # Throws error because new pandas version wants to select by callable\ndf[obj1] = obj1()\ndf[obj2] = obj2()\n```\n#### Expected Output\n\n``` python\nexpected = pd.DataFrame([[\"Result 1\", \"Result 2\"]], columns=[obj1, obj2])\ntm.assert_frame_equal(df, expected)\n```\n#### How I fixed the test for the moment\n\n``` python\nclass myClass1:\n    def __call__(self, *args):\n        if args:\n            return self\n        return \"Result 1\"\n    def __str__(self):\n        return \"Class 1\"\n\nclass myClass2:\n    def __call__(self, *args):\n        if args:\n            return self\n        return \"Result 2\"\n    def __str__(self):\n        return \"Class 2\"\n```\n\nIf pandas invokes a callable object it passes the frame itself as a parameter. Because i dont need arguments for my call function i can prevent this call by checking if a argument is passed. Is there a better way of doing this or can you add an option so i can disable the new behavior.\n#### output of `pd.show_versions()`\n\n``` python\ncommit: None\npython: 3.4.3.final.0\npython-bits: 32\nOS: Windows\nOS-release: 8\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 19.6.2\nCython: None\nnumpy: 1.11.0\nscipy: 0.16.1\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: 1.4.1\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n```\n"},{"labels":["api",null],"text":"I deal with DateTime columns extensively. Using strings to slice and select items is extremely useful, but it requires a DatetimeIndex.\n\nI think it would be great to include the same functionality for any column.\n\nConsider this DF:\n\n```\n>>> a = pd.DataFrame([pd.to_datetime('2016-4-3 15:32:04'), pd.to_datetime('2016-4-9 15:32:04')], columns=['a_col'])\n>>> print(a)\n\n                a_col\n0 2016-04-03 15:32:04\n1 2016-04-09 15:32:04\n```\n\nThis is what I usually do, but it's extremely cluttered, and fails when the index is already something useful.\n\n```\n>>>print(a.set_index('a_col')['2016-4-3'].reset_index())\n                a_col\n0 2016-04-03 15:32:04\n```\n\nI imagine this is the preferred way of doing the same thing (please correct me if I'm wrong). Requires a lot of typing, and not elegant at all (especially for chosing a single day, as in this example).\n\n```\n>>> print(a[(a.a_col >= pd.to_datetime('2016-4-3')) & (a.a_col < pd.to_datetime('2016-4-4'))])\n                a_col\n0 2016-04-03 15:32:04\n```\n\nI think something like the `ds` would be great:\n\n```\n>>> print(a.ds['a_col', '2016-4-3'])\n```\n\nThe second element could be any argument that is accepted to DatetimeIndex elements.\n\nWhat do you think? Am I missing something?\n"},{"labels":["api",null,null],"text":"Hi,\n\nWhen using `pd.read_excel(..., sheetname=None, converters={...})` to load all data in an Excel workbook, the columns specified in `converters` gets applied to any matching column names across _all_ spreadsheets. I have a use case where I would like to use different converter functions for columns of the same name across multiple sheets.\n\nOne potential implementation could yield:\n\n``` python\npd.read_excel(\"my_data.xlsx\", sheetname=None, converters={\n  \"Sheet1\": { \"my_column\": do_something },\n  \"Sheet2\": { \"my_column\": do_something_else }})\n```\n\nAnother implementation could be:\n\n``` python\npd.read_excel(\"my_data.xlsx\", sheetname=None, converters={\n  (\"Sheet1\", \"my_column\"): do_something,\n  (\"Sheet2\", \"my_column\"): do_something_else })\n```\n\nThanks! Please let me know if I can help out with an implementation -- I'd love to contribute to Pandas!\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\n``` python\n\nnp.random.seed(12345)\npanel = pd.Panel(np.random.randn(125,200,10))\npanel.iloc[:,:,0] = np.round(panel.iloc[:,:,0])\npanel.iloc[:,:,1] = np.round(panel.iloc[:,:,1])\nx = panel\ncols = [0,1]\n_x = x.swapaxes(0, 2).to_frame()\nnumeric_cols = []\nfor df_col in _x:\n    if df_col not in cols and pd.core.common.is_numeric_dtype(_x[df_col].dtype):\n        numeric_cols.append(df_col)\n\ncalls = 0\n\n\ndef _print_type(df):\n    print(type(df))\n    return df\n\nindex = _x.index\n_x.index = pd.RangeIndex(0, _x.shape[0])\ngroups = _x.groupby(cols)\nout = groups.transform(_print_type)\n```\n#### Comment\n\nThe `slow_path` operated series by series rather than on a group DataFrame.  Once the slow path is accepted, it operated on the group DataFrames.  I have 59 groups in my example with 8 columns, and so it runs 8 times with Series from the first group DataFrame and then, once happy, runs 58 more times on the DataFrames.\n\nThe description says that it onlly operated on the group DataFrames (which is the correct behavior IMO)\n#### Expected Output\n\nMany lines of\n\n```\n<class 'pandas.core.frame.DataFrame'>\n```\n#### Actual Output\n\n```\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.frame.DataFrame'>\n<class 'pandas.core.frame.DataFrame'>\n<class 'pandas.core.frame.DataFrame'>\n<class 'pandas.core.frame.DataFrame'>\n...\n<class 'pandas.core.frame.DataFrame'>\n```\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: 0.8.0.dev0+f060948\nxarray: 0.7.2\nIPython: 4.2.0\nsphinx: 1.3.1\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: 1.0.0\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: 0.2.1\n```\n"},{"labels":["api",null],"text":"It would be great if Pandas Series got an info method, like Pandas DataFrame. My main interest is to find out memory usage.\n"},{"labels":["api",null],"text":"Using groupby + shift seems to have changed behaviour in 0.17 and 0.18 compared to 0.16.\nWith as_index=False, I would expect the columns that the groupby is made over to remain in the output dataframe, but they are no longer present.\n#### Code Sample, a copy-pastable example if possible\n\n``` python\n>>>import pandas as pd\n>>>import numpy as np\n>>>df = pd.DataFrame({'A': [1.0, 1, 1, 2, 2, 3], 'B': [1.0, 2, 3, 4, 5, 6]})\n>>>df\n     A    B\n0  1.0  1.0\n1  1.0  2.0\n2  1.0  3.0\n3  2.0  4.0\n4  2.0  5.0\n5  3.0  6.0\n>>>df_sorted.groupby('A', as_index=False).shift(1)\n     B\n0  NaN\n1  1.0\n2  2.0\n3  NaN\n4  4.0\n5  NaN\n```\n#### Expected Output\n\n``` python\n>>>pd.DataFrame({'A':[np.nan, 1, 1, np.nan, 2, np.nan], 'B':[np.nan, 1, 2, np.nan, 4, np.nan]})\n     A    B\n0  NaN  NaN\n1  1.0  1.0\n2  1.0  2.0\n3  NaN  NaN\n4  2.0  4.0\n5  NaN  NaN\n```\n#### output of `pd.show_versions()`\n\n``` python\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.24\nnumpy: 1.10.4\nscipy: 0.17.1\nstatsmodels: None\nxarray: 0.7.2\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.4.1\npytz: 2016.4\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.40.0\npandas_datareader: None\n```\n\nI have also confirmed the issue on an similar install Linux installation using pandas 0.18.1\n"},{"labels":["api",null,null],"text":"xref #8640 #12699 #13361 #13410 \n\nThere's been discussion of a few overlapping uses of `Categorical`:\n1. as 'true' categorical data with a known set of values\n2. as 'lazy' categorical data which adds new categories as needed\n3. as an interned string data type, with no particular categorical interpretation\n\nOption 1 is currently well-supported. Option 2 can be achieved explicitly by `union_categorical`, see #13361 #13410, but will not happen automatically e.g. when setting values or concatenating, see #12699. Option 3 is similar to 2, and has been discussed as a new `String` type, see #8640, but may have different semantics to 2.\n\nWhile I completely agree that option 1 should be the default, I'd like to see more support for option 2 if possible; there are cases where I really do want to work with categorical data, but I don't yet know what the categories are, e.g. when concatenating a table together from several different source files.\n\nOne option would be to mimic Matlab's 'protected' flag for categorical data. By default, a `Categorical` would be created protected, and throw errors when performing actions which implicitly change the category set. However, the user could choose to declare a `Categorical` as unprotected, in which case these operations would be performed as intended.\n\nWhile this behaviour could be achieved with the proposed `String` type, it's unclear whether that type would share the `Categorical` API, or be efficiently convertible to `Categorical`. Having the behaviour as part of `Categorical` would allow the user to build up a `Categorical` iteratively, from multiple sources, and then quickly mark it protected once the category set is known.\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\nTeaching a pandas course. Attendee just came across this. Note that we index with a list instead of a tuple at the bottom.\n\n``` python\nframe = pd.DataFrame(np.arange(12).reshape(( 4, 3)),\n                  index =[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n                  columns =[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])\nframe.index.names = ['key1', 'key2']\nframe.columns.names = ['state', 'color']\nframe\nframe.loc[['b', 2], 'Colorado']\nframe.loc[['b', 1], 'Colorado']\n```\n\nReturns \n\n```\ncolor      Green\nkey1 key2\nb    1         8\n     2        11\n```\n\nin both cases on pandas 0.18.1\n#### Expected Output\n\nError?\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.4.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.1\nsetuptools: 20.3\nCython: None\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: None\nxarray: None\nIPython: 4.0.3\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.6\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.9999999\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.8\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: None\n```\n"},{"labels":["api",null],"text":"Idea after seeing the gitter between @ssanderson  and @jorisvandenbossche \n\nConsider this data:\n\n```\nIn [13]: df = pd.DataFrame({'date': pd.date_range('2015-01-01', freq='W', periods=5),\n                            'a': np.arange(5)}, \n                            index=pd.MultiIndex.from_arrays([\n                                [1,2,3,4,5], \n                                pd.date_range('2015-01-01', freq='W', periods=5)], \n                            names=['v','d']))\n\nIn [14]: df\nOut[14]: \n              a       date\nv d                       \n1 2015-01-04  0 2015-01-04\n2 2015-01-11  1 2015-01-11\n3 2015-01-18  2 2015-01-18\n4 2015-01-25  3 2015-01-25\n5 2015-02-01  4 2015-02-01\n```\n\nRight now to resample by `MultiIndex` level `'d'` or column `'date'` it would be:\n\n```\nIn [15]: df.groupby(pd.TimeGrouper(key='date', freq='M')).sum()\nOut[15]: \n            a\ndate         \n2015-01-31  6\n2015-02-28  4\n\nIn [16]: df.groupby(pd.TimeGrouper(level='d', freq='M')).sum()\nOut[16]: <same>\n```\n\nWhat if we instead allowed this as a convenience?\n\n``` python\nIn [17]: df.resample('M', level='d').sum()\nIn [17]: df.resample('M', key='date').sum()\n```\n"},{"labels":["api"],"text":"Hey everyone,\n\nI was working on a project that involves working with data acquired via the world bank pandas api (i.e. `pandas.io.wb`). This library has a function called `pandas.io.wb.search(str)` which is very useful. In data transformation terms, it's basically does this- given a string 's', filter the dataframe by retaining only the tuples where a string match occurs with the given string 's'.\n\nThis operation occurs in my workflow on a regular basis, and I was wondering if anyone else feels the need to have this function natively in pandas. Would be a huge time saver in my opinion!\n\nWas thinking of a function that looks like this:\n`DataFrame.search(search_str, is_ignore_case=True, is_match_full_word=False, **kwargs)`\n\nWould love to hear from the community! :)\n"},{"labels":["api",null],"text":"#### Code Sample:\n\nDataframe:\n\n```\n    gre         gpa  prestige\n0   380.0   3.61     3.0\n1   660.0   3.67     3.0\n2   800.0   4.00     1.0\n3   640.0   3.19     4.0\n4   0520.0  2.93     4.0\n```\n\n`df.plot.hist(figsize=(20, 10), layout=(2,3), subplots=True, sharex=False, bins=10);`\n\nResult:\n![image](https://cloud.githubusercontent.com/assets/604138/16180645/654894b4-3659-11e6-95ab-880e421fe6a3.png)\n\nNote that scales are all off\n#### Expected Output\n\nIndependent axis scale for subplots\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.0.0\nCython: None\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: 1.2.2\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: 0.9999999\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n```\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\n`Index`/`Series` results in `datetime64` dtype if all inputs are `pd.NaT`.\n\n```\npd.Index([pd.NaT, pd.NaT])\n# DatetimeIndex(['NaT', 'NaT'], dtype='datetime64[ns]', freq=None)\n\npd.Series([pd.NaT, pd.NaT])\n#0   NaT\n#1   NaT\n# dtype: datetime64[ns]\n```\n\nHowever, it coerces to `timedelta64` if `np.nan` is included. \n\n```\npd.Index([pd.NaT, np.nan])\n# TimedeltaIndex([NaT, NaT], dtype='timedelta64[ns]', freq=None)\n\npd.Series([pd.NaT, np.nan])\n#0   NaT\n#1   NaT\n# dtype: timedelta64[ns]\n```\n\nThis looks to be caused by following internals:\n\n```\npd.tslib.is_timestamp_array(np.array([pd.NaT, np.nan]))\n# False\npd.lib.is_timedelta_array(np.array([pd.NaT, np.nan]))\n# True\n```\n#### Expected Output\n\n`datetime64` dtype?\n#### output of `pd.show_versions()`\n\non current master\n"},{"labels":["api",null,null],"text":"Using `sum()` on a Series of Decimal datatypes returns a Decimal. However, using `sum()` on a DataFrame of Decimal datatypes returns floats. \n\nI think it would be more consistent for the DataFrame `sum()` to return Decimals.\n#### Code Sample\n\n``` python\nimport pandas as pd\nfrom decimal import Decimal\n\ndf = pd.DataFrame({'a':[Decimal('1.0'), Decimal('2.0')],\n                   'b':[Decimal('3.0'), Decimal('4.0')]})\n\ntype(df['a'].sum()), type(df.sum()[0])\n\nOut[1]: (decimal.Decimal, numpy.float64)\n```\n#### Expected Output\n\n``` python\nOut[1]: (decimal.Decimal, decimal.Decimal)\n```\n#### output of `pd.show_versions()`\n\n``` python\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 32\nOS: Windows\nOS-release: 7\nmachine: x86\nprocessor: x86 Family 6 Model 42 Stepping 7, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0\nnose: 1.3.7\npip: 7.1.2\nsetuptools: 18.5\nCython: 0.23.4\nnumpy: 1.10.1\nscipy: 0.16.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.0.1\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5\nmatplotlib: 1.5.0\nopenpyxl: 2.2.6\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.7.7\nlxml: 3.4.4\nbs4: 4.4.1\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.9\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.38.0\n```\n"},{"labels":["api",null,null,null],"text":"The boat may have long sailed on this, but just for consideration.\n\nI semi-frequently get the `.levels` and `.labels` of a `MultiIndex` backwards.  Maybe it's just me, but I think `labels` is the culprit, because in other pandas contexts, \"labels\" refer to the actual value of the thing.  E.g.\n- `.loc` indexes by \"labels\" (values)\n-  the values inside of a single row index or columns are the \"labels\" for those items\n\nSo, consistent with `Categorical`, would it make sense for the integer mapping inside a `MultiIndex` to also be called `.codes`?\n"},{"labels":["api",null,null],"text":"Situation:\n.xlsx file having a column with label and hyperlinks (see screenshot collage)\nIn Excel, label: Python (that has a hyperlink to i.e.: www.python.org)\n#### Code Sample, a copy-pastable example if possible\n\ndata = pd.read_excel('Book1.xlsx')\ncolumn has just \"Python\" as content. Hyperlink information is lost.\n(see screenshot)\n\n![excel-hyperlink](https://cloud.githubusercontent.com/assets/19929980/16044286/25e27eca-3244-11e6-8ce4-2ae0ec34ba77.png)\n#### Expected Output, Proposal:\n\nAdd an argument  for example named hyperlinkparser to read_excel()\nthat offers options: \"label\", \"hyperlink\", \"both\"\nlabel ... just the label (this is how it is handled now)\nhyperlink ... just the hyperlink, drop the label\nsplit ... created two separate columns for label and hyperlink each\n\nwith example it could look like:\n         | MyLinks | MyLinks_hyperlink\n0       | Python   | www.python.org\n\nbtw. openpyxl 2.4.+ has this feature now included (was broken in previous versions). Maybe this helps for implementation.\n\nThank you!\n"},{"labels":["api",null,null],"text":"xref #13992 \n\nSome of the issues and inconsistencies I noticed. \n(Apologies for a bit lengthy input.)\n##### Settings:\n\n``` python\ndf = pd.DataFrame({\"A\": [2, 1, 2, 2],\n                   \"B\": [3, 3, 4, 4],\n                   \"C1\": pd.Categorical([5, 6, 6, 6], [5, 6, 7]),\n                   \"C2\": pd.Categorical(list(\"aaab\"), list(\"abc\")),\n                   \"D\": pd.date_range('2011-11-11', periods=4),\n                   \"E\": [10, 20, 30, 40]})\ndf\nOut[7]: \n   A  B C1 C2          D   E\n0  2  3  5  a 2011-11-11  10\n1  1  3  6  a 2011-11-12  20\n2  2  4  6  a 2011-11-13  30\n3  2  4  6  b 2011-11-14  40\n\ndf.dtypes\nOut[8]: \nA              int64\nB              int64\nC1          category\nC2          category\nD     datetime64[ns]\nE              int64\ndtype: object\n\ndf_ac = df[['A', 'C1', 'C2']]\ndf_ad = df[['A', 'D']]\ndf_abc = df[['A', 'B', 'C1', 'C2']]\ndf_abd = df[['A', 'B', 'D']]\ndf_abe = df[['A', 'B', 'E']]\ndf_acd = df[['A', 'C1', 'C2', 'D']]\ndf_abcd = df[['A', 'B', 'C1', 'C2', 'D']]\n```\n\nUsually, non-numeric types are skipped in aggregation functions:\n\n``` python\ndf.groupby('A').mean()\nOut[16]: \n          B          E\nA                     \n1  3.000000  20.000000\n2  3.666667  26.666667\n\ndf.groupby('A').sum()\nOut[17]: \n    B   E\nA        \n1   3  20\n2  11  80\n\ndf.groupby(['A', 'B']).mean()\nOut[18]: \n      E\nA B    \n1 3  20\n2 3  10\n  4  35\n\ndf.groupby(['A', 'B'], as_index=False).sum()\nOut[19]: \n   A  B   E\n0  1  3  20\n1  2  3  10\n2  2  4  70\n```\n#### Issues:\n\nBut if there are no numeric types, an output varies. (I use here subframes `df_xxx` of the original data frame.)\n\n``` python\n# .mean() always raises.\ndf_ac.groupby('A').mean()                               # (1)\npandas.core.base.DataError: No numeric types to aggregate\n\n# .sum() adds categoricals\ndf_ac.groupby('A').sum()                                # (2)\nOut[21]: \n   C1   C2\nA         \n1   6    a\n2  17  aab\n\n# and tries to do something with datetimes.\ndf_ad.groupby('A').sum()                                # (3)\nOut[22]: \n           D\nA           \n1 2011-11-12\n2        NaT\n\ndf_acd.groupby('A').sum()                               # (4)\nOut[23]: \n    C1   C2                    D\nA                               \n1  6.0    a  2011-11-12 00:00:00\n2  NaN  NaN                  NaN\n\n# It's even worse for multiple groupers.\ndf_abcd.groupby(['A', 'B']).sum()                       # (5)\nOut[24]: \nA  B    \n1  3  C1                      6\n      C2                      a\n      D     2011-11-12 00:00:00\n2  3  C1                      5\n      C2                      a\n      D     2011-11-11 00:00:00\ndtype: object\n\ndf_abcd.groupby(['A', 'B'], as_index=False).sum()       # (6)\nOut[25]: \n0  A                       1\n   B                       3\n   C1                      6\n   C2                      a\n   D     2011-11-12 00:00:00\n1  A                       2\n   B                       3\n   C1                      5\n   C2                      a\n   D     2011-11-11 00:00:00\n2  A                       4\n   B                       8\ndtype: object\n\n# Additionally, the index is not reset here and the grouper columns are transformed.\ndf_abc.groupby(['A', 'B'], as_index=False).sum()        # (7)\nOut[26]: \n     A  B  C1  C2\nA B              \n1 3  1  3   6   a\n2 3  2  3   5   a\n  4  4  8  12  ab\n\n# Sometimes an empty data frame is returned  (a reasonable output):\ndf_abd.groupby(['A', 'B']).sum()                        # (8)\nOut[27]: \nEmpty DataFrame\nColumns: []\nIndex: []\n\n# but not always:\ndf_abd.groupby(['A', 'B'], as_index=False).sum()        # (9)\nOut[28]: \n0  A                      1\n   B                      3\n   D    2011-11-12 00:00:00\n1  A                      2\n   B                      3\n   D    2011-11-11 00:00:00\n2  A                      4\n   B                      8\ndtype: object\n\ndf_abcd.groupby(['A', 'B']).sum()                       # (10)\nOut[29]: \nA  B    \n1  3  C1                      6\n      C2                      a\n      D     2011-11-12 00:00:00\n2  3  C1                      5\n      C2                      a\n      D     2011-11-11 00:00:00\ndtype: object\n```\n##### Some other issues:\n\nMultiple groupers with a categrical one (it's already addressed in #13204).\n\n``` python\ndf.groupby(['A', 'C1'], as_index=False).sum()           # (11)\nOut[36]: \n       A  C1   B   E\nA C1                \n1 5  NaN NaN NaN NaN\n  6  NaN NaN NaN NaN\n  7  NaN NaN NaN NaN\n2 5  NaN NaN NaN NaN\n  6  NaN NaN NaN NaN\n  7  NaN NaN NaN NaN\n```\n\n`apply`\n\n``` python\n# mean() as expected:\ndf.groupby(['A', 'B']).apply(lambda x: np.mean(x))\nOut[30]: \n       A    B     E\nA B                \n1 3  1.0  3.0  20.0\n2 3  2.0  3.0  10.0\n  4  2.0  4.0  35.0\n\n# sum() not:\ndf.groupby(['A', 'B']).apply(lambda x: np.sum(x))       # (12)\nOut[31]: \nA  B    \n1  3  A                       1\n      B                       3\n      C1                      6\n      C2                      a\n      D     2011-11-12 00:00:00\n      E                      20\n2  3  A                       2\n      B                       3\n      C1                      5\n      C2                      a\n      D     2011-11-11 00:00:00\n      E                      10\n   4  A                       4\n      B                       8\n      E                      70\ndtype: object\n```\n\n`transform`\n\n``` python\n# it's ok with numeric types only\ndf_abe.groupby(['A', 'B']).transform('sum')\nOut[32]: \n    E\n0  10\n1  20\n2  70\n3  70\n\n# Doesn't transform anything with mixed types:\ndf.groupby(['A', 'B']).transform('sum')                 # (13)\nOut[33]: \n  C1 C2          D   E\n0  5  a 2011-11-11  10\n1  6  a 2011-11-12  20\n2  6  a 2011-11-13  30\n3  6  b 2011-11-14  40\n\n# but someimtes transforms categoricals\ndf_abc.groupby(['A', 'B']).transform('sum')             # (14)\nOut[34]: \n   C1  C2\n0   5   a\n1   6   a\n2  12  ab\n3  12  ab\n\n# and sometimes not:\ndf_abc.groupby(['A', 'B'], as_index=False).transform('sum')     # (15)\nOut[35]: \n  C1 C2\n0  5  a\n1  6  a\n2  6  a\n3  6  b\n```\n#### What should be the expected output?\n\nSome ideas for aggregation (with `sum()`) when there's no numeric types (1)-(10):\n1. always raise (as `.mean()` does)\n2. return an empty DataFrame as in (8)\n   (but (a) should `.mean()` do the same? (b) should groupers be returned when `as_index=False`?)\n3. ???\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Linux\nmachine: x86_64\n\npandas: 0.18.1\nCython: 0.24\nnumpy: 1.11.0\nIPython: 4.0.1\n```\n\nExactly the same output with:\n\n```\npandas: 0.18.1+119.gd405bf2\n```\n##### Just a thought about a possible approach:\n\nA call to `_python_apply_general` inside `_python_agg_general`\nhttps://github.com/pydata/pandas/blob/master/pandas/core/groupby.py#L795\nseems to trigger some of the issues. (It doesn't exclude grouper columns when `as_index=False`, treats categoricals as their underlying types, and possibly just replicates some actions from earlier part of `_python_agg_general`.)\n\nThe following change affects (and possibly solves, at least partially) \nissues (5), (6), (7), (9), (10), (14):\n\n``` python\ndiff --git a/pandas/core/groupby.py b/pandas/core/groupby.py\nindex bea62e9..0d401e2 100644\n--- a/pandas/core/groupby.py\n+++ b/pandas/core/groupby.py\n@@ -791,10 +791,7 @@ class _GroupBy(PandasObject, SelectionMixin):\n             except TypeError:\n                 continue\n\n-        if len(output) == 0:\n-            return self._python_apply_general(f)\n-\n-        if self.grouper._filter_empty_groups:\n+        if len(output) > 0 and self.grouper._filter_empty_groups:\n\n             mask = counts.ravel() > 0\n             for name, result in compat.iteritems(output):\n```\n"},{"labels":["api",null,null],"text":"xref #13361\r\n- [x] support union w `Series/CategoricalIndex` as well as `Categorical` #14199\r\n- [x] add `ignore_order` to ignore the raising on an ordered Categorical (and just have it work) #15219 \r\n- [ ] do we want to put this in the `pd` namespace (or change its name).  Consider `Categorical.from_union(...)`\r\n"},{"labels":["api",null],"text":"In the following examples there are three different merge behaviours when it comes to handling NaNs.\nThey are all based on pd.merge(..., how=\"left\", ...)\n\nThe difference depends on: \n##### 1) Whether we are merging using an index or a column.\n##### 2) Whether the column keys we are merging on are the same value or not (i.e. if left_on = right_on).\n\nArguably, if we specify \"left\" as the merging criterion, the desired behaviour is to have NaNs in the columns coming from the right dataframe where there is no match between the left and right dataframes' key columns (see first merge in example below, 'd' and 'e' columns). \nThe problem is, if we are merging on left's index, the NaNs get filled with the index values from the left dataframe even if the names of the two columns don't match ('c' and 'd' in the example). We are thus led to believe there was a perfect match between the index of the left dataframe and the \"key\" column of the right dataframe ('d' here).\n#### Gotchas:\n###### -There is something puzzling going on with the new indices of the resulting dataframe (when merging on index).\n###### -Type casting occurs when merging on index, perhaps suggesting NaNs are explicitly filled in a second step.\n#### Proposed behaviour:\n\nMaybe it is simply a matter of removing this NaN filling step.\n##### Better yet, the \"key\" column in the merged dataframe should perhaps bear the name of left's index not of the \"right_on\" key (provided we used left's index to merge). I.e. in the second merge of the example, the 'd' column should be called 'c'.\n\nThis is really the source of the confusion when the two names are different. When they are the same the \"no NaN\" behaviour is arguably legitimate.\n\nAlso it might be worthwhile to cast the final column back to the original dtype if there are no NaNs.\n\nMaybe this is not really an issue though, more something to be aware of. I would be interested in hearing any motivation behind this behaviour. \n##### Looking forward to reading your thoughts!\n### Code Sample:\n\n```\ndf1 = pd.DataFrame(columns=['a','b','c'], data=[[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\ndf1_c_index = df1.set_index('c')\ndf2 = pd.DataFrame(columns=['d','e'], data=[[3,14],[6,15],[13,16]])\n\nprint 'df1', '\\n', df1, '\\n'\nprint 'df1_c_index', '\\n', df1_c_index, '\\n'\nprint 'df2', '\\n', df2, '\\n'\n\nprint \"pd.merge(df1, df2, how='left', left_on='c', right_on='d')\", '\\n'\nprint pd.merge(df1, df2, how='left', left_on='c', right_on='d'), '\\n'\n\nprint \"pd.merge(df1_c_index, df2, how='left', left_index=True, right_on='d')\", '\\n'\nprint pd.merge(df1_c_index, df2, how='left', left_index=True, right_on='d'), '\\n'\n\ndf2.rename(columns={'d':'c'}, inplace=True)\n\nprint 'df1', '\\n', df1, '\\n'\nprint 'df1_c_index', '\\n', df1_c_index, '\\n'\nprint 'df2', '\\n', df2, '\\n'\n\nprint \"pd.merge(df1, df2, how='left', left_on='c', right_on='c')\", '\\n'\nprint pd.merge(df1, df2, how='left', left_on='c', right_on='c'), '\\n'\n\nprint \"pd.merge(df1_c_index, df2, how='left', left_index=True, right_on='c')\", '\\n'\nprint pd.merge(df1_c_index, df2, how='left', left_index=True, right_on='c'), '\\n'\n```\n### Output:\n\n![screen shot 2016-06-05 at 21 06 48](https://cloud.githubusercontent.com/assets/5802525/15807927/8aef526e-2b61-11e6-856f-e74d2bffd970.png)\n\n![screen shot 2016-06-05 at 21 07 05](https://cloud.githubusercontent.com/assets/5802525/15807930/98086efe-2b61-11e6-8de6-97332f436980.png)\n### output of `pd.show_versions()`:\n#### INSTALLED VERSIONS\n\n---\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_GB.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 21.0.0\nCython: None\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: None\nxarray: 0.7.2\nIPython: 4.2.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: 1.0.0\ntables: None\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: None\n"},{"labels":["api",null],"text":"In `df.apply(f, axis=1)` , it means applying `f` on rows. However, In `df.drop([column_name], axis=1)`, it means dropping columns. \nSo, what is the trick to determine in which cases `axis=1` means functioning on rows while in other cases it means functioning on columns ?\n"},{"labels":["api",null],"text":"`skipfooter`  : \"This line <a href=\"https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L729\">here</a> says I am the right choice.\"\r\n\r\n`skip_footer` : \"This line <a href=\"https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L519\">here</a> says I am the right choice.\"\r\n\r\nWhatever the case is, something should certainly be done to avoid having duplicate arguments for the same thing, as can be seen <a href=\"https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L582\">here</a>\r\n"},{"labels":["api",null,null],"text":"Example of something that previously worked (before 7bbd031104ee161b2fb79ba6f5732910661f94f8):\n\n``` .python\n>>> s = pd.Series([lambda x: x] * 10)\n>>> s[s.index > 5] = lambda x: x + 1\n```\n\nBut now the second line tries to apply the function in the rhs to the elements of the Series, rather than assigning them (and throws an exception). This is very counter-intuitive when using `__setitem__` rather than calling `Series.where` directly.\n"},{"labels":["api",null,null,null],"text":"I _think_ this would be just: `.ewm(span).mean() * span`\n\nNow:\n\n``` python\n\nIn [4]: pd.Series(range(10)).ewm(span=2).sum()\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-4-8e0e72f35eb3> in <module>()\n----> 1 pd.Series(range(10)).ewm(span=2).sum()\n\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pandas/core/window.py in __getattr__(self, attr)\n    122 \n    123         raise AttributeError(\"%r object has no attribute %r\" %\n--> 124                              (type(self).__name__, attr))\n    125 \n    126     def _dir_additions(self):\n\nAttributeError: 'EWM' object has no attribute 'sum'\n```\n\nCC @erbian\n"},{"labels":["api",null,null,null,null],"text":"When constructing a `Series` object using a numpy structured data array, if you try and cast it to a `str` (or print it), it throws:\n\n```\nTypeError(ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n```\n\nYou can print a single value from the series, but not the whole series.\n#### Code Sample, a copy-pastable example if possible\n\n``` python\nimport pandas as pd\nimport numpy as np\n\nc_dtype = np.dtype([('a', 'i8'), ('b', 'f4')])\ncdt_arr = np.array([(1, 0.4), (256,  -13)], dtype=c_dtype)\n\npds = pd.Series(cdt_arr, index=['A', 'B'])\n\nprint('pds.iloc[0]: {}'.format(str(pds.iloc[0])))   # (1, 0.4000000059604645)\nprint('pds.iloc[1]: {}'.format(str(pds.iloc[1])))   # (256, -13.0)\nprint('pds.loc[\"A\"]: {}'.format(str(pds.loc['A']))) # Works\nprint('pds.loc[\"B\"]: {}'.format(str(pds.loc['B']))) # Works\n\ndef print_error(x):\n    try:\n        o = str(x)      # repr(x) also causes the same errors\n        print(o)\n    except TypeError as e:\n        print('TypeError({})'.format(e.args[0]))\n\na = pds.iloc[0:1]\nb = pds.loc[['A', 'B']]\n\nprint('pds.iloc[0:1]:')\nprint_error(a)\nprint('pds.loc[\"A\", \"B\"]:')\nprint_error(b)\nprint('pds:')\nprint_error(pds)\n\nprint('pd.DataFrame([pds]).T:')\nprint_error(pd.DataFrame([pds]).T)\n\nprint('pds2:')\ncdt_arr_2 = np.array([(1, 0.4)], dtype=c_dtype)\npds2 = pd.Series(cdt_arr_2, index=['A'])\nprint_error(pds2)\n```\n#### Output (actual):\n\n```\n$ python demo_index_bug.py \npds.iloc[0]: (1, 0.4000000059604645)\npds.iloc[1]: (256, -13.0)\npds.loc[\"A\"]: (1, 0.4000000059604645)\npds.loc[\"B\"]: (256, -13.0)\npds.iloc[0:1]:\nTypeError(ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\npds.loc[\"A\", \"B\"]:\nTypeError(ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\npds:\nTypeError(ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\npd.DataFrame([pds]).T:\n                         0\nA  (1, 0.4000000059604645)\nB             (256, -13.0)\npds2:\nTypeError(ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n```\n#### output of `pd.show_versions()`:\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.5.2-1-ARCH\nmachine: x86_64\nprocessor: \nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 21.0.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: 1.4.1\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n```\n#### Stack Trace\n\nI swallowed the stack traces to show where this was failing, so here's the traceback for that last error:\n\n```\nTraceback (most recent call last):\n  File \"demo_dtype_bug.py\", line 37, in <module>\n    print(pds2)\n  File \"~/.local/lib/python3.5/site-packages/pandas/core/base.py\", line 46, in __str__\n    return self.__unicode__()\n  File \"~/.local/lib/python3.5/site-packages/pandas/core/series.py\", line 984, in __unicode__\n    max_rows=max_rows)\n  File \"~/.local/lib/python3.5/site-packages/pandas/core/series.py\", line 1025, in to_string\n    dtype=dtype, name=name, max_rows=max_rows)\n  File \"~/.local/lib/python3.5/site-packages/pandas/core/series.py\", line 1053, in _get_repr\n    result = formatter.to_string()\n  File \"~/.local/lib/python3.5/site-packages/pandas/formats/format.py\", line 225, in to_string\n    fmt_values = self._get_formatted_values()\n  File \"~/.local/lib/python3.5/site-packages/pandas/formats/format.py\", line 215, in _get_formatted_values\n    float_format=self.float_format, na_rep=self.na_rep)\n  File \"~/.local/lib/python3.5/site-packages/pandas/formats/format.py\", line 2007, in format_array\n    return fmt_obj.get_result()\n  File \"~/.local/lib/python3.5/site-packages/pandas/formats/format.py\", line 2026, in get_result\n    fmt_values = self._format_strings()\n  File \"~/.local/lib/python3.5/site-packages/pandas/formats/format.py\", line 2059, in _format_strings\n    is_float = lib.map_infer(vals, com.is_float) & notnull(vals)\n  File \"~/.local/lib/python3.5/site-packages/pandas/core/common.py\", line 250, in notnull\n    res = isnull(obj)\n  File \"~/.local/lib/python3.5/site-packages/pandas/core/common.py\", line 91, in isnull\n    return _isnull(obj)\n  File \"~/.local/lib/python3.5/site-packages/pandas/core/common.py\", line 101, in _isnull_new\n    return _isnull_ndarraylike(obj)\n  File \"~/.local/lib/python3.5/site-packages/pandas/core/common.py\", line 192, in _isnull_ndarraylike\n    result = np.isnan(values)\nTypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n```\n"},{"labels":["api",null],"text":"In writing some math code in pandas, I find it necessary to do things like \n\n```\ndf2 = df.sub(ser, axis='columns')\n```\n\ninstead of the shorter and more intuitive\n\n```\ndf2 = df - ser\n```\n\nin order to control the axis along which the series is broadcast.\n\nI think it would be a big improvement syntactically if pandas would automatically broadcast down the axis that didn't have a matching name.\n\nExample:\n\n```\ndf = pd.DataFrame(np.random.rand(3, 2), columns=['a', 'b'])\ndf.index.name = 'dim0'\ndf.columns.name = 'dim1'\ndf\ndim1         a         b\ndim0                    \n0     0.755744  0.321682\n1     0.915464  0.413154\n2     0.647672  0.457927\n```\n\nsubtract:\n\n```\ndf - df['a']    # does not give the desired result\n       a   b   0   1   2\ndim0                    \n0    NaN NaN NaN NaN NaN\n1    NaN NaN NaN NaN NaN\n2    NaN NaN NaN NaN NaN\n```\n\nsubtract, specifying which axis to match on (broadcasting happens on the other axis):\n\n```\ndf.sub(df['a'], axis='index')   # gives the desired result\ndim1    a         b\ndim0               \n0     0.0 -0.434062\n1     0.0 -0.502310\n2     0.0 -0.189745\n```\n\nI am suggesting that the \"-\" operator would look at the names of the indices in the operands and match on the axis that has the same name in the two operands.\n\nBy way of motivation, I'm doing mass spectral matching of compounds, so I could name my indices 'chemical' and 'mass'.\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\n```\nimport pandas as P\nS=P.Series([0.6,0.2,15])\n```\n\n**pandas 0.18+numpy 0.10**:\n\n```\nIn [1]: print S.dtype\nfloat64\n\nIn [2]: print S.values.dtype\nfloat64\n\nIn [3]: print S.map(type)\n0    <type 'numpy.float64'>\n1    <type 'numpy.float64'>\n2    <type 'numpy.float64'>\ndtype: object\n```\n\n**pandas 0.18.1+numpy 0.11.0**:\n\n```\nIn [5]: print S.dtype\nfloat64\n\nIn [6]: print S.values.dtype\nfloat64\n\nIn [7]: print S.map(type)\n0    <type 'float'>\n1    <type 'float'>\n2    <type 'float'>\ndtype: object\n```\n\nI expect to get the same dtype for the 3 print, why this is changed in last version?\n#### output of `pd.show_versions()`\n\n```\npandas: 0.18.1\nnose: 1.3.7\npip: 1.5.4\nsetuptools: 21.0.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: 1.3.5\npatsy: 0.4.1\ndateutil: 2.4.2\npytz: 2016.4\nblosc: 1.2.7\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.1\nopenpyxl: 2.3.5\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: None\nlxml: 3.4.0\nbs4: 4.4.1\nhtml5lib: 0.9999999\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n```\n\nThank you\nGla\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\n```\npd.PeriodIndex([1.1], freq='M')\n# ValueError: Given date string not likely a datetime.\n\npd.PeriodIndex(np.array([1.1]), freq='M')\n# PeriodIndex(['1970-02'], dtype='int64', freq='M'\n```\n#### Expected Output\n\nConsistently raise or coerce to `Period`\n#### output of `pd.show_versions()`\n\n0.18.1\n\nCC: @MaximilianR\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\n\n```\ndf = pd.DataFrame(np.random.randn(6,4), columns=list('ABCD'))\n\n# In ipython-notebook, I'm now just looking at the DF by calling it. This seems to create a view of the DF? If df is not called, the warning doesn't show. It will also show when doing x = df for example.\ndf\n\ndf = df.loc[df['A'] > 0]\n\n# Now calling this, SettingWithCopyWarning shows suggesting to use .loc which is what I do?\ndf.loc[:, 'A'] = 1\n\n```\n#### Expected Output\n\nWhat works without a warning is to use df.assign. But what about single row changes a la df.loc[1,'A'] = 1\n\nLooking at the docs, df.loc seems to be the suggested way that should work fine.\n\nMaybe I'm just missing something and the mistake is on my side. If so I'd be happy to know.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: de_DE.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 20.3\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: 0.2.1\n"},{"labels":["api",null,null,null,null],"text":"```\nimport pandas as pd\nN = 6\ndf = pd.DataFrame(data={\"size\":[0.1]*N,\n                             \"group\":[None]*N}, \n                       index=range(N))\ngb = df.groupby(by=['group'])\nd2 =  gb.transform('sum')\nprint d2\n```\n#### Expected Output\n\n```\n# not actually sure what should be returned, perhaps just the original values\nHere is what I got\n            size\n0  2.605041e-316\n1  6.899144e-310\n2  2.584561e+161\n3  1.485397e-313\n4  1.916117e+214\n5  3.075369e+203\n\n\n#### output of ``pd.show_versions()``\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.2.0-54-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.7.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: 0.7.2\nIPython: 4.2.0\nsphinx: 1.3.6\npatsy: 0.4.1\ndateutil: 2.2\npytz: 2016.4\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.1\nopenpyxl: 2.2.0-b1\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.5\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: None\n```\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\n\n``` python\nIn [108]: i = pd.Index([])\n\nIn [109]: bool(i)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-109-43b3c3723ad4> in <module>()\n----> 1 bool(i)\n\nC:\\Users\\Chris\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\indexes\\base.py in __nonzero__(self)\n   1227         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n   1228                          \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n-> 1229                          .format(self.__class__.__name__))\n   1230 \n   1231     __bool__ = __nonzero__\n\nValueError: The truth value of a Index is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\nIn [110]: i.empty\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-110-1524e0ba750e> in <module>()\n----> 1 i.empty\n\nAttributeError: 'Index' object has no attribute 'empty'\n```\n"},{"labels":["api",null,null],"text":"```\ndef get_desc(x):\n    return \"Intermission\" if x[\"ID\"] == \"0*\" else x.workTitle.string + \" by \" + x.composerName.string\nworks[\"nice\"] = works[0].apply(get_desc)\n```\n\non this https://github.com/nyphilarchive/PerformanceHistory dataset\n\nI would like pandas to tell me which line the function fails on, rather than something annoying like\n\n```\nTypeErrorTraceback (most recent call last)\n<ipython-input-139-e53aa56df99a> in <module>()\n      1 def get_desc(x):\n      2     return \"Intermission\" if x[\"ID\"] == \"0*\" else x.workTitle.string + \" by \" + x.composerName.string\n----> 3 works[\"nice\"] = works[0].apply(get_desc)\n\n/usr/local/lib/python3.4/dist-packages/pandas/core/series.py in apply(self, func, convert_dtype, args, **kwds)\n   2235             values = lib.map_infer(values, boxer)\n   2236 \n-> 2237         mapped = lib.map_infer(values, f, convert=convert_dtype)\n   2238         if len(mapped) and isinstance(mapped[0], Series):\n   2239             from pandas.core.frame import DataFrame\n\npandas/src/inference.pyx in pandas.lib.map_infer (pandas/lib.c:63043)()\n<ipython-input-139-e53aa56df99a> in get_desc(x)\n      1 def get_desc(x):\n----> 2     return \"Intermission\" if x[\"ID\"] == \"0*\" else x.workTitle.string + \" by \" + x.composerName.string\n      3 works[\"nice\"] = works[0].apply(get_desc)\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n```\n"},{"labels":["api",null,null,null],"text":"They provide scalars are the values rather all the corresponding labels:\r\n\r\n```\r\nIn [81]: index = pd.date_range('2000-01-01', periods=5)\r\n\r\nIn [82]: s = pd.Series(np.arange(index.size), index)\r\n\r\nIn [83]: s\r\nOut[83]:\r\n2000-01-01    0\r\n2000-01-02    1\r\n2000-01-03    2\r\n2000-01-04    3\r\n2000-01-05    4\r\nFreq: D, dtype: int64\r\n\r\nIn [84]: g = s.groupby(pd.Grouper(freq='2D'))\r\n\r\nIn [85]: g.groups\r\nOut[85]:\r\n{Timestamp('2000-01-01 00:00:00', offset='2D'): 2,\r\n Timestamp('2000-01-03 00:00:00', offset='2D'): 4,\r\n Timestamp('2000-01-05 00:00:00', offset='2D'): 5}\r\n```\r\n\r\nPer the [groupby docs](http://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-object-attributes), I expected something like the following instead:\r\n\r\n```\r\nIn [85]: g.groups\r\nOut[85]:\r\n{Timestamp('2000-01-01 00:00:00', offset='2D'):\r\n [Timestamp('2000-01-01 00:00:00', offset='D'),\r\n  Timestamp('2000-01-02 00:00:00', offset='D')],\r\n Timestamp('2000-01-03 00:00:00', offset='2D'):\r\n [Timestamp('2000-01-03 00:00:00', offset='D'),\r\n  Timestamp('2000-01-04 00:00:00', offset='D')],\r\n Timestamp('2000-01-05 00:00:00', offset='2D'):\r\n [Timestamp('2000-01-05 00:00:00', offset='D')]}\r\n```\r\n\r\nWe see the same behavior for `resample.groups` as well:\r\n\r\n```\r\nIn [103]: s.resample('2D').groups\r\nOut[103]:\r\n{Timestamp('2000-01-01 00:00:00', offset='2D'): 2,\r\n Timestamp('2000-01-03 00:00:00', offset='2D'): 4,\r\n Timestamp('2000-01-05 00:00:00', offset='2D'): 5}\r\n```\r\n"},{"labels":["api",null],"text":"Many DataFrame methods ([now including `__getitem__`](https://github.com/pydata/pandas/issues/11485)) accept callables that take the DataFrame as input, e..g, `df[lambda x: x.sepal_length > 3]`.\n\nHowever, this is annoyingly verbose. I recently suggested (https://github.com/pydata/pandas/issues/13040) enabling argument-free lambdas like `df[lambda: sepal_length > 3]`, but this isn't a viable solution (too much magic!) because it's impossible to implement with Python's standard scoping rules.\n\n[pandas-ply](https://github.com/coursera/pandas-ply) and [dplython](https://github.com/dodger487/dplython) provide an alternative approach, based on a magic `X` operator, e.g.,\n\n``` python\n(flights\n  .groupby(['year', 'month', 'day'])\n  .ply_select(\n    arr = X.arr_delay.mean(),\n    dep = X.dep_delay.mean())\n  .ply_where(X.arr > 30, X.dep > 30))\n```\n\npandas-ply also introduces (injects onto pandas.DataFrame) two new dataframe methods `ply_select` and `ply_where` that accept these symbolic expression build from `X`. dplython takes a different approach, introducing it's own dplyr like API for chaining expressions instead of using method chaining. The pandas-ply approach is much closer to what makes sense for pandas proper, given that we already support method chaining.\n\nI think we should consider introducing an object like `X` into pandas proper and supporting its use on all pandas methods that accept callables that take the DataFrame as input. \n\nI don't think we need to port `ply_select` and `ply_where`, because support for expressions in `DataFrame.assign` and indexing is a good substitute.\n\nSo my proposed syntax (after `from pandas import X`) looks like the following:\n\n``` python\n(flights\n .groupby(['year', 'month', 'day'])\n .assign(\n     arr = X.arr_delay.mean(),\n     dep = X.dep_delay.mean())\n [(X.arr > 30) & (X.dep > 30)])\n```\n\nIndexing is a little uglier than using the `ply_where` method, but otherwise this is a nice improvement. \n\nBest of all, we don't need do any special tricks to introduce new scopes -- we simply define `X.__getattr__` to looking attributes as columns in the DataFrame context. I expect we could even reuse the expression engines from pandas-ply or dplython directly, perhaps with a few modifications.\n\nIn my mind, this would mostly obviate the need for pandas-ply, though the alternate API provided by dpython would still be independently useful. In an ideal world, our `X` implementation in pandas would be something that could be reused by dplython.\n\ncc @joshuahhh @dodger487\n"},{"labels":["api",null,null],"text":"xref #5751\n\n[questions from SO](http://stackoverflow.com/questions/37078880/status-of-parallelization-of-pandas-apply?noredirect=1#comment61713215_37078880).\n[mrocklins nice example of using .apply](http://stackoverflow.com/questions/31361721/python-dask-dataframe-support-for-trivially-parallelizable-row-apply)\n\nSo here is an example of how to do a parallel apply using [dask](https://dask.readthedocs.io/en/latest/). This could be baked into `.apply()` in pandas by the following signature enhancement:\n\ncurrent:\n\n```\nDataFrame.apply(self, func, axis=0, broadcast=False, raw=False, reduce=None, \n                args=(), **kwds)\n```\n\nproposed:\n\n```\nDataFrame.apply(self, func, axis=0, broadcast=False, raw=False, reduce=None, \n                engine=None, chunksize=None, args=(), **kwds)\n```\n\nwhere `engine='dask'` (or `numba` at some point) are possibilities\n`chunksize` would map directly to `npartitions` and default to the number of cores if not specified.\nfurther would allow `engine` to be a meta object like `Dask(scheduler='multiprocessing')` to support other options one would commonly pass (could also move `chunksize` inside that instead of as a separate object).\n\nimpl and timings:\n\n```\nfrom functools import partial\nimport pandas as pd\nimport dask\nimport dask.dataframe as dd\nfrom dask import threaded, multiprocessing\nfrom time import sleep\n\npd.__version__\ndask.__version__\n\ndef make_frame(N):\n    return pd.DataFrame({'A' : range(N)})\ndef slow_func(x):\n    sleep(0.5)\n    return x\ndf = make_frame(40)\n\n# reg apply\ndef f1(df):\n    return df.apply(slow_func, axis=1)\n# dask apply\ndef f2(df, get):\n    ddf = dd.from_pandas(df, npartitions=8, sort=False)\n    return ddf.apply(slow_func, columns=df.columns, axis=1).compute(get=get)\n\nf1 = partial(f1, df)\nf2_threaded = partial(f2, df, threaded.get)\nf2_multi = partial(f2, df, multiprocessing.get)\n\nresult1 = f1()\nresult2 = f2_threaded()\nresult3 = f2_multi()\n```\n\n```\nIn [18]: result1.equals(result2)\nOut[18]: True\n\nIn [19]: result1.equals(result3)\nOut[19]: True\n\nIn [20]: %timeit -n 1 -r 1 f1()\n1 loop, best of 1: 20.6 s per loop\n\nIn [21]: %timeit -n 1 -r 1 f2_threaded()\n1 loop, best of 1: 3.03 s per loop\n\nIn [22]: %timeit -n 1 -r 1 f2_multi()\n1 loop, best of 1: 3.07 s per loop\n```\n\nNow for some caveats.\n\nPeople want to parallelize a poor implementation. Generally you proceed thru the following steps first:\n- get your problem correct; optimizing incorrect results is useless\n- profile profile profile. This is always the first thing to do\n- use built-in pandas / numpy vectorized routines\n- use `cython` or `numba` on the user defined function\n- `.apply` is always the last choice\n- if its still not enough, parallelizaton.\n\nYou always want to make code simpler, not more complex. Its hard to know a-priori where bottlenecks are. People think `.apply` is some magical thing, its NOT, its JUST A FOR LOOP. The problem is people tend to throw in the kitchen sink, and just everything, which is just a terrible idea.\n\nOk my 2c about optimizing things. \n\nIn order for parallelization to actually matter the function you are computing should take some non-trivial amount of time to things like:\n- iteration costs of the loop\n- serialization time (esp if using multi-processing / distributed computing)\n- does the function release the GIL if not, then threading will probably not help much\n- development resources (your time)\n\nIf these criteria are met, then sure give it a try.\n\nI think providing pandas a first class way to parallelize things, even tough people will just naively use it is probably not a bad thing.\n\nFurther extensions to this are: `to_dask()` (return a `dask.dataframe` to the user directly), and `engine='dask'` syntax for `.groupby()`\n"},{"labels":["api",null],"text":"A simple `import pandas` apparently does a `np.seterr(all='ignore')`: https://github.com/pydata/pandas/blob/23eb483d17ce41c0fe41b0bfa72c90df82151598/pandas/compat/numpy/__init__.py#L8-L9\n\nThis is a problem when using Pandas as a library, particularly during testing: a test (completely unrelated to Pandas) that should have produced warnings can fail to do so just because some earlier test happened to import a library that imported a library that imported something from Pandas (for example). Or a test runner that's done a `np.seterr(all='raise')` specifically to catch potential issues can end up catching nothing, because some Pandas import part-way through the test run turned the error handling off again.\n\nI'm working around this right now by wrapping every pandas import in `with np.errstate():`. For example:\n\n``` python\nwith np.errstate():\n    from pandas import DataFrame\n```\n\nBut that's (a) ugly, and (b) awkward if the pandas import is in a third-party library out of your control.\n\nPlease consider removing this feature!\n#### Code Sample, a copy-pastable example if possible\n\n``` python\nEnthought Canopy Python 2.7.11 | 64-bit | (default, Mar 18 2016, 14:49:17) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import numpy, pandas\n>>> 1.0 / numpy.arange(10)\narray([        inf,  1.        ,  0.5       ,  0.33333333,  0.25      ,\n        0.2       ,  0.16666667,  0.14285714,  0.125     ,  0.11111111])\n```\n#### Expected Output\n\nI expected to see a `RuntimeWarning` from the division by zero above, as in:\n\n``` python\nEnthought Canopy Python 2.7.11 | 64-bit | (default, Mar 18 2016, 14:49:17) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import numpy\n>>> 1.0 / numpy.arange(10)\n__main__:1: RuntimeWarning: divide by zero encountered in divide\narray([        inf,  1.        ,  0.5       ,  0.33333333,  0.25      ,\n        0.2       ,  0.16666667,  0.14285714,  0.125     ,  0.11111111])\n```\n#### output of `pd.show_versions()`\n\n```\n>>> pandas.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 13.4.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_GB.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.6.7\nCython: None\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\n```\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\nSplit from #13071. Currently it returns int and loses freq info.\n\n```\npd.Period('2011-03', freq='M') - pd.Period('2011-01', freq='M') \n#2\n```\n#### Expected Output\n\nCurrent ideas are:\n1. Return `Timedelta` adding support of `Monthly` freq.\n2. Return `Perioddelta` newly created.\n3. Return `DateOffset`.\n#### output of `pd.show_versions()`\n\non current master.\n\nCC: @jreback @MaximilianR \n"},{"labels":["api",null,null],"text":"It would be nice if the `tz_localize` function of a `DatetimeIndex` had an optional flag for silently returning `NaT` instead of throwing a `NonExistentTimeError`, if the timestamp is not valid in the given timezone (for example due to DST changes).\nI ran into this problem while trying to `tz_localize` a large index, and it seems to me that this would be a much better solution than manually handling the exception with a lambda expression in a (slow) python loop.\n"},{"labels":["api",null,null,null,null],"text":"related issues:\r\n- [x] #13121 \r\n- [x] #5839\r\n- [x] #9867\r\n- [x] #13255 \r\n- [x] #20066 \r\n- [x] #14927 (about detecting mutating functions)\r\n\r\n_Thank you very much for provide us pandas which is really a good packages I have used!_\r\n### There were several times when I use pandas but get inconsistent return format which finally leads to a break!\r\n- for example:\r\n\r\n``` python\r\nimport numpy as np\r\nimport pandas as pd\r\ndf = pd.DataFrame(np.random.randn(10,1), columns = ['x'])\r\ndf['y'] = 1\r\ndf.groupby('y').apply(lambda x: x.x)\r\n\r\n#x        0         1         2         3         4         5         6  \\\r\n#y                                                                        \r\n#1 -1.12114  0.679616 -1.392863  0.032637 -0.051134  0.594201 -0.238833   \r\n\r\n#x        7        8         9  \r\n#y                              \r\n#1  0.95173  1.07469 -0.062198  \r\n\r\ndf2 = df.copy()\r\ndf2['y'] = np.random.randint(1,3,10)\r\n# do something similar like above\r\ndf2.groupby('y').apply(lambda x: x.x)\r\n#y   \r\n#1  3    0.032637\r\n#   8    1.074690\r\n#2  0   -1.121140\r\n#   1    0.679616\r\n#   2   -1.392863\r\n#   4   -0.051134\r\n#   5    0.594201\r\n#   6   -0.238833\r\n#   7    0.951730\r\n#   9   -0.062198\r\n#Name: x, dtype: float64\r\n```\r\n\r\neven though, I knew the way to avoid this issue,  I still believe it is better to return the same format. It's really annoying when some inconsistent return format happened which force me to   look backwards to use some awkward methods to avoid it.\r\n\r\nSo, if possible, I do suggest the above function will only return one kind of format.\r\n"},{"labels":["api",null],"text":"With [a little bit of magic](https://github.com/shoyer/pandas-magic/blob/master/pandas_magic/__init__.py#L13-L33), we could make the following syntax work:\n\n``` python\n(df[lambda: sepal_length > 3]\n .groupby(lambda: pd.cut(sepal_width, 5))\n .apply(lambda: petal_width.mean()))\n```\n\nSyntax like `df[lambda: sepal_length > 3]` is an alternative to more verbose alternatives like the recently added `df[lambda x: x.sepal_length > 3]` (https://github.com/pydata/pandas/issues/11485). Here we use `lambda` essentially in place of a macro that would allow for delayed evaluation (which of course are not supported by Python syntax).\n\nMy proposal is to add support for such \"thunks\" to every pandas method that accepts a callable function that currently must take a single argument work on DataFrames.\n\nUnder the covers, this works by (1) copying the `globals()` dictionary at evaluation time and (2) injecting the current DataFrame into it. We would further ensure that this only works on lambda functions, by checking `f.func_name == '<lambda>'`.\n\nThe main gotcha is that [it isn't possible to dynamically override ~~local~~ non-global variables](http://stackoverflow.com/questions/8028708/dynamically-set-local-variable-in-python) without [some true dark magic](https://gist.github.com/njsmith/2347382#file-inject-py-L40). This means that code like the following is going to behave contrary to expectations:\n\n``` python\ndef x_plus_one(df):\n    x = 0\n    # uses x = 0 instead of df.x\n    return df.pipe(lambda: x + 1)\n\ndf = pd.DataFrame({'x': np.arange(100)})\nresult = x_plus_one(df)  # all 1s, not range(1, 101)\n```\n\nIs this so bad? Shadowing variables in an outer scope is already poor design, but this is a pretty serious departure from expectations.\n\nThe other danger is that this could mask bugs, e.g., if a user mistakenly types `df.pipe(lambda: x)` instead of `df.pipe(lambda x: x)`. This is an unavoidable danger of spelling two APIs with similar syntax.\n\nOn the plus side, this proposal is safer than @njsmith's \"true dark magic\" context manager (see above) for injecting DataFrame columns, because there's no possibility of variable assignment inside a `lambda`.\n\nWould this be a good idea for pandas?\n"},{"labels":["api",null],"text":"My understanding of how multiindexes work is fuzzy, but as far as I can tell set_levels isn't updating the labels properly. The example probably explains better than I can:\n#### Code\n\n``` python\ndf = pd.DataFrame(data = [\"foo\",\"bar\"])\ndf2 = pd.concat([df,df,df],keys = [\"a\",\"b\",\"c\"])\nprint df2\nnewIndex = df2.index.get_level_values(0).map(lambda x:x + \"!\")\ndf2.index.set_levels(newIndex,level=0,inplace=True)\nprint df2\n```\n#### Expected Output\n\n```\n       0\na 0  foo\n  1  bar\nb 0  foo\n  1  bar\nc 0  foo\n  1  bar\n        0\na! 0  foo\n   1  bar\nb! 0  foo\n   1  bar\nc! 0  foo\n   1  bar\n```\n#### Actual Output\n\n```\n       0\na 0  foo\n  1  bar\nb 0  foo\n  1  bar\nc 0  foo\n  1  bar\n        0\na! 0  foo\n   1  bar\n   0  foo\n   1  bar\nb! 0  foo\n   1  bar\n```\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.3.0-040300-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: None\npip: 8.1.1\nsetuptools: 20.3.1\nCython: None\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: 4.0.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.0\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: 0.8\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n```\n"},{"labels":["api",null,null],"text":"deprecate `between` in favor of other indexing options (e.g. `loc`)\n\nxref: https://github.com/pydata/pandas/issues/12398, #12402 (not merged)\n"},{"labels":["api",null,null],"text":"Pandas used to allow the writing of empty HDF5 datasets through its pytables interface code. However, after upgrading to 0.17 (from 0.11), we've discovered that this behaviour is intentionally\nshort circuited. The library behaves as though the dataset is being written, but simply ignores the request and the resulting HDF5 file doesn't contain the requested table.\n\nThe offending code is in pandas/io/pytables.py:_write_to_group()\n\n```\n    # we don't want to store a table node at all if are object is 0-len\n    # as there are not dtypes\n    if getattr(value, 'empty', None) and (format == 'table' or append):\n        return\n```\n\nWe've worked around it by patching our installed copy of pandas, but we'd like to know the provocation behind this code before submitting a pull request. The comment implies that the lack of dtypes in the dataset is the cause, however each pandas column has type information even if empty.\n\nAny clarification would be appreciated\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\n```\npd.Series(index=pd.to_datetime(['2016-04-15 21:18:04.683115', '2016-04-15 21:18:05.384231',\n               '2016-04-15 21:18:05.504638', '2016-04-15 21:18:05.510117',\n               '2016-04-15 21:18:05.370865', '2016-04-15 21:18:05.577177',\n               '2016-04-15 21:18:06.088652', '2016-04-15 21:18:05.865269',\n               '2016-04-15 21:18:05.867652', '2016-04-15 21:18:07.207526'])).truncate(before = '2016-04-15 21:18:03.577077', after ='2016-04-15 22:18:03.577077')\n```\n\nI get the error:\n\n> ValueError: index must be monotonic increasing or decreasing \\pandas\\indexes\\base.py\", line 2791, in _searchsorted_monotonic\n#### Expected Output\n\n```\n2016-04-15 21:18:04.683115   NaN\n2016-04-15 21:18:05.384231   NaN\n2016-04-15 21:18:05.504638   NaN\n2016-04-15 21:18:05.510117   NaN\n2016-04-15 21:18:05.370865   NaN\n2016-04-15 21:18:05.577177   NaN\n2016-04-15 21:18:06.088652   NaN\n2016-04-15 21:18:05.865269   NaN\n2016-04-15 21:18:05.867652   NaN\n2016-04-15 21:18:07.207526   NaN\n```\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.3\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\n"},{"labels":["api",null,null],"text":"#### Code Sample\n\n`>>> import pandas as pd`\n `>>> df = pd.DataFrame(np.zeros((4,5),dtype=int))`\n`>>> sparse = df.to_sparse()`\n`>>> sparse.loc[[1]] != 0`\n#### Expected Output\n\n  `0      1      2      3      4`\n `1  False  False  False  False  False`\n### Observed Output\n\n`TypeError: _combine_const() got an unexpected keyword argument 'raise_on_error'`\n#### output of `pd.show_versions()`\n\nINSTALLED VERSIONS\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-35-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: None\npip: 8.1.1\nsetuptools: 20.9.0\nCython: None\nnumpy: 1.10.4\nscipy: 0.16.1\nstatsmodels: None\nxarray: None\nIPython: None\nsphinx: 1.3.1\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.3.1\nopenpyxl: None\nxlrd: None\nxlwt: 1.0.0\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.999\nhttplib2: 0.8\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: None\n### Reason\n\n`pandas.core.frame` implements `_combine_const(self, other, func, raise_on_error=True)`\n\nwhile `pandas.sparse.frame` implements `_combine_const(self, other, func)`\n\nand the missing argument in the signature is the cause of the error when using sparse frames. This is raised due to the call `res = self._combine_const(other, func, raise_on_error=False)` from `_comp_method_FRAME` at `pandas.core.ops` \n"},{"labels":["api",null,null],"text":"Floating an idea: `pd.PeriodIndex.from_range` & `pd.DatetimeIndex.from_range` as `classmethod`s to mirror `pd.period_range` & `pd.date_range` respectively. This encapsulates constructors on the class.\n\nAlso could have `pd.DatetimeIndex.from_dataframe` for https://github.com/pydata/pandas/pull/12967\n"},{"labels":["api",null,null,null,null],"text":"``` python\nIn [127]: isinstance(pd.Timestamp(np.nan), pd.tslib.Timestamp)\nOut[127]: False\n\nIn [128]: isinstance(pd.Timestamp('2015'), pd.tslib.Timestamp)\nOut[128]: True\n\nIn [129]: isinstance(pd.Timestamp(np.nan), datetime.datetime)\nOut[129]: True\n\nIn [130]: isinstance(pd.Timestamp('2015'), datetime.datetime)\nOut[130]: True\n```\n"},{"labels":["api",null,null],"text":"I will be happy to write up a pull request, but first wanted to gauge the sanity of my suggestion:\n\nI think that `swaplevel()` deserves default values for its parameters, just like its friends like `stack()` and `unstack()` and `sortlevel()` that also all take an initial `level` argument. I suggest:\n\n```\ndef swaplevel(self, i=-2, j=-1, axis=0):\n```\n\nThis provides the greatest symmetry with the other methods that operate on levels: they all, if no level is specified, operate on the innermost level as their default.\n\nIn the very common case where there are only two levels to the multi-index anyway, this would reduce this frequent operation to simply `.swaplevel()` or `.swaplevel(axis='columns')` without, I don't think, any more loss of readability than when `stack()` or `unstack()` fail to specify the level upon which they are operating.\n"},{"labels":["api",null],"text":"Sorry to open a issue for such a minor thing..\nin a DataFrame object, both the `columns` and `index` properties are actually `Index` objects.\nWouldn't be cleaner to rename `index` as `rows` ?\nAs you are working with n-dimensional panels, `rows`  and `columns` could then be just convenient aliases for something like `axis0` and `axis1` respectively (in a df), while further dimensional container could use `axis2`, `axis3` and so on (the current axis names `labels`, `items`, `major_axis` and `minor_axis` do not suggest any particular order).\n"},{"labels":["api",null,null,null],"text":"As discussed here: https://github.com/pydata/pandas/pull/12874, some ideas:\r\n- ~Only return `PeriodIndex`s when resampling `PeriodIndex`~ (#16153)\r\n- Don't use `convention` when doing simple upsampling & downsampling ('simple' meaning a one to many relationship between subperiods and superperiods)\r\n- Define how to handle 'complicated' resampling (meaning a many to many relationship between subperiods & superperiods, such as Week->Month)\r\n\r\nref: https://github.com/pydata/pandas/issues/7744, https://github.com/pydata/pandas/issues/12871, https://github.com/pydata/pandas/pull/8707, https://github.com/pandas-dev/pandas/issues/15944\r\n\r\nEDIT: strikethrough above after completion"},{"labels":["api",null,null,null],"text":"I'm am trying to call the to_dict function on the following DataFrame:\n\n```\nimport pandas as pd\n\ndata = {\"a\": [1,2,3,4,5], \"b\": [90,80,40,60,30]}\n\ndf = pd.DataFrame(data)\n```\n\n```\ndf\n   a   b\n0  1  90\n1  2  80\n2  3  40\n3  4  60\n4  5  30\n```\n\n```\ndf.reset_index().to_dict(\"r\")\n[{'a': 1, 'b': 90, 'index': 0},\n {'a': 2, 'b': 80, 'index': 1},\n {'a': 3, 'b': 40, 'index': 2},\n {'a': 4, 'b': 60, 'index': 3},\n {'a': 5, 'b': 30, 'index': 4}]\n\n```\n\nHowever my problem occurs if I perform a float operation on the dataframe, which mutates the index into a float:\n\n```\n(df*1.0).reset_index().to_dict(\"r\")\n[{'a': 1.0, 'b': 90.0, 'index': 0.0},  \n{'a': 2.0, 'b': 80.0, 'index': 1.0},  \n{'a': 3.0, 'b': 40.0, 'index': 2.0},  \n{'a': 4.0, 'b': 60.0, 'index': 3.0},  \n{'a': 5.0, 'b': 30.0, 'index': 4.0}]\n```\n\nCan anyone explain the above behaviour or recommend a workaround, or verify whether or not this could be a pandas bug? None of the other outtypes in the to_dict method mutates the index as shown above.\n\nI've replicated this on both pandas 0.14 and 0.18 (latest)\n\nMany thanks!\n\nlink to stackoverflow: http://stackoverflow.com/questions/36548151/pandas-to-dict-changes-index-type-with-outtype-records\n"},{"labels":["api",null,null,null],"text":"I think we can move the functionaility to the regular `Business` freqs but just accepting \nthe additional arguments from `CustomBusinesssDay`, IOW, expand `BusinessDay` with default args of `None` for `weekmask` and `holidays` (and just default them).\n\nI think this can be done without any user changes at all. We can then deprecate `CustomBusiness*` in favor of simple `Business*` in 0.19.0.\n"},{"labels":["api",null],"text":"It would be great if there is a support for `inplace` argument for `str.replace`, so I can do\n\n``` python\naddresses.str.replace('\\s+\\d{5}', '', inplace=True)\n```\n\ninstead of \n\n``` python\naddresses = addresses.str.replace('\\s+\\d{5}', '')\n```\n\nThank you!\n"},{"labels":["api",null],"text":"Hi, \n\nI am using Pandas more and more for preparing my dataset before feeding it to other statistical software. \n\nI believe something really useful and, somehow, missing from the current version of Pandas is a simple way to generate dummy variables based on a logical test. \n\nDummy variable coding (0-1) is everywhere in the academic literature (I have a phd in economics) and are a key component of many statistical tests.\n\nSomething like \n\n`pd.get_dummies(df.A > df.C)` that would generate a dummy that is one if A>C and zero otherwise, allowing the user to \n- include/exclude missing variables\n- specify what happens when columns have mixed types (see (https://stackoverflow.com/questions/36373681/how-to-create-dummy-variables-in-pandas-when-columns-can-have-mixed-types)\n\nOf course, we can always generate the dummies above manually, but the experience shows that this simply dummy variable creation can be, at times, tricky.\n\nWhat do you think?\n"},{"labels":["api",null,null],"text":"Numeric indexes are pretty common with scientific datasets and [xarray](http://xarray.pydata.org). For such data, resampling to a new resolution is a pretty common operation.\n\nCurrently, resampling only works for dates. Any thoughts on making it work for all numeric axes? I think every argument in the [signature](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html) would still make sense for numbers, except the rule would be expected to be a number, not a string.\n\nI expect the implementation would be relatively straightforward, given that we don't need to handle the complexity of datetime frequencies.\n\nxref https://github.com/pydata/xarray/pull/818\n"},{"labels":["api",null],"text":"``` python\ndf2=pd.DataFrame([[0,1],[2,3]],columns=['c1','c2'],index=['i1','i2'])\ndf2.index.name='index'\ndf2.to_html()\n```\n\n``` html\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>c1</th>\\n      <th>c2</th>\\n    </tr>\\n    <tr>\\n      <th>index</th>\\n      <th></th>\\n      <th></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>i1</th>\\n      <td>0</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <th>i2</th>\\n      <td>2</td>\\n      <td>3</td>\\n    </tr>\\n  </tbody>\\n</table>\n```\n\n``` python\ndf2.index = df2.index.map(lambda x: x.upper())\ndf2.to_html()\n```\n\n``` html\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>c1</th>\\n      <th>c2</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>I1</th>\\n      <td>0</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <th>I2</th>\\n      <td>2</td>\\n      <td>3</td>\\n    </tr>\\n  </tbody>\\n</table>\n```\n\nI think `map` should accept `inplace`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 18.2\nCython: None\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.0.0\ntables: None\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n```\n"},{"labels":["api",null,null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\n`Timedelta` returns `NaT` when input can be regarded as `NaT`. Otherwise `Period` returns its special representation (This was done in #7485 prior to `Timedelta`).\n\n```\npd.Timedelta('NaT')\n# NaT\n\npd.Period('NaT', freq='M')\n# Period('NaT', 'M')\n```\n#### Expected Output\n\n```\npd.Period('NaT', freq='M')\n# NaT\n```\n\nThe fix should affects:\n- [ ] `Period` and `PeriodIndex`, `Series` ops\n  - [ ] add, sub, comp\n- [x] `PeriodIndex` creation from list-like which contains `Period` and `NaT` (#13430)\n- [ ] `PeriodIndex` boxing\n- [ ] `.to_period`\n- [ ]  `__contains__` any NaT-like(`pd.NaT, None, float('nan'), np.nan`)  (#13582)\n#### output of `pd.show_versions()`\n\nCurrent master.\n"},{"labels":["api",null,null,null],"text":"A DataFrame index has the map function, which expects a mapper. However, unlike mapping a Series, it only accepts callables, and won't work when mapping through dictionaries or series.\n#### Code Sample:\n\n``` python\ndf = pd.DataFrame({'col1': range(5)})\nd = {0: 'zero', 1: 'one', 2: 'two', 3: 'three', 4: 'four'}\ndf.col1.map(d) # perfectly valid\ndf.index.map(d) # TypeError: 'dict' object is not callable\ns = pd.Series(d)\ndf.col1.map(s) # perfectly valid\ndf.index.map(s) # TypeError: 'Series' object is not callable\n```\n#### Expected Output:\n\nI would expect the same output for `df.col1.map(s)` and `df.index.map(s)`, or at least that it returns an nparray with the mapped values, as it does when we do pass a callable, but not that it breaks like it does.\n#### output of `pd.show_versions()`:\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.16.0-67-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_DE.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.6.6\nCython: 0.23.5\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.4\npatsy: 0.4.1\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.5.1\nmatplotlib: 1.5.1\nopenpyxl: 2.3.4\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: 3.6.0\nbs4: None\nhtml5lib: 0.9999999\nhttplib2: 0.9.2\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: 2.39.0\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\r\n##### 1. replace bool with int (#12780)\r\n\r\nNG, should be int Series\r\n\r\n```\r\npd.Series([True]).replace({True: 3})\r\n#0    True\r\n# dtype: bool\r\n```\r\n##### 2. replace datetime/timedelta with int  (#12780)\r\n\r\nNG, should be int Series\r\n\r\n```\r\npd.Series([pd.Timestamp('2011-01-01')]).replace({pd.Timestamp('2011-01-01'): 3})\r\n#0   1970-01-01 00:00:00.000000003\r\n# dtype: datetime64[ns]\r\n\r\npd.Series([pd.Timedelta('1 days')]).replace({pd.Timedelta('1 days'): 3})\r\n#0   00:00:00.000000\r\n# dtype: timedelta64[ns]\r\n```\r\n##### 3. replace datetime/timedelta with object  (#12780)\r\n\r\nNG, coerced to internal repr unexpectedly.\r\n\r\n```\r\nrep = {pd.Timestamp('2011-01-01'): 'a', pd.Timestamp('2011-01-02'): 'b'}\r\npd.Series([pd.Timestamp('2011-01-01'), pd.Timestamp('2011-01-02')]).replace(rep)\r\n#0    1293840000000000000\r\n#1                      b\r\n# dtype: object\r\n```\r\n##### 4. replace with datetimetz (#12780)\r\n\r\nNG, coerced to GMT\r\n\r\n```\r\npd.Series(['a']).replace({'a': pd.Timestamp('2011-01-01', tz='US/Eastern')})\r\n#0   2011-01-01 05:00:00\r\n# dtype: datetime64[ns]\r\n```\r\n\r\n##### 5. replacement of string (open, see #15743)\r\n\r\n```\r\nIn [37]: pd.Series([1,2,3]).replace('1', np.nan)\r\nOut[37]: \r\n0    NaN\r\n1    2.0\r\n2    3.0\r\ndtype: float64\r\n```\r\n\r\n#### output of `pd.show_versions()`\r\n\r\nLatest master.\r\n"},{"labels":["api",null,null],"text":"I ran into this issue today, and it seems like it should be a fairly common situation. I have imported two dataframes (using `pandas.read_stata`) of categorical data that I want to concatenate. One of them might not have an instance of every category that the other one has, so pandas won't concatenate. It seems like it would be more flexible if it could add all missing categories.\n\nI know that this inflexibility is in the [documentation](http://pandas.pydata.org/pandas-docs/stable/categorical.html#merging), but I wonder why it exists. Is there a good reason why pandas shouldn't automatically append new categories as they are encountered?\n#### In\n\n```\ns = pd.Series(['a', 'b'], dtype=\"category\")\ns2 = pd.Series(['a', 'c'], dtype=\"category\")\ns.append(s2)\n```\n#### Expected Output\n\n```\n0    a\n1    b\n0    a\n1    c\ndtype: category\nCategories (3, object): [a, b, c]\n\n```\n#### Actual Output\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-40-e1bb85501be1> in <module>()\n      4 s = pd.Series(['a', 'b'], dtype=\"category\")\n      5 s2 = pd.Series(['a', 'c'], dtype=\"category\")\n----> 6 s.append(s2)\n\n/Users/will/anaconda/lib/python2.7/site-packages/pandas/core/series.pyc in append(self, to_append, verify_integrity)\n   1575             to_concat = [self, to_append]\n   1576         return concat(to_concat, ignore_index=False,\n-> 1577                       verify_integrity=verify_integrity)\n   1578 \n   1579     def _binop(self, other, func, level=None, fill_value=None):\n\n/Users/will/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\n    833                        verify_integrity=verify_integrity,\n    834                        copy=copy)\n--> 835     return op.get_result()\n    836 \n    837 \n\n/Users/will/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in get_result(self)\n    979             # stack blocks\n    980             if self.axis == 0:\n--> 981                 new_data = com._concat_compat([x._values for x in self.objs])\n    982                 name = com._consensus_name_attr(self.objs)\n    983                 return (Series(new_data, index=self.new_axes[0], name=name)\n\n/Users/will/anaconda/lib/python2.7/site-packages/pandas/core/common.pyc in _concat_compat(to_concat, axis)\n   2722     elif 'category' in typs:\n   2723         from pandas.core.categorical import _concat_compat\n-> 2724         return _concat_compat(to_concat, axis=axis)\n   2725 \n   2726     if not nonempty:\n\n/Users/will/anaconda/lib/python2.7/site-packages/pandas/core/categorical.pyc in _concat_compat(to_concat, axis)\n   1948     for x in categoricals[1:]:\n   1949         if not categories.is_dtype_equal(x):\n-> 1950             raise ValueError(\"incompatible categories in categorical concat\")\n   1951 \n   1952     # we've already checked that all categoricals are the same, so if their\n\nValueError: incompatible categories in categorical concat\n```\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.4\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.21\nnumpy: 1.10.2\nscipy: 0.16.1\nstatsmodels: 0.5.0\nxarray: None\nIPython: 4.0.1\nsphinx: 1.2.3\npatsy: 0.3.0\ndateutil: 2.5.0\npytz: 2016.1\nblosc: None\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.5.0\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.7\nlxml: 3.4.0\nbs4: 4.3.2\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.32.1\n\n```\n"},{"labels":["api",null,null,null],"text":"It appears that `Series.argsort()` is implemented as `s.dropna().argsort().reindex(s.index, fill_value=-1) = np.argsort(s.dropna()).reindex(s.index, fill_value=-1)`.\n\nThere are two problems with this:\n(a) Since the result represents integer indices into the original series `s`, the result should not have the same `index` as `s.index` -- it should either be a `Series` with index `[0, 1, ...]`, or more likely simply be a NumPy`array`;\n(b) The way `NaN`s are effectively removed before calling `np.argsort()` leads to indexes that are no longer appropriate into the original `Series`, resulting in the nonsensical results shown in `[9]` and `[10]` below.\n\nNote that: \"As of NumPy 1.4.0 `argsort` works with real/complex arrays containing nan values.\" So it's not obvious to me that there's much to be gained from mucking around with `np.argsort(s.values)`.\n\n```\nPython 3.4.4 (v3.4.4:737efcadf5a6, Dec 20 2015, 20:20:57) [MSC v.1600 64 bit (AMD64)]\nType \"copyright\", \"credits\" or \"license\" for more information.\n\nIPython 4.1.2 -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\n\nIn [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: s = pd.Series([200, 100, 400, 500, np.nan, 300], index=list('abcdef'))\n\nIn [4]: s\nOut[4]:\na    200.0\nb    100.0\nc    400.0\nd    500.0\ne      NaN\nf    300.0\ndtype: float64\n\nIn [5]: s.argsort()\nOut[5]:\na    1\nb    0\nc    4\nd    2\ne   -1\nf    3\ndtype: int64\n\nIn [6]: s.dropna().argsort().reindex(s.index, fill_value=-1)\nOut[6]:\na    1\nb    0\nc    4\nd    2\ne   -1\nf    3\ndtype: int64\n\nIn [33]: np.argsort(s.dropna()).reindex(s.index, fill_value=-1)\nOut[33]:\na    1\nb    0\nc    4\nd    2\ne   -1\nf    3\ndtype: int64\n\nIn [34]: np.argsort(s.dropna().values)\nOut[34]: array([1, 0, 4, 2, 3], dtype=int64)\n\nIn [7]: np.argsort(s.values)\nOut[7]: array([1, 0, 5, 2, 3, 4], dtype=int64)\n\nIn [8]: s[np.argsort(s.values)]  # desired result\nOut[8]:\nb    100.0\na    200.0\nf    300.0\nc    400.0\nd    500.0\ne      NaN\ndtype: float64\n\nIn [9]: s[s.argsort()]  # nonsensical result\nOut[9]:\nb    100.0\na    200.0\ne      NaN\nc    400.0\nf    300.0\nd    500.0\ndtype: float64\n\nIn [10]: s[s.argsort().values]  # nonsensical result\nOut[10]:\nb    100.0\na    200.0\ne      NaN\nc    400.0\nf    300.0\nd    500.0\ndtype: float64\n\nIn [11]: pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.4.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 62 Stepping 4, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.3.1\nCython: None\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: 0.4.1\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: 2.3.4\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: 1.0b8\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: None\n```\n"},{"labels":["api",null,null],"text":"I initially posted this on #12136, as I intended to fix it with the same patch, but I think it deserves its own issue as my initial idea for a fix may in fact change the behaviour for strings suffixed by capital `M`. Previously, those would be interpreted as minutes because of the `unit.lower()` in https://github.com/pydata/pandas/blob/master/pandas/tslib.pyx#L2820\n\nApparently, `timedelta_from_spec()` doesn't recognize all the abbreviations that are allowed in the `unit` parameter. Consider for example:\n\n```\n>>> pd.Timedelta('1Y')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"pandas\\tslib.pyx\", line 2373, in pandas.tslib.Timedelta.__new__ (pandas\\tslib.c:43241)\n    value = np.timedelta64(parse_timedelta_string(value, unit, False))\n  File \"pandas\\tslib.pyx\", line 3010, in pandas.tslib.parse_timedelta_string (pandas\\tslib.c:52603)\n    raise\n  File \"pandas\\tslib.pyx\", line 3005, in pandas.tslib.parse_timedelta_string (pandas\\tslib.c:52506)\n    r = timedelta_from_spec(number, frac, unit)\n  File \"pandas\\tslib.pyx\", line 2822, in pandas.tslib.timedelta_from_spec (pandas\\tslib.c:50601)\n    raise ValueError(\"invalid abbreviation: {0}\".format(unit))\nValueError: invalid abbreviation: Y\n```\n\nwhile\n\n```\n>>> pd.Timedelta(1, unit='Y')\nTimedelta('365 days 05:49:12')\n```\n\nSame goes for Week.\n\nMonth is slightly trickier. As noted above, we have this inconsistency that we might want to do something about:\n\n```\n>>> pd.Timedelta('1M')\nTimedelta('0 days 00:01:00')\n>>> pd.Timedelta(1, unit='M')\nTimedelta('30 days 10:29:06')\n```\n"},{"labels":["api",null,null,null],"text":"## Known differences between Python & C engines\r\n\r\nUpdate [here](http://pandas.pydata.org/pandas-docs/stable/io.html#specifying-the-parser-engine)\r\n### Features supported in the Python engine only\r\n- [ ] `skipfooter` / `skip_footer` (#13349) - num of lines at the bottom of the file to skip\r\n- [ ] sniffing (`sep=None`) - deduce the `sep` #9645\r\n- [ ] regex `sep` - regular expression/multicharacter seperator\r\n### Features supported in the C engine only\r\n- [x] `dtype` - specify dtype for providing dtype or `{column_name: dtype}` (related as this is a conflicting option: https://github.com/pydata/pandas/issues/5232) (done in #14295)\r\n- [x] `warn_bad_lines` -  issue a warnings for each bad line (#15925)\r\n- [x] `error_bad_lines` - if `False`, drop bad lines instead of raising (#15925)\r\n- [ ] `lineterminator` - specify the line terminating character\r\n- [x] C engine accepts `float` for `nrows` but Python engine raises #10476 (closed by #13275)\r\n- [x] `decimal` option, #12933  (closed by #13189)\r\n- [x] `delim_whitespace` #12958 \r\n- [x] `na_filter` #13321\r\n- [x] `float_precision`, documented [here](https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L1526) and [here](https://github.com/pydata/pandas/blob/master/doc/source/io.rst#specifying-method-for-floating-point-conversion), #13377 \r\n  #### In C engine only (but undocumented)\r\n  - [x] `low_memory` (PR #13293)\r\n### marked as internal on C engine only (maybe be a bit louder about this in the internal code)\r\n- [x] `buffer_lines` #13360\r\n### Undocumented arguments to `read_csv`\r\n- [x] `doublequote` #13368\r\n- [x] `compact_ints` #13323\r\n- [x] `use_unsigned` #13323 \r\n- [x]  `as_recarray`  #(#13373)\r\n- [x] `memory_map`, #7477, #13381\r\n### Differences\r\n- [ ] validity of `names` and its length with respect to `usecols` #16469\r\n- [ ] different handling of `na_values` when `converters` is also present. #13302\r\n- [ ] different handling of columns aggregated to create date columns #23845\r\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\n\n``` python\n>>> panel.tz_localize(tz = pytz.UTC, axis=1)\n>>> pp.pprint(panel.axes[1].tzinfo)\n```\n\n``` python\n[Index([u'ddhm', u'ffhm', u'nnhv', u'pshv', u'tahv', u'uuhv', u'rrhs'], dtype='object'),\n DatetimeIndex(['2016-03-09 12:05:05'], dtype='datetime64[ns]', name=u'date', freq=None),\n DatetimeIndex(['2016-03-09 13:00:00', '2016-03-09 14:00:00',\n               '2016-03-09 15:00:00', '2016-03-09 16:00:00',\n               '2016-03-09 17:00:00', '2016-03-09 18:00:00',\n               '2016-03-09 19:00:00', '2016-03-09 20:00:00',\n               '2016-03-09 21:00:00', '2016-03-09 22:00:00',\n               ...\n               '2016-03-19 03:00:00', '2016-03-19 04:00:00',\n               '2016-03-19 05:00:00', '2016-03-19 06:00:00',\n               '2016-03-19 07:00:00', '2016-03-19 08:00:00',\n               '2016-03-19 09:00:00', '2016-03-19 10:00:00',\n               '2016-03-19 11:00:00', '2016-03-19 12:00:00'],\n              dtype='datetime64[ns]', name=u'forecast', length=240, freq=None)]\nNone\n```\n#### Expected Output\n\n``` python\n[Index([u'ddhm', u'ffhm', u'nnhv', u'pshv', u'tahv', u'uuhv', u'rrhs'], dtype='object'),\n DatetimeIndex(['2016-03-09 12:05:05+00:00'], dtype='datetime64[ns, UTC]', name=u'date', freq=None),\n DatetimeIndex(['2016-03-09 13:00:00', '2016-03-09 14:00:00',\n               '2016-03-09 15:00:00', '2016-03-09 16:00:00',\n               '2016-03-09 17:00:00', '2016-03-09 18:00:00',\n               '2016-03-09 19:00:00', '2016-03-09 20:00:00',\n               '2016-03-09 21:00:00', '2016-03-09 22:00:00',\n               ...\n               '2016-03-19 03:00:00', '2016-03-19 04:00:00',\n               '2016-03-19 05:00:00', '2016-03-19 06:00:00',\n               '2016-03-19 07:00:00', '2016-03-19 08:00:00',\n               '2016-03-19 09:00:00', '2016-03-19 10:00:00',\n               '2016-03-19 11:00:00', '2016-03-19 12:00:00'],\n              dtype='datetime64[ns]', name=u'forecast', length=240, freq=None)]\n<UTC>\n```\n#### output of `pd.show_versions()`\n\nhttp://paste.pound-python.org/show/zeZJO5S6OCJJ3Ohk3UWf/\n\nusing `panel = panel.tz_locaize()`fixes it, but there is no information about inplace operation or a return value @ http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Panel.tz_localize.html \n"},{"labels":["api",null],"text":"Currently, the parameter for values that one needs to insert into an array-like object such as `DataFrame` or `Series` is inconsistent across implementations.  For example, `base.py` surfaces it as `key`, whereas `series.py` surfaces it as `v`.\r\n\r\nThis issue was identified while fixing an issue for `v0.18.1` (xref #12413), but in the interests of maintaining backwards compatibility for a minor release, this change should be made as early as `v0.19.0` with the following API comment:\r\n\r\n\"The signature for `searchsorted` has changed. It is now `searchsorted(self, value, side='left', sorter=None)`.\"\r\n\r\nList of functions with signature inconsistencies:\r\n`searchsorted`\r\n`repeat`\r\n\r\n@jreback \r\n@jorisvandenbossche \r\n"},{"labels":["api",null,null],"text":"xref https://github.com/pydata/pandas/issues/8662\nxref #12652 \n\nThis makes pandas code jump thru hoops and is a complete anti-pattern for pandas. let's nuke it. users should _never_ do this.\n"},{"labels":["api",null,null],"text":"#### Code Sample, a copy-pastable example if possible\n\n```\nimport pandas as pd\ns = pd.Series(['2012-01-01 10:00', '2012-02-10 23:20', '2012-03-22 12:40', '2012-05-02 02:00', '2012-06-11 15:20', '2012-07-22 04:40','2012-08-31 18:00', '2012-10-11 07:20', '2012-11-20 20:40', '2012-12-31 10:00'])\nprint s\n0    2012-01-01 10:00\n1    2012-02-10 23:20\n2    2012-03-22 12:40\n3    2012-05-02 02:00\n4    2012-06-11 15:20\n5    2012-07-22 04:40\n6    2012-08-31 18:00\n7    2012-10-11 07:20\n8    2012-11-20 20:40\n9    2012-12-31 10:00\ndtype: object\nprint pd.to_datetime(s, format='%Y-%m-%d', errors='raise', infer_datetime_format=False, exact=True)\n0   2012-01-01 10:00:00\n1   2012-02-10 23:20:00\n2   2012-03-22 12:40:00\n3   2012-05-02 02:00:00\n4   2012-06-11 15:20:00\n5   2012-07-22 04:40:00\n6   2012-08-31 18:00:00\n7   2012-10-11 07:20:00\n8   2012-11-20 20:40:00\n9   2012-12-31 10:00:00\ndtype: datetime64[ns]\n```\n#### Expected Output\n\nA ValueError exception because string `'2012-01-01 10:00'` (and all others of this series) does not comply with format string `'%Y-%m-%d'` and exact conversion is required.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\n\npandas: 0.17.1\nnose: 1.3.6\npip: 8.0.3\nsetuptools: 18.0\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.16.0\nstatsmodels: None\nIPython: 3.0.0\nsphinx: None\npatsy: None\ndateutil: 2.5.0\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.4.3\nopenpyxl: 2.2.2\nxlrd: 0.9.3\nxlwt: 0.8.0\nxlsxwriter: 0.7.3\nlxml: 3.4.4\nbs4: None\nhtml5lib: None\nhttplib2: 0.9.1\napiclient: None\nsqlalchemy: 1.0.9\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\nJinja2: None\n"},{"labels":["api",null,null],"text":"Unless I am missing something obvious, we can do this:\n`df.groupby('x').cumcount()`\nbut not:\n`df.cumcount()`\n\nObviously, there are workarounds, but seems like we should be able to do on a dataframe/series if we can do on a groupby object?\n"},{"labels":["api",null],"text":"A recent spate of issues/PR's stemming from calling functions defined in numpy's `fromnumeric.py` module <a href=\"https://github.com/numpy/numpy/blob/master/numpy/core/fromnumeric.py\">here</a> that have identically-named but differently implemented methods/functions in `pandas` is indicative of a much larger compatibility issue between the two libraries with this module.  A thorough overview of all of the functions from the `fromnumeric.py` module and cross-referencing them to implementations in `pandas` is needed to avoid similar issues.\n\nRelevant PRs:\n#12413 (issue: #12238)\n#12603 (issue: #12600)\n#12638\n\n<a href=\"https://github.com/numpy/numpy/pull/7325\">#7325</a> (from `numpy`)\n"},{"labels":["api",null],"text":"from `pandas.tseries.api`\n\nif someone can take a look and see when this was added?\n\nI don't think this is useful at all. Let's just deprecate and remove this function entirely. Could always add a `Period.now()`.\n"},{"labels":["api",null],"text":"xref https://github.com/pydata/pandas/pull/12578\r\n\r\nso its pretty easy to add (and eventually deprecate) certain top-level functions that could/should be methods. Promotes a cleaner syntax and method chaining.\r\n\r\nIn 0.18.1 would propose adding these, deprecating in 0.19.0.\r\n\r\nThis would eliminate the ability to use directly with `np.ndarrays`, but I view this is as a positive (note we effectively did this for the `.rolling/expanding/ewm` with `np.ndarrays` as well, though deprecated for a while).\r\n- [ ] `pd.crosstab` (DataFrame)\r\n- [x] `pd.melt` (DataFrame) (PR #15521); added to ``DataFrame``\r\n- [ ] `pd.get_dummies` (both)\r\n- [ ] `pd.cut/qcut` (only on Series)\r\n"},{"labels":["api",null,null],"text":"Normal `Index` ignores `fill_value` (this is desribed in docstring)\n\n```\npd.Index([1, 2, 3]).take(np.array([1, 0, -1]), fill_value=np.nan)\n# Int64Index([2, 1, 3], dtype='int64')\n```\n\nBut `DatetimeIndex` does (used in reshape ops at least). \n\n```\npd.DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01'], tz='US/Eastern').take(np.array([1, 0, -1]), fill_value=pd.NaT)\n# DatetimeIndex(['2011-02-01 00:00:00-05:00', '2011-01-01 00:00:00-05:00',\n#                'NaT'],\n#               dtype='datetime64[ns, US/Eastern]', freq=None)\n```\n\nOtherwise `PeriodIndex` doesn't.\n\n```\npd.PeriodIndex(['2011-01', '2011-02', '2011-03'], freq='M').take(np.array([1, 0, -1]), fill_value=pd.NaT)\n# PeriodIndex(['2011-02', '2011-01', '2011-03'], dtype='int64', freq='M')\n```\n\nSo I'd like to discuss:\n1. All Index should handle `fill_value`?\n2. Only `PeriodIndex` should handle `fill_value` to be compat with other datetime-likes.\n"},{"labels":["api",null,null],"text":"It would be nice if there was some way to join based on just some levels of a MultiIndex. For example, given a DataFrame like:\n\n```\n>>> df = pd.DataFrame({\"a\": [1, 1, 2], \"b\": [11, 12, 13], \"c\": [21, 22, 23]}).set_index([\"a\", \"b\"])\n>>> df\n       c\na b     \n1 11  21\n  12  22\n2 13  23\n```\n\nI'd like to join it to another DataFrame along, for example, just the first level of the index:\n\n```\n>>> other = pd.DataFrame({\"a\": [1, 2], \"x\": [91, 92]}).set_index(\"a\")\n>>> other\n    x\na    \n1  91\n2  92\n>>> df.join(other, left_index=[\"a\"])\n       c   x\na b         \n1 11  21  91\n  12  22  91\n2 13  23  92\n```\n\nOr something similar.\n\nAs far as I can tell, the only way to do this now is with a combination of `reset_index` and `set_index`:\n\n```\n>>> df.reset_index().join(other, on=\"a\").set_index([\"a\", \"b\"])\n       c   x\na b         \n1 11  21  91\n  12  22  91\n2 13  23  92\n```\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\n\nMost of the time when I write a csv to file I don't need the index in the CSV output. I end up writing `to_csv('file/path.csv', index=False)` basically everytime. I am wondering if `Index=False` should be the default.\n#### Expected Output\n\n`pd.to_csv('file/path.csv')`\n#### output of `pd.show_versions()`\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.0.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.1\nnose: 1.3.7\npip: 7.1.2\nsetuptools: 1.1.6\nCython: None\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nIPython: 4.0.0\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.0\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: 1.0b8\nhttplib2: 0.9.2\napiclient: None\nsqlalchemy: 1.0.10\npymysql: None\npsycopg2: None\nJinja2: None\n"},{"labels":["api",null,null],"text":"We can create / convert using datetimetz dtype, but it doesn't work some cases.\n\n```\ns = pd.Series([pd.Timestamp('2011-01-31', tz='US/Eastern')])\ns\n#0   2011-01-31 00:00:00-05:00\n# dtype: datetime64[ns, US/Eastern]\n\n# OK\ns.astype('datetime64[ns, Asia/Tokyo]')\n#0   2011-01-31 14:00:00+09:00\n# dtype: datetime64[ns, Asia/Tokyo]\n```\n### astype\n\n```\n# numpy (OK)\npd.Series([1296432000000000000]).astype('datetime64[ns]')\n#0   2011-01-31\n# dtype: datetime64[ns]\n\n# extended (NG)\npd.Series([1296432000000000000]).astype('datetime64[ns, Asia/Tokyo]')\n# TypeError: Invalid datetime unit in metadata string \"[ns, Asia/Tokyo]\"\n```\n### dtype arg\n\n```\n# extended (OK ? I think the result should be 2011-01-31 00:00:00-05:00... see below)\npd.Series([1296432000000000000], dtype='datetime64[ns, US/Eastern]')\n#0   2011-01-31 05:00:00-05:00\n# dtype: datetime64[ns, US/Eastern]\n\n# ref\npd.Series([1296432000000000000], dtype='datetime64[ns]').dt.tz_localize('US/Eastern')\n#0   2011-01-31 00:00:00-05:00\n# dtype: datetime64[ns, US/Eastern]\n```\n\n```\n# extended (NG)\npd.Series([pd.Timestamp('2011-01-01', tz='US/Eastern')], dtype='datetime64[ns, US/Eastern]')\n# TypeError: data type not understood\n```\n"},{"labels":["api",null,null,null],"text":"Hello,\n\nI think it would make sense to make `read_csv()`, `read_table()`, `read_fwf()` able to read multiple  files with the same structure. It might be tricky to read multiple files into one string, especially when all of them have header line(s) and when you want to use the following parameters: `skiprows`, `skipfooter`, `nrows`.\nThe logic  for (`skiprows`, `skipfooter`, `nrows`) is already implemented, so IMO it shouldn't be very difficult. The `header` parameter (if header exists) must be read/parsed only from the first (from one) file.\n\nOf course one can always do something like: \n\n`df = pd.concat([pd.read_csv(f, **kwargs) for f in flist], ignore_index=True)\n`\n\nbut it's not very efficient when working with big files.\n\nIn the last days there were plenty of similar questions stackoverflow.com, asking how to merge CSV files with the same structure.\n\nThank you!\n"},{"labels":["api"],"text":"I think it would be useful to have a command similar to `table` in R or `tab` in Stata. \n\nGiven a vector `v`   `table(v)` should return the frequency of each value in `v`.    `table(v1, v2)` should return the cross tabulation of `v1` and `v2`\n"},{"labels":["api",null],"text":"Following builtin `set`, I would expect the method name to be `symmetric_difference`.\n\nHow about deprecating `sym_diff` in favour of this (see also #6016, #8226)?\n"},{"labels":["api",null,null],"text":"It'd be great to have a simple `normalization` option for cross tab to get shares rather than frequencies.\n\nSomething that would do something like:\n\n```\ndef normalize(x):\n    return len(x)/len(w_mobile.language)\n\npd.crosstab(w_mobile.language,w_mobile.carrier, values=w_mobile.language, aggfunc=normalize)\n```\n\nas just an option.  \n\n(The ability to do row-normalizations and column normalizations would also be great -- so all entries in a row add to 1 or all entries in a column add to 1). Similar in behavior (for row normalizations) as:\n\n```\nl = list()\ndf = pd.DataFrame({'carrier':['a','a','b','b','b'], 'language':['english','spanish', 'english','spanish','spanish']})\n\nfor i in df.carrier.unique():\n    temp = df.query('carrier==\"{}\"'.format(i)).language.value_counts(normalize=True)\n    temp.name = i\n    l.append(temp)\n\nctab = pd.concat(l, axis=1)\n\n\nOut[1]: \n                a         b\n english  0.5  0.333333\nspanish  0.5  0.666667\n```\n\nBut with a command like: `pd.crosstab(df.language, df.carrier, normalization='row')`\n"},{"labels":["api",null,null,null],"text":"I think there should be a way to take a DataFrame and use the values in its columns as indices into a MultiIndexed Series.  Here is an example:\n\n```\n>>> d = pandas.DataFrame({\"Let\": [\"A\", \"B\", \"C\"], \"Num\": [1, 2, 3]})\n>>> d\n  Let  Num\n0   A    1\n1   B    2\n2   C    3\n>>> ser = pandas.Series(\n...     ['a', 'b', 'c', 'd', 'e', 'f'],\n...     index=pandas.MultiIndex.from_arrays([[\"A\", \"B\", \"C\"]*2, [1, 2, 3, 4, 5, 6]])\n... )\n>>> ser\nA  1    a\nB  2    b\nC  3    c\nA  4    d\nB  5    e\nC  6    f\ndtype: object\n```\n\nWith this data, you should be able to do `d.map(ser)` (or whatever method name instead of `map`) and get the same result as this:\n\n```\n>>> ser.ix[d.apply(tuple, axis=1)]\nA  1    a\nB  2    b\nC  3    c\ndtype: object\n```\n\nYou currently cannot do this without converting the rows to tuples (`ser[d]` gives `ValueError: Cannot index with multidimensional key`).  Converting to tuple is an awkward way to do a very natural task, which is using a tabular array of data to look up values in a tabular index (i.e., a MultiIndex).\n\nI'm creating an issue to resurrect this request from much earlier discussion [here](https://groups.google.com/forum/#!searchin/pydata/dataframe$20map/pydata/sibkQ5Ea-Hc/RKMwiNsH7TMJ) and [here](http://stackoverflow.com/questions/22293683/equivalent-of-series-map-for-dataframe).\n"},{"labels":["api",null],"text":"i want use qcut() to process my data?but i don't want to direct to binning by width.\nso i want use a supervised binning.such as Entropy-bor based Binning.or can you tell another package for this?\nthank you?\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\n\nAllow `.where` and `.mask` to accept `callable` as cond. This is useful if `DataFrame` is changed during method chaining.\n\n```\ndf = pd.DataFrame(np.random.randn(2, 2))\ndf.where(lambda x: x > 0)\n# currently raises ValueError\n```\n#### Expected Output\n\nShould be the same as:\n\n```\ndf.where(df > 0)\n#           0         1\n#0  0.689202       NaN\n#1       NaN  0.261644\n```\n"},{"labels":["api",null,null,null],"text":"There are known ways of droping inf and nan (.replace(np.inf, np.nan).dropna()) but still, the API could be expanded:\n\n```\n.dropinf(inplace=False) \n\n.dropna(drop_inf = False)\n```\n\nAre both self explanatory wishes and it is no stretch to see how they are usefull. \nNote for instance that today's API does not provide a one liner to drop inf in place while keeping nans.\n\nBest\n\nPS: not sure how to submit wishes.\n"},{"labels":["api",null,null],"text":"unlike when I use .aggregate('mean'), I lose the groupedby columns and any nonnumeric columns when using transform\n\n``` python\n>>> d = {'a':['a', 'a', 'c', 'c'], 'b':['z','z','z','y'], 'dat':[4.2, 2.4, 3.5, 5.3], 'dat2':[1.3,2.6,3.4,4.1]}\n>>> df = pd.DataFrame(d)\n>>> zscore = lambda x: (x - x.mean()) / x.std()\n>>> df.groupby('a').transform(zscore)\n        dat      dat2\n0  0.707107 -0.707107\n1 -0.707107  0.707107\n2 -0.707107 -0.707107\n3  0.707107  0.707107\n```\n\nIf I add 'a' to the index, and then transform, I still lose column b \n\n``` python\n>>> df = df.set_index('a')\n>>> df.groupby(level='a').transform(zscore)\n        dat      dat2 \na                     \na  0.707107 -0.707107 \na -0.707107  0.707107 \nc -0.707107 -0.707107 \nc  0.707107  0.707107 \n```\n\nThe only way to keep non-numeric information columns is to add them all to the index, regardless of whether I need to group on them or not\n\n``` python\n>>> df = df.set_index(['a', 'b'])\n>>> df.groupby(level='a').transform(zscore)\n          dat      dat2 \na b                     \nc z  0.993399 -0.475293 \n  z  0.198680 -0.095059 \n  z  0.198680  1.425880 \n  y -1.390759 -0.855528 \n```\n#### Expected Output\n\nI would expect the columns that I grouped on to remain, like they do when using aggregate('mean') for example.\n\n``` python\n   a    b        dat      dat2\n0  a    z    0.707107 -0.707107\n1  a    z    -0.707107  0.707107\n2  c    z    -0.707107 -0.707107\n3  c    y    0.707107  0.707107\n```\n#### output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS                                                               \n------------------                                                               \ncommit: None                                                                     \npython: 2.7.11.final.0                                                           \npython-bits: 64                                                                  \nOS: Windows                                                                      \nOS-release: 7                                                                    \nmachine: AMD64                                                                   \nprocessor: Intel64 Family 6 Model 70 Stepping 1, GenuineIntel                    \nbyteorder: little                                                                \nLC_ALL: None                                                                     \nLANG: None                                                                       \n\npandas: 0.17.1                                                                   \nnose: 1.3.7                                                                      \npip: 8.0.2                                                                       \nsetuptools: 18.5                                                                 \nCython: 0.23.4                                                                   \nnumpy: 1.10.1                                                                    \nscipy: 0.16.0                                                                    \nstatsmodels: 0.6.1                                                               \nIPython: 4.0.1                                                                   \nsphinx: 1.3.1                                                                    \npatsy: 0.4.0                                                                     \ndateutil: 2.4.2                                                                  \npytz: 2015.7                                                                     \nblosc: None                                                                      \nbottleneck: 1.0.0                                                                \ntables: 3.2.2                                                                    \nnumexpr: 2.4.4                                                                   \nmatplotlib: 1.5.0                                                                \nopenpyxl: 2.2.6                                                                  \nxlrd: 0.9.4                                                                      \nxlwt: 1.0.0                                                                      \nxlsxwriter: 0.7.7                                                                \nlxml: 3.4.4                                                                      \nbs4: 4.4.1                                                                       \nhtml5lib: None                                                                   \nhttplib2: None                                                                   \napiclient: None                                                                  \nsqlalchemy: 1.0.9                                                                \npymysql: 0.7.1.None                                                              \npsycopg2: None                                                                   \nJinja2: None                                                                     \n```\n"},{"labels":["api",null,null],"text":"xref #12448 / #12449 \n\nand on [SO](http://stackoverflow.com/questions/36293620/pandas-0-18-changes-to-resample-how-to-upsample-with-groupby/36294398#36294398)\n\n```\nIn [1]:         df = DataFrame({'date': pd.date_range(start='2016-01-01',\n   ...:                                               periods=4,\n   ...:                                               freq='W'),\n   ...:                         'group': [1, 1, 2, 2],\n   ...:                         'val': [5, 6, 7, 8]}).set_index('date')\n\nIn [2]: df\nOut[2]: \n            group  val\ndate                  \n2016-01-03      1    5\n2016-01-10      1    6\n2016-01-17      2    7\n2016-01-24      2    8\n```\n\nThis replicates 0.17.1 (something slightly off with it including the grouper column)\n\n```\nIn [3]: df.groupby('group').apply(lambda x: x.resample('1D').ffill())[['val']]\nOut[3]: \n                  val\ngroup date           \n1     2016-01-03    5\n      2016-01-04    5\n      2016-01-05    5\n      2016-01-06    5\n      2016-01-07    5\n      2016-01-08    5\n      2016-01-09    5\n      2016-01-10    6\n2     2016-01-17    7\n      2016-01-18    7\n      2016-01-19    7\n      2016-01-20    7\n      2016-01-21    7\n      2016-01-22    7\n      2016-01-23    7\n      2016-01-24    8\n\n# ideally this would work. Its possible but requires some intelligently filling according to each group level.\nIn [4]: df.groupby('group').resample('1D').ffill()\nOut[4]: \n            group  val\ndate                  \n2016-01-03      1    5\n2016-01-10      1    6\n2016-01-17      2    7\n```\n\nA pure asfreq operation\n\n```\ndata = [['2010-01-01', 'A', 2], ['2010-01-02', 'A', 3], ['2010-01-05', 'A', 8], \n        ['2010-01-10', 'A', 7], ['2010-01-13', 'A', 3], ['2010-01-01', 'B', 5], \n        ['2010-01-03', 'B', 2], ['2010-01-04', 'B', 1], ['2010-01-11', 'B', 7], \n        ['2010-01-14', 'B', 3]]\n\ndf = pd.DataFrame(data, columns=['Date', 'ID', 'Score'])\ndf.Date = pd.to_datetime(df.Date)\n\nIn [27]: df.groupby('ID').apply(lambda x: x.set_index('Date').Score.resample('D').asfreq())\nOut[27]: \nID  Date      \nA   2010-01-01    2.0\n    2010-01-02    3.0\n    2010-01-03    NaN\n    2010-01-04    NaN\n    2010-01-05    8.0\n    2010-01-06    NaN\n    2010-01-07    NaN\n    2010-01-08    NaN\n    2010-01-09    NaN\n    2010-01-10    7.0\n    2010-01-11    NaN\n    2010-01-12    NaN\n    2010-01-13    3.0\nB   2010-01-01    5.0\n    2010-01-02    NaN\n    2010-01-03    2.0\n    2010-01-04    1.0\n    2010-01-05    NaN\n    2010-01-06    NaN\n    2010-01-07    NaN\n    2010-01-08    NaN\n    2010-01-09    NaN\n    2010-01-10    NaN\n    2010-01-11    7.0\n    2010-01-12    NaN\n    2010-01-13    NaN\n    2010-01-14    3.0\nName: Score, dtype: float64\n```\n\nWould be nice for this to work\n\n```\ndf.groupby(['ID',pd.Grouper(key='Date',freq='D')]).asfreq()\n```\n"},{"labels":["api"],"text":"Hey folks, I'm starting to use the pyspark.ml library and it'd be cool to create a translation layer from pandas to spark to make it easy for local code to easily translate into spark. I have two ideas and would would be great to hear feedback. If positive I'd be happy to create a PR. \n\n1) Direct translation  \n\n```\nspark_df = pandas_df.to_spark(spark_context)\n```\n\n2) Pseudo translation: only permit spark compatible operations on this DataFrame \n\n```\nrestricted_spark_df = pandas_df.restrict_spark()\n```\n\nWould error if any operation that would not work on a spark df were performed on this df. Harder, but potentially useful for seeing if code designed for local use can be used for spark, without even installing spark. \n"},{"labels":["api",null,null,null,null],"text":"Timestamp stores the value in a `datetime64[ns]` internal representation. For most purposes this is fine, but I'd love if I could choose the internal precision, for example to use `datetime64[s]` instead.\n\nWhen I do timeseries manipulations that span more than a day, I generally do not need sub-second resolution and I could definitely use the extended range instead. But, more conveniently, this would also allow to pretty-print the result without a ton of trailing zeros without setting some arbitrary threshold.\n\nIncidentally, this would remove the limit of ~500 years, but it's more of an ergonomic change than an actual need. For example, when I need to perform direct manipulations of the values in a DatetimeIndex, I often have to scale up to seconds if I want to perform meaningful fitting and avoid numerical instability. When plotting, it's an extra indirection I need to care about.\n"},{"labels":["api",null],"text":"While using master a bit, I discovered some more cases where the new resample API breaks things:\n- Plotting. `.plot` is a dedicated groupby/resample method (which adds each group individually to the plot), while I think it is a very common idiom to quickly resample your timeseries and plot it with (old API) eg `s.resample('D').plot()`. \n  Example with master:\n  \n  ```\n  In [1]: s = pd.Series(np.random.randn(60), index=date_range('2016-01-01', periods=60, freq='1min'))\n  \n  In [3]: s.resample('15min').plot()\n  Out[3]:\n  2016-01-01 00:00:00    Axes(0.125,0.1;0.775x0.8)\n  2016-01-01 00:15:00    Axes(0.125,0.1;0.775x0.8)\n  2016-01-01 00:30:00    Axes(0.125,0.1;0.775x0.8)\n  2016-01-01 00:45:00    Axes(0.125,0.1;0.775x0.8)\n  Freq: 15T, dtype: object\n  ```\n  \n  ![figure_1](https://cloud.githubusercontent.com/assets/1020496/13317218/79e8937e-dbb4-11e5-80ad-87e663ef5de8.png)\n  \n  while previously it would just have given you one continuous line. \n  This one can be solved I think by special casing `plot` for `resample` (not have it a special groupby-like method, but let it warn and pass the the `resample().mean()` result to `Series.plot()` like the 'deprecated_valids')\n- When you previously called a method on the `resample` result that is also a valid Resampler method now. Eg `s.resample(freq).min()` would previously have given you the \"minimum daily average\" while now it will give you the \"minimum per day\". \n  This one is more difficult/impossible to solve I think? As you could detect that case if you know it is old code, but cannot distinguish it from perfectly valid code with the new API. If we can't solve it, I think it deserves some mention in the whatsnew explanation.\n- Using `resample` on a `groupby` object (xref #12202). Using the example of that issue, with 0.17.1 you get:\n  \n  ```\n  In [1]: df = pd.DataFrame({'date': pd.date_range(start='2016-01-01', periods=4,\n  freq='W'),\n  ...:                'group': [1, 1, 2, 2],\n  ...:                'val': [5, 6, 7, 8]})\n  \n  In [2]: df.set_index('date', inplace=True)\n  \n  In [3]: df\n  Out[3]:\n        group  val\n  date\n  2016-01-03      1    5\n  2016-01-10      1    6\n  2016-01-17      2    7\n  2016-01-24      2    8\n  \n  In [4]: df.groupby('group').resample('1D', fill_method='ffill')\n  Out[4]:\n                  val\n  group date\n  1     2016-01-03    5\n    2016-01-04    5\n    2016-01-05    5\n    2016-01-06    5\n    2016-01-07    5\n    2016-01-08    5\n    2016-01-09    5\n    2016-01-10    6\n  2     2016-01-17    7\n    2016-01-18    7\n    2016-01-19    7\n    2016-01-20    7\n    2016-01-21    7\n    2016-01-22    7\n    2016-01-23    7\n    2016-01-24    8\n  \n  In [5]: pd.__version__\n  Out[5]: u'0.17.1'\n  ```\n  \n  while with master you get:\n  \n  ```\n  In [29]: df.groupby('group').resample('1D', fill_method='ffill')\n  Out[29]: <pandas.core.groupby.DataFrameGroupBy object at 0x0000000009BA73C8>\n  ```\n  \n  which will give you different results/error with further operations on that. Also, this case does not raise any FutureWarning (which should, as the user should adapt the code to `groupby().resample('D').ffill()`)\n"},{"labels":["api",null,null,null],"text":"It'd be nice if there was a way to easily broadcast arithmetic operations on Series/DataFrames with `DatetimeIndexes`. For example, say you have monthly data, calculate the monthly mean, and want to multiply the original series by the monthly factor.\n\n``` python\nimport numpy as np\nimport pandas as pd\nidx = index=pd.date_range('1970-01-01', end='2015-01-01', freq='MS')\ns = pd.Series(np.random.randn(len(idx)), name='ts',\n              index=idx)\nmf = s.groupby(lambda x: x.month).mean()\n\nIn [3]: s\nOut[3]:\n1970-01-01   -1.032080\n1970-02-01   -0.706686\n1970-03-01   -0.380895\n1970-04-01    0.322074\n1970-05-01   -1.545298\n                ...\n2014-09-01   -0.787646\n2014-10-01   -2.444045\n2014-11-01   -0.646474\n2014-12-01   -0.291925\n2015-01-01    0.399430\nFreq: MS, Name: ts, dtype: float64\n\nIn [4]: mf\nOut[4]:\n1     0.184326\n2    -0.069534\n3    -0.146372\n4     0.018643\n5     0.050393\n        ...\n8    -0.088188\n9     0.148328\n10    0.105042\n11    0.219308\n12    0.000997\nName: ts, dtype: float64\n```\n\nRight now you need something like (this could be simplified)\n\n``` python\ntmp = s.copy()\ntmp.index = s.index.month\ntmp = tmp * mf\ntmp.index = s.index\ns\n```\n\nIt'd be nice to use the fact that `s` is a datetimeindex to allow some broadcasting over the years. Something like\n\n```\ns.mul(mf, freq='M')\n```\n\nA similar case is when both `s` and `mf` have DatetimeIndexes. Then `s.mul(mf, freq='M')` upsamples the lower frequency `mf` to a monthly frequency, fills, and multiplies.\n\nAnother example from [SO](http://stackoverflow.com/questions/35579455/how-to-multiply-all-hourly-values-in-one-pandas-dataframe-with-yearly-values-in)\n"},{"labels":["api",null],"text":"#### Code Sample, a copy-pastable example if possible\n\n```\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,2,3], 'b':[4,5,6]})\nassert type(df.apply(lambda column: [1,2])) == pd.Series\nassert type(df.apply(lambda column: [1,2,3])) == pd.DataFrame\n# when list length matches the relevant df size, the result is DF; otherwise, a Series of lists\n```\n#### Expected Output\n\nThis behavior is not documented, but more importantly it's really dangerous. A slight change in the inputs may cause a really hard-to-track bug. I think only two alternatives are safe:\n\n1) When the function in `apply` returns a list, the result should always be a Series of lists. \n2) When the function in `apply` returns a list with the correct size, the result should be a DataFrame; otherwise, an error should be raised.\n\nApproach 2) is obviously too restrictive, so I'd recommend approach 1).\n\nOn a related note, it would be nice to be able to explicitly tell `apply` whether I want a Series of lists or a DF as a result.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.0.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 70 Stepping 1, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.17.0\nnose: None\npip: 8.0.2\nsetuptools: 18.2\nCython: 0.23.4\nnumpy: 1.10.1\nscipy: 0.16.0\nstatsmodels: None\nIPython: 4.0.0\nsphinx: None\npatsy: 0.4.1\ndateutil: 2.4.2\npytz: 2015.6\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.5\nmatplotlib: 1.5.0\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\n\n"},{"labels":["api",null,null,null],"text":"do we need label selectors? we should for sure just have a single method for this. maybe call it `query_labels`? to be consistent with `.query` as the workhorse for data selection.\r\n\r\n- [x] ``.select`` (#17633)\r\n- [ ] ``.filter``\r\n\r\nxref #6599 \r\n"},{"labels":["api",null],"text":"Allow ability to distinguish inclusiveness of left and right boundaries.  I've written the code and tests, but need a GH tracking number prior to submitting a PR.\n\n```\n    def between(self, left, right, inclusive=True):\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right. NA values\n        will be treated as False\n\n        Parameters\n        ----------\n        left : scalar\n            Left boundary\n        right : scalar\n            Right boundary\n        inclusive : boolean for both left and right or iterable pair.\n            Whether or not to include the left and right boundary.\n            If a tuple pair, the boundaries can be set separately\n            (e.g. (False, True) for left < series <= right).\n```\n"},{"labels":["api",null],"text":"I find the `Index.to_series` method is a convenient way to allow indices to act as columns of a dataframe where desired. However, the behavior of `MultiIndex.to_series`, which gives a `Series` of tuples, is less useful.\n\nWould it be convenient to provide a `to_dataframe` method for index classes? This would be a natural extension of the utility of `to_series`, and more useful for `MultiIndex` objects I would think.\n\nI'm   something equivalent to:\n\n> ```\n> def to_dataframe(self):\n>    DataFrame(self.tolist(), columns=self.names, index=self)\n> ```\n"},{"labels":["api",null],"text":"`rename` accepts a `columns` argument or an `index` argument, while `drop` looks for a `labels` and `axis` pair. I don't know about anyone else, but I have to check the help file every time I come back to pandas to remember which takes which. \n\nHow would people feel about adding `columns` and `index` arguments to `drop`? They could just be added in addition to `labels`/`axis` if we want to provide backwards compatibility and just raise an exception if the user tries to mix them. \n"},{"labels":["api",null,null],"text":"I find myself running into a situation where I don't want to see small numbers as scientific notation fairly frequently, things like:\n\n```\nIn [3]: pd.set_option('display.precision', 2)\n\nIn [4]: pd.DataFrame(np.random.randn(5, 5)).corr()\nOut[4]: \n      0     1         2         3     4\n0  1.00 -0.57  2.15e-02 -3.48e-02 -0.64\n1 -0.57  1.00  2.59e-01 -5.56e-01  0.51\n2  0.02  0.26  1.00e+00  2.91e-03 -0.06\n3 -0.03 -0.56  2.91e-03  1.00e+00  0.36\n4 -0.64  0.51 -6.21e-02  3.63e-01  1.00\n```\n\nor\n\n```\nIn [16]: pd.Series(np.random.poisson(size=1000)).value_counts(normalize=True)\nOut[16]: \n0    3.80e-01\n1    3.63e-01\n2    1.75e-01\n3    5.70e-02\n4    1.80e-02\n5    5.00e-03\n7    1.00e-03\n6    1.00e-03\ndtype: float64\n```\n\nScientific notation isn't helpful when you are trying to make quick comparisons across elements, and have a well-defined notion of a -1 to 1 or 0 to 1 range.\n\nI propose adding some sort of display flag to suppress scientific notation on small numbers, and just report zeros in these cases instead. Alternatively we could also suppress it on large numbers, but I am not sure how helpful that is. I usually only find myself going up against it on small numbers, in exactly the use cases (correlations, proportions) above.\n"},{"labels":["api",null],"text":"xref 12329\n\nFor consistency, the proposal is to make `[4]` have a multi-level index `('C','sum'),('C','std')` as its not a rename (like `[5]`). This previously raised in 0.17.1\n\n```\nIn [2]: df = DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n                              'foo', 'bar', 'foo', 'foo'],\n                        'B': ['one', 'one', 'two', 'two',\n                              'two', 'two', 'one', 'two'],\n                        'C': np.random.randn(8) + 1.0,\n                        'D': np.arange(8)})\n\nIn [3]: g = df.groupby(['A', 'B'])\n\nIn [4]: g['D'].agg({'C': ['sum', 'std']})\nOut[4]: \n         sum       std\nA   B                 \nbar one    1       NaN\n    two    8  1.414214\nfoo one    6  4.242641\n    two   13  2.516611\n\nIn [5]: g['D'].agg({'C': 'sum', 'D' : 'std'})\nOut[5]: \n          C         D\nA   B                \nbar one   1       NaN\n    two   8  1.414214\nfoo one   6  4.242641\n    two  13  2.516611\n```\n"},{"labels":["api",null,null],"text":"from #12246 \n\n@jorisvandenbossche this looks odd\n\n```\nIn [11]: s = pd.Series([1,2,3])\n\nIn [12]: s.loc[0.0] = 10\n\nIn [13]: s.loc[0.0]\nTypeError: cannot do label indexing on <class 'pandas.indexes.range.RangeIndex'>\n with these indexers [0.0] of <type 'float'>\n```\n"},{"labels":["api",null,null],"text":"Suppose the following dataframe and reindex operation:\n\n```\nIn [65]: df = pd.DataFrame(np.arange(12).reshape(4,3), columns=pd.MultiIndex.from_tuples([('A', 'a'), ('A', 'b'), ('B', 'a')]))\n\nIn [66]: df\nOut[66]:\n   A       B\n   a   b   a\n0  0   1   2\n1  3   4   5\n2  6   7   8\n3  9  10  11\n\nIn [67]: df.reindex(columns=['a', 'b'], level=1)\nOut[67]:\n   A       B\n   a   b   a\n0  0   1   2\n1  3   4   5\n2  6   7   8\n3  9  10  11\n```\n\nShould this give the following?\n\n```\nIn [67]: df.reindex(columns=['a', 'b'], level=1)\nOut[67]:\n   A       B\n   a   b   a    b\n0  0   1   2  NaN\n1  3   4   5  NaN\n2  6   7   8  NaN\n3  9  10  11  NaN\n```\n\nI am not sure what the exact behaviour of the `level` keyword should be, but eg in the following example it does the selection of columns for each of label of the other level:\n\n```\nIn [69]: df2 = pd.DataFrame(np.arange(18).reshape(3,6), columns=pd.MultiIndex.from_product([('A', 'B'), ('a', 'b', 'c')]))\nIn [70]:\n\nIn [70]: df2\nOut[70]:\n    A           B\n    a   b   c   a   b   c\n0   0   1   2   3   4   5\n1   6   7   8   9  10  11\n2  12  13  14  15  16  17\n\nIn [71]: df2.reindex(columns=['a', 'c'], level=1)\nOut[71]:\n    A       B\n    a   c   a   c\n0   0   2   3   5\n1   6   8   9  11\n2  12  14  15  17\n```\n"},{"labels":["api",null],"text":"`df.max(axi=1)` does not throw an error and evaluates to `df.max()`\n"},{"labels":["api",null,null],"text":"First of all, I have to admit that I have some trouble finding a catchy title for this issue. Better suggestions are welcome...\n\nThere is some issue with crosstab, when applied on a DataFrame which has been derived from a DataFrame consisting of categorized columns.\n\nLet's consider the following example df:\n\n```\nimport pandas as pd\ndf = pd.DataFrame({'col0':list('abbabcabcab'), 'col1':[1,1,1,1,2,3,2,2,3,3,1],\n                  'crit':[1,1,1,1,1,0,0,0,0,0,0]})\n```\n\nWe apply some filter according to some criterion which happens to eliminate  all occurrences of 'c' in 'col0' (and '3' in 'col1'). Then we build a crosstab:\n\n```\ndf_filtered = df[df.crit == 1]\npd.crosstab(df_filtered.col0, df_filtered.col1)\n```\n\nThe result is as expected:\n\n```\ncol1    1   2\ncol0        \na   2   0\nb   2   1\n```\n\nNow we try the same on categorized columns:\n\n```\nfor col in df.columns:\n    df[col] = df[col].astype('category')\ndf_filtered = df[df.crit == 1]\npd.crosstab(df_filtered.col0, df_filtered.col1\n```\n\nIn this case, value 'c' and '3' are again listed, although they shouldn't be:\n\n```\ncol1    1   2   3\ncol0            \na   2   0   0\nb   2   1   0\nc   0   0   0\n```\n\nMy guess is that the reason for this behavior can be found in the lookup-table of the category, which does not account for values that are not represented any longer:\n`df_filtered.col0.cat.categories`\nOutput:\n`Index(['a', 'b', 'c'], dtype='object'`\n\nSo, either this lookup-table has to be fixed upon filtering, or pd.crosstab has to respect this condition.\n"},{"labels":["api",null],"text":"In line with \n- [ENH: Conditional HTML Formatting](https://github.com/pydata/pandas/pull/10250)\n- [Support for Conditional HTML Formatting](http://pandas.pydata.org/pandas-docs/version/0.17.1/whatsnew.html#whatsnew-0171-style)\n- [Style](http://pandas-docs.github.io/pandas-docs-travis/style.html)\n\nadd the same possibility for mpl tables.\n\nProof of concept:\n[Conditional formatting for 2- or 3-scale coloring of cells of a table](http://stackoverflow.com/questions/17748570/conditional-formatting-for-2-or-3-scale-coloring-of-cells-of-a-table)\n"},{"labels":["api",null,null],"text":"`DatetimeIndex` looks like it can benefit from `RangeIndex` #11892 .  \nUse case 1: Evenly-spaced time-series analysis\nUse case 2: I sometimes (ab)use `date_range` like a PY3 `range` in a for loop, which I think is quite neat.  It would be cooler if it were lazily evaluated.\n"},{"labels":["api",null,null],"text":"I often find myself building a chain of logic, and then wanting to slice over some expression. Here's a contrived example:\n\n``` python\nIn [12]: pd.Series(range(10)).mul(5).pipe(lambda x: x**2).pipe(lambda x: x-500)\nOut[12]: \n0    -500\n1    -475\n2    -400\n3    -275\n4    -100\n5     125\n6     400\n7     725\n8    1100\n9    1525\ndtype: int64\n```\n\n...and then having to assign to a separate variable and slice using that. Which is not the worst affliction that could strike western civilization, but can disrupt the flow in a repl a little.\n\n``` python\n\nIn [13]: s=pd.Series(range(10)).mul(5).pipe(lambda x: x**2).pipe(lambda x: x-500)\n\nIn [14]: s[s>200]\nOut[14]: \n6     400\n7     725\n8    1100\n9    1525\ndtype: int64\n```\n\nInstead, would it be reasonable to extend query or filter to do this. For example:\n\n`pd.Series(range(10)).mul(5).pipe(lambda x: x**2).pipe(lambda x: x-500).query('>200')`\nor\n`pd.Series(range(10)).mul(5).pipe(lambda x: x**2).pipe(lambda x: x-500).filter(lambda x: x>200)`\n"},{"labels":["api",null],"text":"This maybe is just a doc warning - but caught me by surprise.  I would prefer the `read_csv` behavior, but it would be backwards incompatible.\n\n``` python\nIn [57]: pd.read_csv('path/doesn_exist')\n---------------------------------------------------------------------------\n\nIOError: File path/doesn_exist does not exist\n\nIn [56]: pd.read_msgpack('path/doesn_exist')\nOut[56]: [112, 97, 116, 104, 47, 100, 111, 101, 115, 110, 95, 101, 120, 105, 115, 116]\n\nIn [59]: pd.read_msgpack(u'path/doesn_exist')\n---------------------------------------------------------------------------\nValueError: path_or_buf needs to be a string file path or file-like\n```\n"},{"labels":["api",null],"text":"I try to read a csv file were some lines are longer than the rest.\n\nPandas `C engine` throws an error with these lines. \nBut I do not was to skip these as discussed in [ a comment on a similar issue](https://github.com/pydata/pandas/issues/2886#issuecomment-30425534).\n\nI prefer to \"cut\" the \"bad columns\" off using usecols. But I get the following errors:\n\n```\ndf = pd.read_csv(file_path, sep=',', skiprows=1, header=None, \n                       usecols=range(0,23), nrows=None, \n                         engine='python')\n```\n\nThrows:\n\n`ValueError: Expected 10 fields in line 100, saw 20`\n\nWhy is the `ValueError` raised although I explicitly defined the columns to use?\n\nReferences:\n- #2886 \n- [Dealing with bad lines](http://github.com/pydata/pandas/issues/2886) -- does not apply\n- [Dealing with bad lines II](http://nipunbatra.github.io/2013/06/reading-unclean-data-csv-using-pandas/)\n- [Handling Variable Number of Columns with Pandas - Python](http://stackoverflow.com/questions/15242746/handling-variable-number-of-columns-with-pandas-python)\n- [pandas read_csv and filter columns with usecols](http://stackoverflow.com/questions/15017072/pandas-read-csv-and-filter-columns-with-usecols), especially: http://stackoverflow.com/a/27791362\n"},{"labels":["api",null],"text":"I thought that `reindex_axis` with argument `copy=False` should change it inplace. From [docs](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.reindex_axis.html):\n\n> **copy** : _boolean_, _default True_  \n>     Return a new object, even if the passed indexes are the same\n\nHowever:\n\n```\ndf = pd.DataFrame({'A': [1], 'B': [2]})\n\nIn [105]: df.reindex_axis(['B', 'A'], axis=1, copy=False)\nOut[105]:\n   B  A\n0  2  1\n\nIn [106]: df\nOut[106]:\n   A  B\n0  1  2\n```\n\nAm I misunderstood that argument?\n"},{"labels":["api",null,null,null],"text":"Currently, Pandas interpolation interpolates all gaps, regardless of there size and the limit parameter is used to limit the number of replacement :  if there is a gap of 3 values and limit=2, pandas replaces the first 2 values. \n\nI have difficulty understanding why someone would want to do interpolation on only a few missing items in a consecutive series of missing.\n\nPersonally, depending on the length of the gap, I would like to decide to interpolate the whole gap or none of it. For example, in an hourly time-series, interpolation of missing hours up to a maximum of 3 consecutive hours:\n gaps <= 3 would be interpolated\n gaps > 3 remain untouched.\n\nI would appreciated a option for interpolation such as R \"na.approx\" in which a \"maxgap\" parameters is available. maxgap:  maximum number of consecutive NAs to fill. Any longer gaps will be left unchanged. \n"},{"labels":["api",null,null],"text":"See [this comment](https://github.com/pydata/pandas/pull/12173#discussion_r51347252). Basically we now have\n\n```\nIn [8]: read_csv(StringIO(u'A,B\\n1,2\\n3,4'))\nOut[8]: \n   A  B\n0  1  2\n1  3  4\n```\n\nbut\n\n```\nIn [9]: concat(read_csv(StringIO(u'A,B\\n1,2\\n3,4'), chunksize=1))\nOut[9]: \n   A  B\n0  1  2\n0  3  4\n```\n\nWe want the latter to also have index (0,1).\n"},{"labels":["api",null],"text":"I am using Python 3.\n\nI tried saving a DataFrame with Unicode labels using the `.to_msgpack` method. I didn't specify an encoding, because I assumed it would use UTF-8, which is the default encoding for Python in my locale (en_US.UTF-8) as well as just a sensible encoding to use in general.\n\nInstead, it tried to encode labels in Latin-1, which failed. Latin-1 seems like a strangely antiquated default to use in modern code.\n\nI can work around it by passing the `encoding='utf-8'` option, but it would be helpful if UTF-8 were the default, as it is in other Python I/O.\n\nHere's my version information:\n\n```\n>>> pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.0.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-51-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.1\nnose: 1.3.7\npip: 1.5.4\nsetuptools: 2.2\nCython: 0.23.3\nnumpy: 1.10.4\nscipy: 0.16.1\nstatsmodels: 0.6.1\nIPython: 4.0.0\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: 0.7.3\nlxml: None\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\nJinja2: None\n```\n"},{"labels":["api",null],"text":"```\ncsv = pd.read_csv(file_path, chunksize=10)\nchunk = next(csv)\n```\n\nyields\n\n```\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-21-ec4c9826eb96> in <module>()\n      1 csv = pd.read_csv(file_path, chunksize=10)\n----> 2 chunk = next(csv)\n\nTypeError: TextFileReader object is not an iterator\n```\n\nwhich I didn't expect. Sure, I can do `csv = iter(pd.read_csv(file_path, chunksize=10))`, but since I don't see any advantage (i.e. indexing) in having a non-iterator iterable, maybe read_csv could directly return an iterator? In other words,\n\n```\nIn [3]: csv = pd.read_csv(file_path, chunksize=100)\n\nIn [4]: print(len(list(iter(csv))), len(list(iter(csv))))\n(23, 0)\n```\n\nis maybe not what people expect from a non-iterator iterable.\n\n(Even) I should be able to submit a PR. The only question is whether 1) this is worth fixing 2) this is worth fixing in pandas rather than in pytables.\n"},{"labels":["api",null,null],"text":"When reading in a CSV file, you can easily convert a column into the `datetime` format using:\n\n```\npd.read_csv(filepath, parse_dates=['TIMESTAMP'])\n```\n\nIf the column holding the original time information uses an unsupported format, you can currently pass a parser function to it too, for example if the timestamp is \"unixtime in milliseconds\":\n\n```\nconvert = lambda x: datetime.datetime.fromtimestamp(float(x) / 1e3)\ndf = pd.read_csv(filepath, parse_dates=['TIMESTAMP'], date_parser=convert)\n```\n\nHowever, if the data is already read in, you can also do such conversions easily with `.to_datetime()` which provides a `unit` parameter for the most common cases:\n\n```\npd.to_datetime(df['UNIXTIME'], unit='ms')\n```\n\nI think it would be quite useful to have this `unit` parameter in `.read_csv()` too, calling it `date_unit` to avoid confusion. The example with the extra conversion function would then look like:\n\n```\ndf = pd.read_csv(filepath, parse_dates=['TIMESTAMP'], date_unit='ms')\n```\n\nOr using a list of units to allow multiple columns with different units:\n\n```\ndf = pd.read_csv(filepath, parse_dates=['TIMESTAMP', 'ANOTHER_TIMESTAMP], date_unit=['ms', 'us'])\n```\n"},{"labels":["api",null,null,null],"text":"xref #11892 (see changes and discussion)\n\nRangeIndex, a new Int64Index subclass that will be used as the new default index (currently) has the following signature: `RangeIndex(start=None, stop=None, step=None, name=None...)`\n\nFor converting an `xrange`/`range` object into a RangeIndex, we have the constructor `RangeIndex.from_range`.\n\nAlthough it isn't documented, the current constructor supports accepting a `RangeIndex` instances as the `start` argument, in which case a copy of the original index is produced, e.g., `RangeIndex(RangeIndex(0, 10, 1)) -> RangeIndex(0, 10, 1)`.\n\nI think this is a mistake: it is reusing the `start` parameter for a different type of argument, at odds with the documented behavior and predictable function signatures. If a user wants to create another RangeIndex from a RangeIndex, they can simply use the Index construtor, e.g., `Index(RangeIndex(...))`.\n\n@jreback argues that not accepting a `RangeIndex` as the first argument to `RangeIndex` would be an API break, because it would create an Index constructor that is not idempotent. It would be \"a major break with the current Index design\".\n\nThoughts from anyone else?\n"},{"labels":["api",null,null],"text":"xref #12060\n"},{"labels":["api",null,null],"text":"`.to_csv` has a `decimal` parameter to specify the decimal mark for floats since version 0.16\n\nIt would be nice to have the same thing in `.to_latex` and `.to_html`.\n\nI'll work on a PR.\n\nThis issue is a spin-off of #7061, in which @TomAugspurger  said:\n\n> There’s a _possibility_ that we’ll be able to replace some of the `to_latex` code with a Jinja template, similar to the Style stuff. So don’t spend too much time on it :)\n"},{"labels":["api",null],"text":"I'm working on a `from_dummies` right now, and I'm trying to achieve (roughly)\n\n``` python\ndf = pd.from_dummies(pd.get_dummies(df))\n```\n\nOne problem is that `get_dummies` will append all the dummy columns to the end of the DataFrame, after the numeric columns.\n\n``` python\nIn [15]: df = pd.DataFrame({\"A\": [1, 2], \"B\": ['a', 'b'], 'C': [1, 2]})\nIn [18]: pd.get_dummies(df)\nOut[18]:\n   A  C  B_a  B_b\n0  1  1    1    0\n1  2  2    0    1\n```\n\nSo when we go to invert with `from_dummies` we don't know when column goes where. If instead we had\n\n``` python\n   A  B_a  B_b  C\n0  1    1    0  1\n1  2    0    1  2\n```\n\nwe'd know what the original ordering was by position.\nLooking through the docs, I don't see anywhere where we say that the ordering must be this way, but it'll still be an API change and I'm not sure whether it's worth it.\n"},{"labels":["api",null,null],"text":"One feature about DataFrame that I found uncomfortable is the inconsistency of results:\n\n```\nx = pd.DataFrame({'A': [1,2,3,4,5]}, index=['a','b','c','d','e'])\nx.loc[['a', 'f']]  # fine, value of 'f' is numpy.nan\nx.loc[['f']]  # throws \n```\n\nWhen developing data applications, I spend lot of time fixing this issue.  I think we should make it more consistent. Either always returns (preferred) or always throws.\n"},{"labels":["api",null,null],"text":"When using pd.ExcelWriter, it appears that calling the save method prevents you from subsequently creating new sheets. E.g. see the following test script\n\n``` python\nimport pandas as pd\nimport numpy as np\nimport os\n\n\nif __name__ == \"__main__\" :\n    dates = pd.date_range('20130101', periods=6)\n    df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\n\n    currentpath = os.path.realpath(__file__)\n    outputpath = os.path.join(currentpath, \"..\\\\test.xlsx\")\n\n    writer = pd.ExcelWriter(outputpath)\n    df.to_excel(writer, \"thing1\")\n\n    writer.save() #if you remove this line it works as expected\n\n    new_sheet = writer.book.add_worksheet(\"thing2\")\n    new_sheet.write(1,1,\"test\")\n\n    print writer.book.sheetnames\n\n    writer.save()\n    writer.close()\n```\n"},{"labels":["api",null,null,null],"text":"Seems surprising, I thought `.name` would come along. \n\n``` python\nIn [31]: idx = pd.MultiIndex.from_product([['A'], ['a', 'b', 'c']])\n\nIn [32]: idx.name = 'foo'\n\nIn [33]: idx2 = idx.set_names(['l1', 'l2'])\n\nIn [34]: idx2.name\n```\n\nBug, or user error since MultiIndex.name isn't all that useful?\n"},{"labels":["api",null],"text":"It would be nice to have a method to change the `.name` attribute on the columns/index, to permit it in a method chain. (This is a suggestion based on [this stackoverflow question](http://stackoverflow.com/questions/34598733/dataframe-method-for-renaming-axis).)\n\nInstead of \n\n```\ndf.columns.name = 'A'\ndf.index.name = 'B'\ndf\n    A   0   1\n    B        \n    0   9   4\n    1  15   0\n    2  17  16\n```\n\nit would allow something like\n\n```\ndf.rename_columns('A').rename_index('B')\n```\n\nor perhaps be a keyword argument to the existing `.rename()`. Does this seem like something that would fit in with pandas?\n"},{"labels":["api",null],"text":"pandas generally tries to coerce values to fit the column dtype, or upcasts the dtype to fit.\n\nFor a setting operation this is convenient & I think expected as a user\n\n```\nIn [35]: df = DataFrame({'A' : Series(dtype='M8[ns]'), 'B' : Series([np.nan],dtype='object'), 'C' : ['foo'], 'D' : [1]})\n\nIn [36]: df\nOut[36]:\n    A    B    C  D\n0 NaT  NaN  foo  1\n\nIn [37]: df.dtypes\nOut[37]:\nA    datetime64[ns]\nB            object\nC            object\nD             int64\ndtype: object\n\nIn [38]: df.loc[0,'D'] = 1.0\n\nIn [39]: df.dtypes\nOut[39]:\nA    datetime64[ns]\nB            object\nC            object\nD           float64\ndtype: object\n```\n\nHowever for a `.fillna` (or `.replace`) operation this might be a bit unexpected. So `A` was coerced to `object` dtype, even though it was `datetime64[ns]`.\n\n```\nIn [40]: df.fillna('')\nOut[40]:\n  A B    C  D\n0      foo  1\n\nIn [41]: df.fillna('').dtypes\nOut[41]:\nA     object\nB     object\nC     object\nD    float64\ndtype: object\n```\n\nSo a possibility is to add a keyword `errors='raise'|'coerce'|'ignore'`. This last behavior would be equiv of `errors='coerce'`. While skipping this column would be done with `errors='coerce'`. (and of course `raise` would raise.\n\nIdeally would have a default of `coerce` I think (to skip for non-compat values). Any thoughts on this? \n"},{"labels":["api",null,null,null],"text":"xref: https://github.com/pydata/pandas/pull/11832 allows `pandas.tseries.tools.to_time` to parse `time` types (into `datetime.time`) objects. These are not first class within the pandas data structures. But still may want to expose this.\n"},{"labels":["api",null,null,null],"text":"for example:\n\n``` python\nSecond().rollback(pd.Timestamp('2015-02-02 08:35:22.555'))\n```\n\ndoes not change the timestamp and returns:\n\n``` python\nTimestamp('2015-02-02 08:35:22.555000')\n```\n\nThis will probably also affect `DatetimeIndex.snap()` method.\n(using pandas 0.17.1)\n"},{"labels":["api",null],"text":"``` python\npandas.DataFrame({\"stuff\":[1,2,3,4,5]}).head(0)\n```\n\nreturns\n\n``` python\n   stuff\n0      1\n1      2\n2      3\n3      4\n4      5\n\n```\n\nI would expect an empty data frame, in the same way that\n\n``` python\n[1,2,3,4,5][0:0]\n```\n\nreturns an empty list.\n\nThis is with pandas version 0.15.0.\n\nThe documentation (http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.head.html) says `head(n)` returns 'first n rows', so I'd expect 0 rows if n is 0.\n"},{"labels":["api",null],"text":"(I know that the following bug report will be dismissed, since what I refer to here as a \"bug\" is clearly intentional.  I'm posting it only for-the-record.)\n\nCurrently, an exception is raised if DataFrame.groupby is called with an empty tuple as argument:\n\n```\n>>> df.groupby(by=())\n...\nValueError: No group keys passed!\n```\n\nIndeed, the underlying code is pretty direct about it:\n\n```\n.../python2.7/site-packages/pandas/core/groupby.pyc in _get_grouper(obj, key, axis, level, sort)\n   2277 \n   2278     if len(groupings) == 0:\n-> 2279         raise ValueError('No group keys passed!')\n   2280 \n```\n\nThis is not the correct behavior for this condition.  It is therefore a \"design bug\".  The correct behavior should be returning a trivial grouping, consisting of exactly 1 group (containing, of course, all the rows).  This is consistent with the semantics \"each group consists of the rows that agree on the grouping columns\": when there are no grouping columns, all rows fall into one group.\n\nIn particular, the proposed behavior will render the expression\n\n```\ndf.groupby(by=tuple(df.columns))\n```\n\nvalid even when `df` has no columns.  The expression above groups identical rows together, and it should work for all valid dataframes.  In a dataframe with no columns, all the rows are identical.\n"},{"labels":["api",null],"text":"I'd like to be able to separately trap errors in indexing that are due to the lack of the `MultiIndex` being lexsorted.  Right now, a `KeyError` is raised, and the only way to tell if the error was due to the lack of a lexsort versus a missing key is to look at the text of the message.\n\nSo I'd like to avoid code like this:\n\n``` python\ntry:\n    subdf = df.loc['A':'D','Vals']\nexcept KeyError as ker:\n    if ker.args[0].startswith(\"MultiIndex Slicing requires the index to be fully lexsorted\"):\n        print (\"Need to handle fact index not sorted\")\n    else:\n        print (\"Need to handle fact that key was missing\")\n```\n\nI'd rather write something like:\n\n``` python\ntry:\n    subdf = df.loc['A':'D','Vals']\nexcept UnsortedIndexError:\n    print (\"Need to handle fact index not sorted\")\nexcept KeyError:\n    print (\"Need to handle fact that key was missing\")\n```\n\nSo could the designers accept a change where a different error is raised in case the index is not lexsorted?  If so, I'll implement it.\n"},{"labels":["api",null],"text":"If you call the new `round` method on a mixed dataframe (not only numerical columns), you get an error:\n\n```\nIn [1]: df = pd.DataFrame({'floats':[0.123, 0.156, 5.687], 'strings': ['a', 'b', 'c']})\n\nIn [2]: df.round(2)\nTypeError: can't multiply sequence by non-int of type 'float'\n\nIn [3]: df.round()\nAttributeError: 'str' object has no attribute 'rint'\n```\n\nWould it be better to ignore these non-numerical columns instead of raising an error? \nAnother option would be to drop these columns (like numerical aggregations like `df.mean()` do).\n"},{"labels":["api",null,null],"text":"The DataFrame.describe() method docs seem to indicate that you can pass percentiles=None to not compute any percentiles, however by default it still computes 25%, 50% and 75%. The best I can do is pass an empty list  to only compute the 50% percentile. I would think that passing an empty list would return no percentile computations. \n\nShould we allow passing an empty list to not compute any percentiles?\n\npandas 0.17.1\n\n``` python\nIn [1]: import pandas as pd\n\nIn [2]: import numpy as np\n\nIn [3]: df = pd.DataFrame(np.random.randn(10,5))\n\nIn [4]: df.describe(percentiles=None)\nOut[4]:\n               0          1          2          3          4          5  \ncount  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000\nmean   -0.116736  -0.160728   0.066763  -0.068867  -0.242050   0.390091\nstd     0.771704   0.837520   0.875747   0.955985   1.093919   0.923464\nmin    -1.347786  -1.140541  -1.297533  -1.347824  -2.085290  -0.825807\n25%    -0.580527  -0.613640  -0.558291  -0.538433  -0.836046  -0.275567\n50%    -0.261526  -0.395307   0.007595  -0.248025   0.000515   0.314278\n75%     0.329780   0.154053   0.708768   0.407732   0.366278   1.192338\nmax     1.285276   1.649528   1.485076   1.697162   1.551388   1.762939\n\nIn [15]: df.describe(percentiles=[])\nOut[15]:\n               0          1          2          3          4          5  \ncount  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000\nmean   -0.116736  -0.160728   0.066763  -0.068867  -0.242050   0.390091\nstd     0.771704   0.837520   0.875747   0.955985   1.093919   0.923464\nmin    -1.347786  -1.140541  -1.297533  -1.347824  -2.085290  -0.825807\n50%    -0.261526  -0.395307   0.007595  -0.248025   0.000515   0.314278\nmax     1.285276   1.649528   1.485076   1.697162   1.551388   1.762939\n\n```\n"},{"labels":["api",null],"text":"I would find it more natural for PeriodIndexed data to be plotted by default as a stepped line. A step better conveys the fact that the y-value one is plotting corresponds to that entire x-region, whereas a (linearly or otherwise) interpolated line suggests that one is using Timestamped data (since it suggests that the y-value is different at each point in time, which for periods is impossible to know).\n\nIt is, of course, possible to do this manually with `drawstyle=steps`, but this has the annoying problem that one of the Period \"bins\" will be missing (the one at the end). `steps-mid` is ugly, since it cuts off half of the steps at both edges.\n\nIt would be nice if the default would be changed to this `drawstyle`, but it would be even better if this style would be extended some way to fully handle Periods. Ideally, each Period step would be plotted between the edges of the Period and irregular Periods would be supported as well.\n"},{"labels":["api",null,null],"text":"numpy's `sum()` function allows specifying a tuple of axis values for higher-dimensional arrays, but when it is called this way on a Pandas `Panel` or `Panel4D` it raises a ValueError:\n\n```\n>>> import numpy as np\n>>> import pandas as pd\n>>> a = np.random.randn(3, 3, 3)\n>>> a.sum(axis=(0, 1))\narray([-1.91974326,  1.49781967, -1.29698099])\n>>> pan = pd.Panel(a)\n>>> pan.sum(axis=(0, 1))  # should return the same vector\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas/core/generic.py\", line 4554, in stat_func\n    skipna=skipna, numeric_only=numeric_only)\n  File \"/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas/core/panel.py\", line 1084, in _reduce\n    axis_name = self._get_axis_name(axis)\n  File \"/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas/core/generic.py\", line 327, in _get_axis_name\n    .format(axis, type(self)))\nValueError: No axis named (0, 2) for object type <class 'pandas.core.panel.Panel'>\n>>> np.sum(pan, axis=(0, 1))  # try it another way\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/numpy/core/fromnumeric.py\", line 1828, in sum\n    return sum(axis=axis, dtype=dtype, out=out)\n  File \"/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas/core/generic.py\", line 4554, in stat_func\n    skipna=skipna, numeric_only=numeric_only)\n  File \"/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas/core/panel.py\", line 1084, in _reduce\n    axis_name = self._get_axis_name(axis)\n  File \"/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/pandas/core/generic.py\", line 327, in _get_axis_name\n    .format(axis, type(self)))\nValueError: No axis named (0, 1) for object type <class 'pandas.core.panel.Panel'>\n```\n\nRight now the workaround is to call `np.asarray()` on the Pandas object first and then convert it back to `Series` (or `DataFrame`, I suppose) once the result is returned, being sure to apply the correct index to the resulting object.\n"},{"labels":["api",null,null],"text":"One \"gotcha\" in pandas that's always bugged me is that you can generally access and change columns in a DataFrame by dot-messaging (attribute assignment) _except_ for adding new columns. For example, one can play with existing columns easily:\n\n```\ndf = pd.DataFrame({'col1':range(10,14),\n                   'col2':range(20,24)})\n\ndf.col1 = 3\ndf\nOut[5]: \n   col1  col2\n0     3    20\n1     3    21\n2     3    22\n3     3    23\n```\n\nBut you can't set new columns:\n\n```\ndf.col3 = pd.Series(range(30,40))\ndf\n\nOut[6]: \n\n   col1  col2\n0     3    20\n1     3    21\n2     3    22\n3     3    23\n```\n\nSeems like it wouldn't be hard to modify `__setattr__` so that if the setting value is a Series, NumPy Array, or scalar value, just call `df[key] = value`.\n\nSeem reasonable?\n\n(Still wrestling with copy-on-write PR, but happy to come back to this if others support and no on addresses till then)\n"},{"labels":["api",null],"text":"When you save MultiIndex DataFrame to csv and then loading it with no additional parameters pandas not able to read multiindices properly and reconstructed dataframe is incorrect, however all MultiIndices are stored. Creating MultiIndex DataFrame:\n\n``` python\n>>> import pandas as pd\n>>> arrays = [np.array(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux']),\nnp.array(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'])]\n>>> tuples = list(zip(*arrays))\n>>> index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\n>>> df = pd.DataFrame(np.random.randn(6, 6), index=index[:6], columns=index[:6])\n```\n\n``` python\n>>> df\nfirst              bar                 baz                 foo          \nsecond             one       two       one       two       one       two\nfirst second                                                            \nbar   one     0.608879 -1.128122 -0.238610  1.089921  0.521766  1.116998\n      two     1.161840  0.876528 -1.417781 -1.150305 -1.043776  0.458298\nbaz   one     1.761382  0.634688 -1.146492 -0.082379  0.414079 -0.520011\n      two     1.079547  0.667917  0.122468 -1.112053  0.408232  0.703068\nfoo   one    -1.031256  1.439310  0.604550 -0.275087 -0.316610 -0.589309\n      two     1.730061 -1.213560 -0.581617 -0.260879  0.069140  0.970397\n```\n\nNow saving to .csv\n\n``` python\n>>> df.to_csv('test.csv')\n```\n\nFile content:\n\n```\nfirst,,bar,bar,baz,baz,foo,foo\nsecond,,one,two,one,two,one,two\nfirst,second,,,,,,\nbar,one,-0.218970051706,-0.37813933444,-0.7634843365,-0.704262374956,-0.839464555892,0.541422908532\nbar,two,0.681850126209,-0.12138626844,0.361273042684,0.143076786043,0.89542332917,-1.4010526032\nbaz,one,-0.818985292735,-1.09454292831,0.162462638192,-1.03231385607,0.553385654032,0.585854960842\nbaz,two,-1.54054784208,1.00696690064,0.976815889071,-1.39618300884,0.283526891003,-0.942377368698\nfoo,one,1.08113073993,-0.29494275034,0.738820962724,-1.62865612926,1.10227198603,-0.628396328368\nfoo,two,0.135391399079,0.410272615409,0.59806563553,0.172448276698,0.11403085441,1.47679237362\n```\n\nLoading file:\n\n``` python\n>>> df = pd.read_csv('test.csv')\n```\n\n```\n    first Unnamed: 1             bar           bar.1              baz  \\\n0  second        NaN             one             two              one   \n1   first     second             NaN             NaN              NaN   \n2     bar        one  0.608879221531  -1.12812151143  -0.238610193059   \n3     bar        two   1.16184022363  0.876527615412   -1.41778054545   \n4     baz        one   1.76138211998  0.634687797737   -1.14649190565   \n5     baz        two    1.0795474718  0.667917354209   0.122468371132   \n6     foo        one  -1.03125632895   1.43930972259   0.604550496274   \n7     foo        two   1.73006141389  -1.21356047236  -0.581617481203   \n\n              baz.1              foo            foo.1  \n0               two              one              two  \n1               NaN              NaN              NaN  \n2     1.08992073325   0.521766323704    1.11699823915  \n3    -1.15030481312   -1.04377642582   0.458297920053  \n4  -0.0823790312557   0.414078987249  -0.520010569472  \n5    -1.11205302607    0.40823221543   0.703068488788  \n6    -0.27508709062  -0.316610246209  -0.589308900542  \n7   -0.260878563358  0.0691397545038     0.9703965012 \n```\n\nYes, I know that I need to specify columns and index parameters, but why I should do this? All information needed to restore original DataFrame with MultiIndex are already in the file. Is it possible to make it default to parse MultiIndices in csv, for example, using multiple separators in a row:\n\n```\nfirst,second,,,,,,\n```\n"},{"labels":["api",null,null],"text":"The generic function `.mad()` calculates the mean absolute difference of a set of data, but in some cases the median absolute difference is more appropriate. In `R`, the `mad()` function accepts a `center` argument to specify how the average absolute difference should be calculated. I propose to add the same to the _pandas_ function. \n"},{"labels":["api",null],"text":"when `pd.to_numeric` is called with `errors='coerce'` on a `DataFrame`, it doesn't raise and just returns the original `DataFrame`. \n\nThis may be related to the discussion here https://github.com/pydata/pandas/issues/11221 as this function currently doesn't support anything more than 1-d. \n\n```\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'a': [1, 2, 'foo'], 'b': [2.3, -1, 'bar']})\n\nIn [3]: df\nOut[3]:\n     a    b\n0    1  2.3\n1    2   -1\n2  foo  bar\n\nIn [4]: pd.to_numeric(df)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-4-9febd95a7c0a> in <module>()\n----> 1 pd.to_numeric(df)\n\n/Users/mortada_mehyar/code/github/pandas/pandas/tools/util.py in to_numeric(arg, errors)\n     94         conv = lib.maybe_convert_numeric(arg,\n     95                                          set(),\n---> 96                                          coerce_numeric=coerce_numeric)\n     97     except:\n     98         if errors == 'raise':\n\n/Users/mortada_mehyar/code/github/pandas/pandas/src/inference.pyx in pandas.lib.maybe_convert_numeric (pandas/lib.c:52369)()\n    518 cdef int64_t iINT64_MIN = <int64_t> INT64_MIN\n    519\n--> 520 def maybe_convert_numeric(object[:] values, set na_values,\n    521                           bint convert_empty=True, bint coerce_numeric=False):\n    522     '''\n\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\n\nIn [5]: pd.to_numeric(df, errors='coerce')\nOut[5]:\n     a    b\n0    1  2.3\n1    2   -1\n2  foo  bar\n```\n\nNote that the last expression doesn't raise but the previous one does. \n\nSeems like we should either\n1. make `pd.to_numeric` work with `DataFrame` or `NDFrame` in general \n2. simply raise here too if a `DataFrame` or something more than 1-d is passed\n"},{"labels":["api",null,null],"text":"Is there some philosophical reason that round(Series) and round(Dataframe) is not defined?  See the example below.  Being able to write round() as a function of a Dataframe or Series, as opposed to doing it afterwards would be nice.  I'm willing to implement `__round__()` to make it work, but don't want to go down that path if there is some reason `__round__()` is not implemented.\n\nExample:\n\n```\n    import pandas as pd\n    import numpy as np\n    df =  pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'])\n    print (df.A.round())\n    print (round(df))\n```\n\nProduces\n\n```\n0   -2\n1    1\n2   -1\n3   -1\n4    2\n5    0\n6    0\n7    0\n8   -2\n9    0\nName: A, dtype: float64\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-8-9d2be9cbdc70> in <module>()\n      3 df =  pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'])\n      4 print (df.A.round())\n----> 5 print (round(df))\n\nTypeError: type DataFrame doesn't define __round__ method\n```\n"},{"labels":["api",null,null],"text":"```\nIn [19]: df = DataFrame({'a':['A1', 'A1', 'A1'], 'b':['B1','B1','B2'], 'c':1})\n\nIn [20]: df.set_index('a').groupby('b').rank(method='first')\nOut[20]: \n    c\na    \nA1  1\nA1  2\nA1  1\n\nIn [21]: df.set_index('a').groupby('c').rank(method='first')\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-21-6b8d4cae9d91> in <module>()\n----> 1 df.set_index('a').groupby('c').rank(method='first')\n\n/home/nicolas/Git/pandas/pandas/core/groupby.pyc in rank(self, axis, numeric_only, method, na_option, ascending, pct)\n\n/home/nicolas/Git/pandas/pandas/core/groupby.pyc in wrapper(*args, **kwargs)\n    618                     # mark this column as an error\n    619                     try:\n--> 620                         return self._aggregate_item_by_item(name, *args, **kwargs)\n    621                     except (AttributeError):\n    622                         raise ValueError\n\n/home/nicolas/Git/pandas/pandas/core/groupby.pyc in _aggregate_item_by_item(self, func, *args, **kwargs)\n   3076             # GH6337\n   3077             if not len(result_columns) and errors is not None:\n-> 3078                 raise errors\n   3079 \n   3080         return DataFrame(result, columns=result_columns)\n\nTypeError: rank() got an unexpected keyword argument 'numeric_only'\n```\n\nI'm trying to obtain what I would get with a `row_number()` in SQL...\n\nNotice that if I replace the value in the `'c'` column with the string `'1'`, then even `df.set_index('a').groupby('b').rank(method='first')` fails.\n\nAm I doing something wrong?\n"},{"labels":["api",null],"text":"When using SAS or Stata data, dates are represented as the number of days since 1/1/1960, and other statistical software uses different [origin dates](http://blog.stata.com/2011/01/05/using-dates-and-times-from-other-software/). With that in mind, it would be nice to have an origin date that can be specified. See also, #3969.\n\nIt's a relatively simple thing, and not hard to work around, of course. However, I end up dealing with date formatting on just about every data set I import, and I imagine that lots of others do, too.\n\nCurrently, I do something like this:\n\n``` python\nimport pandas as pd\nimport datetime\n\nEPOCH1960 = datetime.date(1970, 1, 1) - datetime.date(1960, 1, 1)\n\ndata = pd.read_stata('./data.dta')\ndata['date'] = pd.to_datetime(data['date'],unit='D') - EPOCH1960\n\n```\n\nIn R, the `as.Date()` function takes an origin parameter for numeric types (see, [manual](https://stat.ethz.ch/R-manual/R-devel/library/base/html/as.Date.html)). So, in R, the date part would simply be:\n\n``` r\ndata$date <- as.Date(data$date, origin = '1960-01-01')\n\n```\n"},{"labels":["api",null,null,null],"text":"If I group a `DataFrame` by a column of dates, the return type varies depending on whether I just group or whether I also apply a frequency in the `Grouper`. \n\nGrouping without resampling dates returns a `DataFrame` when I apply a function which returns a labeled `Series`, or a `Series` if the function returns a scalar:\n\n```\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'date': ['10/10/2000', '11/10/2000'], 'value': [10, 13]})\n\nIn [3]: def sumfunc(x):\n   ...:     return pd.Series([x['value'].sum()], ('sum',))\n   ...: \n\nIn [4]: df.groupby(pd.Grouper(key='date')).apply(sumfunc)\nOut[4]: \n            sum\ndate           \n10/10/2000   10\n11/10/2000   13\n\nIn [5]: type(df.groupby(pd.Grouper(key='date')).apply(sumfunc))\nOut[5]: pandas.core.frame.DataFrame\n\nIn [17]: df.groupby(pd.Grouper(key='date')).apply(lambda x: x.value.sum())\nOut[17]: \ndate\n2000-10-10    10\n2000-11-10    13\ndtype: int64\n\nIn [18]: type(df.groupby(pd.Grouper(key='date')).apply(lambda x: x.value.sum()))\nOut[18]: pandas.core.series.Series\n```\n\nIf I apply a frequency in the `Grouper`, I get a `Series` with a multi-index when the function returns a labeled `Series`, or a `TypeError` when it returns a scalar.\n\n```\nIn [6]: df['date'] = pd.to_datetime(df['date'])\n\nIn [7]: df.groupby(pd.Grouper(freq='M', key='date')).apply(sumfunc)\nOut[7]: \ndate           \n2000-10-31  sum    10\n2000-11-30  sum    13\ndtype: int64\n\nIn [8]: type(df.groupby(pd.Grouper(freq='M', key='date')).apply(sumfunc))\nOut[8]: pandas.core.series.Series\n\nIn [16]: df.groupby(pd.Grouper(freq='M', key='date')).apply(lambda x: x.value.sum())\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-16-ad73d0ebc475> in <module>()\n----> 1 df.groupby(pd.Grouper(freq='M', key='date')).apply(lambda x: x.value.sum())\n\n/Users/shoover/.py35/lib/python3.5/site-packages/pandas/core/groupby.py in apply(self, func, *args, **kwargs)\n    713         # ignore SettingWithCopy here in case the user mutates\n    714         with option_context('mode.chained_assignment',None):\n--> 715             return self._python_apply_general(f)\n    716 \n    717     def _python_apply_general(self, f):\n\n/Users/shoover/.py35/lib/python3.5/site-packages/pandas/core/groupby.py in _python_apply_general(self, f)\n    720 \n    721         return self._wrap_applied_output(keys, values,\n--> 722                                          not_indexed_same=mutated)\n    723 \n    724     def aggregate(self, func, *args, **kwargs):\n\n/Users/shoover/.py35/lib/python3.5/site-packages/pandas/core/groupby.py in _wrap_applied_output(self, keys, values, not_indexed_same)\n   3253             # Handle cases like BinGrouper\n   3254             return self._concat_objects(keys, values,\n-> 3255                                         not_indexed_same=not_indexed_same)\n   3256 \n   3257     def _transform_general(self, func, *args, **kwargs):\n\n/Users/shoover/.py35/lib/python3.5/site-packages/pandas/core/groupby.py in _concat_objects(self, keys, values, not_indexed_same)\n   1271                 group_names = self.grouper.names\n   1272                 result = concat(values, axis=self.axis, keys=group_keys,\n-> 1273                                 levels=group_levels, names=group_names)\n   1274             else:\n   1275 \n\n/Users/shoover/.py35/lib/python3.5/site-packages/pandas/tools/merge.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\n    810                        keys=keys, levels=levels, names=names,\n    811                        verify_integrity=verify_integrity,\n--> 812                        copy=copy)\n    813     return op.get_result()\n    814 \n\n/Users/shoover/.py35/lib/python3.5/site-packages/pandas/tools/merge.py in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\n    866         for obj in objs:\n    867             if not isinstance(obj, NDFrame):\n--> 868                 raise TypeError(\"cannot concatenate a non-NDFrame object\")\n    869 \n    870             # consolidate\n\nTypeError: cannot concatenate a non-NDFrame object\n```\n\nSince in this example, assigning dates to months still leaves the same groups, I would have expected identical results whether I set `freq='M'` or not. I'm guessing that the difference is that the `freq='M'` causes an extra `groupby` to happen under the hood, yes? When I ran into this, what I expected to happen was for `pd.Grouper(freq='M', key='date')` to do a single `groupby`, combining rows where dates happened to fall into the same month.\n\nPandas version:\n\n```\nIn [9]: pd.__version__\nOut[9]: '0.17.1+22.g0c43fcc'\n```\n"},{"labels":["api",null],"text":"similar to #11603 \n\nthis would transform:\n\n`s.resample('D',how='max')`\n\nto\n\n`s.resample('D').max()`\n\nThis would be a breaking API change, as the default is `how='mean'`, meaning, that `s.resample('D')` returns the `mean` of the resampled data. However it would be visible at the very least and not simply change working code.\n\nThis would bring `.resample` (which is just a groupby type operation under the hood anyhow) into the API syntax for `.groupby` and `.rolling` et. al.\n\nFurthermore this would allow geitem / aggregate type operations with minimal effort\ne.g.\n\n`s.resample('D').agg(['min','max'])`\n"},{"labels":["api",null],"text":"When I work with Pandas DataFrames, I prefer to keep the full column names for clarity. So when I print out the `head`, or use describe, I get a meaningful table. However, this also means I have column names like \"Time of Sale\" that become annoying to type out. \n\nA nice compromise seems like it would be to have short \"aliases\" for column names. For instance, I can define the `tos` average for the above, perhaps like so:\n\n```\ndf = pd.read_csv(...)\ndf.set_alias({'Time of Sale' : 'tos'})\n```\n\nThen, the `__get_attribute__` method can look up aliases in addition to column names, so I can refer to that column simply as `df.tos`. But for all other purposes, the columns name is still the descriptive full name.\n\nWould this make sense?\n"},{"labels":["api",null],"text":"### Expected\n\nRegardless of the arguments specified, the return of `DataFrame.join` always has the left side's index values and name. In the case of `how='right'`, there should be `np.nan` in the index when the right side does not 'match' a row on the left.\n\nIn this case:\n\n``` python\n>>> pd.DataFrame([], columns=['gene'], index=pd.Index([], name='family')).join(s), how='right', on='gene')\n     gene  gene_id\nfamily              \nNaN     NaN        1\n```\n### Actual / reproduce\n\nRight index is returned for empty dataframe:\n\n``` python\n>>> import pandas as pd\n>>> s = pd.Series([1], name='gene_id', index=pd.Index([1], name='gene')\n>>> pd.DataFrame([], columns=['gene'], index=pd.Index([], name='family')).join(s), how='right', on='gene')\n     gene  gene_id\ngene              \n1     NaN        1\n```\n\nNote that it doesn't happen with non-empty frames:\n\n``` python\n>>> pd.DataFrame([1], columns=['gene'], index=pd.Index([1], name='family')).join(s), how='right', on='gene')\n        gene  gene_id\nfamily               \n1          1        1\n```\n### Version\n\n``` python\n>>> pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.0.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.5-1-ARCH\nmachine: x86_64\nprocessor: \nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.0\nnose: None\npip: 7.1.2\nsetuptools: 18.5\nCython: None\nnumpy: 1.10.1\nscipy: 0.16.1\nstatsmodels: None\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: None\nnumexpr: 2.4.6\nmatplotlib: 1.5.0\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.1.0b1\npymysql: None\npsycopg2: None\n```\n"},{"labels":["api",null,null,null],"text":"Implement `to_html` / notebook repr based on `.style`.\r\n\r\nprob need to expand this to take a `use` argument (to select the style, needs to be 'classic' for a while, to replicate the current `.to_html` one).\r\n"},{"labels":["api",null,null],"text":"When using the new CSS styling, there doesn't seem to be a way to add formatters for the numbers. So trying to make the number data display as $10,000 when using any styling options currently seems off the table.\nDocumented here:\nhttp://stackoverflow.com/questions/33875937/apply-number-formatting-to-pandas-html-css-styling\n\nRelated to this PR: https://github.com/pydata/pandas/pull/11667\n"},{"labels":["api"],"text":"pandas 0.17.1\n\n```\nimport pandas as pd\nfrom io import StringIO\nsio=StringIO(\"\"\"\nA\n2\nnan\n\"\"\")\ndf = pd.read_csv(sio,dtype={'A':str})  \nprint(type(df.A[0]),df.A[0])\nprint(type(df.A[1]),df.A[1])\nprint()\ndf=df.astype(str)\nprint(type(df.A[0]),df.A[0])\nprint(type(df.A[1]),df.A[1])\n```\n\nprints:\n\n```\n<class 'str'> 2\n<class 'float'> nan\n\n<class 'str'> 2\n<class 'str'> nan\n```\n\n---\n\n`read_csv`,even I've specified the dtype to str , reads column A into a mixed object dtype (str + float nan) .\n`astype` is as expected as it interpreting the `nan` into string `'nan'` thus it results in pure str dtype.\nThese two semantic is not consistent .\n"},{"labels":["api",null,null,null],"text":"I'll propose a\n\n`cache_datetime=False` keyword as an addition to `read_csv` and `pd.to_datetime`\n\nthis would use a lookup cache (a dict will probably work), to map datetime strings to Timestamp objects. For repeated dates this will lead to some dramatic speedups.\n\nCare must be taken if a `format` kw is provided (in `to_datetime` as the cache will have to be exposed). This would be an optional (and default `False`) as I think if you have unique dates this could modestly slow down things (but can be revisted if needed).\n\nThis _might_ need also want to accept a list of column names (like `parse_dates`) to enable per-column caching (e.g. you might want to apply to a column, but not the index of example).\n\npossibly we could overload `parse_dates='cache'` to mean this as well\n\ntrivial example\n\n```\nIn [1]: pd.DataFrame({'A' : ['20130101 00:00:00']*10000}).to_csv('test.csv',index=True)\n\nIn [14]: def parser(x):\n   ....:         uniques = pd.Series(pd.unique(x))\n   ....:         d = pd.to_datetime(uniques)\n   ....:         d.index = uniques\n   ....:         return Series(x).map(d).values\n   ....: \nIn [3]: df1 = pd.read_csv('test.csv',index_col=0, parse_dates=['A'])\n\nIn [4]: df2 = pd.read_csv('test.csv',index_col=0, parse_dates=['A'], date_parser=parser)\n\nIn [17]: %timeit pd.read_csv('test.csv',index_col=0, parse_dates=['A'])\n1 loops, best of 3: 969 ms per loop\n\nIn [18]: %timeit pd.read_csv('test.csv',index_col=0, parse_dates=['A'], date_parser=parser)\n100 loops, best of 3: 5.31 ms per loop\n\nIn [7]: df1.equals(df2)\nOut[7]: True\n```\n"},{"labels":["api",null,null],"text":"Hello,\nPlease consider the following code :\n\n```\nimport pandas as pd\nimport numpy as ny\n\ndates = pd.date_range(\"2015-01-01\", periods=10, freq=\"D\")\nts = pd.TimeSeries(data=range(10), index=dates, dtype=ny.float64)\nts_mean = pd.rolling_mean(ts, 5)\nprint(ts) \n2015-01-01    0\n2015-01-02    1\n2015-01-03    2\n2015-01-04    3\n2015-01-05    4\n2015-01-06    5\n2015-01-07    6\n2015-01-08    7\n2015-01-09    8\n2015-01-10    9\nFreq: D, dtype: float64\n\nprint(ts_mean)\n2015-01-01   NaN\n2015-01-02   NaN\n2015-01-03   NaN\n2015-01-04   NaN\n2015-01-05     2\n2015-01-06     3\n2015-01-07     4\n2015-01-08     5\n2015-01-09     6\n2015-01-10     7\nFreq: D, dtype: float64\n```\n\nFor the last date (2015-01-10), you should obtain 7, which corresponds to [5, 6, 7, 8, 9] mean value.\nNow, please replace the 2015-01-03 value by -9+33 extreme value.\n\n```\ndates = pd.date_range(\"2015-01-01\", periods=10, freq=\"D\")\nts = pd.TimeSeries(data=range(10), index=dates, dtype=ny.float64)\nts[2] = -9e+33\nprint(ts)\n2015-01-01    0.000000e+00\n2015-01-02    1.000000e+00\n2015-01-03   -9.000000e+33\n2015-01-04    3.000000e+00\n2015-01-05    4.000000e+00\n2015-01-06    5.000000e+00\n2015-01-07    6.000000e+00\n2015-01-08    7.000000e+00\n2015-01-09    8.000000e+00\n2015-01-10    9.000000e+00\nFreq: D, dtype: float64\n```\n\nAnd compute rolling_mean again :\n\n```\nts_mean = pd.rolling_mean(ts, 5)\nprint(ts_mean)\n2015-01-01             NaN\n2015-01-02             NaN\n2015-01-03             NaN\n2015-01-04             NaN\n2015-01-05   -1.800000e+33\n2015-01-06   -1.800000e+33\n2015-01-07   -1.800000e+33\n2015-01-08    0.000000e+00\n2015-01-09    1.000000e+00\n2015-01-10    2.000000e+00\nFreq: D, dtype: float64\n```\n\nAs you can see,  from the 2015-01-08,  computation returns an incorrect result i.e [1, 2, 3] instead of [5, 6, 7]. The extreme value has introduced some perturbations in following date computation.\n\nBest regards,\n"},{"labels":["api",null,null],"text":"Sometimes it's handy to have access to a distinct integer for each group.  For example, using the (internal) grouper:\n\n```\n>>> df = pd.DataFrame({\"a\": list(\"xyyzxy\"), \"b\": list(\"ab\"*3), \"c\": range(6)})\n>>> df[\"group_id\"] = df.groupby([\"a\",\"b\"]).grouper.group_info[0]\n>>> df\n   a  b  c  group_id\n0  x  a  0         0\n1  y  b  1         2\n2  y  a  2         1\n3  z  b  3         3\n4  x  a  4         0\n5  y  b  5         2\n```\n\nThis can be achieved in a number of ways but none of them are particularly elegant, esp. if we're grouping on multiple keys and/or Series.  Accordingly, after a brief discussion on gitter, I propose a new method `transform(\"enumerate\")` which returns a Series of integers from 0 to ngroups-1 matching the order the groups will be iterated in.  In other words, we'll simply be applying the following map:\n\n```\n>>> m = {k: i for i, (k,g) in enumerate(df.groupby([\"a\",\"b\"]))}\n>>> m\n{('x', 'a'): 0, ('y', 'b'): 2, ('y', 'a'): 1, ('z', 'b'): 3}\n```\n\n(Note this is only to shows the desired behaviour, and wouldn't be how it'd be implemented!)\n"},{"labels":["api",null,null,null,null],"text":"Particularly now that `shift` only works on datetime-like indexes (https://github.com/pydata/pandas/pull/11211)\n\nThey look almost the same, although not exactly:\n\n``` python\nIn [11]: df=pd.DataFrame(pd.np.random.rand(5,2), index=pd.date_range(periods=5, start='2000'))\n\nIn [12]: df\nOut[12]: \n                   0         1\n2000-01-01  0.640148  0.781291\n2000-01-02  0.261649  0.652372\n2000-01-03  0.642422  0.734348\n2000-01-04  0.582657  0.601868\n2000-01-05  0.848645  0.078437\n\nIn [13]: df.shift()\nOut[13]: \n                   0         1\n2000-01-01       NaN       NaN\n2000-01-02  0.640148  0.781291\n2000-01-03  0.261649  0.652372\n2000-01-04  0.642422  0.734348\n2000-01-05  0.582657  0.601868\n\nIn [14]: df.tshift()\nOut[14]: \n                   0         1\n2000-01-02  0.640148  0.781291\n2000-01-03  0.261649  0.652372\n2000-01-04  0.642422  0.734348\n2000-01-05  0.582657  0.601868\n2000-01-06  0.848645  0.078437\n\nIn [15]: df.shift(freq='D')\nOut[15]: \n                   0         1\n2000-01-02  0.640148  0.781291\n2000-01-03  0.261649  0.652372\n2000-01-04  0.642422  0.734348\n2000-01-05  0.582657  0.601868\n2000-01-06  0.848645  0.078437\n\nIn [16]: df.tshift(freq='D')\nOut[16]: \n                   0         1\n2000-01-02  0.640148  0.781291\n2000-01-03  0.261649  0.652372\n2000-01-04  0.642422  0.734348\n2000-01-05  0.582657  0.601868\n2000-01-06  0.848645  0.078437\n\n```\n"},{"labels":["api"],"text":"As discussed: https://github.com/pydata/pandas/pull/11596#issuecomment-156552785\n"},{"labels":["api",null],"text":"I would like to know how many bytes my dataframe takes up in memory.  The standard way to do this is the `memory_usage` method\n\n```\ndf.memory_usage(index=True)\n```\n\nFor object dtype columns this measures 8 bytes per element, the size of the reference not the size of the full object.  In some cases this significantly underestimates the size of the dataframe.\n\nIt might be nice to optionally map `sys.getsizeof` on object dtype columns to get a better estimate of the size.  If this ends up being expensive then it might be good to have this as an optional keyword argument.\n\n```\ndf.memory_usage(index=True, measure_object=True)\n```\n"},{"labels":["api",null],"text":"Would be nice to be able to perform a fuzzy join between two dataframes based on string columns. This [thread](http://stackoverflow.com/questions/13636848/is-it-possible-to-do-fuzzy-match-merge-with-python-pandas) shows how to do it using difflib, but it may be nice to have this as built-in functionality.\n"},{"labels":["api",null,null],"text":"I was happy to see in the release notes for 0.17.0 that value_counts no longer discards the series name, but the implementation wasn't what I expected.\n\n0.17.0 gives\n\n``` py\n>>> series = pd.Series([1731, 364, 813, 1731], name='user_id')\n>>> series.value_counts()\n1731    2\n813     1\n364     1\nName: user_id, dtype: int64\n```\n\nwhich doesn't set the index name.\n\nIn my opinion the old series name belongs in the index, not in the series name:\n\n``` py\n>>> series.value_counts()\nuser_id\n1731    2\n813     1\n364     1\ndtype: int64\n```\n\nWhy:\n- It's logical: the user_id has moved to the index, and the values now represent occurrence counts\n- This would be consistent with how `.groupby().size()` behaves\n- Adding a missing index name is cumbersome and requires creating a temporary variable\n- In many cases the series name is discarded, while index names tend to stick around: for example, `pd.DataFrame({'n': series.value_counts(), 'has_duplicates': series.value_counts() > 1})` should really have user_id as an index name\n\nThere are three options:\n- result.name = None and result.index.name = series.name\n- result.name = series.name and result.index.name = series.name\n- result.name = 'size' or 'count' and result.index.name = series.name\n\nThe first option seems more elegant to me but @sinhrks, who reported #10150, apparently expected result.name to be filled, so perhaps there are use cases where the second option is useful.\n"},{"labels":["api",null,null],"text":"I stumbled upon this today while working with the chunking feature. I was trying to implement a CSV cleaner that would set aside error rows (or a chunk that has an error) for future review.\n\nGiven input table as 'test.csv':\n\n```\n1,2,3,4\n1,2,3,4\n1,2,3,4\n1,2,3,4\n1,2,3,4\n1,2,3,4\n1,2,3,4,5,6,7,8\n1,2,3,4\n1,2,3,4\n```\n\nAnd reading it with chunking...\n\n```\ndf = pd.read_csv(\n    'test.csv',\n    header=None,\n    chunksize=1,\n    error_bad_lines=True  \n)\n\nfor chunk in df:\n    print(chunk)\n```\n\n...results in:\n\n```\n   0  1  2  3\n0  1  2  3  4\n   0  1  2  3\n0  1  2  3  4\n   0  1  2  3\n0  1  2  3  4\n   0  1  2  3\n0  1  2  3  4\n   0  1  2  3\n0  1  2  3  4\n   0  1  2  3\n0  1  2  3  4\n   0  1  2  3\n0  1  2  3  4\n   0  1  2  3\n0  1  2  3  4\n   0  1  2  3\n0  1  2  3  4\n```\n\n...silently truncating row 7.\n\nIt does the expected if the error row is in the first chunk. But error rows in subsequent chunks are silently truncated. I assume this is not expected behavior -- correct me if I'm wrong.\n\nI'd love to tackle this. But I've never contributed to this codebase. I figured I'd raise it first before diving in headfirst.\n"},{"labels":["api",null,null],"text":"This works:\n\n```\ndf = pd.DataFrame({'A': [1, 2, 1, 2, 1, 2, 3], 'B': [1, 1, 1, 2, 2, 2, 2]})\ndf.groupby('B').agg(pd.Series.mode)\n```\n\nbut this doesn't:\n\n```\ndf.groupby('B').agg('mode')\n\n...\nAttributeError: Cannot access callable attribute 'mode' of 'DataFrameGroupBy' objects, try using the 'apply' method\n```\n\nI thought all the series aggregate methods propagated automatically to groupby, but I've probably misunderstood?\n"},{"labels":["api",null,null],"text":"xref #11551 \n\nParameter `float_format` and `decimal` options are ignored in an Index, but work in the data itself.\n\n```\nIn [2]: pd.DataFrame({'a': [0.1,1.1], 'b': [2, 3]}).to_csv(float_format='%.2f', index=False)\nOut[2]: 'a,b\\n0.10,2\\n1.10,3\\n\n\nIn [3]: pd.DataFrame({'a': [0.1,1.1], 'b': [2, 3]}).set_index('a').to_csv(float_format='%.2f')\nOut[3]: 'a,b\\n0.1,2\\n1.1,3\\n'\n```\n\nand\n\n```\nIn [4]: pd.DataFrame({'a': [0.1,1.1], 'b': [2, 3]}).to_csv(decimal='^', index=False)\nOut[4]: 'a,b\\n0^1,2\\n1^1,3\\n'\n\nIn [4]: pd.DataFrame({'a': [0.1,1.1], 'b': [2, 3]}).set_index('a').to_csv(decimal='^')\nOut[4]: 'a,b\\n0.1,2\\n1.1,3\\n'\n```\n\nI'll do a PR, soon I hope :)\n"},{"labels":["api",null],"text":"From #11497. Found inconsistencyr regarding `fillna` downcasting.\n\nI understand `fillna(downcast=None)` should downcast values appropriately. It doesn't work on `float` dtype.\n\n```\n# NG\npd.Series([1, 2, np.nan, 4]).fillna(3)\n#0    1\n#1    2\n#2    3\n#3    4\n# dtype: float64\n\n# OK\npd.Series([1, 2, np.nan, 4], dtype=object).fillna(3)\n#0    1\n#1    2\n#2    3\n#3    4\n# dtype: int64\n```\n\nBased on the internal logic, `downcast` can accept dtype. It works on `float`, but not on `object`.\n- https://github.com/pydata/pandas/blob/master/pandas/core/internals.py#L366\n\n```\n# OK\npd.Series([1, 2, np.nan, 4]).fillna(3, downcast='int')\n#0    1\n#1    2\n#2    3\n#3    4\n# dtype: int64\n\n# NG\npd.Series([1, 2, np.nan, 4], dtype=object).fillna(3, downcast='int')\n#0    1\n#1    2\n#2    3\n#3    4\n# dtype: object\n```\n\nI understood the expected behavior as below:\n- all dtypes should be downcast by default (downcast=None)\n- if any dtype is passed via `downcast` kw, downcast to the specified dtype if possible. \n- `downcast=False` will not perform downcast. \n- [ ] Add `Index.fillna` to API (follow-up of #11343)\n"},{"labels":["api",null,null],"text":"While it's possible to find the `max` of a Series containing strings, it's not possible to find the `idxmax`:\n\n```\n>>> s = pd.Series(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n>>> s.max()\n'Z'\n\n>>> s.idxmax()\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-292-28e0d79e56be> in <module>()\n----> 1 s.idxmax()\n\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/core/series.py in idxmax(self, axis, out, skipna)\n   1218         numpy.ndarray.argmax\n   1219         \"\"\"\n-> 1220         i = nanops.nanargmax(_values_from_object(self), skipna=skipna)\n   1221         if i == -1:\n   1222             return np.nan\n\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/core/nanops.py in nanargmax(values, axis, skipna)\n    492     \"\"\"\n    493     values, mask, dtype, _ = _get_values(values, skipna, fill_value_typ='-inf',\n--> 494                                          isfinite=True)\n    495     result = values.argmax(axis)\n    496     result = _maybe_arg_null_out(result, axis, mask, skipna)\n\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/core/nanops.py in _get_values(values, skipna, fill_value, fill_value_typ, isfinite, copy)\n    178     values = _values_from_object(values)\n    179     if isfinite:\n--> 180         mask = _isfinite(values)\n    181     else:\n    182         mask = isnull(values)\n\n/Users/jakevdp/anaconda/envs/python3.4/lib/python3.4/site-packages/pandas/core/nanops.py in _isfinite(values)\n    221             is_integer_dtype(values) or is_bool_dtype(values)):\n    222         return ~np.isfinite(values)\n--> 223     return ~np.isfinite(values.astype('float64'))\n    224 \n    225 \n\nValueError: could not convert string to float: 'Z'\n```\n\nThis surprised me because it works without a problem in numpy:\n\n```\n>>> arr = np.array(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n>>> arr.argmax()\n25\n```\n\nIt seems that pandas `idxmax` implementation implicitly assumes numerical types.\n"},{"labels":["api",null],"text":"Related to #11456. Currently, `DatetimeIndex` handles mixed tz values like below. This behavior sometimes triggers coercion between tz-aware/tz-naive.\n\n```\npd.Index([pd.Timestamp('2011-01-01'), pd.Timestamp('2011-01-02', tz='US/Eastern')])\n# DatetimeIndex(['2010-12-31 19:00:00-05:00', '2011-01-02 00:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None)\n# -> should be normal Index with object dtype?\n\npd.Index([pd.Timestamp('2011-01-01', tz='Asia/Tokyo'), pd.Timestamp('2011-01-02', tz='US/Eastern')])\n# DatetimeIndex(['2010-12-31 15:00:00', '2011-01-02 05:00:00'], dtype='datetime64[ns]', freq=None)\n# -> should be normal Index with object dtype?\n\n pd.Index([pd.Timestamp('2011-01-01'), pd.Timestamp('2011-01-02', tz='US/Eastern')], tz='Asia/Tokyo')\n# DatetimeIndex(['2011-01-01 09:00:00+09:00', '2011-01-02 14:00:00+09:00'], dtype='datetime64[ns, Asia/Tokyo]', freq=None)\n# -> OK, localized  to explicitly passed tz ('Asia/Tokyo')\n```\n"},{"labels":["api",null,null,null,null,null],"text":"accept callables\n- ~~`.query`~~ (not changed, see #12539)\n- [x] `.where/.mask` (#12539)\n- [x] `.loc` (and indexers) (#12539)\n\n```\nIn [4]: df = DataFrame({'A' : [1,2,3,4], 'B' : ['a',np.nan,'b','a']})\n\nIn [5]: df\nOut[5]: \n   A    B\n0  1    a\n1  2  NaN\n2  3    b\n3  4    a\n```\n\nan operation that changes the shape of the DataFrame\n\n```\nIn [9]: res = df.dropna()\n\nIn [10]: res[res.B=='a']\nOut[10]: \n   A  B\n0  1  a\n3  4  a\n```\n\ncan be done like this\n\n```\nIn [8]: df.dropna().pipe(lambda x: x[x.B=='a'])\nOut[8]: \n   A  B\n0  1  a\n3  4  a\n```\n\nSQL calls this `select`, which pandas has, but both `select/filter` are used for filtering LABELS (and not data).\n\nI suppose making this work:\n\n`df.dropna().loc[lambda x: x[x.B=='a']]` is _maybe_ a slight enhancement of this\n\nany thoughts?\n"},{"labels":[null,"api",null,null],"text":"Found this when working on \n\nhttps://github.com/pydata/pandas/pull/11441\n\n``` python\nimport pandas as pd\nd = {'one' : ['A', 'A', 'B', 'B', 'C'],\n     'two' : [4., 3., 2., 2, 1],\n     'three' : [10., 8., 3, 5, 7.]}     \ndf = pd.DataFrame(d)\n\n# this works\ndf.hist('two', by='one', bins=range(0, 10))\n\n# this does not work (everything in one plot), also no way to specify column\ndf.plot.hist(by='one', bins=range(0, 10))\n```\n\nMy idea was to make the df.plot.hist version similar to the df.hist. But the code is much more complex. Would it not be best to point the df.plot.hist to the df.hist version? Instead of having two separate logics for this?\n\nOh, and the by keyword does not seem to work for df.plot.box, have not found any it worked for. At least the way I expected it to work :)\n"},{"labels":["api",null,null],"text":"Is it possible to pass all columns to query? e.g. \n\n```\ndf.query('substring in all_columns')\n```\n"},{"labels":["api",null,null],"text":"This code generates an error in Pandas 0.17.0, but works fine in pandas 0.16.2\n\n``` python\nimport datetime\nimport pandas as pd\nimport numpy as np\n\nprint pd.show_versions()\n\nSHOW_BUG=True\nif SHOW_BUG:\n    df = pd.DataFrame({\n            'a': 1 * np.ones(10),\n            'b': [datetime.datetime.now() for nn in range(10)],\n        })\nelse:\n    df = pd.DataFrame({\n            'a': 1 * np.ones(10),\n            'b': range(10),\n        })\n\n\ndef _compute_length(batch):\n    return pd.Series({'c': 2})\n\ndfg = df.groupby(by=['a']).apply(_compute_length)\n```\n\nHere is the version information:\n\n```\nprint pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_US.UTF-8\n\npandas: 0.17.0\nnose: 1.3.7\npip: 7.1.2\nsetuptools: 18.4\nCython: 0.22.1\nnumpy: 1.10.1\nscipy: 0.16.0\nstatsmodels: 0.6.1\nIPython: 3.2.0\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.4.1\npytz: 2015.6\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.0\nnumexpr: 2.4.3\nmatplotlib: 1.4.3\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 1.0.0\nxlsxwriter: 0.7.3\nlxml: 3.4.4\nbs4: 4.3.2\nhtml5lib: None\nhttplib2: 0.9.1\napiclient: 1.4.1\nsqlalchemy: 1.0.5\npymysql: None\npsycopg2: None\nNone\n```\n\nAnd here is the stack trace\n\n```\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-3-072dccab0584> in <module>()\n     21     return pd.Series({'c': 2})\n     22 \n---> 23 dfg = df.groupby(by=['a']).apply(_compute_length)\n\n/Users/rob/anaconda/lib/python2.7/site-packages/pandas/core/groupby.py in apply(self, func, *args, **kwargs)\n    711         # ignore SettingWithCopy here in case the user mutates\n    712         with option_context('mode.chained_assignment',None):\n--> 713             return self._python_apply_general(f)\n    714 \n    715     def _python_apply_general(self, f):\n\n/Users/rob/anaconda/lib/python2.7/site-packages/pandas/core/groupby.py in _python_apply_general(self, f)\n    718 \n    719         return self._wrap_applied_output(keys, values,\n--> 720                                          not_indexed_same=mutated)\n    721 \n    722     def aggregate(self, func, *args, **kwargs):\n\n/Users/rob/anaconda/lib/python2.7/site-packages/pandas/core/groupby.py in _wrap_applied_output(self, keys, values, not_indexed_same)\n   3125                     date_cols = self._selected_obj.select_dtypes(\n   3126                         include=list(_DATELIKE_DTYPES)).columns\n-> 3127                     result[date_cols] = (result[date_cols]\n   3128                                          ._convert(datetime=True,\n   3129                                                           coerce=True))\n\n/Users/rob/anaconda/lib/python2.7/site-packages/pandas/core/frame.py in __getitem__(self, key)\n   1906         if isinstance(key, (Series, np.ndarray, Index, list)):\n   1907             # either boolean or fancy integer index\n-> 1908             return self._getitem_array(key)\n   1909         elif isinstance(key, DataFrame):\n   1910             return self._getitem_frame(key)\n\n/Users/rob/anaconda/lib/python2.7/site-packages/pandas/core/frame.py in _getitem_array(self, key)\n   1950             return self.take(indexer, axis=0, convert=False)\n   1951         else:\n-> 1952             indexer = self.ix._convert_to_indexer(key, axis=1)\n   1953             return self.take(indexer, axis=1, convert=True)\n   1954 \n\n/Users/rob/anaconda/lib/python2.7/site-packages/pandas/core/indexing.py in _convert_to_indexer(self, obj, axis, is_setter)\n   1119                 mask = check == -1\n   1120                 if mask.any():\n-> 1121                     raise KeyError('%s not in index' % objarr[mask])\n   1122 \n   1123                 return _values_from_object(indexer)\n\nKeyError: \"['b'] not in index\"\n```\n"},{"labels":["api",null],"text":"I am currently writing a script to take data from an API and save it to an Excel file for non-technical users who need to access the data. One of the requirements is to have separate date and time column, rather than a single datetime column. \n\nI am using pandas 0.17.0. \n\n``` python\nimport pandas as pd\n\n# creating datetime series\ndates = pd.date_range('2015-01-01 00:00', '2015-01-01 03:00', freq='H')\ndates = pd.Series(dates)\n# setting one record to NaT\ndates.iloc[0] = pd.NaT\n# converting to date \ndates.apply(lambda x: x.date()) # ignores the NaT\n# converting to time\ndates.apply(lambda x: x.time()) # fails with error \"NaTType does not support time\"\n```\n\nI expected NaTType to behave in the same way for converting into a time as it did for a date i.e. returning NaT. \n\nThe full error is: \n\n``` python\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-98-b34fccfc4c06> in <module>()\n      9 dates.apply(lambda x: x.date()) # ignores the NaT\n     10 # converting to time\n---> 11 dates.apply(lambda x: x.time()) # fails with error \"NaTType does not support time\"\n\nC:\\Users\\Nathan\\Anaconda\\lib\\site-packages\\pandas\\core\\series.pyc in apply(self, func, convert_dtype, args, **kwds)\n   2158             values = lib.map_infer(values, lib.Timestamp)\n   2159 \n-> 2160         mapped = lib.map_infer(values, f, convert=convert_dtype)\n   2161         if len(mapped) and isinstance(mapped[0], Series):\n   2162             from pandas.core.frame import DataFrame\n\npandas\\src\\inference.pyx in pandas.lib.map_infer (pandas\\lib.c:62187)()\n\n```\n"},{"labels":["api",null,null],"text":"I find this surprising as the rest of the pandas `Series.str.*` API ignores NaN values.\n\n``` python\nIn [1]: import pandas as pd\n\nIn [2]: import numpy as np\n\nIn [3]: pd.__version__\nOut[3]: u'0.17.0'\n\nIn [4]: s = pd.Series(['asdf','sdfg',np.nan,'qwer','wert'])\n\nIn [5]: s.str.cat(sep=' ')\nOut[5]: nan\n```\n\nI think this should return\n\n``` python\nIn [5]: s.str.cat(sep=' ')\nOut[5]:'asdf sdfg qwer wert'\n```\n"},{"labels":["api",null,null],"text":"I assume that this was officially supported before. Haven't narrowed it down any more than sometime between 0.16.2 and 0.17.0.\n\n``` python\nIn [1]: pd.__version__\nOut[1]: '0.16.2'\n\nIn [2]: pd.date_range(\"Jan 1\", \"March 31\", name=\"date\")\nOut[2]:\nDatetimeIndex(['2015-01-01', '2015-01-02', '2015-01-03', '2015-01-04',\n               '2015-01-05', '2015-01-06', '2015-01-07', '2015-01-08',\n               '2015-01-09', '2015-01-10', '2015-01-11', '2015-01-12',\n...\n```\n\n``` python\nIn [1]: pd.__version__\nOut[1]: '0.17.0'\n\nIn [2]: pd.date_range(\"Jan 1\", \"March 31\", name=\"date\")\n---------------------------------------------------------------------------\nOutOfBoundsDatetime                       Traceback (most recent call last)\n<ipython-input-2-8eaca08051ac> in <module>()\n----> 1 pd.date_range(\"Jan 1\", \"March 31\", name=\"date\")\n\n/Users/tom.augspurger/Envs/py3/lib/python3.5/site-packages/pandas/tseries/index.py in date_range(start, end, periods, freq, tz, normalize, name, closed)\n   1912     return DatetimeIndex(start=start, end=end, periods=periods,\n   1913                          freq=freq, tz=tz, normalize=normalize, name=name,\n-> 1914                          closed=closed)\n   1915\n   1916\n\n/Users/tom.augspurger/Envs/py3/lib/python3.5/site-packages/pandas/util/decorators.py in wrapper(*args, **kwargs)\n     87                 else:\n     88                     kwargs[new_arg_name] = new_arg_value\n---> 89             return func(*args, **kwargs)\n     90         return wrapper\n     91     return _deprecate_kwarg\n\n/Users/tom.augspurger/Envs/py3/lib/python3.5/site-packages/pandas/tseries/index.py in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, closed, ambiguous, dtype, **kwargs)\n    234             return cls._generate(start, end, periods, name, freq,\n    235                                  tz=tz, normalize=normalize, closed=closed,\n--> 236                                  ambiguous=ambiguous)\n    237\n    238         if not isinstance(data, (np.ndarray, Index, ABCSeries)):\n\n/Users/tom.augspurger/Envs/py3/lib/python3.5/site-packages/pandas/tseries/index.py in _generate(cls, start, end, periods, name, offset, tz, normalize, ambiguous, closed)\n    383\n    384         if start is not None:\n--> 385             start = Timestamp(start)\n    386\n    387         if end is not None:\n\npandas/tslib.pyx in pandas.tslib.Timestamp.__new__ (pandas/tslib.c:8967)()\n\npandas/tslib.pyx in pandas.tslib.convert_to_tsobject (pandas/tslib.c:22303)()\n\npandas/tslib.pyx in pandas.tslib.convert_str_to_tsobject (pandas/tslib.c:24364)()\n\npandas/tslib.pyx in pandas.tslib.convert_to_tsobject (pandas/tslib.c:23344)()\n\npandas/tslib.pyx in pandas.tslib._check_dts_bounds (pandas/tslib.c:26590)()\n\nOutOfBoundsDatetime: Out of bounds nanosecond timestamp: 1-01-01 00:00:00\n```\n"},{"labels":["api",null,null],"text":"I might have missed it, but I couldn't find this behavior documented anywhere.  It is occasionally useful, if a bit magical.\n\n``` python\nIn [1]: df1 = pd.DataFrame([1,2,3], columns=[['a'],['b']])\n\nIn [2]: df2 = pd.DataFrame([1,2,3], columns=[['a'],['']])\n\nIn [5]: df1['a'], type(df1['a'])\nOut[5]: \n(   b\n 0  1\n 1  2\n 2  3, pandas.core.frame.DataFrame)\n\nIn [6]: df2['a'], type(df2['a'])\nOut[6]: \n(0    1\n 1    2\n 2    3\n Name: a, dtype: int64, pandas.core.series.Series)\n```\n"},{"labels":["api",null,null],"text":"xref #8435 (different issue though)\nnormally the semantic is to roll forward to the next period for `n=0`\n\n``` python\nIn [15]: pd.Timestamp('2014-3-2') + pd.offsets.MonthBegin(n=0)\nOut[15]: Timestamp('2014-04-01 00:00:00')\n\nIn [16]: pd.Timestamp('2014-3-2') + pd.offsets.QuarterBegin(n=0)\nOut[16]: Timestamp('2014-03-01 00:00:00')\n\nIn [17]: #expected: Timestamp('2014-06-01 00:00:00')\n```\n"},{"labels":["api",null,null,null],"text":"Please quickly recall how awesome named arguments for Python are and then read through the following idea:\n\nIn case of identical or almost identical values of `index` and `columns`, it can be annoying to remember the assignment of the two data-axes to index and columns respectively. This could be solved by assigning names to `index` and `columns`, e.g. `index_name=y`and `columns_name=x`, and hence being able to retrieve values via `pd.DataFrame.name_loc(x=10, y=20)` (syntax not final).\n\nI have no idea, if such a feature can be implemented with reasonable effort. I do however believe, that it would highly increase the usability of DataFrames for certain usecases.\n"},{"labels":["api",null,null],"text":"Currently there are some inconsistent results. We should clarify the policy first.\n\n```\nimport pandas as pd\n\ns = pd.Series([pd.Timestamp('2011-01-01'), pd.NaT])\ns\n#0   2011-01-01\n#1          NaT\n# dtype: datetime64[ns]\n```\n#### 1. coerced to object dtype\n\nI prefer this behavior.\n\n```\ns.fillna(pd.Timestamp('2011-01-02', tz='Asia/Tokyo'))\n#0          2011-01-01 00:00:00\n#1    2011-01-02 00:00:00+09:00\n# dtype: object\n```\n#### 2. converted to GMT\n\n```\ns[1] = pd.Timestamp('2011-01-02', tz='Asia/Tokyo')\ns\n#0   2011-01-01 00:00:00\n#1   2011-01-01 15:00:00\n# dtype: datetime64[ns]\n```\n#### 3. raise explicit error\n\nThis looks second best.\n\n```\nstz = pd.Series(pd.DatetimeIndex(['2012-01-01', '2012-01-02'], tz='Asia/Tokyo'))\nstz\n#0   2012-01-01 00:00:00+09:00\n#1   2012-01-02 00:00:00+09:00\n# dtype: datetime64[ns, Asia/Tokyo]\n\npd.Index(s).union(pd.Index(stz))\n# TypeError: Cannot join tz-naive with tz-aware DatetimeIndex\n```\n#### 4. not handled properly (bug)\n\n```\ns.append(stz)\n# AttributeError: 'numpy.ndarray' object has no attribute 'tz_localize'\n```\n\n-> Split to #11455, closed by #12195.\n"},{"labels":["api"],"text":"Hi all,\n\nI've enjoyed using pandas for a little while now. Thanks for developing such a useful and convenient tool!\n\nA little nit has been nagging at me about pct_change function (and the .pct_change method of Series, DataFrame, etc.).  If I run a sequence of numbers that increase by 100% (python 0.16.2):\n\n``` python\nimport pandas as pd\n\nseries = pd.Series([0.5,1,2,4])\nprint series.pct_change()\n```\n\nthe output is:\n\n```\n0   NaN\n1     1\n2     1\n3     1\ndtype: float64\n```\n\nBased on the name pct_change, I'd expect the result should the percentage change (i.e., scaled by 100):\n\n```\n0   NaN\n1     100\n2     100\n3     100\ndtype: float64\n```\n\nIn general it looks to me like in general pct_change is really a _fractional_ change. Perhaps the name would suggest the behavior more closely if the function and method were called frac_change? I've become familiar with pandas enough now to know the expected behavior, but this could be a confusing turn-off for newer users.\n"},{"labels":["api",null,null],"text":"```\ndf = pd.read_sql('select idx1,idx2,metric1 from table1 ', dbcon)\ndf.set_index(list(df.columns)[:ncol]).T.to_dict('list')\n```\n\nsuch operations are very frequent in product\n"},{"labels":["api",null],"text":"Currently:\n- `corr` on a `DataFrame` requires another `DataFrame`, and fails on a `Series`\n- `corrwith` on a `DataFrame` takes a `Series`\n\nIs there a good reason these are separate? Should `corr` do whatever `corrwith` does when passed a `Series`, and `corrwith` could be deprecated?\n"},{"labels":["api",null,null],"text":"`HDFStore.append_to_multiple` only lets your specify `data_columns` for the \"selector\" table; all other tables you make with it have no data columns.  Is there a reason it needs to be that way?\n\nI think the `append_to_multiple`/`select_as_multiple` workflow would be somewhat more flexible if we didn't view the set of tables being appended/selected as a monolithic grouping that must always be used together, with the \"selector\" (queryable) table specified up front.  For write operations it makes sense to append together to keep them in sync, but for read operations, is there any particular reason why you always need to be querying on the same table?\n\nI am working with some census data that has many columns representing different sorts of data.  Some are geographic identifiers like FIPS codes, some are overall population counts, some are detailed population counts for subcategories like racial/age groups, some may be other kinds of data like housing units or income.  It would be nice if you could use `append_to_multiple` to write a row of all the data to all the tables, but then later still be able to query or retrieve some of the data depending on what you need.\n\nFor instance, suppose you have a dataset with columns GEO1, GEO2, POP1, POP2, INCOME1, INCOME2.  Sometimes I might need the population columns but not income, or maybe the income columns but not the population ones.  It would be nice if I could use `append_to_multiple` to write my dataset to three separate tables, but then do something like `select_as_multiple(['GeoTable', 'PopTable'], where=['POP1>1000'], selector='PopTable')` or, on the same data, `select_as_multiple(['GeoTable', 'IncomeTable', where=['INCOME1>20000'], selector='IncomeTable')`.  In other words, I want to specify the selector table at query time, not write time.\n\nI think just modifying `append_to_multiple` to allow setting data columns on any table, not just the \"selector\" table.  In fact, there's no need to specify a \"selector\" table at write time at all (although it could still be done for convenience).  This would mean that every one of the sub-tables created by `append_to_multiple` would be nicely queryable on its own.  As far as I can tell, there's nothing in the existing implementations of append/select that would preclude this; they just don't expose any interface for it.\n\nFor now this would probably stick to allowing just one selector table per query (e.g., in the above example you can query on population columns or income columns, but not both in one query).  But in the future it could be possible to allow multi-selector queries by intersecting the resulting row sets of each sub-query.\n"},{"labels":[null,"api",null,null],"text":"In pandas 0.17.0.rc1 there is an inconsistency in the color arguments: once it is \"color\", other times it is \"colors\":\n\ndf.plot(kind='bar', color= ....)\ndf.plot(kind='pie', colors= ...)\n"},{"labels":["api",null,null],"text":"xref #11173 \n\nor IMHO simply replace by use of `pd.to_datetime,pd.to_timedelta,pd.to_numeric`.\n\nHaving an auto-guesser is ok, but when you try to forcefully coerce things can easily go awry.\n"},{"labels":["api",null,null],"text":"I'm using `resample()` to aggregate data in a timeframe.\n\n``` python\n>>> s = pandas.Series((3, 4), (pandas.Timestamp(\"2014-1-1\"), pandas.Timestamp(\"2015-1-1\")))\n>>> x = s.resample(\"1s\")\n>>> len(x)\n31536001\n```\n\nWhen doing such a call, `resample` fills with NaN all the (31536001 - 2) inexistent values, which ends up creating thousands of points and making Python using 500M+ RAM. The thing is that I don't care about the NaN point, so I would like to _not_ fill them in the `Series` and having so much memory used. AFAICS `resample` does not offer such as `fill_method`.\n"},{"labels":["api",null,null],"text":"```\nimport pandas as pd\nidx = pd.Index([1, 2 ,3], name='x')\npd.Index(idx)\n# Int64Index([1, 2, 3], dtype='int64')\n```\n\nA point is whether to reset existing Index name in below case. Current impl can't distinguish whether name is not passed, or passed `None` explicitly.\n\n```\npd.Index(idx, name=None)\n# -> should reset name?\n```\n"},{"labels":["api",null],"text":"Currently, passing a list of `namedtuple`s (or SQLAlchemy results, which is our specific case) to `DataFrame.from_records` doesn't resolve the field names. \n\nI think it would make sense to, at least if the names are all the same. Happy to do a PR if people agree.\n\n``` python\n\nIn [17]: from collections import namedtuple\n\nIn [18]: Record = namedtuple('Record',('date','value'))\n\nIn [19]: records = [Record(d, v) for (d, v) in zip(pd.date_range('2000',freq='D', periods=20), range(20))]\n\nIn [20]: records\nOut[20]: \n[Record(date=Timestamp('2000-01-01 00:00:00', offset='D'), value=0),\n Record(date=Timestamp('2000-01-02 00:00:00', offset='D'), value=1),\n...\n Record(date=Timestamp('2000-01-19 00:00:00', offset='D'), value=18),\n Record(date=Timestamp('2000-01-20 00:00:00', offset='D'), value=19)]\n\nIn [21]: pd.DataFrame.from_records(records)\nOut[21]: \n            0   1\n0  2000-01-01   0\n1  2000-01-02   1\n...\n18 2000-01-19  18\n19 2000-01-20  19\n```\n\nDesired behavior:\n\n``` python\nOut[21]: \n            date   value\n0  2000-01-01   0\n1  2000-01-02   1\n...\n18 2000-01-19  18\n19 2000-01-20  19\n```\n"},{"labels":["api",null],"text":"I use both Stata and Pandas. Many Stata users save variable labels to describe the columns in a clearer way than the names. Running this in Stata\n\n```\nsysuse auto.dta\ndescribe\n```\n\ngives something like\n\n| variable name | storage type | variable label |\n| --- | --- | --- |\n| make | str18 | Make and Model |\n| price | int | Price |\n| mpg | int | Mileage (mpg) |\n\nFor me (maybe for others too) it would be useful to have an optional field in a DataFrame with a column label dictionary. The keys would be the columns (not necessarily all of them) and the values the string labels.\n\nThis is used in the pandas.io.stata.StataReader field `variable_labels`(see [the docs](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.stata.StataReader.variable_labels.html)], that allows you to import these labels when one reads in a Stata `.dta` file.\n\nI know I could just carry around a dictionary with this information, but I think it's cleaner and less error prone to set it and save it within a DataFrame.\n\nAdditionally, storing this would allow doing a cycle on Stata/Pandas without loss of information, since the `to_stata` would check if this field exists. (`to_stata` might already have the option to pass the `variable_labels` dictionary as an option, but I didn't see it documented at least)\n\nMy coding prowess is quite limited, but I'd be happy to at least write test code and help out if somebody starts out.\n"},{"labels":["api",null],"text":"When creating the union of an index with itself, the frequency may end up being reset. I would have expected the call to union in the following example to be invariant and return the original index:\n\n```\nIn [56]: index = pd.bdate_range('20150101', periods=10)\nIn [57]: index.freq = None\nIn [58]: index\nOut[58]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2015-01-01, ..., 2015-01-14]\nLength: 10, Freq: None, Timezone: None\n\nIn [59]: index.union(index)\nOut[59]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2015-01-01, ..., 2015-01-14]\nLength: 10, Freq: B, Timezone: None\n```\n\nThis then propagates through to e.g. `combine_first`, which is also not invariant. This might be desirable behavior in some cases but definitely surprising imo.\n"},{"labels":["api",null],"text":"This is more of a request rather than a call for help I believe, as I think there isn't a way pandas can currently help me with this. Here's my stackoverflow thread relating the issue. http://stackoverflow.com/questions/32523083/iterator-to-iterate-over-excel-file-in-python\n\nI work with large excel files which I use as databases. Most of the time, I need the raw data from the database only once but with read_excel() method, I have to load it all into my memory even though I'll need that data only once. Sad thing is, the amount of time I take for calculations and data arrangement is far shorter than the time my script takes to read excel data.\n\nIt would be pretty nice if pandas had a feature which would allow us to have iterator objects that iterate over certain row/column ranges. As I have stated in my stackoverflow thread, people would be able to make their own iterators if pandas had a method to return a single cell value. Or you could just pass a range of columns or rows to it and it could return a generator.\n\nIf these are already all there or there are methods for this kind of applications, or anything that can help me deal with large excel files, I would like to know. Thank you.\n"},{"labels":["api",null,null],"text":"As `pandas` approaches its 1.0 release, I would like to raise a concern about one aspect of the `pandas` architecture that I think is a threat to its widespread adoption: how `pandas` works with copies and views when setting values (what I will refer to here as the `SettingWithCopyWarning` issue). \n\nThe summary of my concern is the following:\n\n```\n1. SettingWithCopyWarning is a threat to data integrity\n2. It is unreasonable to expect the average user to avoid a `SettingWithCopyWarning` issue, as doing\n    so requires keeping track of the plethora of factors that determine what generates a copy and what\n    generates a view.\n    2a. Views made sense in `numpy`, but not in `pandas`\n    2b. Chain-indexing is a much more subtle problem than suggested in the `pandas` docs. \n3. Given (1) and (2), data integrity in `pandas` relies on users noticing a non-exception warning in the\n    flow of their output.\n4. Even aside from the threat to data integrity, this behavior is unpythonic, and likely to frustrate\n    alienate lots of potential users of `pandas`. \n5. I think solutions can be found that would have only limited effects on performance for the majority of  \n    users\n```\n\nTaking each of these in turn:\n\n(1) **`SettingWithCopyWarning` is a threat to data integrity**\n\nThe fact that assignment operations do different things depending on whether the target is a view or a copy has already been recognized as a threat to the predictability of `pandas`. Indeed, the reason a warning was added is because users were consistently asking why `pandas` was doing un-anticipated things when `SettingWithCopyWarning` came into play. \n\n(2) **It is unreasonable to expect the average user to avoid a `SettingWithCopyWarning` issue, as doing so requires keeping track of the plethora of factors that determine what generates a copy and what generates a view.**\n\nFiguring out when a function will return a copy and when it will return a view in `pandas` is not simple. Indeed, the `pandas` documentation doesn't even try to explain when each will occur (link http://pandas-docs.github.io/pandas-docs-travis/indexing.html?highlight=views#indexing-view-versus-copy):\n\n> The reason for having the SettingWithCopy warning is this. Sometimes when you slice an array you will simply get a view back, which means you can set it no problem. However, even a single dtyped array can generate a copy if it is sliced in a particular way. A multi-dtyped DataFrame (meaning it has say float and object data), will almost always yield a copy. Whether a view is created is dependent on the memory layout of the array.\"\n\n(2a) **Views made sense in `numpy`, but not in `pandas`**\nViews entered the pandas lexicon via `numpy`. But the reason they were so useful in `numpy` is that they were _predictable_ because `numpy` arrays are always single-typed. In `pandas`, no such consistent, predictable behavior exists. \n\n(2b) **Chain-indexing is a much more subtle problem than suggested in the `pandas` docs**\nAt first glance, the `pandas` docs suggest that the `SettingWithCopyWarning` is easily avoided by avoiding chain-indexing or using `.loc`. This, I fear, is misleading for two reasos. First, the canonical example of chain indexing in the docs (`dfmi['one']['second'] = value`) seems to suggest that one can avoid chain indexing by just not falling into the trap of this kind of double slicing. The problem, however, is that these slices need not appear near one another. I know I've had trouble with code of the form:\n\n```\ndf2 = dfmi['one']\n\n# Lots of intermediate code that doesn't change dfmi or df2\n\ndf2['second'] = 5\n```\n\nMoreover, using `.loc` only solves this problem if one notices the chained indexing and attempts to fix it in one place. Just consistently using `.loc[]` (for example, in both the first and second problematic slicings above) would not solve the problem. \n\n(3) **Given (1) and (2), data integrity in `pandas` relies on users noticing a non-exception warning in the flow of their output.**\n\nThis seems really problematic. If a users is printing values as they go along (which CS developers may not do, but interactive casual users often do to monitor the progress of their code), these warnings are easy to miss. And that seems very dangerous. \n\n(4) **Even aside from the threat to data integrity, this behavior is unpythonic, and likely to frustrate alienate lots of potential users of `pandas`**\n\nI suspect I come to `pandas` from a different perspective than many developers. I am an economist and political scientist who has gotten deeper and deeper into computer science over the past several years for instrumental purposes. As a result, I think I have a pretty good sense of how applied users approach something like `pandas`, and I can just see this peculiarity of `pandas` driving this class of users batty. I've taken a year of computer science course work and am one of the most technically trained social scientists I know, and it drives me batty.\n\nIt's also unpythonic -- the behavior of basic operators (like `=`) should not depend on the type of columns in a `DataFrame`. Python 3 changed the behavior of the `/` operator because it was felt the behavior of `/` should not do depend on whether you were working with `float`s or `int`s. Since whether functions return a view or copy is in large part (but not exclusively) a function of whether a `DataFrame` is single or multi-typed (which occurs when some columns are `floats` and some are `ints`), we have the same problem -- the operation of a basic operation (`=`) depends on data types.\n\nIn other words, if one of the aims of `pandas` is to essentially surplant `R` among applied data scientists, then I think this is a major threat to achieving that goal. \n\n(5) **I think solutions can be found that would have only limited effects on performance for the majority of users**\n`pandas` uses views because they're so damn fast, so I understand the reluctance to drop them, but I think there are ways to minimize the performance hit. Obviously more talented developers with a better understanding of the innards of `pandas` may have better suggestions, but hopefully this can get the ball rolling. \n\n```\n* Solution 1: Move `views` to the background.\n   When a user tries to look at an object and it's possible to return a view, do so. But just never let a \n   user assign values to a view -- any time an attempt is made to set on a view, convert it to a copy\n   before executing the assignment. Views will still operate in the background providing high speed \n   data access in read-only environments, but users don't have to worry about what they're dealing \n   with. Users who *really* need access to views can work with `numpy` arrays. \n\n  (I would also note that given the unpredictability of when one will get a view or copy, it's not clear to \n  me how anyone can write code that takes advantage of the behavior of views, which makes me \n  doubt there are many people for whom this would seriously impact performance or written code, but \n  I'd be happy to hear if anyone has workarounds!)\n\n* Solution 2: Create an indexer that always returns copies (like .take(), but for axis labels). \n   This would at least give users who want to avoid views all together a way to do so without littering\n   their code with `.copy()`s. \n\n* Solution 3: Change the `SettingWithCopyWarning` to an exception by default. \n  This is currently a setting, but the global default is for it to be a warning. Personally, I still don't like \n  this solution since, as a result of (2) this means `pandas` will now raise exceptions unpredictably, but \n  at least data integrity will be preserved.     \n```\n\n`pandas` is a brilliant tool, and a huge improvement on everything else out there. I am eager to see it becomes the standard not only among python users, but among data analysts more broadly. Hopefully, by addressing this issue, we can help make this happen. \n\nWith that in mind, I would like to suggest the need for two things:\n1. A discussion about the desirability of the various solutions proposed above\n2. Volunteers to help implement this change. Unfortunately, I don't have the programming sophistication or knowledge of `pandas` internals to take this on alone, and this is likely too big an undertaking for any one individual anyone, so a team is likely to be necessary. \n"},{"labels":["api",null,null],"text":"In [#928](https://github.com/pydata/pandas/issues/928), support for list of boolean/binary values was added to control ascending/descending when sorting on multiple columns.\n\nThis doesn't work with `sortlevel`, but nor does it raise an error. This seems potentially \"dangerous\" to me.\n\n```\nIn [23]: levels = list('ABC')\n\nIn [24]: length = 10\n\nIn [25]: df = pd.DataFrame(dict(alpha=np.random.choice(levels, length),\n   ....:     num=np.random.random(length), vals=np.random.random(length)))\n\nIn [26]: df = df.set_index(['alpha', 'num'])\n\nIn [27]: df\nOut[27]: \n                    vals\nalpha num               \nC     0.101632  0.566144\n      0.334399  0.242917\nA     0.741676  0.807468\nC     0.406477  0.756441\nA     0.225341  0.214034\nC     0.623214  0.696784\nA     0.576516  0.181786\n      0.509289  0.144184\nB     0.510504  0.155965\nA     0.093283  0.356627\n\nIn [28]: # Sort reverse alphabetically. But then `num` is backwards!\n\nIn [29]: df.sortlevel(['alpha', 'num'], ascending=False)\nOut[29]: \n                    vals\nalpha num               \nC     0.623214  0.696784\n      0.406477  0.756441\n      0.334399  0.242917\n      0.101632  0.566144\nB     0.510504  0.155965\nA     0.741676  0.807468\n      0.576516  0.181786\n      0.509289  0.144184\n      0.225341  0.214034\n      0.093283  0.356627\n\nIn [30]: # Attempt to sort reverse alphabetically with `num` sorting forwards:\n\nIn [31]: df.sortlevel(['alpha', 'num'], ascending=[False, True])\nOut[31]: \n                    vals\nalpha num               \nA     0.093283  0.356627\n      0.225341  0.214034\n      0.509289  0.144184\n      0.576516  0.181786\n      0.741676  0.807468\nB     0.510504  0.155965\nC     0.101632  0.566144\n      0.334399  0.242917\n      0.406477  0.756441\n      0.623214  0.696784\n```\n\nAs you can see, this last `sortlevel` does not behave as expected. I'd expect to see `alpha` sorted descending (Z - A) and `num` sorted ascending (0 - 1).\n"},{"labels":["api",null,null,null],"text":"Right now using between_time on an empty series will raise an TypeError: Index must be DatetimeIndex. I would think that it would make more sense to return an empty list (or at least return a more helpful Error).\n\n``` python\nimport datetime as dt\nimport pandas as pd\ns = pd.Series([])\ns.between_time(dt.time(0, 0), dt.time(23, 0))\n```\n\nI would assume that expanding the Error Handler of the method should do the trick:\n\n``` python\n[...]\nexcept AttributeError:\n    if self.empty:\n        return self\n    else:\n         raise TypeError('Index must be DatetimeIndex')\n```\n\nThe Error persists on this configuration:\nINSTALLED VERSIONS\ncommit: None\npython: 2.7.6.final.0\npython-bits: 32\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.16.2\nnose: 1.3.3\nCython: 0.20.1\nnumpy: 1.8.1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.1.0\nsphinx: 1.2.2\npatsy: 0.2.1\ndateutil: 2.2\npytz: 2014.3\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.4.0\nopenpyxl: 2.0.3\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: None\nlxml: 3.3.5\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\n"},{"labels":["api",null,null],"text":"Background: I am trying to build a horizonal bar graph that shows me when various builds on a server started and ended, so that I can see visually which builds overlapped in time. But the idea that a horizontal bar graph's bar-width would be a timedelta seems completely foreign to all of the plotting libraries that I have tried today (matplotlib, bokeh, ggplot), so I want to punt and instead reduce my timedelta64's to a floating-point number of days. I can then fudge the axes to display dates anyway.\n\nProblem: there is no easy way to move from a timedelta64 to the standard Python idea of an ordinal date. I would have to extract all of the components manually (days, seconds, fractions of a second) and then multiply them each by an appropriate factor and then add them back together.\n\nThe Python `timedelta` class has an alternative that I like very much: a `total_seconds()` method that expresses then entire value of the `timedelta` as a single scalar number of seconds.\n\nThis is not a perfect solution, since the factors it uses internally neglect the fact that a day might have included a leap second, but it works pretty well for non-astronomical use.\n\nCould a `Series.dt.total_seconds()` method be added that returns the floating point value `days * 86400 + seconds + microseconds / 1e6`? It would be a great convenience for moving from date-land to scalar-land before plotting. Thanks!\n"},{"labels":["api",null],"text":"Say you have a multi-index:\n\n```\nIn [34]: idx = pd.MultiIndex.from_product([['a', 'b', 'c'], [1, 2, 3], ['f', 'g'\n]], names=['lev0', 'lev1', 'lev2'])\n\nIn [35]: df = pd.DataFrame(range(len(idx)), index=idx)\n\nIn [36]: df\nOut[36]:\n                 0\nlev0 lev1 lev2\na    1    f      0\n          g      1\n     2    f      2\n          g      3\n     3    f      4\n          g      5\nb    1    f      6\n          g      7\n     2    f      8\n          g      9\n     3    f     10\n          g     11\nc    1    f     12\n          g     13\n     2    f     14\n          g     15\n     3    f     16\n          g     17\n```\n\nand you want to select certain levels of the Index (like you select columns of a frame, I want to select levels of an index and get a subset of the index).\n\nAt the moment, some possibilities:\n\n```\nIn [37]: pd.MultiIndex.from_arrays([df.index.get_level_values(0), df.index.get_level_values(1)])\nOut[37]:\nMultiIndex(levels=[[u'a', u'b', u'c'], [1, 2, 3]],\n           labels=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], [0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2]],\n           names=[u'lev0', u'lev1'])\n\nIn [38]: idx.droplevel(-1)    # if you know the ones to drop\nOut[38]:\nMultiIndex(levels=[[u'a', u'b', u'c'], [1, 2, 3]],\n           labels=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], [0, 0\n, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2]],\n           names=[u'lev0', u'lev1'])\n\nIn [39]: df.reset_index().set_index(['lev0','lev1']).index\nOut[39]:\nMultiIndex(levels=[[u'a', u'b', u'c'], [1, 2, 3]],\n           labels=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], [0, 0\n, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2]],\n           names=[u'lev0', u'lev1'])\n\nIn [40]: df.reset_index(-1).index       # if you know the ones to drop\nOut[40]:\nMultiIndex(levels=[[u'a', u'b', u'c'], [1, 2, 3]],\n           labels=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], [0, 0\n, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2]],\n           names=[u'lev0', u'lev1'])\n```\n\nAm I missing an easy way to do this?\n\nAnd if not, I think we should have a better way to do this.\n\nNote: triggerd by this SO question: http://stackoverflow.com/questions/31991388/combinations-of-multiindex-levels-which-occur-in-a-dataframe (but had already encountered this multiple times)\n"},{"labels":["api",null],"text":"something along the lines: \n\n```\ndf.reindex(df.index.tolist()+new_values).sort().interpolate().ix[new_values]\n```\n"},{"labels":["api",null,null,null],"text":"Is there a reason that `notnull()` and `isnull()` consider an empty string to not be a missing value? \n\n```\npd.isnull('')\nFalse\n```\n\nSeems like in string data, people usually think of the empty string as \"missing\". \n"},{"labels":["api",null,null,null],"text":"`PeriodIndex`s should resolve when `datetime.date` is passed into `in`, but doesn't. Example below.\n\nTracing this through, it looks like `parse_time_string` is expected to return a tuple in `get_loc` (which is called by `__contains__`): https://github.com/pydata/pandas/blob/master/pandas/tseries/period.py#L610\n\nBut if passed something that isn't a string, just returns it back as a single item: https://github.com/pydata/pandas/blob/master/pandas/tseries/tools.py#L447\n\n...so in this instance the lookup raises an exception and mistakenly propagates up that the value can't be found.\n\nWhat's the best way forward here? Should the return type always be the same? Or do we add a check in `get_loc` to stop it calling `parse_time_string`? Or broaden the `except` here to be any exception: https://github.com/pydata/pandas/blob/master/pandas/tseries/period.py#L612\n\nMy inclination is the first, but whether that will have some blast radius / what the standards are? The third seems like a reasonable cut through too.\n\n``` python\nIn [9]:\n\nperiod_index\nOut[9]:\nPeriodIndex(['2015-01-01', '2015-01-02', '2015-01-05', '2015-01-06',\n             '2015-01-07', '2015-01-08', '2015-01-09', '2015-01-12',\n             '2015-01-13', '2015-01-14', '2015-01-15', '2015-01-16',\n             '2015-01-19', '2015-01-20', '2015-01-21', '2015-01-22',\n             '2015-01-23', '2015-01-26', '2015-01-27', '2015-01-28',\n             '2015-01-29', '2015-01-30'],\n            dtype='int64', freq='B')\nIn [10]:\n\n'2015-01-06' in period_index\nOut[10]:\nTrue\nIn [15]:\n\ndt = datetime.datetime(2015,1,6) \ndt\nOut[15]:\ndatetime.datetime(2015, 1, 6, 0, 0)\nIn [16]:\n\ndt in period_index\nOut[16]:\nFalse\nIn [17]:\n\nd = datetime.date(2015,1,6) \nd\nOut[17]:\ndatetime.date(2015, 1, 6)\nIn [18]:\n\nd in period_index\nOut[18]:\nFalse\n```\n"},{"labels":["api",null,null],"text":"Running this code generates an error, since columns are now considered an Index and not a list. I'm not sure if the error that gets raised (set_index ends up asking for an index that is the entire length of the data frame) is actually by design. In this case, it was a bit unintuitive to figure out how to catch, since Index objects and python lists often work so similarly.\n\n```\nimport string\nimport pandas as pd\n\ndata1 = 'x'*5 + 'y'*5\ndata2 = string.lowercase[:5]*2\ndata3 = range(10)\ndata_dict = {'Cat': list(data1), 'SubCat': list(data2), 'Vals':data3}\ndf = pd.DataFrame(data_dict)\nordered_df = df[['Cat', 'SubCat', 'Vals']]\n\ncorrect_df = ordered_df.reset_index([x for x in ordered_df.columns[:2]])\nerror_df = ordered_df.set_index(ordered_df.columns[:2])\n```\n"},{"labels":["api",null],"text":"`nlargest/nsmallest` on frame should have `take_last` -> `keep`, in-line with #10236 \n\n[here](https://github.com/pydata/pandas/blob/master/pandas/core/frame.py#3187)\n\nno deprecation is necessary as this is new in 0.17.0\n\ncc @sinhrks \ncc @cpcloud \n"},{"labels":["api",null],"text":"This may have been intended, but this change (https://github.com/pydata/pandas/commit/3d54482bbd8086c27a01d489a0ae751e0b9c3731) released in 0.16.2 broke some of my code because `Index.to_native_types()` started converting all index types to strings, even integers. By reading the source I found I can prevent that by passing `quoting=True` to `to_native_types`, but since there's no documentation for `to_native_types` I wanted to get a little clarification: What is the expected behavior of `to_native_types` and what does the `quoting` keyword mean? Thanks!\n"},{"labels":["api",null],"text":"Currently the exponential functions take one of three arguments to specify the length of backhistory. Each of these numerically converts to alpha. Outlined here: http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-moment-functions\n\nIs there a reason we don't allow users to just pass in `alpha`? This is how I think about weighing some of the time.\n\nI'm happy to do a PR in time; this issue is to solicit feedback.\n"},{"labels":["api",null,null],"text":"Derived from #10507. Consider better API for testing functions.\n### 1. How to specify comparison tolerance\n\nCurrent assert functions have `check_less_precise` and `check_exact` flags to specify how to compare values:\n- `check_exact=True`: Use `assert_equal`, comparison using `==`.\n- `check_exact=False` and `check_less_precise=False`: Use `assert_almost_equal`, comparing 5 digits after decimal points (default)\n- `check_exact=False` and `check_less_precise=True`: Use `assert_almost_equal`, comparing 3 digits after decimal points\n\nThere can be single kw to specify above behaviors?\n### 2. Make integrated assert  function\n\nI think it's nice to have test functions which supports all `pandas` objects for users who is starting contribution / using pandas as their dependencies. Changing `assertEquals` and `assertAlmostEquals` to internally use current `assert_index/series/frame_equal` is one idea.\n### 3. Remove `assert_almost_equal`\n\nRemove `assert_almost_equal` and use class-based validation method. Also add input type validation to `numpy_assert_array_equal`.\n\nxref #9895.\n"},{"labels":["api",null],"text":"If one uses `series.value_counts(dropna=False)` it includes `NaN` values, but `crosstab(series1, series2, dropna=False)` does not include `NaN` values. I think of `cross_tab` kind of like 2-dimensional `value_counts()`, so this is surprising.\n\n``` python\nIn [31]: x = pd.Series(['a', 'b', 'a', None, 'a'])\n\nIn [32]: y = pd.Series(['c', 'd', 'd', 'c', None])\n\nIn [33]: y.value_counts(dropna=False)\nOut[33]: \nd      2\nc      2\nNaN    1\ndtype: int64\n\nIn [34]: pd.crosstab(x, y, dropna=False)\nOut[34]: \n       c  d\nrow_0      \na      1  1\nb      0  1\n```\n\nI believe what `crosstab(..., dropna=False)` really does is just include rows/columns that would otherwise have all zeros, but that doesn't seem the same as `dropna=False` to me. I would have expected to see a row and column entry for `NaN`, something like:\n\n``` python\n       c  d  NaN\nrow_0      \na      1  1    1\nb      0  1    0\nNaN    1  0    0\n```\n\nThoughts on this?\n\nThis is on current master (almost 0.17).\n"},{"labels":["api",null,null],"text":"from [SO](http://stackoverflow.com/questions/21599633/how-to-create-a-pandas-timestamp-object)\n\n`Timestamp(year, month, day, second, millisecond, microsecond, nanosecond)`\n\nas optional keyword args might be nice for compat with `datetime.datetime`. We do something similar for `Timedelta` construction.\n"},{"labels":["api",null,null,null],"text":"This came out of work on https://github.com/pydata/pandas/pull/10729\n\nIn the [documentation](http://pandas.pydata.org/pandas-docs/version/0.16.2/categorical.html#missing-data), we mention that\n\n> There are two ways a np.nan can be represented in categorical data: either the value is not available (“missing value”) or np.nan is a valid category.\n\nIn the first case, `NaN` is not in `.categories`, and in the second case it is. I think we should only\nrecommend the first.\n\nThe option of `NaN`s in the categories makes the code in #10729 less pleasant that it would be otherwise. I don't think we should error if NaNs are included, just advise against it in the docs. Perhaps a deprecation, but I worry that I'm missing some obvious reason why NaNs were allowed in `.categories`.\n\n@JanSchulz  do you remember the initial reason for allowing either representation?\n\nSome bad things that come out of `NaN` in `.categories`:\n- Can't rely on a value of `nan` mapping to a code of `-1`:\n\n``` python\nIn [2]: s = pd.Categorical(['a', 'b', 'a', np.nan], categories=['a', 'b', np.nan])\n\nIn [3]: s\nOut[3]:\n[a, b, a, NaN]\nCategories (3, object): [a, b, NaN]\n\nIn [4]: s.categories\nOut[4]: Index(['a', 'b', nan], dtype='object')\n\nIn [5]: s.codes\nOut[5]: array([0, 1, 0, 2], dtype=int8)\n```\n- potentially have to upcast the index type or mix strings and floats (`nan`) in the `.categories` Index.\n- extra code if you want to generically handle Categoricals that may or may not have `NaN` in categories.\n"},{"labels":["api",null,null],"text":"When creating a DataFrame with a millisecond timestamp, created with `dtype='datetime64[ms]'`, this works as expected when there are no missing values in the data:\n\n```\nimport pandas as pd\ndf = pd.DataFrame([1036713600000], dtype='float64')\nprint(df[0].astype('datetime64[ms]'))\n```\n\nOutput:\n\n```\n0   2002-11-08\nName: 0, dtype: datetime64[ns]\n```\n\nAdding a missing value to the data causes the values to get parsed as nanoseconds rather than microseconds, which causes an exception:\n\n```\ndf = pd.DataFrame([1036713600000, None], dtype='float64')\nprint(df[0].astype('datetime64[ms]'))\n```\n\nOutput:\n\n```\nTraceback (most recent call last):                                          \n  File \"./f.py\", line 6, in <module>                                        \n    print(df[0].astype('datetime64[ms]'))                                   \n  File \"/Users/dsc/.virtualenvs/p3default/lib/python3.4/site-packages/pandas/core/generic.py\", line 2411, in astype\n    dtype=dtype, copy=copy, raise_on_error=raise_on_error, **kwargs)        \n  File \"/Users/dsc/.virtualenvs/p3default/lib/python3.4/site-packages/pandas/core/internals.py\", line 2504, in astype\n    return self.apply('astype', dtype=dtype, **kwargs)                      \n  File \"/Users/dsc/.virtualenvs/p3default/lib/python3.4/site-packages/pandas/core/internals.py\", line 2459, in apply\n    applied = getattr(b, f)(**kwargs)                                       \n  File \"/Users/dsc/.virtualenvs/p3default/lib/python3.4/site-packages/pandas/core/internals.py\", line 373, in astype\n    values=values, **kwargs)                                                \n  File \"/Users/dsc/.virtualenvs/p3default/lib/python3.4/site-packages/pandas/core/internals.py\", line 407, in _astype\n    fastpath=True, dtype=dtype, klass=klass)                                \n  File \"/Users/dsc/.virtualenvs/p3default/lib/python3.4/site-packages/pandas/core/internals.py\", line 2101, in make_block\n    placement=placement)                                                    \n  File \"/Users/dsc/.virtualenvs/p3default/lib/python3.4/site-packages/pandas/core/internals.py\", line 1795, in __init__\n    values = tslib.cast_to_nanoseconds(values)                              \n  File \"pandas/tslib.pyx\", line 2622, in pandas.tslib.cast_to_nanoseconds (pandas/tslib.c:43295)\n  File \"pandas/tslib.pyx\", line 1333, in pandas.tslib._check_dts_bounds (pandas/tslib.c:23332)\npandas.tslib.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 292278994-08-16 16:47:04\n```\n\nExpected output was something like:\n\n```\n0   2002-11-08\n1   NaT\nName: 0, dtype: datetime64[ns]\n```\n\nThis was using python 3.4.2, Pandas 0.16.2.\n"},{"labels":["api",null],"text":"I've realized the `.sample()` function is sometimes returning a view rather than a copy. This will cause the \"setting on a view/copy\" error:\n\n```\ndf = pd.DataFrame({'col1':[5,6,7], 'col2':['a','b','c']})\na_sample = df.sample(1)\na_sample['col1'] = 'z'\n```\n\nI believe the behavior is the result of it using the `.take()` function, which is what the `.sample()` command uses to pull the sample the passed frame. \n\nI'd like to suggest either:\n- replace `.take()` with `.iloc()`\n- change `.take()` to `.take().copy()` if that's faster (@shoyer suggested `.take()` would be a faster -- is `.take().copy()`?)\n\nThoughts?\n"},{"labels":["api",null],"text":"On a DataFrame, we have the [`combineAdd`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.combineAdd.html) and [`combineMult`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.combineMult.html) methods:\n- they are a redundant, as they are defined as `self.add(other, fill_value=0.)` and `self.mul(other, fill_value=1.)` (that is the actual implementation), which is also just cleaner to use IMHO.\n- they have awkward (and non-pep8) names\n- I assume they are not used a lot (I found 3 relevant references on StackOverflow for combineAdd ([SO1](http://stackoverflow.com/questions/28421137/subtract-two-dataframes-with-non-overlapping-indexes/28421255#28421255), [SO2](http://stackoverflow.com/questions/27744908/computing-sum-of-pandas-dataframes/27746836#27746836) and [SO2](http://stackoverflow.com/questions/20927701/pandas-numpy-how-to-turn-column-data-into-sparse-matrix/20929504#20929504), which is more than I expected), and none for combineMult\n\nTherefore, I was thinking we could maybe deprecate these (as kind of clean-up)?\n"},{"labels":["api",null,null,null],"text":"Currently `DataFrame.update()` modifies the dataframe in place. I feel this is not symmetric to how most other methods work, e.g. `set_index()`, that returns a new `DataFrame`. It is probably too late to change this default behavior (isi it?), but if at the very least an `inplace` option was added (with default value `True` to support the current behaviour), it will be easier to chain a sequence of operators that includes `update()`.\n"},{"labels":["api",null],"text":"Thoughts on this?\n\n``` python\nIn [8]: import pandas.util.testing as tm\n\nIn [9]: s = pd.Series(tm.makeCategoricalIndex(k=100))\n\nIn [10]: s.value_counts()\nOut[10]:\nvcKH    40\nvXR9    31\nZn8J    29\ndtype: int64\n\nIn [11]: s.value_counts().index\nOut[11]: Index(['vcKH', 'vXR9', 'Zn8J'], dtype='object')\n```\n\nNow that we have `CategoricalIndex` (thanks Jeff), should that type be preserved so that `Out[11]` is a `CategoricalIndex`? My use-case (not shown in this example) is when the original categories are ordered, you get your `value_counts` and then want to sort the index.\n"},{"labels":["api",null,null],"text":"I just upgraded to latest Pandas 0.16, and the new error has hit me (with Pandas 0.15) when slicing with multiple values. (df.ix[[list_of_values]]). I actually think it is more valid to return an empty DataFrame, than to throw an error.\n\nThe best I've been able to come up with to reproduce the previous behaviour (fail silently, return empty DataFrame) is:\n\n``` python\n# I want to filter a DataFrame by the first level of the index\ndf.ix[df.reset_index().IndexValue.isin(list_of_values).values]\n```\n\nNot saying I'm right on the error/empty argument; but is the above the most elegant solution?\n\nPerhaps we should consider three distinct slicing operations:\n1. Insists on everything being there (and throws errors)\n2. Creates spaces where there is nothing there (reindex)\n3. Just returns what's present, leaving the rest out (silent error)\n\nI would think anyone indexing would be vary aware of what they are expecting from the above?\n"},{"labels":["api",null],"text":"```\nIn[10]: a = pandas.Series(pandas.Categorical(list(\"abc\")))\nIn[11]: a.cat.labels = [1,2]\nIn[12]: a.cat.labels\nTraceback (most recent call last):\n  File \"C:\\portabel\\miniconda\\envs\\pandas_dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2883, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-a68ee763e4e8>\", line 1, in <module>\n    a.cat.labels\nAttributeError: 'CategoricalAccessor' object has no attribute 'labels'\n```\n"},{"labels":["api",null,null],"text":"It seems strange to me that `pandas.read_csv` is not a direct reciprocal function to `df.to_csv`.  In this illustration, notice how when using all the default settings the original and final DataFrames differ by the \"Unnamed\" column.\n\n```\nIn [1]: import pandas as pd\n\nIn [2]: orig_df = pd.DataFrame({'AAA' : [4,5,6,7], 'BBB' : [10,20,30,40],'CCC' : [100,50,-30,-50]}); orig_df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\n[4 rows x 3 columns]\n\nIn [3]: orig_df.to_csv('test.csv')\n\nIn [4]: final_df = pd.read_csv('test.csv'); final_df\nOut[4]: \n   Unnamed: 0  AAA  BBB  CCC\n0           0    4   10  100\n1           1    5   20   50\n2           2    6   30  -30\n3           3    7   40  -50\n\n[4 rows x 4 columns]\n```\n\nIt seems the default `read_csv` should instead be \n\n```\nIn [6]: final2_df = pd.read_csv('test.csv', index_col=0); final2_df\nOut[7]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\n[4 rows x 3 columns]\n```\n\nor the default `to_csv` should instead be\n\n```\nIn [8]: df.to_csv('test2.csv', index=False)\n```\n\nwhich when read gives\n\n```\nIn [9]: pd.read_csv('test2.csv')\nOut[9]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n```\n\n[4 rows x 3 columns]\n\nThis inconsistency make using these functions unintuitive.  I do not really care which way it goes but I think the default behavior should be consistent with options for differences rather than different with options for consistency.\n"},{"labels":["api",null,null],"text":"Accessors should be enabled depending on `categories`. Should care `CategoricalIndex` also.\n\n```\nimport pandas as pd\ns = pd.Series(['A', 'B'], dtype='category')\ns.str\n# AttributeError: Can only use .str accessor with string values, which use np.object_ dtype in pandas\n\ns = pd.Series([pd.Timestamp('2011-01-01'), pd.Timestamp('2011-01-01')], dtype='category')\ns.dt\n# AttributeError: Can only use .dt accessor with datetimelike values\n```\n"},{"labels":["api",null],"text":"From #10636 \n\nWhen parsing an invalid date, there are some possible different actions to take:\n- ignore -> just returning the original value unparsed (`errors='ignore'`)\n- raise an error (`errors='raise'`)\n- coerce / convert to NaN (`coerce=True`)\n\nAFAIK, these three options are just three different options and mutually exclusive (or is there somewhere in the codebase an interaction between both?), so they could be controlled by a single keyword. \nHaving two keyword can make it even confusing what you should expect when combining those two, eg the example of coerce=True and error=raise.\n\nProposal: add another `'coerce'` option to the `errors` keyword that is equivalent to `coerce=True`\n\n-> this gives the `errors` keyword three options: `'ignore'|'raise'|'coerce'`\n"},{"labels":["api",null,null],"text":"Pandas is not able to export a matrix as csv file and reimport it and keeping the data consistent. Using default arguments for `to_csv`, it will add an additional column for the indeces. Importing with default parameters will treat this column as data, not as index....\n\n```\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10,10))\ndf.to_csv('np.csv')\nprint pd.read_csv('np.csv')\n\n\n   Unnamed: 0         0         1         2         3         4         5  \\\n0           0  0.055663  0.492976  0.936424  0.931585  0.043748  0.931660   \n1           1  0.946510  0.481707  0.935273  0.987895  0.982537  0.735273   \n2           2  0.429818  0.090192  0.923747  0.973678  0.432166  0.318196   \n3           3  0.579657  0.599554  0.794318  0.631867  0.700044  0.834421   \n4           4  0.438074  0.747774  0.034653  0.113885  0.982059  0.736432   \n5           5  0.379523  0.094214  0.435573  0.729742  0.778312  0.341792   \n6           6  0.542644  0.175657  0.913459  0.532352  0.607791  0.369434   \n7           7  0.132935  0.052179  0.145688  0.549158  0.127237  0.475737   \n8           8  0.454960  0.872086  0.006616  0.444334  0.435469  0.435362   \n9           9  0.141345  0.512531  0.900547  0.570482  0.366632  0.992289   \n\n          6         7         8         9  \n0  0.385482  0.432543  0.927187  0.408233  \n1  0.385019  0.905481  0.852093  0.368507  \n2  0.641478  0.966683  0.706884  0.229032  \n3  0.592390  0.091528  0.969585  0.177480  \n4  0.805170  0.585675  0.024259  0.961815  \n5  0.818240  0.688166  0.175099  0.583955  \n6  0.697869  0.202709  0.458018  0.546078  \n7  0.597875  0.625422  0.055143  0.720858  \n8  0.866318  0.348642  0.855215  0.689258  \n9  0.723096  0.194654  0.681293  0.941478  \n```\n"},{"labels":["api",null],"text":"xref #8894 \nxref #10615 \nxref #10154\n\nThe idea is that if you have completely unparseable dates with or w/o a format specified, then the default will be to raise. Further, since `coerce` is by default `False`. You will get an immediate exception rather than have strings returned.\n\nI think some tests will need to be adjusted and a simple example for the user in the whatsnew.\n"},{"labels":["api",null,null],"text":"Using `combine_first` on an empty frame with a single-column index fails if the other frame has a multi-index and is not empty.\n\nThis used to work for pandas version 0.14.1\n\n``` python\nimport pandas as pd\n\nmidx = pd.MultiIndex.from_arrays([[-1], [1]])\n\na = pd.DataFrame({'a': [1]}, index=midx)\nprint a\n\nb = pd.DataFrame({'a': []})\nprint b\n\nprint b.combine_first(a)\n```\n\n```\n$ python x.py \n      a\n-1 1  1\nEmpty DataFrame\nColumns: [a]\nIndex: []\nTraceback (most recent call last):\n  File \"x.py\", line 11, in <module>\n    print b.combine_first(a)\n  File \"/users/is/ahlpypi/egg_cache/p/pandas-0.16.2_ahl1-py2.7-linux-x86_64.egg/pandas/core/frame.py\", line 3388, in combine_first\n    return self.combine(other, combiner, overwrite=False)\n  File \"/users/is/ahlpypi/egg_cache/p/pandas-0.16.2_ahl1-py2.7-linux-x86_64.egg/pandas/core/frame.py\", line 3283, in combine\n    this, other = self.align(other, copy=False)\n  File \"/users/is/ahlpypi/egg_cache/p/pandas-0.16.2_ahl1-py2.7-linux-x86_64.egg/pandas/core/generic.py\", line 3374, in align\n    fill_axis=fill_axis)\n  File \"/users/is/ahlpypi/egg_cache/p/pandas-0.16.2_ahl1-py2.7-linux-x86_64.egg/pandas/core/generic.py\", line 3395, in _align_frame\n    return_indexers=True)\n  File \"/users/is/ahlpypi/egg_cache/p/pandas-0.16.2_ahl1-py2.7-linux-x86_64.egg/pandas/core/index.py\", line 2005, in join\n    return self._join_multi(other, how=how, return_indexers=return_indexers)\n  File \"/users/is/ahlpypi/egg_cache/p/pandas-0.16.2_ahl1-py2.7-linux-x86_64.egg/pandas/core/index.py\", line 2098, in _join_multi\n    raise ValueError(\"cannot join with no level specified and no overlapping names\")\nValueError: cannot join with no level specified and no overlapping names\n```\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.3.final.0\npython-bits: 64\nOS: Linux\nOS-release: 2.6.18-400.1.1.el5\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_GB\n\npandas: 0.16.2\nnose: 1.3.0\nCython: 0.22\nnumpy: 1.9.2\nscipy: 0.9.0\nstatsmodels: None\nIPython: 3.2.0-1\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.2\nbottleneck: None\ntables: 3.2.0\nnumexpr: 2.4.3\nmatplotlib: 1.3.1\nopenpyxl: None\nxlrd: None\nxlwt: 0.7.4\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: $Rev$\napiclient: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\n"},{"labels":["api",null,null],"text":"The behavior of these methods differs in confusing ways:\n\nDF:\n- [`DataFrame.sort_index`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort_index.html) takes a `by` parameter, in which case the index is resorted by a specific column's values\n- [`DataFrame.sort`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort.html) defaults to `inplace=False`\n- `DataFrame.order` does not exist\n\nSeries:\n- [`Series.sort`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort.html) defaults to `inplace=True`\n- [`Series.order`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.order.html) defaults to `inplace=False`, but is otherwise identical to `Series.sort`\n- [`Series.sort_index`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort_index.html) does not take a `by` parameter\n\nThis makes it very hard to remember which one to use, especially after an aggregating function. For example:\n\n```\ndf.max(axis=1).order(ascending=False)      # max returns a Series in this case, use Series methods\ndf.max(level='Foo').sort_index(by='Bar')      # max returns a DF in this case, use DF methods\n```\n\nAny reason not to make the `inplace` behavior identical for `DataFrame.sort` and `Series.sort`? Also, `Series.sort_index` could take `by=0` to replicate the behavior of `DataFrame`.\n"},{"labels":["api",null],"text":"First, here's the example DataFrame\n\n``` python\n                    0  1  2  3  4  5  6  7\nfirst second third                        \nbar   one    three  4  9  7  8  5  0  7  8\n             four   6  8  1  5  9  9  1  7\n      two    three  7  5  2  6  7  8  5  9\n             four   0  8  8  5  3  5  8  3\nbaz   one    three  6  0  8  0  0  9  8  8\n             four   9  3  2  0  2  7  4  9\n      two    three  0  3  7  7  4  3  7  0\n             four   6  3  2  8  3  9  7  8\nfoo   one    three  6  7  3  7  3  0  3  6\n             four   5  8  0  8  1  5  1  5\n      two    three  2  0  8  2  8  1  8  3\n             four   9  0  2  7  0  6  8  3\nqux   one    three  1  2  5  5  0  7  0  1\n             four   6  6  7  0  0  4  5  3\n      two    three  1  8  2  8  7  5  7  5\n             four   1  1  3  8  8  6  0  3\n```\n\nFor `DataFrame`s with `MultiIndex`ed rows, pandas allows this type of indexing\n\n`df.loc[('foo','bar'), ('one','two'), ('three','four')]`\n\nTo be taken to mean\n\n`df.loc[(('foo','bar'), ('one','two'), ('three','four')), :]`\n\nBut this type of indexing is ambiguous in the case when the number of indexing tuples is 2 since\n\n`df.loc[('foo','bar'), ('one','two')]`\n\ncould mean incomplete indexing as in\n\n`df.loc[(('foo','bar'), ('one','two')),:]`\n\nor row,column indexing as in\n\n`df.loc[(('foo','bar'),), (('one','two'),)]`\n\nI appreciate that there is already a warning for this in the [documentation](http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers), but I wonder if the functionality is worth the complications it adds to the code/docs.\n\nPersonally, I would suggest offloading the responsibility of complete indexing on a MultiIndex DataFrame to the user (obviously this doesn't apply to `Series` as they are 1d so to speak). This would take away the _minor_ syntactical convenience of not specifying the column index, but it simplifies the code and gives the user only one way to index on a MultiIndex DataFrame (which makes usage less confusing).\n\nThe consequence to the user in the specific case of selecting multiple levels of a row-MultiIndex on a DataFrame is that instead of writing\n\n`df.loc['foo','one']`\n\nthey would have to write\n\n`df.loc[('foo','one'), :]`\n\nAnd, in the syntactically worst case, instead of writing\n\n`df.loc[('foo','bar'), ('one','two'), ('three','four')]`\n\nthey would have to write\n\n`df.loc[(('foo','bar'), ('one','two'), ('three','four')), :]`\n\nI'm fairly new to pandas (don't think I started using it until v0.16), so I realize I may be missing the bigger picture. If so, enlighten me!\n"},{"labels":["api",null],"text":"pd.read_excel() takes \"sheetname\" as an argument.\npd.DataFrame.to_excel() takes \"sheet_name\" as an argument.\n\nFor consistency, I think one ought to be changed to match the other.\nOn my windows installation of pandas 0.16.2, I modified to_excel to take sheetname rather than sheet_name.  All I had to do was locate frame.py within the core directory.  to_excel() is defined here, and sheet_name is written twice in the function and once in the docstring.  \n\nVery new to github so if ideas like these are more appropriately filed through PRs I apologize.\n"},{"labels":["api",null,null,null],"text":"In my discussion with Jonathan and others and at the SciPy sprints, we agreed that it would be really nice to expose some minimal tools for manipulating and view the internal pandas blocks system. For example, it should be possible to:\n1. manually consolidate blocks\n2. view a representation of the internal blocking of a dataframe (via matplotlib?)\n\nIt's not so much that we want to create and use blocks directly, but that we want to make it easier to understand the internal data model and make performance with more predictable.\n\nAt the same time, we would like to disable automatic consolidation of blocks in the DataFrame constructor and when inserting new columns. Consolidation is certainly a useful feature, but it is currently not always possible to even predict when it will happen.\n\nMost users never notice or care about consolidation. Power users (concerned about memory or performance) are at least as likely to find it frustrating as helpful, so we should make this something that they can trigger explicitly (as part of the blocks API). This would make it possible to create dataframes while guaranteeing that none of the data is copied (#9216).\n\ncc @jonathanrocher @sinhrks @jreback @cpcloud @TomAugspurger @ARF1 @quicknir\n"},{"labels":["api",null,null],"text":"All of the following should be equivalent:\n\n```\nIn [21]: df = pd.DataFrame({'a': [1], 'b': [2], 'c': [3]}).set_index(['a', 'b'])\n\nIn [22]: df.loc[1]\nOut[22]:\n   c\nb\n2  3\n\nIn [23]: df.loc[(1,), slice(None)]\nOut[23]:\n   c\nb\n2  3\n\nIn [24]: df.loc[(1, slice(None)), slice(None)]\nOut[24]:\n     c\na b\n1 2  3\n```\n\nIn the last output, there should no longer be a level `a`.\n"},{"labels":["api",null,null],"text":"Like R's `stringsAsFactors`, it is nice to create `category` when read data. Currently, it raises `TypeError`.\n\n```\nimport pandas as pd\nimport StringIO\npd.read_csv(StringIO.StringIO(\"\"\"A, B\na, c\nb, d\"\"\"), dtype={'A': 'category'})\n# TypeError: data type \"category\" not understood\n```\n"},{"labels":["api",null,null,null],"text":"So here's my setup (using pandas 0.16.2):\n\n``` python\n>>> midx = pd.MultiIndex.from_product([['bar', 'baz', 'foo', 'qux'], ['one', 'two']],names=['first','second'])\n>>> df = pd.DataFrame(np.random.randint(10,size=(8,8)),index=midx)\n\n>>> df \n              0  1  2  3  4  5  6  7\nfirst second                        \nbar   one     0  5  5  5  6  2  6  8\n      two     2  6  9  0  3  6  7  9\nbaz   one     9  0  9  9  2  5  7  4\n      two     4  8  1  2  9  2  8  1\nfoo   one     2  7  3  6  5  5  5  2\n      two     3  4  6  2  7  7  1  2\nqux   one     0  8  5  9  5  5  7  3\n      two     7  4  0  7  3  6  8  6\n```\n\nI recently found that I can select multiple levels by indexing with a tuple of tuples\n\n```\n>>> df.loc[( ('bar','baz'),  ), :]\n              0  1  2  3  4  5  6  7\nfirst second                        \nbar   one     0  5  5  5  6  2  6  8\n      two     2  6  9  0  3  6  7  9\nbaz   one     9  0  9  9  2  5  7  4\n      two     4  8  1  2  9  2  8  1\n```\n\nOr even select at multiple depths of levels\n\n```\n>>> df.loc[( ('bar','baz'), ('one',) ), :]\n              0  1  2  3  4  5  6  7\nfirst second                        \nbar   one     0  5  5  5  6  2  6  8\nbaz   one     9  0  9  9  2  5  7  4\n```\n\nThe issue is this: if I add any levels to the index tuple that don't exist in the dataframe, pandas drops them silently\n\n```\n>>> df.loc[( ('bar','baz','xyz'), ('one',) ), :]\n              0  1  2  3  4  5  6  7\nfirst second                        \nbar   one     0  5  5  5  6  2  6  8\nbaz   one     9  0  9  9  2  5  7  4\n```\n\nIt seems to me like this should raise an exception since \n1. The shape of the dataframe that is returned in this instance is not what you'd expect\n2. There's no way to unambiguously fill the returned dataframe with NaNs where a level didn't exist (as is done in the case where there is only a single level index)\n"},{"labels":["api",null],"text":"When working with a subclass of `Series` or a `DataFrame` the type is not maintained.\n\n``` py\nIn [1]: import pandas as pd\n\nIn [2]: class Subclassed(pd.Series):\n   ...:     pass\n   ...: \n\nIn [3]: type(Subclassed(range(10)))\nOut[3]: __main__.Subclassed\n\nIn [4]: type(Subclassed(range(10)) + Subclassed(range(10)))\nOut[4]: pandas.core.series.Series\n\nIn [5]: pd.__version__\nOut[5]: '0.16.2'\n```\n"},{"labels":["api",null],"text":"DataFrame.drop_duplicates can be useful to 'sparsify' a frame, requiring less \nmemory/storage. However, it doesn't handle the case where a value later reverts\nto an earlier value. For example:\n\n```\nIn [2]: df = pd.DataFrame(index=pd.date_range('20020101', periods=5, freq='D'),\n                          data={'poll_support': [0.3, 0.4, 0.4, 0.4, 0.3]})\n\nIn [3]: df\nOut[3]:\n            poll_support\n2002-01-01           0.3\n2002-01-02           0.4\n2002-01-03           0.4\n2002-01-04           0.4\n2002-01-05           0.3\n\nIn [4]: df.drop_duplicates()\nOut[4]:\n            poll_support\n2002-01-01           0.3\n2002-01-02           0.4\n```\n\nWould be ideal to be able to do something like:\n\n```\nIn [4]: df.drop_duplicates(consecutive=True)\nOut[4]:\n            poll_support\n2002-01-01           0.3\n2002-01-02           0.4\n2002-01-05           0.3\n```\n\nThis should also be a much faster operation, since you only have to compare each\nrow with its successor, rather with all other rows.\n\nYou can achieve something like this with some shift trickery:\n\n```\nIn [5]: s1 = df.shift(1)\n\nIn [6]: different = (s1 != df) & (s1.notnull() | df.notnull())\n\nIn [7]: df.drop(df.index[~different.any(axis=1)], axis=0)\nOut[7]:\n            poll_support\n2002-01-01           0.3\n2002-01-02           0.4\n2002-01-05           0.3\n```\n\nBut this is somewhat cumbersome, and allocating the intermediate shifted\nframe can be slow (particularly if done via a groupby with a lot of groups).\n"},{"labels":["api",null],"text":"``` python\ndef foo(x):\n    print \"hello\"\n_ = pd.DataFrame({'a':[1]}).groupby([\"a\"]).apply(foo)\n```\n\n```\nhello\nhello\n```\n"},{"labels":["api",null],"text":"While building up a DataFrame in several steps, I found it difficult to add a new \"perpendicular\" column, i.e. a column that adds another dimension to already existing columns. To solve this problem I got the idea that this may be done in two steps:\n1. Add a new column whose values at each cell is a python list of the values the new column takes on.\n2. Unlistify the column thereby creating a new row for each element in the above lists.\n\nI.e. I propose two new DataFrame methods, listify() and unlistify().\n\n`listify(df, column)`: Takes as input a dataframe and the name of a column. It will do a groupby of the df for all columns except column and generate a single row where the values in the column cell is a list of the column `column` values.\n\n`unlistify(df, column)`: Takes as input a dataframe and the name of a column. It will iterate over the values of the contents of `column` for each row and generate a new row for each value.\n\nThe functions may be expanded to support multiple columns. `listify()` may e.g. support a post processing function, that will be applied on the list.\n\nThe following python code illustrates these two functions. But obviously the functionality may be implemented more efficienctly on the C-level.\n\n``` python\n#!/usr/bin/python\n\nimport pandas as pd\n\ndef listify(df, column):\n  matches = [i for i,n in enumerate(df.columns)\n             if n==column]\n\n  if len(matches)==0:\n    raise Exception('Failed to find column named ' + column +'!')\n  if len(matches)>1:\n    raise Exception('More than one column named ' + column +'!')\n\n  old_index = df.index\n  col_idx = matches[0] + len(old_index.shape) # Since we will reset the index\n\n  column_names = list(df.index.names) + list(df.columns)\n  gb_cols = [c for c in column_names\n             if c!= column]\n\n  # Helper function to generate the squashed dataframe\n  def fnc(d):\n    row = list(d.values[0])\n    return pd.DataFrame([row[:col_idx]\n                         + [[v[col_idx] for v in list(d.values)]]\n                         + row[col_idx+1:]])\n\n  return (df\n          .reset_index()\n          .groupby(gb_cols)\n          .apply(fnc)\n          .rename(columns = lambda i : column_names[i])\n          .set_index(old_index.names)\n          )\n\ndef unlistify(df, column):\n  matches = [i for i,n in enumerate(df.columns)\n             if n==column]\n\n  if len(matches)==0:\n    raise Exception('Failed to find column named ' + column +'!')\n  if len(matches)>1:\n    raise Exception('More than one column named ' + column +'!')\n\n  col_idx = matches[0]\n\n  # Helper function to expand and repeat the column col_idx\n  def fnc(d):\n    row = list(d.values[0])\n    bef = row[:col_idx]\n    aft = row[col_idx+1:]\n    col = row[col_idx]\n    z = [bef + [c] + aft for c in col]\n    return pd.DataFrame(z)\n\n  col_idx += len(df.index.shape) # Since we will push reset the index\n  index_names = list(df.index.names)\n  column_names = list(index_names) + list(df.columns)\n  return (df\n          .reset_index()\n          .groupby(level=0,as_index=0)\n          .apply(fnc)\n          .rename(columns = lambda i :column_names[i])\n          .set_index(index_names)\n          )\n\n\n# Examples of how to listify and unlistify a column.\n\ndf = pd.DataFrame([[11,range(5),10],\n                   [22,range(3),20]],\n                   columns = ['A','B','C']).set_index('C')\nprint 'org'\nprint df\nprint '--'\ndf = unlistify(df,'B')\nprint 'unlistify(df,B)'\nprint df\nprint '--'\ndf = listify(df,'B')\nprint 'listify(df,B)'\nprint df\n```\n\nThe corresponding output:\n\n```\norg\n     A                B\nC                      \n10  11  [0, 1, 2, 3, 4]\n20  22        [0, 1, 2]\n--\nunlistify(df,B)\n     A  B\nC        \n10  11  0\n10  11  1\n10  11  2\n10  11  3\n10  11  4\n20  22  0\n20  22  1\n20  22  2\n--\nlistify(df,B)\n     A                B\nC                      \n10  11  [0, 1, 2, 3, 4]\n20  22        [0, 1, 2]\n```\n"},{"labels":["api",null],"text":"See PR below. Includes tests and release doc edits.\n\nhttps://github.com/pydata/pandas/pull/10384\n\nI'm just having a little problem with merging a simple two line conflict. I'm in the process of learning, so be gentle =P\n"},{"labels":["api",null],"text":"```\ns = pd.Series(np.random.randn(100))\ns.plot(kind='hist')\ns.hist()\n```\n\nRunning `s.plot(kind='hist')` labels the y axis with the word 'Degree'. A more informative label might be 'count', 'frequency', or 'density'.\n\nAlso, it seems strange that `s.plot(kind='hist')` and `s.hist()` produce plots with different labels / gridlines.\n\nLooks like this came from PR #7809. cc @sinhrks, @jreback\n"},{"labels":["api",null,null],"text":"```\nIn [1]: import pandas as pd\n\nIn [2]: pd.Series([1,2,3]).sum(numeric_only=False)\nOut[2]: 6\n\nIn [3]: pd.Series([1,2,3]).sum(numeric_only=True)\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n<ipython-input-3-2c46bd289e26> in <module>()\n----> 1 pd.Series([1,2,3]).sum(numeric_only=True)\n\n/users/is/whughes/pyenvs/research/lib/python2.7/site-packages/pandas-0.16.2_ahl1-py2.7-linux-x86_64.egg/pandas/core/generic.pyc in stat_func(self, axis, skipna, level, numeric_only, **kwargs)\n   4253                                               skipna=skipna)\n   4254                 return self._reduce(f, name, axis=axis,\n-> 4255                                     skipna=skipna, numeric_only=numeric_only)\n   4256             stat_func.__name__ = name\n   4257             return stat_func\n\n/users/is/whughes/pyenvs/research/lib/python2.7/site-packages/pandas-0.16.2_ahl1-py2.7-linux-x86_64.egg/pandas/core/series.pyc in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n   2081             if numeric_only:\n   2082                 raise NotImplementedError(\n-> 2083                     'Series.{0} does not implement numeric_only.'.format(name))\n   2084             return op(delegate, skipna=skipna, **kwds)\n   2085 \n\nNotImplementedError: Series.sum does not implement numeric_only.\n```\n"},{"labels":["api",null,null],"text":"`SparseSeries.shape` seems to return incorrect result ignoring shapes filled by `fill_value`.\n\n```\ns = pd.SparseSeries([0, 0, 1, 0], fill_value=0)\ns\n#0    0\n#1    0\n#2    1\n#3    0\n# dtype: int64\n# BlockIndex\n# Block locations: array([2], dtype=int32)\n# Block lengths: array([1], dtype=int32)\n\nlen(s)\n#4\n\n# must be (4, )?\ns.shape\n# (1,)\n\n# OK\ns._data.shape\n# (4,)\n```\n"},{"labels":["api",null,null],"text":"Best shown with an example.\n\n``` python\nimport numpy as np, pandas as pd\ntimestamps = map(pd.Timestamp, ['2014-01-01', '2014-02-01'])\ncategories = ['A', 'B', 'C', 'D']\ndf = pd.DataFrame(index=pd.MultiIndex.from_product([timestamps, categories], names=['ts', 'cat']),\n                  columns=['Col1', 'Col2'])\n\n>>> df\n                Col1  Col2\nts         cat            \n2014-01-01 A     NaN   NaN\n           B     NaN   NaN\n           C     NaN   NaN\n           D     NaN   NaN\n2014-02-01 A     NaN   NaN\n           B     NaN   NaN\n           C     NaN   NaN\n           D     NaN   NaN\n```\n\nI want to set the values for all categories in a single month. These examples work just fine.\n\n``` python\ndf.loc['2014-01-01', 'Col1'] = 5\ndf.loc['2014-01-01', 'Col2'] = [1,2,3,4]\n\n>>> df\n               Col1 Col2\nts         cat          \n2014-01-01 A      5    1\n           B      5    2\n           C      5    3\n           D      5    4\n2014-02-01 A    NaN  NaN\n           B    NaN  NaN\n           C    NaN  NaN\n           D    NaN  NaN\n```\n\nThese examples don't work.\n\n``` python\ndf.loc['2014-01-01', 'Col1'] += 1\ndf.loc['2014-02-01', 'Col2'] = df.loc['2014-01-01', 'Col2']\n\n>>> df\n               Col1 Col2\nts         cat          \n2014-01-01 A    NaN    1\n           B    NaN    2\n           C    NaN    3\n           D    NaN    4\n2014-02-01 A    NaN  NaN\n           B    NaN  NaN\n           C    NaN  NaN\n           D    NaN  NaN\n```\n\nIt doesn't seem to be a \"setting a value on a copy\" issue. Instead, Pandas is writing the NaNs.\n\nMy current workaround is to unstack each column into a DataFrame with simple indexes. This works, but I have lots of columns to work with. One dataframe is much easier to work with than a pile of dataframes.\n\nThe computations for each month depend on the values computed in the previous month, hence why it can't be done fully vectorized on an entire column.\n"},{"labels":["api",null,null],"text":"[This code](https://github.com/pydata/pandas/blob/7d6fb510c1dfdbe7342f32f05ca5fd69b7854081/pandas/core/common.py#L1646) means the interpolator won't handle extrapolating to the front of the Series even though the underlying implementations may have no problem with the extrapolation. \n\nSee for example [`UnivariateSpline`](http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.interpolate.UnivariateSpline.html) whose default behavior is extrapolation.\n\nInterpolation works fine at the end of the Series.\n\nFor example:\n\n``` python\ns = pd.Series([1, 2, 3, 4, np.nan, 6, np.nan])\ns.interpolate(method='spline', order=1)\n0    1\n1    2\n2    3\n3    4\n4    5\n5    6\n6    7\ndtype: float64\n```\n\nbut:\n\n``` python\ns = pd.Series([np.nan, 2, 3, 4, np.nan, 6, 7])\ns.interpolate(method='spline', order=1)\n0   NaN\n1     2\n2     3\n3     4\n4     5\n5     6\n6     7\ndtype: float64\n```\n"},{"labels":["api",null],"text":"Hello,\n\nIf I try to unstack using multiIndex I got some groups divided. However, if I tried to do it by successive unstacking they are correctly grouped.  Please see image below\n\n![image](https://cloud.githubusercontent.com/assets/5657134/8312620/b044c562-19ab-11e5-8f88-f338efdb4ae8.png)\n\nI believe this is due to the fact that the first rows don't have n=3 for \"actor\" and the corresponding column is added only when it appears in a posterior row. Please note this issue is more related with presentation than performance as both operations yield the same results in subsequent processing (e.g. plotting).\n\nBest!\n"},{"labels":["api",null,null],"text":"I get this error  \"NotImplementedError: Writing as Excel with a MultiIndex is not yet implemented. \nwhen running the example  below(as described in your site):\n\"Generating Excel Reports from a Pandas Pivot Table.  \"\n"},{"labels":["api",null,null],"text":"xref #14351\n\nNone of the following merge operations retain the `category` types. Is this expected? How can I keep them?\n#### Merging on a `category` type:\n\nConsider the following:\n\n```\nA = pd.DataFrame({'X': np.random.choice(['foo', 'bar'],size=(10,)), \n                  'Y': np.random.choice(['one', 'two', 'three'], size=(10,))})\nA['X'] = A['X'].astype('category')\n\nB = pd.DataFrame({'X': np.random.choice(['foo', 'bar'],size=(10,)), \n                  'Z': np.random.choice(['jjj', 'kkk', 'sss'], size=(10,))})\nB['X'] = B['X'].astype('category')\n```\n\nif I do the merge, we end up with:\n\n```\n> pd.merge(A, B, on='X').dtypes \nX    object\nY    object\nZ    object\ndtype: object\n```\n#### Merging on a `non-category` type:\n\n```\nA = pd.DataFrame({'X': np.random.choice(['foo', 'bar'],size=(10,)), \n                  'Y': np.random.choice(['one', 'two', 'three'], size=(10,))})\nA['Y'] = A['Y'].astype('category')\n\nB = pd.DataFrame({'X': np.random.choice(['foo', 'bar'],size=(10,)), \n                  'Z': np.random.choice(['jjj', 'kkk', 'sss'], size=(10,))})\nB['Z'] = B['Z'].astype('category')\n```\n\nif I do the merge, we end up with:\n\n```\npd.merge(A, B, on='X').dtypes\nX    object\nY    object\nZ    object\ndtype: object\n```\n"},{"labels":["api",null,null,null],"text":"When slicing data time - indexed dataframe with date in string format, it works:\n\n```\ndf.ix['2014-11-12'].head()\nOut[55]: \n                     calories      gsr  heart-rate  skin-temp  steps\ndate                                                                \n2014-11-12 00:00:00       2.1  7.04640         NaN    85.7750     30\n2014-11-12 00:01:00       1.5  7.40759         NaN    85.7375      0\n2014-11-12 00:02:00       1.4  7.46220          49    85.5500      0\n2014-11-12 00:03:00       1.4  7.55800         NaN    85.5500      0\n2014-11-12 00:04:00       1.5  7.52163         NaN    85.5125      0\n```\n\nBut when i try it with a date object, of datetime class , it slices only for time 00:00:00. \n\n```\nd1=datetime.date(2014,11,12)\ndf.ix[d1]\nOut[57]: \ncalories       2.1000\ngsr            7.0464\nheart-rate        NaN\nskin-temp     85.7750\nsteps         30.0000\nName: 2014-11-12 00:00:00, dtype: float64\n```\n"},{"labels":["api",null,null],"text":"I may be missing an existing method here. Didn't see anything though...\n\nGiven a Categorical\n\n``` python\nIn [10]: s = pd.Series(['A', 'B', 'C', 'A'], dtype='category')\n\nIn [11]: s\nOut[11]:\n0    A\n1    B\n2    C\n3    A\ndtype: category\nCategories (3, object): [A, B, C]\n```\n\nAnd some codes, `new_codes = pd.Series([0, 1, 0, 2], index=[4, 5, 6, 7])`, I'd like to easily categorize the new codes. I think this is essentially\n\n``` python\nIn [22]: catmap = dict(enumerate(s.cat.categories))\n\nIn [23]: new_codes.map(catmap).astype('category')\nOut[23]:\n4    A\n5    B\n6    A\n7    C\ndtype: object\n```\n\nThat's not quite the same (what if `new_codes` doesn't have every code from `s`, then it's \"missing\"? need to make sure the categories / codes are identical), but the basic idea is there.\n\n---\n\nMy use-case here is going from Categorical -> codes -> scikit-learn classifier -> prediction (codes). It'd be nice if the column could easily transform that result back.\n\nNaming wise, I'd say `categorize` or `decode`. Should we also add the symmetrical `encode` method for `category` to `code`?\n\nIf `new_codes` contains a previously unseen `code` I'd say raise, or maybe insert nan (perhaps an option here?)\n"},{"labels":["api",null],"text":"Extend the new \"pipe\" protocol to GroupBy objects to allow for piping of a wider class of functions.  Currently, one can only create pipes that chain together objects inheriting from NDFrame.  But the concept of piping is general and could be extended to other pandas objects, specifically anything inheriting from GroupBy.\n\nThe use case is to write pipe that allow one to freely transform back-and-forth between NDFrames and GroupBy objects.  Example:\n\n```\ndf = DataFrame({A: [...], B: [...]})\n\ndef f(dfgb):\n    return dfgb['B'].value_counts()\n\ndef g(srs):\n    return srs * 2\n\ngrouped = df.groupby('A')\n\ngrouped.pipe(f).pipe(g)\n```\n\nNote that these transformations are transformations are\n- GroupBy -> Series\n- Series -> Series\n  and the chain seamlessly switches from a GroupBy.pipe to a NDFrame.pipe\n\nThere are a few ways to implement this.  A simple way is to break out the core functionality of \"pipe\" into a pure function and then to call that function in any method implementation of pipe.  Another way is to think of piping as a mix-in trait, put it as a method in a base class, and then mix that base class into any class that wants to implement pipe-ability.  I have no strong preference between these options, and I'm open to other implementations that may be more inline with Pandas' design goals or the long-term vision of the \"pipe\" concept.\n\nA strawman implementation of the first implementation suggestion can be found here:\nhttps://github.com/pydata/pandas/compare/master...ghl3:groupby-pipe\n\nCC\n@TomAugspurger \n@shoyer \n"},{"labels":["api",null,null],"text":"pandas converts all strings to 'O' columns.\n\nFor consistency and writing to files it would be good if pandas respects numpy fixed length strings.\n\n```\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'date': np.array(['2015-10-10', '2015-01-01'], dtype='S10')})\nprint(df.date.dtype)\n```\n\nResults in `dtype('O')` and not `dtype('S10')`\n"},{"labels":["api",null,null],"text":"quantities related\nxref #2494 \nxref #1071 \n\ncustom meta-data\nxref #2485 \n\nIt would be very convenient if unit support could be integrated into pandas.\nIdea: pandas checks for the presence of a unit-attribute of columns and - if present - uses it\n- with 'print' to show the units e.g. below the column names\n- to calculate 'under the hood' with these units similar to the example below\n\nFor my example I use the module pint and add an attribute 'unit' to columns (and a 'title'...). \n\nExample:\n\n``` python\nfrom pandas import DataFrame as DF\nfrom pint import UnitRegistry\nunits = UnitRegistry()\n\nclass ColumnDescription():\n    '''Column description with additional attributes.\n\n    The idea is to use this description to be able to add unit and title\n    attributes to a column description in one step.\n\n    A list of ColumnDescriptions is than used as argument to DataFrame()\n    with unit support.\n    '''\n\n    def __init__(self, name, data, title = None, unit = None):\n        '''\n        Args:\n            name (str): Name of the column..\n            data (list): List of the column data.\n            title (str): Title of the column. Defaults to None.\n            unit (str): Unit of the column (see documentation of module pint).\n                Defaults to None.\n\n        '''\n\n        self.data = data \n        '''(list): List of the column data.'''\n\n        self.name = name\n        '''(str): Name of the column, naming convention similar to python variables.\n\n        Used to access the column with pandas syntax, e.g. df['column'] or df.column.\n        '''\n\n        self.title = title \n        '''(str): Title of the column. \n\n        More human readable than the 'name'. E.g.:\n        Title: 'This is a column title'.\n        name: 'column_title'.\n        '''\n\n        self.unit = unit\n        '''Unit of the column (see module pint).\n\n        Intended to be used in calculations involving different columns.\n        '''\n\nclass DataFrame(DF):\n    '''Data Frame with support for ColumnDescriptions (e.g. unit support).\n\n    1. See documentation of pandas.DataFrame.\n    2. When used with ColumnDescriptions supports additional column attributes\n    like title and unit.\n    '''\n\n    def __init__(self, data, title = None):\n        '''\n        Args:\n            data (list or dict):\n                1. Dict, as in documentation of DataFrame\n                2. List of the column data (of type ColumnDescription).\n            title (str): Title of the data frame. Defaults to None.\n        '''\n\n        if isinstance(data, list):\n            if isinstance(data[0], ColumnDescription):\n                d = {}\n\n                for column in data:\n                    d[column.name] = column.data\n\n                super(DataFrame, self).__init__(d)\n\n                for column in data:\n                    self[column.name].title = column.title\n                    self[column.name].unit = column.unit\n\n                self.title = title\n\n        else:\n            super(DataFrame, self).__init__(data)\n\nif __name__ == '__main__':\n\n    data = [ ColumnDescription('length',\n                               [1, 10],\n                               title = 'Length in meter',\n                               unit = 'meter'),\n             ColumnDescription('time',\n                               [10, 1],\n                               title = 'Time in s',\n                               unit = 's') ]\n\n    d = {'length':[1, 10],\n         'time': [10, 1]}\n    df = DataFrame(d)\n    print 'standard df'\n    print df\n\n    df = DataFrame(data)\n    print '\\n' + 'new df'\n    print df\n\n    ####use of dimensions####\n    # pint works with numpy arrays\n    # df[name] is currently not working with pint, but would be I think \n    # it would be a real enhancement if it would...\n    test = df.as_matrix(['length']) * units(df['length'].unit) / \\\n           (df.as_matrix(['time']) * units(df['time'].unit))\n    print '\\n' + 'unit test'\n    print test\n    print '\\n' + 'magnitude'\n    print test.magnitude\n    print '\\n' + 'dimensionality'\n    print test.dimensionality\n\n```\n"},{"labels":["api",null,null,null,null],"text":"I'm trying to get a slice from a multiindex. I find the behavior pretty inconsistent. Here is a simple dataframe:\n\n``` python\ntest = pd.DataFrame({'l': ['A', 'B', 'C', 'C', 'B', 'A'], 'm': [pd.Timestamp('2014-06-11 14:26:27'), pd.Timestamp('2014-06-11 15:26:27'),  pd.Timestamp('2014-06-11 16:26:27'), pd.Timestamp('2014-06-12 14:26:27'), pd.Timestamp('2014-06-12 15:26:27'),  pd.Timestamp('2014-06-12 16:26:27')]})\n```\n\nWith a single index, I can select all the data for a given day as follows:\n\n``` python\ntest_single_index = test.set_index(['m'])\n#partial string indexing works\ntest_single_index.loc['2014-06-11']\n```\n\nBut it doesn't work for a multiindex:\n\n``` python\ntest_multi_index = test.set_index(['m', 'l'])\n#would expect this to work\ntest_multi_index.loc['2014-06-11']\n#or this\ntest_multi_index.loc[('2014-06-11', 'A'),:]\n\n#what works\nidx = pd.IndexSlice\ntest_multi_index.loc[idx['2014-06-11':'2014-06-11',:],:]\ntest_multi_index.loc[idx['2014-06-11','A'],:]\n```\n\nSo I can make it work if I specify the slice explicitely, but it would be nice if the behavior for the 1D index carried over to Multiindices.\n"},{"labels":["api",null],"text":"Perhaps there is some canonical way of doing this that I'm not aware of, but from time to time I find myself wanting to group more than one frame by a key, and apply some function to the result. At the moment, I'm doing this in a somewhat roundabout way:\n\n```\nresults = []\nfor key, t1 in table1.groupby(level='id'):\n    try:\n        t2 = table2.xs(key, level='id')\n    except KeyError:\n        t2 = None\n    try:\n        t3 = table3.xs(key, level='id')\n    except KeyError:\n        t3 = None\n    # ... etc\n    r = some_func(t1, t2, t3)\n    results.append(r)\nresults = pd.concat(results)\n```\n\nI realize that often the easiest thing to do is just to merge the frames, but there are cases where that doesn't really make sense (e.g. non-compatible multiindexes that share one level). I imagine `pd.merge(t1, t2, on=\"id\")` already does something like this to line up tables - do you guys think there is any value in exposing a standard way of doing this? Perhaps something like:\n\n```\npd.multi_frame_groupby([t1, t2, t3, t4], level='id').apply(some_func)\n```\n\nOr, given that the full behaviour of such a groupby object is non trivial, perhaps just a generator for frame tuples?\n\n```\ngen = pd.multi_frame_groupby([t1, t2, t3, t4], level='id')\npd.concat([some_func(*t) for t in gen])\n```\n"},{"labels":["api",null,null,null],"text":"To record the issue we discussed yesterday, to be solved for 0.16.2\n\nPlotting of categorical:\n- y-axis: the 'values' itself are used\n- x-axis: they are not regarded as values, but as all unique items that just gets represented by a `range(len(cat))` x data\n\nOverview: http://nbviewer.ipython.org/gist/jorisvandenbossche/992d9d34dbfcfd8bc326\n\nDisclaimer: didn't yet look into the code to see why this is like this.\n\nWay forward:\n- for now: handle CategoricalIndex the same as column with Categorical values (so use the values itself)\n  - this will also mean that a CategoricalIndex with string categories will raise now (in 0.16.1 it did not but plotted all values just in order, also discarding the fact the values with the same category should be regarded as equal)\n- later, we can try to implement more fancy / intelligent categorical plotting (there are already #9069 and #8712 about this)\n"},{"labels":["api",null],"text":"From the documentation for DataFrame.apply:\n\n`\nIn the current implementation apply calls func twice on the first column/row to decide whether it can take a fast or slow code path. This can lead to unexpected behavior if func has side-effects, as they will take effect twice for the first column/row.\n`\n\nI am sure there are well-thought out reasons why it is currently implemented this way, but this behavior can be a real \"gotcha\".  I grant that having side effects inside an apply function is not good standard practice, but I would argue that there are times when it is a good solution to a problem. ( I can elaborate on this if needed).  Thankfully this behavior is documented, but I think it is reasonable to expect that most users will not always be mindful of secondary notes that exist throughout the documentation, and the source of problems caused by this behavior is not at all obvious.\n\nWhile I don't understand the engineering issues regarding the fast/slow path, I would suggest that some better solution to the problem be introduced.\n"},{"labels":["api",null,null],"text":"Would it make sense to make the backref from ix objects to the dataframe be weakrefs (and presumably, loc/iloc)  The reason I ask is that it seems that the refcount is incremented once you use ix, and as a result the dataframe won't get garbage collected until the cycle detection runs\n"},{"labels":["api",null,null],"text":"Another somewhat degenerate case that works in 0.15.2, fails in 0.16.1\n\n``` python\nimport pandas as pd\n\npd.show_versions()\n\n\nts = pd.TimeSeries()\n\nts[pd.datetime(2012, 1, 1)] = 47\n\n\nts2 = pd.TimeSeries(0, pd.date_range('2011-01-01', '2011-01-01'))[:0]\n\nts2[pd.datetime(2012, 1, 1)] = 47\n```\n\n0.15.2 output:\n\n``` python\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.9.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.15.2\nnose: 1.3.4\nCython: 0.21\nnumpy: 1.9.2\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 3.0.0-dev\nsphinx: 1.2.3\npatsy: 0.3.0\ndateutil: 2.4.2\npytz: 2015.4\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.2\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.5.7\nlxml: 3.4.0\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: None\n```\n\n0.16.1 output:\n\n``` python\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.9.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.16.1-46-g0aceb38\nnose: 1.3.6\nCython: 0.22\nnumpy: 1.9.2\nscipy: 0.14.0\nstatsmodels: 0.6.1\nIPython: 3.1.0\nsphinx: 1.3.1\npatsy: 0.3.0\ndateutil: 2.4.2\npytz: 2015.4\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.7.2\nlxml: None\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.4\npymysql: None\npsycopg2: None\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nc:\\test_set_empty_series_with_freq.py in <module>()\n     11 ts2 = pd.TimeSeries(0, pd.date_range('2011-01-01', '2011-01-01'))[:0]\n     12\n---> 13 ts2[pd.datetime(2012, 1, 1)] = 47\n\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\core\\series.pyc in __setitem__(self, key, value)\n    687         # do the setitem\n    688         cacher_needs_updating = self._check_is_chained_assignment_possible()\n--> 689         setitem(key, value)\n    690         if cacher_needs_updating:\n    691             self._maybe_update_cacher()\n\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\core\\series.pyc in setitem(key, value)\n    660                             pass\n    661                 try:\n--> 662                     self.loc[key] = value\n    663                 except:\n    664                     print \"\"\n\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\core\\indexing.pyc in __setitem__(self, key, value)\n    113     def __setitem__(self, key, value):\n    114         indexer = self._get_setitem_indexer(key)\n--> 115         self._setitem_with_indexer(indexer, value)\n    116\n    117     def _has_valid_type(self, k, axis):\n\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\core\\indexing.pyc in _setitem_with_indexer(self, indexer, value)\n    272                 if self.ndim == 1:\n    273                     index = self.obj.index\n--> 274                     new_index = index.insert(len(index),indexer)\n    275\n    276                     # this preserves dtype of the value\n\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\tseries\\index.pyc in insert(self, loc, item)\n   1523             # check freq can be preserved on edge cases\n   1524             if self.freq is not None:\n-> 1525                 if (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:\n   1526                     freq = self.freq\n   1527                 elif (loc == len(self)) and item - self.freq == self[-1]:\n\nc:\\python\\envs\\pandas-0.16.1\\lib\\site-packages\\pandas\\tseries\\index.pyc in __getitem__(self, key)\n   1351         getitem = self._data.__getitem__\n   1352         if np.isscalar(key):\n-> 1353             val = getitem(key)\n   1354             return Timestamp(val, offset=self.offset, tz=self.tz)\n   1355         else:\n\nIndexError: index 0 is out of bounds for axis 0 with size 0\n\n```\n\nProposed https://github.com/pydata/pandas/pull/10194\n"},{"labels":["api",null,null],"text":"Derived from #10157. Would like to clarify what these results should be.  Basically, I think: \n- The result must be `CategoricalIndex`\n- The result should have the same values as the result of normal `Index` which has the same original values.\n- Result's `category` should only include categories which the result actually has.\n\nFollowings are current results.\n## intersection\n\n```\n# for reference\npd.Index([1, 2, 3, 1, 2, 3]).intersection(pd.Index([2, 3, 4, 2, 3, 4]))\n# Int64Index([2, 2, 3, 3], dtype='int64')\n\npd.CategoricalIndex([1, 2, 3, 1, 2, 3]).intersection(pd.CategoricalIndex([2, 3, 4, 2, 3, 4]))\n# CategoricalIndex([2, 2, 3, 3], categories=[1, 2, 3], ordered=False, dtype='category')\n# -> Is this OK or it should have categories=[2, 3]?\n```\n## union\n\nDoc says \"Form the union of two Index objects and sorts if possible\". I'm not sure whether the last sentence says \"raise error if sort is impossible\" or \"not sort if impossible\"?\n- http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.union.html\n\n```\npd.Index([1, 2, 4]).union(pd.Index([2, 3, 4]))\n# Int64Index([1, 2, 3, 4], dtype='int64')\n\npd.CategoricalIndex([1, 2, 4]).union(pd.CategoricalIndex([2, 3, 4]))\n# CategoricalIndex([1, 2, 4, 3], categories=[1, 2, 3, 4], ordered=False, dtype='category')\n# -> Should be sorted?\n```\n\n```\npd.Index([1, 2, 3, 1, 2, 3]).union(pd.Index([2, 3, 4, 2, 3, 4]))\n# InvalidIndexError: Reindexing only valid with uniquely valued Index objects\n-> This should results Index([1, 2, 3, 1, 2, 3, 4, 4])?\n\npd.CategoricalIndex([1, 2, 3, 1, 2, 3]).union(pd.CategoricalIndex([2, 3, 4, 2, 3, 4]))\n# TypeError: type() takes 1 or 3 arguments\n# -> should raise understandable error, or Int64Index shouldn't raise (and return unsorted result?)\n```\n## difference\n\n```\npd.CategoricalIndex([1, 2, 4, 5]).difference(pd.CategoricalIndex([2, 3, 4]))\n# Int64Index([1, 5], dtype='int64')\n# -> should be CategoricalIndex?\n```\n## sym_diff\n\n```\npd.CategoricalIndex([1, 2, 4, 5]).sym_diff(pd.CategoricalIndex([2, 4]))\n# Int64Index([1, 5], dtype='int64')\n# -> should be CategoricalIndex?\n```\n"},{"labels":["api",null],"text":"I searched the issues list for \"empty dataframe\"  but did not find anything matching. \n\nIn pandas 0.16.1, the result of adding 2 empty dataframes  is not equal to another empty dataframe. \n\nThis code, in pandas 0.15.2, does what is expected: \n\n``` python\nimport pandas as pd\n\npd.show_versions()\n\ndf1 = pd.DataFrame()\n\ndf2 = pd.DataFrame()\n\nassert df1.equals(df1)\n\nassert df2.equals(df1)\n\nassert (df1+df1).equals(df2)\n```\n\nIt fails in 0.16.1.\n\n0.15.2 output:\n\n``` python\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.7.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: en_GB\n\npandas: 0.15.2\nnose: 1.3.6\nCython: 0.21.1\nnumpy: 1.9.2\nscipy: 0.15.1\nstatsmodels: None\nIPython: 3.1.0\nsphinx: 1.3.1\npatsy: 0.3.0\ndateutil: 2.4.2\npytz: 2015.4\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.0\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.7.2\nlxml: 3.4.0\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: None\n```\n\n0.16.1 output:\n\n``` python\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.9.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.16.1\nnose: 1.3.6\nCython: 0.22\nnumpy: 1.9.2\nscipy: 0.14.0\nstatsmodels: 0.6.1\nIPython: 3.1.0\nsphinx: 1.3.1\npatsy: 0.3.0\ndateutil: 2.4.2\npytz: 2015.4\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.7.2\nlxml: None\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.4\npymysql: None\npsycopg2: None\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nc:\\test_empty_add_0.16.1.py in <module>()\n     11 assert df2.equals(df1)\n     12\n---> 13 assert (df1+df1).equals(df2)\n\nAssertionError:\n```\n"},{"labels":["api",null],"text":"interpolation : {‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}\n\nThis would make it consistent with numpy.percentile options (new in 1.9). See http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html\n"},{"labels":["api",null],"text":"read_csv does not return the data table expected if there's only one line in the CSV file and skipfooter is used and the line directly after the table is blank and skip_blank_lines=True.\n\n```\n>>> import pandas as pd\n>>> import StringIO\n>>> test_csv = StringIO.StringIO('a,b,c\\n1,2,3\\n\\nend\\n')                                                                                                                                                                                      \n>>> pd.read_csv(test_csv, skip_footer=2, engine='python')\nEmpty DataFrame\nColumns: [a, b, c]\nIndex: []\n```\n\nBut if the more than one line is in the data table or the line after the data table is not blank or skip_blank_lines is set to False, everything is ok:\n\n```\n>>> test_csv = StringIO.StringIO('a,b,c\\n1,2,3\\n4,5,6\\n\\nend\\n')\n>>> pd.read_csv(test_csv, skip_footer=2, engine='python')\n   a  b  c\n0  1  2  3\n1  4  5  6\n>>> test_csv = StringIO.StringIO('a,b,c\\n1,2,3\\nend\\n\\n')\n>>> pd.read_csv(test_csv, skip_footer=2, engine='python')\n   a  b  c\n0  1  2  3\n>>> test_csv = StringIO.StringIO('a,b,c\\n1,2,3\\n\\nend\\n')\n>>> pd.read_csv(test_csv, skip_footer=2, engine='python', skip_blank_lines=False)\n   a  b  c\n0  1  2  3\n```\n\nThis occurs in every version from 0.15.0 onwards (ie since skip_blank_lines was introduced).\n"},{"labels":["api",null],"text":"It would be nice to be able to read CSVs with categorical variables using read_csv's dtype parameter instead of casting the columns after the fact.\n"},{"labels":["api",null],"text":"Both `Series.value_counts` and `Index.value_counts` should preserve its name in resulted `Series.name`?  Current behaviors are below:\n\n```\ns = pd.Series([1, 2, 1], name='a')\nidx = pd.Index([1, 2, 1], name='a')\n\nresult = s.value_counts()\nresult.name, result.index.name\n# (None, None)      # should be ('a', None)?\n\nresult = idx.value_counts()\nresult.name, result.index.name\n# (None, None)      # should be ('a', None)?\n\ndidx = pd.date_range('2011-01-01', freq='D', periods=3, name='a')\nresult = didx.value_counts()\n# (None, 'a')       # should be ('a', None)?\n```\n\n```\n```\n"},{"labels":["api",null,null,null],"text":"After upgrading from 0.16.0 to 0.16.1, some of my plotting stopped functioning right, and after debugging I figured out that it was an issue with the groupby results---namely, that they're getting a `CategoricalIndex` even though they're being grouped by a number.\n\nThis is basically what I do in my code (simplified for this example):\n\n```\ndf = pd.DataFrame()\ndf['x'] = 100 * np.random.random(100)\ndf['y'] = df.x**2\n\nbinedges = np.arange(0,110,10)\nbinlabels = np.arange(5,105,10)\n\nz = df.groupby(pd.cut(df.x,bins=binedges ,labels=binlabels )).y.mean()\n```\n\nAnd here's what that produces:\n\n```\nx\n5       21.809599\n15     215.432515\n25     556.803743\n35    1282.261483\n45    1941.166247\n55    3104.923704\n65    4256.942052\n75    5862.686054\n85    7304.835127\n95    8810.194492\nName: y, dtype: float64\n```\n\nWhen I tried plotting this, the scale was all messed up---it was treating each of the 10 bins as an element of `[0,1,2,...,9]`, which wasn't apparent until I tried changing the `xticks` parameter and everything went screwy. Took a while to debug, but finally found the culprit, by looking at `z.index`:\n\n```\nCategoricalIndex([5, 15, 25, 35, 45, 55, 65, 75, 85, 95], categories=[5, 15, 25, 35, 45, 55, 65, 75, ...], ordered=True, name=u'x', dtype='category')\n```\n\nThis isn't the desired behavior, right? I know the `CategoricalIndex` feature is new, but I don't think this was an intended feature. This same code previously produced an `Int64Index` that would plot just fine, and there's no use case I can see for changing that. Or if that was what was intended, is there any way to select an `Int64Index` instead (other than converting it after the fact)?\n\nUPDATE: Also note that this screws up using pd.cut() to create bins in a dataframe, because the indices don't line up afterward. So doing something like\n\n```\ndf['bin'] = pd.cut(df.x,bins=mybins,labels=mylabels)\n```\n\nwill just result in an empty column. So I can't imagine that this is the correct behavior.\n"},{"labels":["api",null],"text":"When I pass cursor as data to DataFrame constructor an error occurs.\n\n```\ncursor = sqlite.execute(sql)\npd.DataFrame(cursor)\n\n\n/usr/lib/python3/dist-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)\n    255                                          copy=copy)\n    256         elif isinstance(data, collections.Iterator):\n--> 257             raise TypeError(\"data argument can't be an iterator\")\n    258         else:\n    259             try:\n\nTypeError: data argument can't be an iterator\n```\n\nBut normal generators is accepted\n\n```\ndef gen():\n    yield (1,2,3)\n    yield (4,5,6)\n    yield (7,8,9)\n\npd.DataFrame(gen())\nOut[171]: \n   0  1  2\n0  1  2  3\n1  4  5  6\n2  7  8  9\n```\n\nIt feels like inconsistence.\n"},{"labels":["api"],"text":"Today @shoyer and I were talking about a new \"protocol\" that will let us sidestep the whole macro / method chaining issue.  The basic idea is that pandas objects define a `pipe` method (ideally other libraries will implement this to, assuming this is useful).\n\nBased on the discussions below, we're leaning towards a method like\n\n``` python\ndef pipe(self, func, *args, **kwargs):\n    pipe_func = getattr(func, '__pipe_func__', func)\n    return pipe_func(self, *args, **kwargs)\n```\n\nThat's it. This lets you write code like:\n\n``` python\nimport seaborn as sns\n\n(sns.load_dataset('iris')\n    .query(\"sepal_length > 5\")\n    .assign(sepal_ratio = lambda df: df.sepal_length / df.sepal_width)\n    .pipe(lambda df: sns.violinplot(x='species', y='sepal_ratio', data=df))\n)\n```\n\nseaborn didn't have to do anything! If the DataFrame is the first argument to the function, things are even simpler:\n\n``` python\ndef f(x):\n    return x / 2\n\ndf['sepal_length'].pipe(f)\n```\n\nUsers or libraries can work around the need for the (somewhat ugly) `lambda _:`, by using the `__pipe_func__`  attribute of the function being `pipe`d in. This is where a protocol (de facto or official) would be useful, since libraries that know nothing else about each other can rely on it. As an example, consider seaborn's violin plot, which expects a DataFrame as its fourth argument, `data`. Seaborn can define a simple decorator to attach a `__pipe_func__` attribute, allowing _it_ to define how it expects to be `pipe`d to. \n\n``` python\ndef pipeable(func):\n    def pipe_func(data, *args, **kwargs):\n        return func(*args, data=data, **kwargs)\n    func.__pipe_func__ = pipe_func\n    return func\n\n# now we decorate all the Seaborn methods as pipeable\n\n@pipeable\ndef violinplot(x=None, y=None, hue=None, data=None, ...):\n    # ...   \n```\n\nAnd users write\n\n``` python\n(sns.load_dataset('iris')\n    .query(\"sepal_length > 5\")\n    .assign(sepal_ratio = lambda x: x.sepal_length / x.sepal_width)\n    .pipe(sns.violinplot, x='species', y='sepal_ratio')\n)\n```\n### Why?\n\nHeavily nested function calls are bad. They're hard to read, and can easily introduce bugs. Consider:\n\n``` python\n# f, g, and h are functions that take and receive a DataFrame\nresult = f(g(h(df), arg1=1), arg2=2, arg3=3)\n```\n\nFor pandas, the approach has been to add `f`, `g`, and `h` as methods to (say) `DataFrame`\n\n``` python\n(df.h()\n   .g(arg1=1)\n   .f(arg2=2, arg3=3)\n)\n```\n\nThe code is certainly cleaner. It reads and flows top to bottom instead of inside-out. The function arguments are next to the function calls. But there's a hidden cost. DataFrame has something like 200+ methods, which is crazy. It's less flexible for users since it's hard to get their own functions into pipelines (short of monkey-patching). With `.pipe`, we can\n\n``` python\n(df.pipe(h)\n   .pipe(g, arg1=1)\n   .pipe(f, arg2=2, arg3=3)\n)\n```\n\nThe other way around the nested calls is using temporary variables:\n\n``` python\nr1 = h(df)\nr2 = g(r1, arg1=1)\nr3 = f(r2, arg2=2, arg3=3)\n```\n\nWhich is better, but not as good as the `.pipe` solution.\n\n---\n\nA relevant thread on python-ideas, started by @mrocklin: https://mail.python.org/pipermail/python-ideas/2015-March/032745.html\n\nThis doesn't achieve everything macros could. We still can't do things like `df.plot(x=x_col, y=y_col)` where `x_col` and `y_col` are captured by `df`'s namespace. But it may be good enough.\n\nGoing to cc a bunch of people here, who've had interest in the past.\n\n@shoyer \n@mrocklin\n@datnamer \n@dalejung\n"},{"labels":["api",null],"text":"http://connor-johnson.com/2014/08/28/tidyr-and-pandas-gather-and-melt/ \n\nIn the spirit of the excellent assign method, wondering if there is support for some tidyr style transformations?\n"},{"labels":["api",null],"text":"This is new in 0.16.1\n\n``` python\nIn [1]: import pandas as pd\n\nIn [2]: pd.__version__ \nOut[2]: '0.16.1'\n\nIn [3]: f = pd.DataFrame({'a': [0, 10, 20, 30, 40], 'b': [5, 4 ,3, 2, 1]},\n                          index=[1, 2, 3, 4, 4])\n\nIn [4]: f.dtypes \nOut[4]: \na    int64\nb    int64\ndtype: object\n\nIn [6]: pd.DataFrame(columns=['a', 'b'], dtype=f.dtypes).dtypes\nOut[6]: \na    object\nb    object\ndtype: object\n```\n\nAlso other mechanisms to convey dtype information yield uninformative errors\n\n``` python\nIn [7]: pd.DataFrame(columns=['a', 'b'], dtype={'a': 'i4', 'b': 'f4'}).dtypes\nValueError: entry not a 2- or 3- tuple\n\nIn [8]: pd.DataFrame(columns=['a', 'b'], dtype=[('a', 'i4'), ('b', 'f4')]).dtypes\nNotImplementedError: compound dtypes are not implementedin the DataFrame constructor\n```\n\nIs there a way to create an empty DataFrame with given dtypes in 0.16.1?\n"},{"labels":["api"],"text":"I often have the situation where I would like to apply multiple aggregation functions to all the columns of a grouped dataframe, like:\n\n``` python\ngrouped = df.groupby('somekey')\ndfAggregated = grouped.agg([np.mean, np.std])\n```\n\nThat works well, but sometimes (all the time, actually) I would also like to be able to use lambda functions this way, like:\n\n``` python\ngrouped = df.groupby('somekey')\ndfAggregated = grouped.agg([np.mean, np.std, lambda v: v.mean()/v.max()])\n```\n\nThis works fine, but the resulting column name will now be 'lambda', which is ugly. This can be resolved by using the much more verbose syntax where you specify a dictionary for every column separately, but I would propose to allow the following syntax:\n\n``` python\ngrouped = df.groupby('somekey')\ndfAggregated = grouped.agg([np.mean,np.std,{'normalized_mean': lambda v: v.mean()/v.max()}])\n```\n\nThe dictionary key should then be used as the resulting column name.\n\nInterestingly, using this syntax in the version 0.16 does not produce an error, but produces a column named 'Nan', that is filled with tupple values: ('n','o','r','m','a','l','i','z','e','d','_','m','e','a','n'), which I don't think is of use to anyone:)\n"},{"labels":["api",null,null],"text":"When passing a list OrderedDict objects to `pd.DataFrame`, the ordering of the columns is not kept:\n\n``` python\nod =  OrderedDict([('Z', 5), ('xx', 8), ('uno', 2), ('etc', 1), ('j', 76), ('e', 7)])\nl = [od]*4\npd.DataFrame(l).columns\n#Out[23]: Index(['Z', 'e', 'etc', 'j', 'uno', 'xx'], dtype='object')\n```\n"},{"labels":["api",null,null],"text":"Related with the issue I just opened (#10038), I relooked at the set operation deprecation issues we had before (xref #8227, #9095, #9630).\n\nTo make a summary:\n- in 0.15.0 we deprecated set operations, only for new TimedeltaIndex it is already numeric operation\n- in 0.15.1, for numeric indexes this was by accident converted to numeric operation\n- in 0.16.0, explicit deprecation added for DatetimeIndex\n- in a future release we want to have it all numeric operations (or TypeErrors if the dtype does not support that operation)\n\nBut now, for the CategoricalIndex added, it is a set operation again (but with a warning):\n\n```\nIn [1]: idx = pd.Index(pd.Categorical(['a', 'b']))\n\nIn [2]: idx\nOut[2]:\nCategoricalIndex([u'a', u'b'],\n                 categories=[u'a', u'b'],\n                 ordered=False,\n                 name=None)\n\nIn [3]: idx - idx\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\index.py:1191: FutureWarning: u\nsing '-' to provide set differences with Indexes is deprecated, use .difference(\n)\n  \"use .difference()\", FutureWarning)\nOut[3]: Index([], dtype='object')\n```\n\nAs this is a new index, we should at once use the correct behaviour? \n(Which is raising a TypeError I think?)\n"},{"labels":["api",null,null],"text":"Ilustration:\n\n```\nidx = pd.Index(['a', 'b', 'c', 'd'])\nidx - ['a', 'd']\nidx - pd.Index(['a', 'd'])\n```\n\ngives:\n\n```\nIn [1]: idx = pd.Index(['a', 'b', 'c', 'd'])\n\nIn [2]: idx - ['a', 'd']\nOut[2]: Index([u'b', u'c'], dtype='object')\n\nIn [3]: idx - pd.Index(['a', 'd'])\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\index.py:1192: FutureWarning: u\nsing '-' to provide set differences with Indexes is deprecated, use .difference(\n)\n  \"use .difference()\",FutureWarning)\nOut[3]: Index([u'b', u'c'], dtype='object')\n```\n\nSo a list is treated a an index-like (as it performs a set operation), but does not raise a warning as it should (the same for arrays, series).\n\nWe have an occurrence in the docs of this one.\n"},{"labels":["api",null],"text":"This is currently not the case (e.g., for `TimedeltaIndex`)\n\nas discussed in #10026.\n"},{"labels":["api",null,null],"text":"Currently the `Index.str` methods only support returning `Index` results. However, analogous to the expansion from `Series.str.*` -> `DataFrame`, we should be able to do `Index.str.*` -> `MultiIndex` in certain cases, as discussed in https://github.com/pydata/pandas/pull/9870#issuecomment-92416184 \n\nSome of the string methods that can support this are: `str.get_dummies`, `str.extract`, `str.split`\n- [x] `split` (#10085)\n- [x] `rsplit` (#10303)\n- [x] `partition` / `rpartition` (#9773)\n- [x] `extract` (#11386, only supports one group )\n- [x] `extractall` (not implemented in Index), #13156 \n- [x] `get_dummies` (#12842)\n\nHere are the related PRs: https://github.com/pydata/pandas/pull/9667, https://github.com/pydata/pandas/pull/9843, https://github.com/pydata/pandas/pull/9870, https://github.com/pydata/pandas/pull/9985\n"},{"labels":["api",null,null],"text":"At this moment, `pd.io.sql.get_schema` is not documented (not in the API docs, and not in the io docs). But, it is a potential useful function, so I think it would be good to be more explicit about its status (by mentioning it in the docs).\n\nHowever, there are some quirks about the function:\n- The signature: `pd.io.sql.get_schema(frame, name, flavor='sqlite', keys=None, con=Non\n  e, dtype=None)` -> flavor keyword in the third place, while we want to deprecate it (and this means you cannot do `get_schema(df, 'name', engine)`, but always have to do `get_schema(df, 'name', con=engine)`.  \n  Ideally this should have the same arguments (order) as `to_sql` (`pd.io.sql.to_sql(frame, name, con, flavor='sqlite', schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None)`) (only chunksize is not relevant)\n- It should have all options to modify the resulting sql table as `to_sql`\n- Maybe also the option to return the sqlalchemy `Table` instead of the string itself?\n\nThat we maybe should first solve before making it more explicitely public?\n\nTriggered by http://stackoverflow.com/questions/29749356/python-pandas-export-structure-only-no-rows-of-a-dataframe-to-sql/\n"},{"labels":["api",null,null],"text":"Whilst answering this [SO question](http://stackoverflow.com/questions/29706825/pandas-version-0-16-0-after-changing-dataframe-index-all-values-become-nan) I discovered that the DataFrame constructor is ignoring the fact that I'm passing an index explicitly resulting in a df with all NaNs:\n\n```\nIn [6]:\ndf = pd.DataFrame({'AAA':[4,5,6,7],'BBB':[10,20,30,40],'CCC':[100,50,-30,-50]})\ndf1 = pd.DataFrame(data=df,index=(['a','b','c','d']))\ndf1\nOut[6]:\n   AAA  BBB  CCC\na  NaN  NaN  NaN\nb  NaN  NaN  NaN\nc  NaN  NaN  NaN\nd  NaN  NaN  NaN\n```\n\nStepping through the code I see that it reaches here in frame.py:\n\n```\n            if isinstance(data, BlockManager):\n            mgr = self._init_mgr(data, axes=dict(index=index, columns=columns),\n                                 dtype=dtype, copy=copy)\n```\n\nand then ends up here in generic.py:\n\n```\n119         def _init_mgr(self, mgr, axes=None, dtype=None, copy=False):\n120             \"\"\" passed a manager and a axes dict \"\"\"\n121             for a, axe in axes.items():\n122                 if axe is not None:\n123                     mgr = mgr.reindex_axis(\n124  ->                     axe, axis=self._get_block_manager_axis(a), copy=False)\n```\n\nThe behaviour can be reproduced simply like this:\n\n```\nIn [46]:\n\ndata1 = pd.DataFrame({'AAA':[4,5,6,7],'BBB':[10,20,30,40],'CCC':[100,50,-30,-50]})\ndata1.reindex_axis(list('abcd'))\nOut[46]:\n   AAA  BBB  CCC\na  NaN  NaN  NaN\nb  NaN  NaN  NaN\nc  NaN  NaN  NaN\nd  NaN  NaN  NaN\n```\n"},{"labels":["api",null],"text":"The following fails:\n\n```\npd.DataFrame([], columns=\"a b\".split())\n```\n\nI'd expect this to be equivalent to\n\n```\npd.DataFrame(None, columns=\"a b\".split())\n```\n\nIs there any reason not to do it like this?\n"},{"labels":["api",null,null,null],"text":"What's the expected behavior when grouping on a column containing `NaN` and then applying `transform`? For a `Series`, the current result is to throw an exception:\n\n```\n>>> df = pd.DataFrame({\n...     'a' : range(10),\n...     'b' : [1, 1, 2, 3, np.nan, 4, 4, 5, 5, 5]})\n>>> \n>>> df.groupby(df.b)['a'].transform(max)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"pandas/core/groupby.py\", line 2422, in transform\n    return self._transform_fast(cyfunc)\n  File \"pandas/core/groupby.py\", line 2463, in _transform_fast\n    return self._set_result_index_ordered(Series(values))\n  File \"pandas/core/groupby.py\", line 498, in _set_result_index_ordered\n    result.index = self.obj.index\n  File \"pandas/core/generic.py\", line 1997, in __setattr__\n    return object.__setattr__(self, name, value)\n  File \"pandas/src/properties.pyx\", line 65, in pandas.lib.AxisProperty.__set__ (pandas/lib.c:41301)\n    obj._set_axis(self.axis, value)\n  File \"pandas/core/series.py\", line 273, in _set_axis\n    self._data.set_axis(axis, labels)\n  File \"pandas/core/internals.py\", line 2219, in set_axis\n    'new values have %d elements' % (old_len, new_len))\nValueError: Length mismatch: Expected axis has 9 elements, new values have 10 elements\n```\n\nFor a `DataFrame`, the missing value gets filled in with what looks like an uninitialized value from `np.empty_like`:\n\n```\n>>> df.groupby(df.b).transform(max)\n   a\n0  1\n1  1\n2  2\n3  3\n4 -1\n5  6\n6  6\n7  9\n8  9\n9  9\n```\n\nIt seems like either it should fill in the missing values with `NaN` (which might require a change of dtype), or just drop those rows from the result (which requires the shape to change). Either solution has the potential to surprise.\n"},{"labels":["api",null],"text":"Hello,\n\nI noticed in http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html#pandas.read_excel\nthat `read_excel` can have a parameter `sheetname`set to `None` or a to a `list`\n\nIn such a case a dictionary of `DataFrame` is returned.\n\nMaybe it should output an `OrderedDict` of `DataFrame` to preserve sheets order.\n\nKind regards\n"},{"labels":["api",null],"text":"from [SO](http://stackoverflow.com/questions/29709918/pandas-and-category-replacement/29712287#29712287)\n- the `.add_categories` should be able to take an `Index`/ndarray, ATM it must be converted to a list\n- should be a keyword for something like: add any additional categories that I am passing to you (even if the current ones are duplicates, just add these to the end). e.g.\n\n```\nIn [147]: s = pd.Categorical(list('aabbcd'))\n\nIn [148]: s2 = list('aabbcdefg')\n\nIn [149]: s\nOut[149]: \n[a, a, b, b, c, d]\nCategories (4, object): [a, b, c, d]\n\nIn [150]: s.add_categories(s.categories.sym_diff(Index(s2)).tolist())\nOut[150]: \n[a, a, b, b, c, d]\nCategories (7, object): [a, b, c, d, e, f, g]\n```\n\nI would ideally just like to say:\n\n```\ns.add_categories(s2, take_new=True)\n```\n\n(maybe not the best keyword, but something like this)\n"},{"labels":["api",null,null],"text":"A lot of other projects that use pandas will (like to) use pandas testing functionality like `assert_frame_equal` in their test suite. Although the pandas testing functions are available in the namespace (#6188), they are not really 'officially' labeled as public API that other projects can use (and rely upon). \nNumpy has a similar submodule  `numpy.testing` (http://docs.scipy.org/doc/numpy/reference/routines.testing.html)\n\nSome things we could do:\n- make a selection of the functions in `util.testing` that we want to label as public\n- add this list somewhere to the docs\n- write docstrings for these public ones (the other could use that as well of course ..)\n- add some tests for the public API\n- I would also import them into a `pandas.testing` module, so it is this one we can publicly advertise (and users are less tempted to use other non-public functions in the `pandas.util.testing` namespace)\n"},{"labels":["api"],"text":"I think it is useful to simply add one row specifying a loc.\n"},{"labels":["api",null],"text":"As mentioned in #6552 there is an open question about whether reindex should keep the index name if the new index has no name.\n\nCurrent Behavior:\n- reindex keeps index name if the argument is a list or an ndarray\n- replaces the index name if the argument is an index, even if the new index does not have a new name.\n\nRelated Issues:\n#9854\n"},{"labels":["api",null,null],"text":"Related to #9802. I think these should return sparse.\n\n```\nimport pandas as pd\n\nss = pd.SparseSeries([1, 0, 1])\ntype(ss.to_frame())\n# <class 'pandas.core.frame.DataFrame'>\n\nindex = pd.MultiIndex.from_tuples([(0, 0), (0, 1), (1, 1)])\nsdf = pd.SparseDataFrame([[1, 0, 1], [0, 0, 0], [1, 0, 0]], index=index)\ntype(sdf.to_panel())\n# <class 'pandas.core.panel.Panel'>\n```\n"},{"labels":["api",null],"text":"xref #9667 Now we also have string methods available on the index, the possible options for `return_type` ('series', 'frame') are a bit confusing I think: it will be confusing for users to get a Index back even if he/she supplies `return_type='series'` on a Index.str.split, or to get a series back with `Series.str.split(.., return_type='index')`? \n\nPossible ways to make this a better API:\n\n1) An `expand` keyword (or another name), that indicates for `False`: give same dimension back (so for series/index keep it a series/index), and for `True`: expand series to dataframe. \n  This would then be a duplicate for `return_type` of course. But the `return_type` was only introduced in 0.15.1 (and for `partition` it is still in a PR), so if we want to change this: better now than later. Or has this ship sailed?\n\n2) easier solution of @jreback: `return_type='same'|'expand'` to satisfy this need? (and can be easily back-compat) -> so no need to change the name of the keyword, only the arguments.\n"},{"labels":["api",null,null],"text":"It would be nice if pandas had an exclude_time function which gives you the opposite of between_time - this can be used to filter out periods of time where you don't want to consider the data, for example.  Is this reasonable?  I can submit a PR if so\n"},{"labels":["api",null],"text":"When subsetting an ordered `pd.Categorical` object using less than/greater than on the ordered values, the less/than greater than follow lexicographical order, not categorical order.\n\nIf you create a dataframe and assign categories, you can subset:\n\n```\nIn [1]: import pandas as pd\n\nIn [2]: import numpy as np\n\nIn [3]: df = pd.DataFrame({'x':np.arange(10), 'y':list('AAAABBBCCC')})\n\nIn [4]: df.y = df.y.astype('category')\n\nIn [5]: df.ix[(df.y >= \"A\") & (df.y <= \"B\")]\nOut[5]: \n   x  y\n0  0  A\n1  1  A\n2  2  A\n3  3  A\n4  4  B\n5  5  B\n6  6  B\n```\n\nBut if you try to subset on an ordered category, it does the lexicographical order instead:\n\n```\nIn [6]: df = pd.DataFrame({'x':np.arange(10), 'y':list('AAAABBBCCC')})\n\nIn [7]: df.y = pd.Categorical(df.y, categories=['A', 'C', 'B'], ordered=True)\n\nIn [8]: df.ix[(df.y >= \"A\") & (df.y <= \"C\")]\nOut[8]: \n   x  y\n0  0  A\n1  1  A\n2  2  A\n3  3  A\n4  4  B\n5  5  B\n6  6  B\n7  7  C\n8  8  C\n9  9  C\n```\n"},{"labels":["api",null],"text":"It would make Pandas easier to teach, easier to learn, and easier to use if the sorting behavior were the same between series and dataframes. But the existing `order()` and `sort()` methods are locked into their old behaviors by all of the code that already depends on them.\n\nBut a new `sorted()` method could bring symmetry between series and dataframes for code written from now on:\n\n```\nSeries.sorted()      =>  same as existing Series.order()\nDataFrame.sorted()   =>  same as existing DataFrame.sort()\n```\n\nHaving this new pair of methods with identical conventions, where possible, would solve several different problems that learners have with Pandas today:\n- In Pandas, nearly all methods return a new object by default instead of doing modification in-place, but learners discover that `Series.sort()` is a special case.\n- In Python, a `sort()` method traditionally returns `None` and does an in-place sort, but learners have to discover that `DataFrame.sort()` violates this convention in order to match the behavior of the rest of Pandas.\n- The new-object sorter for series objects is `Series.order()` which is very difficult to discover, as nothing else in the Python ecosystem is named `order()`, and since one would normally expect an `order()` method to _tell_ you the order (ascending? descending? none?) instead of imposing a new order.\n- The standard Python name for a sort that returns a new object is `sorted()`, per the universally loved Python built-in, but learners cannot transfer this knowledge to Pandas, where that concept exists but under the two different names `Series.order()` and `DataFrame.sort()`.\n\nYes, the `ed` at the end of `sorted()` would be one character longer than `order()` and two characters longer than the current practice of `df.sort()`. But, on balance, I think that most programmers would happily cede two characters in order to be able to use the same method name when they are flipping code between handling series and handling dataframes, and happy to have the option of using the standard Python name for the concept of a non-in-place sort.\n\nI suspect that deprecating the old names would be overly disruptive at this point, and they could probably live alongside the new `sorted()` methods without much trouble — new documentation could adopt the new, consistent terminology where possible, if the Pandas developers did not want to disrupt current users of the old inconsistent names.\n"},{"labels":["api",null,null],"text":"epi.txt\n\n```\nOBS ID PERIOD TMT Y0 AGE COUNT\n1 1 1 0 11 31 5\n2 1 2 0 11 31 3\n3 1 3 0 11 31 3\n4 1 4 0 11 31 3\n5 2 1 0 11 30 3\n6 2 2 0 11 30 5\n7 2 3 0 11 30 3\n8 2 4 0 11 30 3\n9 3 1 0 6 25 2\n10 3 2 0 6 25 4\n```\n\n```\nepi = pandas.read_csv(\"./epi.datn.txt\", sep=\" \", index_col=True)\nprint(epi)\n```\n\n```\n    OBS  PERIOD  TMT  Y0  AGE  COUNT\nID                                  \n1     1       1    0  11   31      5\n1     2       2    0  11   31      3\n1     3       3    0  11   31      3\n1     4       4    0  11   31      3\n2     5       1    0  11   30      3\n2     6       2    0  11   30      5\n2     7       3    0  11   30      3\n2     8       4    0  11   30      3\n3     9       1    0   6   25      2\n3    10       2    0   6   25      4\n```\n\nI had some code that used `False` to disable reading indexes from the data, and was expecting `True` to be the opposite. Instead it read the **second** column, because True == 1 in python.\n\nI think that `True` should be explicitly handled, because this behaviour is counterintuitive given that `False` is meaningful. `False` is not the same as `index_col=0`, after all.\n"},{"labels":["api",null],"text":"When using df.plot() to creat a simple line chart from a dataframe, the expected behavior is that, if no 'grid' keyword is passed, the plot will default to the matplotlib 'axes.grid' rcParam setting to determine whether to show a grid or not.  However (in 0.16), the grid is always being shown, regardless of the matplotlib setting, unless I explicitly pass grid=False.\n\nIn general, there are several plot types where grid=True is forced, which basically breaks the consistent styling that you get from matplotlib rcparams or another package like Seaborn.  I can understand forcing a default grid=False for the type of plots where a grid doesn't make sense, but forcing grid=True, overriding the configured defaults, for things like boxplots seems a bit counter intuitive.\n"},{"labels":["api",null,null,null],"text":"``` python\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.normal(size=(10,15)))\ndf.columns = pd.timedelta_range(start='0s',periods=15,freq='1s')\ndf.groupby(['a']*5+['b']*5)\n```\n\nfails (throwing ValueError), whereas\n\n``` python\ndf.transpose().groupby(['a']*5+['b']*5,axis=1)\n```\n\ndoesn't.\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.8.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-46-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: fr_FR.UTF-8\n\npandas: 0.16.0-28-gcb8c130\nnose: 1.3.4\nCython: 0.20.2\nnumpy: 1.9.2\nscipy: 0.14.0\nstatsmodels: None\nIPython: 3.0.0-dev\nsphinx: 1.2.2\npatsy: None\ndateutil: 2.4.1\npytz: 2015.2\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.4.3\nopenpyxl: 1.7.0\nxlrd: 0.9.2\nxlwt: 0.7.5\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: 2.5.3 (dt dec mx pq3 ext)\n```\n"},{"labels":["api"],"text":"The order of columns coming out of `.assign` is hard to predict\n\n``` Python\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'a': [1], 'b': [2]})\n\nIn [3]: df\nOut[3]: \n   a  b\n0  1  2\n\nIn [4]: df.assign(d=df.a*10, a=df.b*10, e=df.a*20)\nOut[4]: \n    a  b   e   d\n0  20  2  20  10\n```\n\nWould it be reasonable to keep all existing keys in their current location and then add the remaining keys in sorted order at the end?\n"},{"labels":["api"],"text":"```\nimport pandas as pd\n\n\nclass TestDataFrame(pd.DataFrame):\n\n    _metadata = ['testattr']\n\n    @property\n    def _constructor(self):\n        return TestDataFrame\n\ndf = TestDataFrame({0: [1, 2, 3], 1: [1, 2, 3]})\ndf.testattr = 'XXX'\ndf.testattr\n# XXX\n\n# Using list is OK\ndf.iloc[[0, 1], :].testattr\n# XXX\n\n# Using slice is NG\ndf.iloc[0:1, :].testattr\n# AttributeError: 'TestDataFrame' object has no attribute 'testattr'\n```\n"},{"labels":["api",null],"text":"`Series.to_frame` uses `pandas.DataFrame` statically as frame constructor. It is nice if `Series` has overridable property to specify frame-like constructor, like `DataFrame` has `_constructor_sliced`. \n\nHow about `Series._constructor_frame`?\n"},{"labels":["api",null,null],"text":"I find this surprising:\n\n```\nimport pandas as pd\npd.Series([pd.Timestamp('2010-01-04 00:00:00')]).astype(str)\nTypeError: cannot astype a datetimelike from [datetime64[ns]] to [|S0]\n```\n\nstr(pd.Timestamp) works though, so why astype shouldn't?\nAlso the type representation \"|S0\" is not entirely clear to me.\n"},{"labels":["api",null,null],"text":"Currently:\n\n``` python\nIn [2]: df = pd.DataFrame({'x':['a', 'a', 'b'], 'y':['j', 'k', 'j'], 'z':[0, 1, 2]})\n\nIn [3]: df.set_index(['x', 'y']).unstack()\nOut[3]:\n   z\ny  j   k\nx\na  0   1\nb  2 NaN\n```\n\nIf I want to fill with -1, i need to `fillna` and then `astype` back to `int`.  Ideally:\n\n``` python\nIn [3]: df.set_index(['x', 'y']).unstack(fill_value=-1)\nOut[3]:\n   z\ny  j   k\nx\na  0   1\nb  2  -1\n```\n"},{"labels":["api",null,null,null],"text":"I recently ran into an issue where pandas is not handling the \"<=\" logical operator when comparing dates in an interesting way.\n\n```\nimport pandas as pd\nrng = pd.date_range('1/1/2011', periods=72, freq='H')\ndf = pd.DataFrame(rng)\ndf.columns=['date']\n\ncrit = (df['date'] >= '2011-01-01') & (df['date'] <= '2011-01-02')\ndf.loc[crit, 'New_Col'] = 'First two days?'\n```\n\nAnything after 2011-01-02 00:00:00 is not included in the less than or equal to 2011-01-02. One would expect that less than or equal to mean that entire day. Is this by design, or is this a bug?\n"},{"labels":["api",null],"text":"As per http://pandas.pydata.org/pandas-docs/stable/io.html#handling-bad-lines, records with too many fields cause (depending on error_bad_lines) an exception to be thrown, or stderr to be written to.\n\nWould it be possible to add an option, defaulting to False, that gave similar warnings/errors if there are _too few_ fields in a record (compared to all the other records. The current behaviour is just to insert NaN's - but there are cases where data integrity is important so knowing that some records are missing fields is important.\n\nCheers ^\n"},{"labels":["api",null,null],"text":"``` python\nts = pd.Series(np.random.normal(size=10),index=pd.timedelta_range(start=0,periods=10,freq='1s'))\nts.foo\n```\n\nraises\n\n```\nValueError: cannot create timedelta string converter for [foo]\n```\n\nShouldn't it raise \n\n```\nAttributeError: 'Series' object has no attribute 'foo'\n```\n\n?\n\n```\npd.show_versions()\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-46-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: fr_FR.UTF-8\n\npandas: 0.16.0rc1-28-gd7d868f\nnose: 1.3.1\nCython: 0.20.1post0\nnumpy: 1.8.2\nscipy: 0.13.3\nstatsmodels: None\nIPython: 1.2.1\nsphinx: 1.2.2\npatsy: None\ndateutil: 2.4.1\npytz: 2013b\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.2.2\nmatplotlib: 1.3.1\nopenpyxl: None\nxlrd: None\nxlwt: 0.7.5\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.999\nhttplib2: 0.8\napiclient: None\nsqlalchemy: 0.9.8\npymysql: None\npsycopg2: None\n```\n"},{"labels":["api",null,null],"text":"Empirically, it seems to be the case that if one sorts a dataframe then takes a groupby, that sort is preserved within each groupby group. For example, it seems to be the case that `my_func` below always gets a df sorted by age. But I can't find any documentation on whether this is guaranteed, so I'm currently re-sorting data in `my_func` to be safe. \n\n```\ndf.sort(columns = ['users', 'age'], inplace = True)\ndf.groupby('users').apply(my_func)\n```\n\nDoes anyone know if this is a guaranteed behavior, and if so, might it be worth adding to docs?\n\nThis is particularly relevant for the `groupby().nth()` command, since if sort isn't preserved, it's not clear what it grabs!\n"},{"labels":["api"],"text":"xref #9617\n\nbtw, if someone wants to do a followup at some point to remove the .str/.cat/.dt from the **dir** on types they don't apply would be gr8. (not that hard, but requires a bit of changing), prob have to make _local_dir a bit more flexible (to add and delete)\n"},{"labels":["api",null],"text":"so that instead of always copying the returned blocks we can have the option\n"},{"labels":["api",null,null],"text":"we have a slightly odd naming convention for several index (and .dt) accessors\n\n`dayofweek`\n`dayofyear`\n\n(and adding `daysinmonth` in #9605)\n\nso these should prob be `day_of_week` \n\nwe should be able to maintain compat on these and just add the new ones.\n"},{"labels":["api",null],"text":"some examples (on Series only) in #12890  \n\nI started making an overview of the indexing semantics with http://nbviewer.ipython.org/gist/jorisvandenbossche/7889b389a21b41bc1063 (only for series/frame, not for panel)\n\nConclusion: it is mess :-)\n\n---\n#### Summary for slicing\n- Slicing with integer labels is:\n  - always _integer location based_\n  - except for a _float indexer_ where it is label based\n- Slicing with other types of labels is always label based if it is of appropriate type for the indexer.\n\nSo, you can say that the behaviour is equivalent to `.ix`, except that the behaviour for integer labels is different for integer indexers (swapped). (For `.ix`, when having an integer axis, it is always label based and no fallback to integer location based).\n#### Summary for single label\n- Indexing with a single label is **always label based**\n- But, there is fallback to integer location based, except for integer and float indexers\n#### Summary for indexing with list of labels\n- It is primarily _label based_, but:\n  - There is fallback to integer location based apart from int/float integer axis\n  - It is a pure reindex, also if no label of the list is found, you just get an all NaN series (which contrasts with loc, where at least one label should be found)\n  - String parsing for a datetime index does not seem to work\n\nThis mainly follows `ix`, apart from points 2 and 3\n#### Summary for boolean indexing\n- This is simple, it just works as expected\n#### Summary for DataFrames\n- It uses the 'information' axis (axis 1) for:\n  - single labels\n  - list of labels\n- It uses the rows (axis 0) for:\n  - slicing\n  - boolean indexing\n\nThis is as documented (only the boolean case is not explicitely documented I think).\n\nFor the rest (on the choses axis), it follows the same semantics as `[]` on a series, **but**:\n- for a list of labels, now all labels must be present (no pure reindex as with series)\n- for single labels: no fallback to integer location based for non-numeric index (but this _does_ fallback for a list of labels ...)\n\n---\n\nQuestions are here:\n- Are there things we _can_ change? (that would not be too disruptive .. maybe not?) And _want_ change?\n- How do we document this best?\n  - Now you have the \"basics\" section (http://pandas.pydata.org/pandas-docs/stable/indexing.html#basics) and the slicing section (http://pandas.pydata.org/pandas-docs/stable/indexing.html#slicing-ranges), but this does not cover all cases at all.\n"},{"labels":["api",null,null],"text":"```\ns=pd.Series(['a','a','a'])\n\ns.convert_objects(convert_numeric=True)\nOut[78]: \n0    a\n1    a\n2    a\ndtype: object\n\ns[0]=1.0\n\ns\nOut[80]: \n0    1\n1    a\n2    a\ndtype: object\n\ns.convert_objects(convert_numeric=True)\nOut[81]: \n0     1\n1   NaN\n2   NaN\ndtype: float64\n```\n\nHaving a single number changes behavior.  Makes `convert_objects` unreliable when the data type must be numeric.\n"},{"labels":["api",null,null,null],"text":"xref #2665\nxref #5440 \n\nResample appears to be use an inconsistent label convention depending on whether the target frequency is sub-daily/daily or super-daily:\n- For sub-daily/daily frequencies, `label='left'` makes labels at the timestamp corresponding to the start of each frequency bin, and `label='right'` that makes labels at that timestamp plus the frequency (at the timestamp dividing exactly dividing bins).\n- For super-daily frequencies, both labels appears to shifted minus one day to the left, so the timestamps no longer cleanly divide the frequencies.  Moreover, the default label shifts from `'left'` to `'right'`! My guess is that the default was changed here because users were confused by `label='left'` no longer falling inside the expected interval. (I guess I could check `git blame` for the details.)\n\nI found this behavior quite surprising and confusing. Is it intentional? I would like to rationalize this if possible, because this strikes me as very poor design. The behavior also couples in a weird way with the `closed` argument (see the linked issues).\n\nFrom my perspective (as someone who uses monthly and yearly data), the sub-daily/daily behavior makes sense and the super-daily behavior is a bug: there's no particular reason why it makes sense to use 1 day as an offset for frequencies with super-daily resolution.\n\nCC @Cd48 @kdebrab\n\n---\n\nHere's my test script:\n\n``` python\nfor orig_freq, target_freq in [('20s', '1min'), ('20min', '1H'), ('10H', '1D'),\n                               ('3D', '10D'), ('10D', '1M'), ('1M', 'Q'), ('3M', 'A')]:\n    print '%s -> %s:' % (orig_freq, target_freq)\n    ind = pd.date_range('2000-01-01', freq=orig_freq, periods=10)\n    s = pd.Series(np.arange(10), ind)\n    print 'default', s.resample(target_freq, how='first').index[0]\n    print 'left', s.resample(target_freq, label='left', how='first').index[0]\n    print 'right', s.resample(target_freq, label='right', how='first').index[0]\n```\n\n```\n20s -> 1min:\ndefault 2000-01-01 00:00:00\nleft 2000-01-01 00:00:00\nright 2000-01-01 00:01:00\n20min -> 1H:\ndefault 2000-01-01 00:00:00\nleft 2000-01-01 00:00:00\nright 2000-01-01 01:00:00\n10H -> 1D:\ndefault 2000-01-01 00:00:00\nleft 2000-01-01 00:00:00\nright 2000-01-02 00:00:00\n3D -> 10D:\ndefault 2000-01-01 00:00:00\nleft 2000-01-01 00:00:00\nright 2000-01-11 00:00:00\n10D -> 1M:\ndefault 2000-01-31 00:00:00\nleft 1999-12-31 00:00:00\nright 2000-01-31 00:00:00\n1M -> Q:\ndefault 2000-03-31 00:00:00\nleft 1999-12-31 00:00:00\nright 2000-03-31 00:00:00\n3M -> A:\ndefault 2000-12-31 00:00:00\nleft 1999-12-31 00:00:00\nright 2000-12-31 00:00:00\n```\n"},{"labels":["api",null],"text":"See http://stackoverflow.com/questions/28833074/aggregate-group-with-multi-level-columns\n\nSo we want to have some built-in functionality for this?\n\nThe example:\n\n```\nimport itertools\nimport pandas as pd\n\nlev1 = ['foo', 'bar', 'baz']\nlev2 = list('abc')\n\nn = 6\n\ndf = pd.DataFrame({k: np.random.randn(n) for k in itertools.product(lev1,lev2)}, \n                  index=pd.DatetimeIndex(start='2015-01-01', periods=n, freq='11D'))\n```\n\n```\n             bar               baz               foo            \n               a     b     c     a     b     c     a     b     c\n2015-01-01 -1.11  2.12 -1.00  0.18  0.14  1.24  0.73  0.06  3.66\n2015-01-12 -1.43  0.75  0.38  0.04 -0.33 -0.42  1.00 -1.63 -1.35\n2015-01-23  0.01 -1.70 -1.39  0.59 -1.10 -1.17 -1.51 -0.54 -1.11\n2015-02-03  0.93  0.70 -0.12  1.07 -0.97 -0.45 -0.19  0.11 -0.79\n2015-02-14  0.30  0.49  0.60 -0.28 -0.38  1.11  0.15  0.78 -0.58\n2015-02-25 -0.26  0.51  0.82  0.05 -1.45  0.14  0.53 -0.33 -1.35\n```\n\nThe question is here if it should be possible in `groupby().aggregate()` to specify that you want to apply a function to all columns of a certain level label. \nE.g. `df.groupby(pd.TimeGrouper('MS')).aggregate({'bar': np.sum, 'baz': np.mean, 'foo': np.min})` does not work at the moment.\n\nOr does this lead to far?\n"},{"labels":["api",null],"text":"The docstring of `to_series` says for `keep_tz=True`: _\"If the timezone is not set or is UTC, the resulting Series will have a datetime64[ns] dtype. Otherwise the Series will have an object dtype.\"_\n\nhttp://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.to_series.html\n\nBut is seems that also for UTC, `keep_tz=True` does the conversion to object/Timestamp?\n\n```\nIn [1]: idx = pd.date_range('1/1/2000', periods=20, freq='1H', tz='UTC')\n\nIn [2]: idx\nOut[2]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2000-01-01 00:00:00+00:00, ..., 2000-01-01 19:00:00+00:00]\nLength: 20, Freq: H, Timezone: UTC\n\nIn [3]: idx.to_series()[0]\nOut[3]: Timestamp('2000-01-01 00:00:00')\n\nIn [4]: idx.to_series(keep_tz=True)[0]\nOut[4]: Timestamp('2000-01-01 00:00:00+0000', tz='UTC', offset='H')\n```\n"},{"labels":["api",null],"text":"So I have a lot of csv's which are clean till say 7 columns and have no missing values but have strings at random places starting after column 7.\n\nI know it is clean till only 7. So, when I say usecols and list the 7 columns, I want it to ignore the other columns, probably truncate the remaining parts in the row when reading too. Shouldn't that be the functionality?\nI don't want to skip over bad lines.\n\nIs there a way in which I can force pandas to only read 7 columns and expect 7 rows while reading and hence not raise an exception? \n\nAnother method is to use names = range(35), an arbitrarily large number. But then I lose the real headers in my file and can't say what they are talking about. These columns are not fixed.\n\nedit: It's my first issue report in a huge python package. Please bear if I didn't follow any protocol.\n"},{"labels":["api",null,null,null],"text":"apologies if this is a manifest of another issue, though I couldn't find one after some searching.\n\nthe following behavior can be seen in 0.14.1 and 0.15.2:\n\n```\npt = pd.Panel({'a': {'b': {'c': 1, 'd': 1}, 'e': {'c': 2, 'd': 2}}})\npt2 = pd.Panel({'a': {'b': {'c': 3, 'd': 5}, 'e': {'c': 4, 'd': 6}}})\npt.a, pt2.a\nmask = (pt.a > 1) & (pt2.a > 4)\npt.a[mask] = pt2.a[mask]\npt.loc[['a'],:,'e']\npt.loc['a',:,'e']\n```\n\nthe pt.loc[['a'],:,'e'] shows the original 2 while pt.loc['a',:,'e'] shows the updated 6 from pt2.a\n\nis there a better way to do this, basically i'd like to set the values of pt.a to that of pt2.a based on some mask.  any direction would be much appreciated, thanks!\n"},{"labels":["api",null,null,null],"text":"groupby-apply workflows are important pandas idioms. Here's a brief example grouping on a named DataFrame column:\n\n```\n>>> df = pd.DataFrame({'key': [1, 1, 1, 2, 2, 2, 3, 3, 3], 'value': range(9)})\n>>> result = df.groupby('key').apply(lambda x: x['key'])\n>>> result\nkey   \n1    0    1\n     1    1\n     2    1\n2    3    2\n     4    2\n     5    2\n3    6    3\n     7    3\n     8    3\nName: key, dtype: int64\n```\n\nAn important highlight of this example is the ability to reference the grouped value -- eg, `x['key']` -- inside the applied function.\n\npandas also supports grouping on arbitrary mapping functions, iterables, and lots of other objects. In these cases, the grouped value is not represented as a named column in the DataFrame. Thus, when using apply(...), there is no apparent way to access the group key value. The only alternative is to use a (slow) for-loop solution as in:\n\n```\nfoo = lambda _k, _g: ...\ngrouped = df.groupby(grouper)\nresult_iter = (foo(key, group) for key, group in grouped) \nkey_iter = (key for key, group in grouped)\npd.DataFrame.from_records(result_iter, index=key_iter)\n```\n\nIMHO, the ability to access the grouped value in an idiomatic way from within the applied function is ergonomically important; the groupby-apply idiom is at best partially realized without it.\n"},{"labels":["api",null],"text":"Hi all,\nI don't know if this could be an enhancement but: why do not use string values such as `'index'` or `'columns'` in the axis parameter for `DataFrame.quantile()` function? Such as in `DataFrame.div()`..\nI think that it would be more straightforward to use.\nThanks\n\nFrancesco  \n"},{"labels":["api",null,null],"text":"Pandas resample bugs when upsampling a time serie with same size splits :\n\nFor instance, I have a time serie of **size 10**:\n\n```\nrng = pd.date_range('20130101',periods=10,freq='T')\nts=pd.Series(np.random.randn(len(rng)), index=rng)\n```\n\n`print(ts)`\n\n```\n2013-01-01 00:00:00   -1.811999\n2013-01-01 00:01:00   -0.890837\n2013-01-01 00:02:00   -0.363520\n2013-01-01 00:03:00   -0.026245\n2013-01-01 00:04:00    1.515072\n2013-01-01 00:05:00    0.920129\n2013-01-01 00:06:00   -0.125954\n2013-01-01 00:07:00    0.588933\n2013-01-01 00:08:00   -1.278408\n2013-01-01 00:09:00   -0.172525\nFreq: T, dtype: float64\n```\n\nWhen trying to resample in **N > 10** parts it doesn't work:\n\n```\nlength = 11\ntimeSpan = (ts.index[-1]-ts.index[0]+timedelta(minutes=1))\nrule = int(timeSpan.total_seconds()/length)\ntsNew=ts.resample(str(rule)+\"S\")\n```\n\n`print(tsNew)`\n\n```\n2013-01-01 00:00:00    1.845181\n2013-01-01 00:00:54         NaN\n2013-01-01 00:01:48         NaN\n2013-01-01 00:02:42         NaN\n2013-01-01 00:03:36         NaN\n2013-01-01 00:04:30         NaN\n2013-01-01 00:05:24         NaN\n2013-01-01 00:06:18         NaN\n2013-01-01 00:07:12         NaN\n2013-01-01 00:08:06         NaN\n2013-01-01 00:09:00   -0.997419\nFreq: 54S, dtype: float64\n```\n\nNote: here is my versions:\n`pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.2.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.1.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.15.2\nnose: 1.3.4\nCython: 0.21\nnumpy: 1.9.1\nscipy: 0.15.1\nstatsmodels: 0.5.0\nIPython: 2.3.1\nsphinx: 1.2.3\npatsy: 0.3.0\ndateutil: 2.1\npytz: 2014.9\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.0\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.5.7\nlxml: 3.4.0\nbs4: 4.3.2\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.7\npymysql: 0.6.3.None\npsycopg2: None\n```\n\nThank you for your help\n"},{"labels":["api",null,null,null],"text":"So this can be fixed by inferring after the set. We need to do this because we first set the value to a null-type (nan/NaT), then set the value. This works fine for datetime/timedelta/floats/strings, but not for integers which get set as `float`.\n\nHowever, this _can_ be an expensive operation as potentially the entire column needs to be scaned for nulls.\n\n```\nIn [4]: df = pd.DataFrame()\n\nIn [5]: df.loc[1,'foo'] = 2\n\nIn [6]: df\nOut[6]: \n   foo\n1    2\n\nIn [7]: df.dtypes\nOut[7]: \nfoo    float64\ndtype: object\n```\n"},{"labels":["api",null],"text":"I already mentioned this in https://github.com/pydata/pandas/issues/9466 but I think it deserves its own bug report:\n\n```\nIn [2]: s = pd.Series([1, 2, 3], index=(1,1,2))\n\nIn [3]: s\nOut[3]: \n1    1\n1    2\n2    3\ndtype: int64\n\nIn [4]: s.loc[1]\nOut[4]: \n1    1\n1    2\ndtype: int64\n\nIn [5]: type(s.loc[1])\nOut[5]: pandas.core.series.Series\n\nIn [6]: s.loc[2]\nOut[6]: 3\n\nIn [7]: type(s.loc[2])\nOut[7]: numpy.int64\n```\n\nQuoting https://github.com/pydata/pandas/issues/5678 , _\"You are selecting out of a duplicated index Series. You could argue that you should get back another Series\"_\n\nI really think life would be easier if s.loc[2] returned a Series of length one (and DataFrames and Panels behaved similarly). One is assumed to know (and can check in O(1)) if an index is unique, but maybe not if a _given label_ is unique.\n\nWith higher dimensions structures it's even more messy because if e.g. .loc[lab_a, lab_b, lab_c] yields a lower dimension structure, but still a pandas structure, you have to find out which dimensions have been lost/kept (i.e. which of the labels were duplicates).\n\nI don't think I have the skills to propose a PR, but I would volunteer to fix the broken tests.\n"},{"labels":["api",null,null],"text":"Recently, I've been working on adding a 'nearest' method to reindexing: https://github.com/pydata/pandas/pull/9258\n\nIt occurs to me that we could easily extend reindexing/get_indexer methods to work with unordered indexes if we were willing to do a sort operation on the index if necessary. This would probably entail saving the sorted result on the parent index, similarly to how get_indexer is currently supported on MultiIndex by creating a tuple index internally.\n\nI think this would be a nice usability gain over the current implementation, and not be _too_ surprising. Sorting indexes (once) is pretty fast, for anything up to millions of rows.\n\nThoughts?\n"},{"labels":["api",null],"text":"``` python\nimport pandas as pd\nfrom datetime import datetime\npd.datetools.thisYearBegin(datetime(2015,2,1))\n```\n\nThis returns Timestamp('2016-01-01 00:00:00'), but should return 201**5**-01-01.  Should thisYearBegin() just be removed?  I don't think it's documented.\n"},{"labels":["api",null],"text":"This is a repost of #9359 with some new fancy code that handles lots of different kinds of edge cases. Please take a look at the code:\n\nhttps://gist.github.com/cloudformdesign/13278001b1a0b0cde647\n\nBasically this allows automatic loading of nested dictionaries, whether those dictionaries are a \"list of dictionaries\" or a \"dictionary of lists\", it handles them as you would expect them to be handled by automatically creating a MultiIndex.\n\nMy hope is that the `dataframe_dict` function be used instead of the standard `DataFrame.from_dict` method. \n\nThis is not an edge case -- this is a use case that arrives frequently for people who use mongo or who store data in nested dictionaries.\n\nIf you agree that it is functionality worth adding, I can make the changes to pandas and make a pull request.\n"},{"labels":["api",null],"text":"It's sometimes convenient to assign/replace the name of a Series, especially if the result is going to be a DataFrame column, for example:\n\n```\ndf.groupby([]).size().set_name('GRP_SIZE').reset_index()\n```\n\nThis should be as simple as:\n\n```\ndef set_name(self, name):\n    self.name = name\n    return self\n```\n\nComments?\n"},{"labels":["api",null,null],"text":"Now it is not possible to choose method when using corrwith() function, which defaults to pearson (I assume). Would be great to see the same methods that are allowed in corr().\n"},{"labels":["api",null],"text":"This should be a pretty straightforward followup on #9258 that will allow us to also support `align` with `method='nearest'`.\n\nfillna internally relies on interpolate which already supports `method='nearest' so this _may_ just work (it will needs lots of tests).\n"},{"labels":["api",null,null],"text":"I am testing the equivalence of two large DataFrames (9084x367).  The two are the same up to 1x10-13 but when `np.array_equal` fails there is a much slower code path (comparing these two frames takes upwards of 20 seconds).  If I'm not mistaken, if the arrays aren't equivalent it does a more complicated version of `np.allclose`.  I think a good intermediate step would be to check for array equivalence and then as a the second step call `np.allclose`--or maybe just do this on the outset.  If that fails, which it will if there are any NaNs or if the tolerance is not met, then it will use the current logic.  Or we could use `np.isclose` to consider NaNs as equivalent.\n\nhttps://github.com/pydata/pandas/blob/master/pandas/src/testing.pyx#L85\n"},{"labels":["api",null,null],"text":"Related to #9448, using the following example:\n\n```\nIn [143]: df = pd.DataFrame(np.random.randn(5,2), columns=['A', 'B'])\n\nIn [144]: df\nOut[144]:\n          A         B\n0 -1.151315 -2.174735\n1 -1.031375 -1.957344\n2 -0.619368 -0.835133\n3  2.562381 -0.294245\n4  2.432353  0.032805\n```\n\n`to_clipboard` is said to pass the `kwargs` to `to_csv` (see docs: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_clipboard.html). However, that seems not be fully true for the `float_format` kwarg:\n\n```\ndf.to_clipboard(float_format='%.3f')\n->\n    A   B\n0   -1.151  -2.175\n1   -1.031  -1.957\n2   -0.619  -0.835\n3   2.562   -0.294\n4   2.432   0.033\n```\n\nNow using a callable:\n\n```\ndf.to_clipboard(float_format='{:.3f}'.format)\n->\n       A      B\n0 -1.151 -2.175\n1 -1.031 -1.957\n2 -0.619 -0.835\n3  2.562 -0.294\n4  2.432  0.033\n```\n\nbut this does not work with `to_csv`:\n\n```\nIn [150]: df.to_csv(float_format='{:.3f}'.format)\n---------------------------------------------------------------------------\nTypeError: unsupported operand type(s) for %: 'builtin_function_or_method' and 'float'\n```\n\nFurthermore, when using a function as `float_format`, the result is no longer pastable in excel, even with `excel=True` (the default).\n"},{"labels":["api",null],"text":"### Summary\r\n\r\nThe question is what the sum of a Series of all NaNs should return (which is equivalent to an empty Series after skipping the NaNs): NaN or 0?\r\n\r\n```\r\nIn [1]: s = Series([np.nan])                 \r\n\r\nIn [2]: s.sum(skipna=True)  # skipping NaNs is the default\r\nOut[2]: nan or 0     <---- DISCUSSION POINT\r\n\r\nIn [3]: s.sum(skipna=False)\r\nOut[3]: nan\r\n```\r\n\r\nThe reason this is a discussion point has the following cause: the internal nansum implementation of pandas returns NaN. But, when bottleneck is installed, pandas will use bottlenecks implementation of nansum, which returns 0 (for the versions >= 1.0). \r\nBottleneck changed the behaviour from returning NaN to returning 0 to model it after numpy's nansum function.\r\n\r\nThis has the very annoying consequence that depending on whether bottleneck is installed or not (which is only an optional dependency), you get a different behaviour.\r\n\r\nSo the decision we need to make, is to either:\r\n\r\n- adapt pandas internal implementation to return 0, so in all cases 0 is returned for all NaN/empty series.\r\n- workaround bottlenecks behaviour or not use it for nansum, in order to consistently return NaN instead of 0\r\n- choose one of both above as the default, but have an option to switch behaviour\r\n\r\n\r\n\r\n\r\n---\r\n\r\nOriginal title: **nansum in bottleneck 1.0 will return 0 for all NaN arrays instead of NaN**\r\n\r\nxref https://github.com/kwgoodman/bottleneck/issues/96\r\nxref https://github.com/pydata/pandas/issues/9421\r\n- [ ] Tests are turned off for bottleneck >1.0 (xref https://github.com/pydata/pandas/pull/10986)\r\n\r\nThis matches a change from numpy 1.8 -> 1.9.\r\n\r\nWe should address this for pandas 0.16.\r\n\r\nShould we work around the new behavior (probably the simplest choice) or change nansum in pandas?\r\n"},{"labels":["api",null],"text":"i recently bumped an unexpected issue with pandas rolling funcs. rolling_quantile for example:\n\n```\n>> row = 10\n>> col = 5\n>> idx = pd.date_range(20100101,periods=row,freq='B')\n>> a = pd.DataFrame(np.random.rand(row*col).reshape((row,-1)),index=idx)\n>> a\n                   0           1           2           3           4\n2010-01-01  0.341434    0.497274    0.596341    0.259909    0.872207\n2010-01-04  0.222653    0.056723    0.064019    0.936307    0.785647\n2010-01-05  0.179067    0.647165    0.931266    0.557698    0.713282\n2010-01-06  0.049766    0.259756    0.945736    0.380948    0.282667\n2010-01-07  0.385036    0.517609    0.575958    0.050758    0.850735\n2010-01-08  0.628169    0.510453    0.325973    0.263361    0.444959\n2010-01-11  0.099133    0.976571    0.602235    0.181185    0.506316\n2010-01-12  0.987344    0.902289    0.080000    0.254695    0.753325\n2010-01-13  0.759198    0.014548    0.139858    0.822900    0.251972\n2010-01-14  0.404149    0.349788    0.038714    0.280568    0.197865\n\n>> a.quantile([0.25,0.5,0.75],axis=0)\n               0           1           2           3           4\n0.25    0.189963    0.282264    0.094964    0.255999    0.323240\n0.50    0.363235    0.503864    0.450966    0.271964    0.609799\n0.75    0.572164    0.614776    0.600761    0.513510    0.777567\n\n>> np.percentile(a,[25,50,75],axis=0)\n[array([ 0.18996316,  0.28226404,  0.09496441,  0.25599853,  0.32323997]),\n array([ 0.36323529,  0.50386356,  0.45096554,  0.27196429,  0.60979881]),\n array([ 0.57216415,  0.61477607,  0.6007611 ,  0.51351021,  0.7775667 ])]\n\n>> pd.rolling_quantile(a,row,0.25).tail(1)\n                   0           1       2           3           4\n2010-01-14  0.179067    0.259756    0.08    0.254695    0.282667\n```\n\nlooks like pandas.DataFrame.quantile member func is consistent with the numpy.percentile func. however the pandas.rolling_quantile func returns diff results. reduce the row number to 5, the problem will be gone (all three methods return the same results). any thoughts?\n\nps: i also tested rolling_std func which will \"randomly\" generate errors with 10^-7 ~ 10^-8 scales (compared to pandas.DataFrame std member func or numpy/scipy std funcs which could limit the error close to np.spacing(1) level) for long (row-wise) pandas.DataFrames\n\npython environment:\n\npython 3.4.2\ncython 0.21.1\nnumpy 1.8.2\nscipy 0.14.0\npandas 0.15.1\nstatsmodels 0.6.0\n"},{"labels":["api",null],"text":"Ravel doesn't seem to flatten lists in a series.\n\n``` python\n\nimport pandas as pd\ndf = pd.DataFrame({ 'data':[[1,2],3]})\ndf['data'].ravel()\n\nOut[3]: array([[1, 2], 3], dtype=object)\n\ndf.data.values.ravel()\n\nOut[4]: array([[1, 2], 3], dtype=object)\n```\n\nOr, in iPython Notebook\n\n![screen shot 2015-02-03 at 1 55 54 pm](https://cloud.githubusercontent.com/assets/925757/6026920/799ed948-abac-11e4-81d6-38c99e55a4cd.png)\n\nSeen this issue on two versions:\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.0.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.15.2\nnose: 1.3.4\nCython: None\nnumpy: 1.9.1\nscipy: None\nstatsmodels: None\nIPython: 2.4.0\nsphinx: None\npatsy: None\ndateutil: 2.4.0\npytz: 2014.10\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.4.2\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.3.2\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.8\npymysql: None\npsycopg2: 2.5.4 (dt dec pq3 ext)\n\nand \n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.9.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.1.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.14.1\nnose: 1.3.4\nCython: 0.21\nnumpy: 1.9.1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.2.0\nsphinx: 1.2.3\npatsy: 0.3.0\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2014.7\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.4.0\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.7\nlxml: 3.4.0\nbs4: 4.3.2\nhtml5lib: None\nhttplib2: 0.8\napiclient: 1.2\nrpy2: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: None\n"},{"labels":["api",null,null,null,null],"text":"As described in https://github.com/pydata/pandas/pull/9023#issuecomment-72707057, the way `DataFrame.stack()` and `DataFrame.unstack()` treat `NaN` indices is rather odd/inconsistent. Despite passing  `test_unstack_nan_index()` in `test_frame.py`, I observe the following (this is from 0.15.2, but I think it's unchanged in the current master for 0.16.0):\n\n```\nIn [140]: df = pd.DataFrame(np.arange(4).reshape(2, 2),\n                            columns=pd.MultiIndex.from_tuples([('A','a'), ('B', 'b')],\n                                                              names=['Upper', 'Lower']),\n                            index=Index([0, 1], name='Num'), dtype=np.float64)\n\nIn [141]: df_nan = pd.DataFrame(np.arange(4).reshape(2, 2),\n                                columns=pd.MultiIndex.from_tuples([('A',np.nan), ('B', 'b')],\n                                                                  names=['Upper', 'Lower']),\n                                index=Index([0, 1], name='Num'), dtype=np.float64)\n\nIn [148]: df\nOut[148]:\nUpper  A  B\nLower  a  b\nNum\n0      0  1\n1      2  3\n\nIn [149]: df.stack()\nOut[149]:\nUpper       A   B\nNum Lower\n0   a       0 NaN\n    b     NaN   1\n1   a       2 NaN\n    b     NaN   3\n\nIn [150]: df.T.unstack().T\nOut[150]:\nUpper       A   B\nNum Lower\n0   a       0 NaN\n    b     NaN   1\n1   a       2 NaN\n    b     NaN   3\n\nIn [151]: df_nan\nOut[151]:\nUpper   A  B\nLower NaN  b\nNum\n0       0  1\n1       2  3\n\nIn [152]: df_nan.stack()\nOut[152]:\nUpper      A  B\nNum Lower\n0   NaN    0  1\n    b      0  1\n1   NaN    2  3\n    b      2  3\n\nIn [153]: df_nan.T.unstack().T\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-153-edbcaeb64f64> in <module>()\n----> 1 df_nan.T.unstack().T\n\nC:\\Python34\\lib\\site-packages\\pandas\\core\\frame.py in unstack(self, level)\n   3486         \"\"\"\n   3487         from pandas.core.reshape import unstack\n-> 3488         return unstack(self, level)\n   3489\n   3490     #----------------------------------------------------------------------\n\nC:\\Python34\\lib\\site-packages\\pandas\\core\\reshape.py in unstack(obj, level)\n    439     if isinstance(obj, DataFrame):\n    440         if isinstance(obj.index, MultiIndex):\n--> 441             return _unstack_frame(obj, level)\n    442         else:\n    443             return obj.T.stack(dropna=False)\n\nC:\\Python34\\lib\\site-packages\\pandas\\core\\reshape.py in _unstack_frame(obj, level)\n    479     else:\n    480         unstacker = _Unstacker(obj.values, obj.index, level=level,\n--> 481                                value_columns=obj.columns)\n    482         return unstacker.get_result()\n    483\n\nC:\\Python34\\lib\\site-packages\\pandas\\core\\reshape.py in __init__(self, values, index, level, value_columns)\n    101\n    102         self._make_sorted_values_labels()\n--> 103         self._make_selectors()\n    104\n    105     def _make_sorted_values_labels(self):\n\nC:\\Python34\\lib\\site-packages\\pandas\\core\\reshape.py in _make_selectors(self)\n    143\n    144         if mask.sum() < len(self.index):\n--> 145             raise ValueError('Index contains duplicate entries, '\n    146                              'cannot reshape')\n    147\n\nValueError: Index contains duplicate entries, cannot reshape\n```\n"},{"labels":["api",null,null],"text":"The following two tests seem inconsistent. Why does the first test for a `KeyError` while the second tests for a `ValueError`? Also, the first error message, `Level foo not found`, doesn't seem correct.\n\n`test_index.py`:\n\n```\n    def test_duplicate_names(self):\n        self.index.names = ['foo', 'foo']\n        assertRaisesRegexp(KeyError, 'Level foo not found',\n                           self.index._get_level_number, 'foo')\n```\n\n`test_frame.py`:\n\n```\n    def test_unstack_non_unique_index_names(self):\n        idx = MultiIndex.from_tuples([('a', 'b'), ('c', 'd')],\n                                     names=['c1', 'c1'])\n        df = DataFrame([1, 2], index=idx)\n        with tm.assertRaises(ValueError):\n            df.unstack('c1')\n\n        with tm.assertRaises(ValueError):\n            df.T.stack('c1')\n```\n"},{"labels":["api",null,null],"text":"For visualization/testing purposes, I'm often interested in looking at the first example group(s) from a groupby operation.\n\nIs there a convenient shortcut for this? The best I could come up with is `pd.concat([x for _, x in itertools.islice(group, 3)])` which seemed awkward to me.\n\nNote that this is a _different_ use-case from `.first()`/`.head()`, which returns the first example from each group -- here I want full examples of the first few groups.\n"},{"labels":["api",null,null,null],"text":"Not sure if this is a bug or not.\n\n``` python\nimport datetime\n\nIn [24]: s = pd.Series(pd.to_datetime(['2015-01-01', '2015-02-02']))\n\nIn [25]: s\nOut[25]:\n0   2015-01-01\n1   2015-02-02\ndtype: datetime64[ns]\n\nIn [26]: s.diff() / datetime.timedelta(1)\nOut[26]:\n0   NaN\n1    32\ndtype: float64\n\nIn [27]: s.diff().div(datetime.timedelta(1))\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-27-1197779902ae> in <module>()\n----> 1 s.diff().div(datetime.timedelta(1))\n\nC:\\Miniconda\\lib\\site-packages\\pandas\\core\\ops.pyc in flex_wrapper(self, other,\nlevel, fill_value, axis)\n    716                                level=level, fill_value=fill_value)\n    717         else:\n--> 718             return self._constructor(op(self.values, other),\n    719                                      self.index).__finalize__(self)\n    720\n\nTypeError: ufunc true_divide cannot use operands with types dtype('<m8[ns]') and\n dtype('O')\n```\n\nWorks with our Timedelta though, as suggested by the error message:\n\n``` python\nIn [28]: s.diff().div(pd.Timedelta(1, unit='D'))\nOut[28]:\n0   NaN\n1    32\ndtype: float64\n```\n\nIt at least seems strange that `/` worked while `.div` didn't.\n\nThis is the released 0.15.2, and python 2.\n"},{"labels":["api",null],"text":"Hi All,\n\nAt the moment, the default behavior for the HDF append() function ( docs:  http://pandas.pydata.org/pandas-docs/stable/generated/pandas.HDFStore.append.html?highlight=append#pandas.HDFStore.append ) is to silently drop all rows that are all NaN except for the index. \n\nAs I understand it from a PyData exchange with Jeff, the reason is that people working with panels often have sparse datasets, so this is a very reasonable default.\n\nHowever, while I appreciate the appeal for time-series analysis, I think this is a dangerous default. The main reason is that the assumption is that if an index has a value but the columns do not, there is no meaningful data in the row. But while true in a time series context -- where it's easy to reconstruct the index values that are dropped -- if indexes contain information like userIDs, sensor codes, place names, etc., the index itself is meaningful, and not easy to reconstruct. Thus the default behavior is potentially deleting user data without a warning. \n\nGiven the trade-off between a default that may lead to inefficient storage (dropna = False) and one that potentially erases user data (dropna = True), I think we should error on the side of data preservation. \n"},{"labels":["api",null],"text":"By sorting the index under the hood, we could support label based slicing even for unordered indexes.\n\nDoes this seem like a good idea? Here is what the implementation would look like, roughly:\n\n``` python\ndef slice_indexer_anyorder(idx, start=None, end=None):\n    # preprocessing takes time O(n * log(n)) and space 2 * n\n    # (these variables could be cached on the index)\n    ordering = idx.argsort()\n    reordered = idx.take(ordering)\n    # query complexity is O(log(n) + k * log(k)),\n    # where k is the size of the result\n    start, end = reordered.slice_locs(start, end)\n    return np.sort(ordering[start:end])\n```\n\nThe main potential for confusion here is that this _conflicts_ with the current behavior for `.loc`, which is to sort all values with labels _between_ the locations of start and end if start and end are found in the index. So adding this to `.loc` would require deprecating the existing usage.\n\nCC @immerrr @jreback @jorisvandenbossche \n"},{"labels":["api",null],"text":"Looking at the API docs, neither the function `pd.unique` nor the order of the unique values from `unique` is mentioned. I would like to:\n1. Add `pd.unique` to API > General functions > Data manipulations\n2. Note that `Series.unique()` and `unique()` return values in order of appearance\n3. Add unit tests to verify the \"order of appearance\" nature `unique` (untested directly AFAICT, though I'm sure it's relied on implicitly)\n\nAny objections? If not, I'll put together a PR.\n\nThis lack of documentation has caused some hesitation for relying on this functionality in Seaborn: https://github.com/mwaskom/seaborn/issues/361\n"},{"labels":["api",null],"text":"When grouping by several levels of a MultiIndex, groupby evaltuates all possible combinations of the groupby keys. When grouping by column name, it only evaluates what exist in the DataFrame. Also, this behavior does not exist in 0.14.1, but does in all final releases from 0.15.0 on.\n\nThis may be a new feature, not a bug, but I couldn't find anything in the [docs](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby), open or closed issues, etc. (closest was Issue #8138). If this is the intended behavior, it would be nice to have in the docs.\n\n``` python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.arange(12).reshape(-1, 3))\ndf.index = pd.MultiIndex.from_tuples([(1, 1), (1, 2), (3, 4), (5, 6)])\nidx_names = ['x', 'y']\ndf.index.names = idx_names\n\n# Adds nan's for (x, y) combinations that aren't in the data\nby_levels = df.groupby(level=idx_names).mean()\n\n# This does not add missing combinations of the groupby keys\nby_columns = df.reset_index().groupby(idx_names).mean()\n\nprint by_levels\nprint by_columns\n\n# This passes in 0.14.1, but not >=0.15.0 final\nassert by_levels.equals(by_columns)\n```\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.7.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.15.2\nnose: 1.3.4\nCython: 0.20.1\nnumpy: 1.9.1\nscipy: 0.15.1\nstatsmodels: 0.7.0.dev-161a0f8\nIPython: 2.3.0\nsphinx: 1.2.2\npatsy: 0.3.0\ndateutil: 1.5\npytz: 2014.9\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.2\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.5\nlxml: 3.3.5\nbs4: 4.3.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\n"},{"labels":["api",null,null],"text":"Shouldn't `convert_objects(convert_dates='coerce')` be coercing a series of strings to NaTs?\n\n``` python\ndf = pd.DataFrame({'date': ['a', 'b', 'c', 'd']})\ndf.convert_objects(convert_dates='coerce')\n  date\n0    a\n1    b\n2    c\n3    d\n\n```\n"},{"labels":["api",null],"text":"First time I used the .interpolate() method I thought that it receives a new index and then interpolates on it, similar to [scipy.interpolate.interp1d](http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d)\nFrom scipy web:\n\n``` python\nfrom scipy import interpolate\nx = np.arange(0, 10)\ny = np.exp(-x/3.0)\nf = interpolate.interp1d(x, y)\nxnew = np.arange(0,9, 0.1)\nynew = f(xnew)   # use interpolation function returned by `interp1d`\n```\n\nLater I saw the .reindex() method, so I understood that this role is done by .reindex(). However .reindex() is not really doing a powerful interpolation, just extending the current values using the [method](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.reindex.html) keyword.\n\nThe current way to achieve it (joining previous and new index and then using .reindex()), in version 0.15.0,\n\n``` python\nindex_joined = df.index.join(new_index, how='outer')\ndf.reindex(index=index_joined).interpolate().reindex(new_index)\n```\n\nA simpler syntax could be accepting the methods from [.interpolate()](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.interpolate.html) into the 'method' keyword of .reindex():\n\n``` python\ndf.reindex(index=new_index, method='linear')\n```\n"},{"labels":["api",null,null],"text":"I have strange results from `.equals` appearing when DataFrame is written to HDF Store and then read back:\n\n```\nimport pandas as pd\ndf = pd.DataFrame({'B':[1,2], 'A':[str('x'), str('y')]})  # str() - just to be sure this is not linked to unicode \nprint 'df:'\nprint df\ndf.to_hdf('hdf_file', 'key', format='t', mode='w')\ndf_out = pd.read_hdf('hdf_file', 'key')\nprint '\\ndf_out:'\nprint df_out\nprint '\\ndf equals df_out:', df.equals(df_out)\nprint '\\ndf_out equals df:', df_out.equals(df)\nprint '\\ndf.shape == df_out.shape:', df.shape == df_out.shape\nprint '\\narray_equivalent(df.values, df_out.values):', pd.core.common.array_equivalent(df.values, df_out.values)\nprint '\\ndf.index equals df_out.index:', df.index.equals(df_out.index)\nprint '\\ndf.columns equals df_out.columns:', df.columns.equals(df_out.columns)\nfor col in df.columns:\n    print '\\ndf.{0} equals df_out.{0}: {1}'.format(col, df[col].equals(df_out[col]))\n```\n\noutput:\n\n```\ndf:\n   A  B\n0  x  1\n1  y  2\n\ndf_out:\n   A  B\n0  x  1\n1  y  2\n\ndf equals df_out: False\n\ndf_out equals df: False\n\ndf.shape == df_out.shape: True\n\narray_equivalent(df.values, df_out.values): True\n\ndf.index equals df_out.index: True\n\ndf.columns equals df_out.columns: True\n\ndf.A equals df_out.A: True\n\ndf.B equals df_out.B: True\n```\n\nThe interesting thing is that if DataFrame is initialized with different columns \"order\" in the dictionary the results are ALL True (i.e. correct):\n\n```\ndf = pd.DataFrame({'A':[1,2], 'B':[str('x'),str('y')]})  # in the code above\n```\n\nwill give:\n\n```\ndf equals df_out: True\ndf_out equals df: True\n```\n\nI have seen similar issues ([#8437](https://github.com/pydata/pandas/issues/8437) and [#7605](https://github.com/pydata/pandas/issues/7605)), which are marked as closed, but seeing this strange results... might be something different?\n\npython 2.7.9, pandas 0.15.2\n\nMy apologies in advance for potential duplicate.\n"},{"labels":["api",null],"text":"to_frame should have more control (be explicit) over what axis becomes index and the columns in the DF.\n"},{"labels":["api",null],"text":"Hello,\n\nI'm using the pandas function \"pandas.core.groupby.GroupBy.get_group()\" to parallelize my code.\nThis function raises a \"KeyError\" exception if the passed index is not contained in the index list of the grouped DataFrame.\n\nA basic example is:\ndata = DataFrame({'ind':[0,0,2,2]})\ngb=a.groupby('ind')\ngb.get_group(1)\n\nWhich raises \"KeyError: 1\"\n\nIn order to use intensively the \"get_group()\" function in code parallelization, an option to avoid this exception would be very useful, similar to \"error_bad_lines=False\" of the \"read_csv()\" function.\n\nAn empty DataFrame could be returned by the \"get_group()\" function when this option and an invalid index are given.\n"},{"labels":["api",null],"text":"When you want to initialise a DF to a particular value you can use `data = 0.`. But this does not work in Panel\n"},{"labels":["api"],"text":"Building off of #9229, it's generally much better to create new dataframes rather than to modify existing ones. It's both more chainable and less prone to the bugs/errors associated with mutable state.\n\nSo it seems to me that a saner API design would be to make assignment operations with eval/query return a new frame rather than modify the existing one, i.e., `df.eval('x = y ** 2')` should be equivalent to `df.assign(x = lambda df: df.y ** 2)` (as currently defined in #9239).\n\nIn my experience, something like `df.query('x = 0')` is more likely intended to be equivalent to `df.query('x == 0')` rather than `df['x'] = 0`, which has the unfortunate effect of also modifying the original data. In fact, I have made this exact mistake before -- and apparently others have as well (see #8664).\n\nThoughts? The `query`/`eval` syntax is marked as experimental, so I think we have the freedom to change this.\n\nCC @TomAugspurger @cpcloud @jreback \n"},{"labels":["api",null,null,null],"text":"Apologies if this feature has been suggested before. Many of the IO functions (e.g. `read_csv`) allow use to easily specify the format for each column using a dictionary. As far as I understand, this is **not**\n possible with the regular dataframe construction, e.g:\n\n``` python\ndf = pd.DataFrame(data=data, columns=columns, dtypes={'colname1': str, 'colname2': np.int})\n```\n\n**Even better**, it would be great if one could change the `dtypes` for the dataframe columns using a similar contruction, e.g.:\n\n``` python\ndf.change_types({'colname1': str, 'colname2': np.int})\n```\n\nIs anything like this planned for already? \n"},{"labels":["api",null],"text":"Currently, pandas makes clever use of `__getitem__` to filter/mask Series and DataFrame.  At times, the current abstraction is leaky - for instance, `df[18 < df['age'] < 25]` would raise an exception, even though cascaded compare symbols in that manner are legal Python syntax.\n\nIt would be awesome to also be able to pass in a function to the `__getitem__` function that would act as a filter.  This would allow users to use all of Python's language features for filtering while still maintaining the brevity intended for these statements.  \n\nFor example:\n\n``` python\ndf[lambda row: 18 < row['age'] < 25]\n```\n\nThis would also allow lengthier filters to be defined outside of the getter square brackets:\n\n``` python\ndef age_filter(row):\n    return 18 < row['age'] < 25 if row['country'] == 'USA' else row['age'] > 13\ndf[age_filter]\n```\n\nFunctions as viable filters would also allow for chaining of filter functions without creating multiple intermediary DataFrames or Series, making optimization of cascaded filters much easier.\n"},{"labels":["api",null],"text":"1. Catalog and document all of the methods that don't currently work on sparse structures.\n2. Raise NotImplemented exceptions for these methods.\n\nWould be happy for suggestions for how to do this systematically.\n\nOriginally suggested by @jreback  in #9265.\n"},{"labels":["api",null],"text":"xref #9226 \n\nNoticing an inconsistency. pd.Timedelta doesn't allow nanoseconds in constructor but its components list till nanoseconds. Here is what I can reproduce. (used then current master on OS X 10.10.1)\n\nExample:\n\n<pre>\nPython 2.7.9 (v2.7.9:648dcafa7e5f, Dec 10 2014, 10:10:46)\n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from pandas.tslib import Timedelta\n>>> td = Timedelta(nanoseconds=1)\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nFile \"pandas/tslib.pyx\", line 1723, in pandas.tslib.Timedelta.__new__ (pandas/tslib.c:29743)\nraise ValueError(\"cannot construct a TimeDelta from the passed arguments, allowed keywords \nare \" [days, seconds, microseconds, milliseconds, minutes, hours, weeks]\n>>> td=Timedelta(seconds=1)\n>>> td.components._fields\n('days', 'hours', 'minutes', 'seconds', 'milliseconds', 'microseconds', 'nanoseconds')\n</pre>\n"},{"labels":["api",null],"text":"Continuing on my recent sparse kick...\n\nI appreciate the fact that `Series()`, `DataFrame()`, and `Panel()` all return empty versions of themselves. I think it's reasonable to expect that their sparse counterparts would behave the same way, but this is only the case for `SparseDataFrame()` (it's even [tested](https://github.com/pydata/pandas/blob/master/pandas/sparse/tests/test_sparse.py#L865)). \n\n``` python\npd.SparseSeries()\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-375-46c1881ac0a7> in <module>()\n----> 1 pd.SparseSeries()\n\nTypeError: __init__() missing 1 required positional argument: 'data'\n```\n\nand similar for `SparsePanel()`.\n\nI can submit a PR to change this behavior.\n"},{"labels":["api",null],"text":"```\nIn [1]: import io\nIn [2]: xx = io.BytesIO()\nIn [4]: from pandas import DataFrame\nIn [5]: from numpy.random import randn\nIn [6]: df = DataFrame(randn(1000000,2),columns=list('AB'))\n\n----> 1 df.to_hdf(xx,'xx')\n\n/root/pyenvs/aft_env/lib/python3.4/site-packages/pandas/core/generic.py in to_hdf(self, path_or_buf, key, **kwargs)\n    900 \n    901         from pandas.io import pytables\n--> 902         return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n    903 \n    904     def to_msgpack(self, path_or_buf=None, **kwargs):\n\n/root/pyenvs/aft_env/lib/python3.4/site-packages/pandas/io/pytables.py in to_hdf(path_or_buf, key, value, mode, complevel, complib, append, **kwargs)\n    267             f(store)\n    268     else:\n--> 269         f(path_or_buf)\n    270 \n    271 \n\n/root/pyenvs/aft_env/lib/python3.4/site-packages/pandas/io/pytables.py in <lambda>(store)\n    260         f = lambda store: store.append(key, value, **kwargs)\n    261     else:\n--> 262         f = lambda store: store.put(key, value, **kwargs)\n    263 \n    264     if isinstance(path_or_buf, string_types):\n\nAttributeError: '_io.BytesIO' object has no attribute 'put'\n```\n"},{"labels":["api",null],"text":"In 0.14:\n\n```\nIn [1]: pd.__version__\nOut[1]: '0.14.1'\n\nIn [2]: intidx = pd.Index(range(10))\n\nIn [3]: intidx % 2\nOut[3]: Int64Index([0, 1, 0, 1, 0, 1, 0, 1, 0, 1], dtype='int64')\n```\n\nnow:\n\n```\nIn [1]: pd.__version__\nOut[1]: '0.15.2'\n\nIn [2]: intidx = pd.Index(range(10))\n\nIn [3]: intidx % 2\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-3-de0ac1282c31> in <module>()\n----> 1 intidx % 2\n\nTypeError: unsupported operand type(s) for %: 'Int64Index' and 'int'\n```\n"},{"labels":["api"],"text":"In my [notebook](http://nbviewer.ipython.org/6e052140eaa5fdb6e8c0) comparing dplyr and pandas, I gained a new level of appreciation for the ability to chain strings of operations together. In my own code, the biggest impediment to this is adding additional columns that are calculations on existing columns. For example\n\n``` R\n# R / dplyr\nmutate(flights,\n   gain = arr_delay - dep_delay,\n   speed = distance / air_time * 60)\n\n# ... calculation involving these\n```\n\nvs.\n\n``` python\nflights['gain'] = flights.arr_delay - flights.dep_delay\nflights['speed'] = flights.distance / flights.air_time * 60\n\n# ... calculation involving these later\n```\n\njust doesn't flow as nicely, especially if this `mutate` is in the middle of a chain.\n\nI'd propose a new method (perhaps stealing `mutate`) that's similar to dplyr's.\nThe function signature could be kwarg only, where the keywords are the new column names. e.g.\n\n``` python\nflights.mutate(gain=flights.arr_delay - flights.dep_delay\n```\n\nThis would return a DataFrame with the new column `gain` in addition to the original columns.\n\nWorked out example\n\n``` python\n\nimport pandas as pd\nimport seaborn as sns\n\niris = sns.load_dataset('iris')\n\n(iris.query('sepal_length > 4.5')\n     .mutate(ratio=iris.sepal_length / iris.sepal_width)  # new part\n     .groupby(pd.cut(iris.ratio)).mean()\n)\n\n```\n\nThoughts?\n"},{"labels":["api",null],"text":"It would be logical to add a `name` field to DataFrame, similar to Series. Then when you construct a Panel it can try to use the `name`, in the same way as when you construct a DataFrame from Series.\n\nIt would also be useful when loading/saving DataFrames to files and DBs, for example at the moment to save to HDF you always have to add a key and for SQL a table name, it would be logical to attach these to the DataFrame itself as its name parameter. \n"},{"labels":["api",null,null],"text":"The current documentation for the limit parameter looks like this:\n\n```\nlimit : int, default None.\n    Maximum number of consecutive NaNs to fill.\n```\n\nSeveral features were not immediately obvious to me and could probably be resolved by slightly better docstring or API design:\n1. What happens where the max number of consecutive NaNs is reached? pandas simply stops replacing values, e.g., if there is a gap of 3 values and limit=2, pandas replaces the first 2 values. The alternative would be to entirely ignore gaps of more than 2 values. I'm not saying this would be a better alternative, but we should clarify this.\n2. The limit refers to forward filling only, even though interpolation is not inherently directional. It would be nice if there was some way to trigger a limit for back filling at the same time, e.g., with `forward_limit` and `backward_limit` arguments:\n   \n   ```\n   >>> pd.Series([1, np.nan, np.nan, np.nan, 5]).interpolate(forward_limit=1, backward_limit=1)\n   0     1\n   1     2\n   2   NaN\n   3     4\n   4     5\n   dtype: float64\n   ```\n\nThoughts?\n"},{"labels":["api",null],"text":"Current behavior:\n\n```\nIn [28]: s = pd.Series([1, 2, np.nan, np.nan, 5])\n\nIn [29]: s\nOut[29]:\n0     1\n1     2\n2   NaN\n3   NaN\n4     5\ndtype: float64\n\nIn [30]: s.interpolate(limit=0)\nOut[30]:\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: float64\n```\n\nBut in fact, `limit=0` in contrast to `limit=None` should mean no interpolation.\n"},{"labels":["api",null],"text":"After hours of tearing my hair, I've come to the conclusion that it is impossible to create a mixed dtype DataFrame without copying all of its data in. That is, no matter what you do, if you want to create a mixed dtype DataFrame, you will inevitably create a temporary version of the data (e.g. using np.empty), and the various DataFrame will constructors will always make copies of this temporary. This issue has already been brought up, a year ago: https://github.com/pydata/pandas/issues/5902. \n\nThis is especially terrible for interoperability with other programming languages. If you plan to populate the data in the DataFrame from e.g. a call to C, the easiest way to do it by far is to create the DataFrame in python, get pointers to the underlying data, which are np.arrays, and pass these np.arrays along so that they can be populated. In this situation, you simply don't care what data the DataFrame starts off with, the goal is just to allocate the memory so you know what you're copying to.\n\nThis is also just generally frustrating because it implies that in principle (depending potentially on the specific situation, and the implementation specifics, etc) it is hard to guarantee that you will not end up using twice the memory you really should.\n\nThis has an extremely simple solution that is already grounded in the quantitative python stack: have a method analagous to numpy's empty. This allocates the space, but does not actually waste any time writing or copying anything. Since empty is already taken, I would propose calling the method from_empty. It would accept an index (mandatory, most common use case would be to pass np.arange(N)), columns (mandatory, typically a list of strings), types (list of acceptable types for columns, same length as columns). The list of types should include support for all numpy numeric types (ints, floats), as well as special Pandas columns such as DatetimeIndex and Categorical. \n\nAs an added bonus, since the implementation is in a completely separate method, it will not interfere with the existing API at all.\n"},{"labels":["api",null,null],"text":"More drive-by reports. I'll try to close some of these issues sometime...\n\n``` python\nIn [8]: s = pd.Series(np.random.randn(10))\n\nIn [9]: s.groupby(s > 0).apply(lambda x: x.head(-1))  # works nicely\nOut[9]:\nFalse  0   -0.277400\n       1   -0.875231\n       3   -0.315087\n       4   -0.966649\n       6   -0.474389\n       7   -0.121225\nTrue   2    0.237861\n       5    1.311601\ndtype: float64\n\nIn [10]: s.groupby(s > 0).head(-1)  # doesn't.\nOut[10]: Series([], dtype: float64)\n\n```\n"},{"labels":["api",null,null],"text":"Depending on the runtime value used in `[` indexing on a series, the semantics may be array-indexing, or slicing based on a MultiIndex. This is extremely error-prone behavior in a very basic operation.\n\nIs the answer that production code should always use `.loc`/`.ix` instead of `[`? If so, I don't think the [docs](http://pandas.pydata.org/pandas-docs/stable/advanced.html#basic-indexing-on-axis-with-multiindex) make that clear.\n\n``` python\n>>> s = pandas.Series({(1, 1): 11, (1, 2): 12, (2, 1): 21, (2, 2): 22})\n\n>>> s\n\n1  1    11\n   2    12\n2  1    21\n   2    22\ndtype: int64\n\n#0 isn't recognized as something to do with the index\n# so treat it as an array index\n>>> s[0]  \n    11\n\n# But this could be asking for a slice,\n# so return a slice rather than an array-indexed element.\n>>> s[1] \n\n1    11\n2    12\ndtype: int64\n\n>>> pandas.__version__\n    '0.15.2'\n```\n\nSee also #3390\n"},{"labels":["api",null,null,null],"text":"I feel like I've encountered a bug. In the following scenario, the first `sort_index` call behaves as expected, but the second does not. Does someone know what the difference is here?\n\n```\nIn [1]: import pandas as pd\n\nIn [2]: pd.__version__\nOut[2]: '0.15.2'\n\nIn [3]: tuples = [(' foo', 'bar'), ('foo', 'bar'), (' foo ()', 'bar')]\n\nIn [4]: cols = pd.MultiIndex.from_tuples(tuples)\n\nIn [5]: df = pd.DataFrame(index=cols, data={'baz': [0, 1, 2]})\n\nIn [6]: df\nOut[6]: \n             baz\n foo    bar    0\nfoo     bar    1\n foo () bar    2\n\nIn [7]: df.sort_index()\nOut[7]: \n             baz\n foo    bar    0\n foo () bar    2\nfoo     bar    1\n\nIn [8]: tuples = [(' foo', 'bar'), ('foo', 'bar')]\n\nIn [9]: cols = pd.MultiIndex.from_tuples(tuples)\n\nIn [10]: df = pd.DataFrame(index=cols, data={'baz': [0, 1]})\n\nIn [11]: df\nOut[11]: \n          baz\n foo bar    0\nfoo  bar    1\n\nIn [12]: df.ix[(' foo ()', 'bar'), 'baz'] = 2\n\nIn [13]: df\nOut[13]: \n             baz\n foo    bar    0\nfoo     bar    1\n foo () bar    2\n\nIn [14]: df.sort_index()\nOut[14]: \n             baz\n foo    bar    0\nfoo     bar    1\n foo () bar    2\n```\n"},{"labels":["api",null],"text":"Is it possible to add in place shifting\n"},{"labels":["api",null],"text":"With the new `Timedelta` type (see https://github.com/pydata/pandas/pull/8184), we introduced behavior that breaks compatibility with the superclass `timedelta`: `td.seconds` refers to seconds since the last minute, rather than seconds since the last day (as in the superclass).\n\nI think breaking compatibility with the superclass here was a mistake. User code clearly _does_ rely on the old behavior, e.g., as manifested in bugs for exporting datetime columns to Excel: #9139. Moreover, it's likely to remain an issue causing silent bugs, because external libraries are going to continue to use `isinstance` checks to verify that something is a `datetime`/`timedelta` object. We chose to subclass `timedelta` here, so we really are stuck with full API compatibility.\n\nSo I think we should make this change in 0.16. It's a breaking change, but 0.15 was already a breaking change in this respect, and in any case I think we should give ourselves some allowance for API changes for new types.\n\nCC @jorisvandenbossche @jreback \n"},{"labels":["api",null,null],"text":"```\nimport pandas as pd\ndf = pd.DataFrame({'a': ['x', 'y', 'z'], 'b': [np.nan, np.nan, np.nan]})\n```\n\nApplying to string column - produces correct result:\n\n```\ndf['a'].str.contains('c', na=False)\n0    False\n1    False\n2    False\nName: a, dtype: bool\n```\n\nApplying to float column - returns **zeroes** instead of **bools** and return type is  **float64**:\n\n```\ndf['b'].str.contains('c', na=False)\n0    0\n1    0\n2    0\nName: b, dtype: float64\n```\n"},{"labels":["api",null,null,null,null],"text":"n.b. I'm looking at how to store a `DataFrame` in BSON format (see #4329) in MongoDB. This is my attempt to gather all the relevant information before adding yet another *SON serializer. Please don't treat this as a change request - I'm just trying to get my head around all the relevant bits. Maybe skip to the summary and start from there ...\n## Refactoring *SON to use common schema\n\nThere are a number of *SON formats which are similar: e.g. JSON, Msgpack (in pandas), BSON (not in pandas) recently announced [JBSON](http://www.phoronix.com/scan.php?page=news_item&px=MTY0MTU). They have some common components:\n1. Conversion of python object graph into Document(s) (think dict) made up only of JSON/Msgpack/BSON primitives. You can think of this in terms of a _schema_, as discussed in #3525, #3297, #2485. In an ideal world, this schema would be shared by all formats.\n2. Serialization into a binary format (Msgpack/BSON only) or text (JSON). This is generally done by an external library separately for each format.\n\nCurrently, I don't think any of the code between JSON and Msgpack is shared. The schema used by Msgpack is entirely different from JSON. I'm guessing this is by design; JSON has been kept as human-readable as possible, whereas Msgpack is more focused on a serialization format. To illustrate this, two examples. First a simple Dataframe:\n\n```\n                          bid     offer\n2014-12-15 10:07:00  0.562804  0.398798\n2014-12-15 10:07:01  0.399399  0.896809\n2014-12-15 10:07:02  0.747191  0.098048\n```\n\nproduces the following document before serialization in msgpack:\n\n``` python\n{'axes': [{'data': ['bid', 'offer'],\n   'dtype': 17,\n   'klass': 'Index',\n   'name': None,\n   'typ': 'index'},\n  {'data': '...',\n   'dtype': 21,\n   'freq': 'S',\n   'klass': 'DatetimeIndex',\n   'name': None,\n   'typ': 'datetime_index',\n   'tz': None}],\n 'blocks': [{'compress': 'blosc',\n   'dtype': 12,\n   'items': {'data': ['bid', 'offer'],\n    'dtype': 17,\n    'klass': 'Index',\n    'name': None,\n    'typ': 'index'},\n   'klass': 'FloatBlock',\n   'shape': (2, 3),\n   'values': '...'}],\n 'klass': 'DataFrame',\n 'typ': 'block_manager'}\n```\n\nI produced this by calling `encode` recursively in `packing.py`, replicating what `pack` does. There have been plenty of discussions regarding storage of metadata in #3525, #3297. Now the result of calling `to_json()`:\n\n``` python\n'{\"bid\":{\"1418638020000\":0.5628044127,\"1418638021000\":0.3993987818,\"1418638022000\":0.7471914537},\"offer\":{\"1418638020000\":0.398797779,\"1418638021000\":0.8968090851,\"1418638022000\":0.0980482752}}'\n```\n\nBoth appear to use blocking (see #9130 ). Also, as described in #3525, dates are stored as integers (in strings) for performance reasons.\n\nThe second example, which has a multi-index:\n\n```\n                       bid                                ask                \\\n                      95.0   95.5   96.0   96.5   97.0   97.5   98.0   98.5   \n2014-12-15 10:07:00  25030  16800  42580  58560  75110  10400  89240   3990   \n2014-12-15 10:07:01  42620  57860  80010  81500  98880  99610  65770  83930   \n2014-12-15 10:07:02  94170  98040  74840  41690  48960  76510  88530  48770   \n2014-12-15 10:07:03  65090  23700  16390  45700    500  29290  32370  68350  \n```\n\nThis kind of works for `to_msgpack()` but not for `to_json()` (throws an exception):\n\n```\n{'axes': [{'data': [('bid', 95.0),\n    ('bid', 95.5),\n    ('bid', 96.0),\n    ...\n    ('ask', 99.5)],\n   'dtype': 17,\n   'klass': 'MultiIndex',\n   'names': FrozenList([None, None]),\n   'typ': 'multi_index'},\n  {'data': '...',\n   'dtype': 21,\n   'freq': 'S',\n   'klass': 'DatetimeIndex',\n   'name': None,\n   'typ': 'datetime_index',\n   'tz': None}],\n 'blocks': [{'compress': 'blosc',\n   'dtype': 12,\n   'items': {'data': [('bid', 95.0),\n     ('bid', 95.5),\n     ('bid', 96.0), \n     ...\n     ('ask', 99.5)],\n    'dtype': 17,\n    'klass': 'MultiIndex',\n    'names': FrozenList([None, None]),\n    'typ': 'multi_index'},\n   'klass': 'FloatBlock',\n   'shape': (10, 4),\n   'values': '...'}],\n 'klass': 'DataFrame',\n 'typ': 'block_manager'}\n```\n\nIt would be nice to expose the API of the 'intermediate' representation (i.e. at the end of step 1) for the advanced user, in order to store the dataframe in multiple 'Documents'. If storing to a file, this doesn't make any sense - you want to have a single document with many embedded documents; This is the JSON paradigm. But if you're storing into a database, you would want flexibility in order to store each nested doc/dict in a different collection/table. There are reasons for this e.g. a technical reason is the MongoDB maximum record/document size; however, it would generally be driven by a use-case. \n\nWhat would this intermediate document look like? A dictionary, with only python (and numpy - see below) primitives? What are the primitives that are different for *SON formats (e.g. dates)?\n## Serialization of vectorized data\n\nA separate issue, that is somewhat at odds with the above, is the storage of vectorized data. i.e. the underlying numpy array in `to_msgpack()` is encoded into a string and stored in binary format.  While this makes sense for performance reasons (see https://github.com/pydata/pandas/pull/3525#issuecomment-17442698), it's giving up one of the advantages of using msgpack (or BSON going forward), in my opinion: portability. If you go to http://msgpack.org, you'll see they have APIs for every conceivable language (likewise for MongoDB). Wouldn't it be nice if you could use one of these APIs and load up your pandas data in e.g. Java, Haskell or C? At present, this is possible with the exception of the 'values' fields, which you would have to decode by hand. Sure you could argue to use JSON for portability, but then you're trading off performance.\n\nThis is at odds with adding compression - which we still want. A compromise could be to use the native *SON list type when `compress=None`, when we assume that speed isn't important, and the current solution (encode as string) when compression is active, and speed is important (and compressed data is not handled by the abovementioned APIs)?\nNote also, as described in links in https://github.com/pydata/pandas/issues/4329#issuecomment-27095723, BSON/MongoDB appears to have some support for vectorised lists that avoids having to convert to a list and then serialize each element separately.\n## Summary \n\nIn adding yet another *SON serializer, it would not make sense to have a third codebase that handles the issues discussed above in a new way:\n- Would it make sense to combine existing msgpack/JSON 'packing' code, to have a standard intermediate 'schema', shared by all *SON? cons: breaks backward compatibility, makes JSON document much more complex, pros: support more complex pandas structures (DF, panel, sparse df etc.).\n- Could code for storing vector data be shared? Could we allow native storage of vector data (e.g. supported by BSON), for example, when not using the `compress` argument in `to_msgpack`, to allow portability? Ideally also using compress? could we somehow replicate what _monary_ is doing for BSON in the msgpack serializer, i.e. altogether avoiding conversion to lists and memory copies?\n\nAgain, I want to stress I'm just trying to create some discussion here rather than opening a specific change req... Sorry if raising an issue wasn't the correct procedure.\n"},{"labels":["api",null,null,null],"text":"I had mentioned this at one of the pydata talks. Just to put a couple of ideas down.\n\nCurrently the internal structure of a `NDFrame` is a collection of `Block` objects, managed by a `BlockManager`. Each of these is a nd-dim array of a single dtype (generally these are the same dim as the parent object, eg. a DataFrame will have 2-d objects). Certain types are currently treated slightly differently, e.g. `Categorical` and `Sparse` in they are always unconsolidated (meaning you can have multiple ones).\n\nI had 2 thoughts:\n- allow these to be numpy `mmap` objects\n- allow these to be a `bcolz` `carray`(or possible the entire structure as a `ctable`)\n\nIn theory this should allow transparent seemless serialization/deserialization to disk. Hence you could have a virtual DataFrame larger that actual memory.\n\nI am not sure how feasible this is in practice. But might be worth trying out.\n"},{"labels":["api",null],"text":"I rarely use Pandas without calling some_dataframe.plot() at the end of some processing.\nSaving a plot to disk is a very common requirement.\nHaving a handy df.plot(save_as=\"filename\") would feel very natural.\nBy the way, the figsize keyword requires a tuple in inches. It would be nice to support sizes in pixels as well.\n\nThanks!\n"},{"labels":["api",null],"text":"Ideally some utility method that works to shuffle both dataframes, series and numpy arrays.\n\nFor example to shuffle a Dataframe: df.ix[shuffle(df.index)]. \n\nSee https://github.com/scikit-learn/scikit-learn/issues/4008 for discussion.\n"},{"labels":["api",null],"text":"To explicitly encourage / support serialisation of other objects (e.g. numpy arrays) and better support custom JSON serialisation of Pandas types. As discussed in #9130.\n\nWould welcome more thoughts / ideas on this. Not 100% sold myself.\n"},{"labels":["api",null],"text":"Add a round-trippable json orient (maybe \"roundtrip\") to provide enough metadata to reconstruct a frame or series with 100% fidelity. As discussed in #9130 \n"},{"labels":["api",null],"text":"The following seems like very natural syntax to specify the dtypes of various columns passed into the DataFrame constructor:\n\n```\ndf = pd.DataFrame({'a':[1,2,3], 'b':[2.5,3.5,6.7]}, dtype={'a':'int32','b':'float32'})\n```\n\nHowever, right now it raises an error.  Perhaps something worth implementing?  See also http://stackoverflow.com/questions/21197774/assign-pandas-dataframe-column-dtypes \n"},{"labels":["api",null,null],"text":"There is an inconsistency apparently in the way dtypes are parsed when DataFrames are created from Series:\n\n```\n>>> import pandas as pd\n>>> df = pd.DataFrame({'f':pd.Series([1.5,], dtype='float32'), 'i':pd.Series([1,], dtype='int32')})\n>>> df.info()\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1 entries, 0 to 0\nData columns (total 2 columns):\nf    1 non-null float32\ni    1 non-null int32\ndtypes: float32(1), int32(1)\nmemory usage: 16.0 bytes\n\n>>> df2 = pd.DataFrame([pd.Series([1.5,], dtype='float32'), pd.Series([1,], dtype='int32')], columns=['f','i'])\n>>> df2.info()\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2 entries, 0 to 1\nData columns (total 2 columns):\nf    0 non-null float64\ni    0 non-null float64\ndtypes: float64(2)\nmemory usage: 48.0 bytes\n\n```\n"},{"labels":["api",null],"text":"Currently, we use the `plot` for every possible kind of plot, and switch between them using the `kind` keyword argument.\n\nFrom an API design and discoverability perspective, this seems less than ideal. If we had separate methods, standard introspection tools like IPython and autocomplete could help users figure out which arguments are applicable for which type of plot with less reliance on the documentation. \n\nAnother option is to make a `.plot` also an accessor, so we could have `df.plot.scatter` and so forth. But I suspect `.plot_scatter` would be a bit more discoverable.\n"},{"labels":["api",null],"text":"Should `StringMethods` be compatible with standard `str`, or have only useful methods? Sometimes I have to adjust text formats before converting other types, and I've chosen additionals which makes formatting / adjustments easier. Any ideas?\n- [x] `str.find` (#9386)\n- [x] `str.rfind` (#9386)\n- [x] `str.isalnum` (#9282)\n- [x] `str.isalpha` (#9282)\n- [x] `str.isdigit` (#9282)\n- [x] `str.isspace` (#9282)\n- [x] `str.islower` (#9282)\n- [x] `str.isupper` (#9282)\n- [x] `str.istitle` (#9282)\n- [x] `str.ljust` (#9352)\n- [x] `str.rjust` (#9352): `StringMethods.pad` works as the same as `ljust`, `rjust` and `center` by switching `side` option. And `StringMethods.center` exists, `ljust` and `rjust` don't.\n- [x] `str.rsplit`: `split` also supports regex, and there seems no straightforward way to `rsplit` using regex. (#10303)\n- [x] `str.zfill` (#9387)\n- [x] `StringMethods.center` and `StringMethods.pad` should have 'fillchar` kw to specify padding character.  (#9352)\n- [x] `str.capitalize` (#9766)\n- [x] `str.swapcase` (#9766)\n- [x] `str.translate` (#10052)\n- [x] `str.index` (#10045)\n- [x] `str.rindex` (#10045)\n- [x] `str.partition` (#9773)\n- [x] `str.rpartition` (#9773)\n- [x] `unicode.isnumeric` (#9439)\n- [x] `unicode.isdecimal` (#9439)\n- [x] `unicodedata.normalize` (#10031)\n\n**NOTE** For python 2.x, once convert target to unicode internally then call the func.    \n## Which may not be very userful:\n- `expandtabs`\n- `splitlines`\n\nI think there is an option to implement these to say \"support the same methods as standard `str`\".\n"},{"labels":["api",null,null],"text":"from [SO](http://stackoverflow.com/questions/27502406/convert-pandas-datetimeindex-to-fractional-day-of-year) and previous discussions\n\ncurrently you cannot subtract 2 datetimeindexes ('-' is actually a set op). This is the ONLY valid numerical type of operation (e.g. you cannot add dti).\n\nWe current support pretty much the fully gamut of obvious ops, with `dti-dti` being the exception.\n\nfull list of tests are here: https://github.com/pydata/pandas/blob/master/pandas/tseries/tests/test_base.py#L434\n\nI will propose\n\n`dti.sub(dti)` to rectify this\n\nthoughts\n"},{"labels":["api",null,null],"text":"The current [plot()](http://pandas.pydata.org/pandas-docs/version/0.15.1/generated/pandas.DataFrame.plot.html#pandas.DataFrame.plot) method for Dataframe cannot set the x/y label in it. [Result](http://stackoverflow.com/questions/21487329/add-x-and-y-labels-to-a-pandas-plot?lq=1) is more lines of code for just doing this. \n\nAs someone commented in that post:\n\n> Is there a particular reason why x and y labels can't be added as arguments to pd.plot()? Given the additional concision of pd.plot() over plt.plot() it seems it would make sense to make it even more succinct instead of having to call ax.set_ylabel()\n"},{"labels":["api",null],"text":"A user may expect that the end_time of a Period minus the start_time of a Period returns its duration. Unfortunately, the end_time of a Period is inclusive, consequently, subtracting start from end yields an incorrect period duration:\n\n```\njan = Period('2014-01', freq='M')\n(jan.end_time - jan.start_time).days # returns 30, not 31\n```\n\nThis can be manually rectified by the user:\n\n```\njan = Period('2014-01', freq='M')\n(Timestamp(jan.end_time.value + 1) - jan.start_time).days # correctly returns 31\n```\n\nHowever, this is not intuitive.\n\nThe end_time should either be exclusive or Period should provide a duration property.\n\nAlthough a strong argument can be made that the definitions of duration, start_time, and end_time should all be consistent, changing the end_time definition to exclusive is a significant modification of long-standing behavior.\n\nConsequently, I propose the addition of a duration property that is logically inconsistent with subtracting start_time from end_time.\n"},{"labels":["api",null],"text":"When using `df.plot(kind='bar')` pandas adds a dotted line to the x-axis (`kind='barh'`, line is added to y-axis). Why is the dotted line added ([line in the code](https://github.com/pydata/pandas/blob/master/pandas/tools/plotting.py#L1899))? \n\nThis line is undocumented and there is no way to remove it without a hack. It'd be nice to have it documented (if it is necessary) or removed.\n"},{"labels":["api",null,null],"text":"Now rolling functions like _rolling_mean_ give NaN's if the window size is outside the input series. I would like to have an extra option that the window size shrinkes if the window is outside the input series. Or to put it different: use an expanding mean for the first values and the rolling mean for all the intermediate values. Below an example of such a code and my workaround. \n\n``` python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#%% create data\ndf = pd.DataFrame()\ndata = np.cumsum(np.random.random(10)-0.5)\ndf['data'] = data\n\n#%% do the calculations\n# now:\nwindow_size = 5\ndf['rolling_mean'] = pd.rolling_mean(df['data'], window=window_size)\n\n# or: \ndf['expanding']= pd.expanding_mean(df['data'],)\n\n# what I wont\ndf['whatIwant'] = df['rolling_mean'].fillna(df['expanding'])\n\n#%% create picture\nfig, ax = plt.subplots()\nax.plot(df.index, df['data'], lw=5, label='data')\nax.plot(df.index, df['rolling_mean'], lw=5, label='rolling mean')\nax.plot(df.index, df['expanding'], lw=5, label='expanding')\nax.plot(df.index, df['whatIwant'], lw=2, label='what I want')\nax.legend()\n```\n\nThis workaround works quite good, and is sort of clear. But if _rolling_mean_ is used in combination with _center=True_ this becomes more complicated. This workaround is also not very usefull when some data is missing. I think pandas can be improved by adding this option into the rolling_statistics functions. \n\nDisclaimer: This is the first issue ever that I add. \n"},{"labels":["api",null],"text":"currently\n\n`Series.str.strip()` works\nbut \n\n`Index(Series(Index.values).str.strip())` would be necessary for an index\n\nshould be very straightforward to move the `.str` accessor to `IndexOpsMixin` (from Series)\nto support this (would need to prevent Index sub-classes from using it though as they don't support string ops.\n"},{"labels":["api",null],"text":"I didn't realize this was possible, and didn't see it in the docs.\n\n```\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'a', 'b'], 'C': [3, 4, 5]})\ndf.groupby('B').agg({'A': {'mean1': 'mean', 'med1': 'median'}, 'C': {'mean2': 'mean', 'med2': 'median'}})\n```\n"},{"labels":["api",null,null],"text":"from [SO](http://stackoverflow.com/questions/27354281/query-term-is-not-valid-condition-none/27356827#27356827)\n\n```\nIn [1]: df = pd.DataFrame.from_dict({'A':[1,2],'B':[100,200], 'C':[42,11]})\nIn [2]: df_a = df.set_index('A')\nIn [3]: df_a\nOut[3]: \n     B   C\nA         \n1  100  42\n2  200  11\n\nIn [4]: store = pd.HDFStore('foo.h5','w')\nIn [5]: store.put('bar', df_a, format='table', data_columns=True)\nIn [7]: store.select('bar','index==1')\n```\n\n```\nstore.select('bar','A==1)\n```\n\nshould work as well (obviously only for a named index)\n"},{"labels":["api",null],"text":"xref #8994 \nxref #5904 \nxref #8572 \n\nThis might be a bit controversial, but the issues raised in #8994 and #5904 \npoint to some continued confusion w.r.t. attribute setting 'being' column setting\n\nso if we now have\n\n```\ndf = DataFrame({'A' : [1,2,3], 'B' : 5 })\ndf.C = 5\n```\n\nis an attribute set\n\nit _could_ be a column set\ne.g. de-facto `df['C'] = 5`\n\nIf someone actually wants an attribute to 'stick' around. (meaning they would have to intercept the `__finalize__` methods and actually deal with them properly, then I think it is reasonable to also have them add to the `_internal_names` as well (see #8572)\n\nSo basically would try to set a column (unless its an internal name).\n\n(note that `getattr` is de-facto already equivalent to `__getitem__`, e.g. `df.B === df['B']`)\n(don't mind my JS equivalence notion :)\n"},{"labels":["api",null,null],"text":"It seems like `pd.Series([x]) | pd.Series([y])` with `x, y` integers returns `pd.Series([x | y]).astype(bool)`. This is a reasonable semantic, but `pd.Series([x]) & pd.Series([y])` seems to return `pd.Series([x & y % 2]) == 1`, which is a lot weirder. Is there a justification for this? I couldn't find one in the documentation (`&` is hard to search!), so it may be a bug.\n"},{"labels":["api",null],"text":"Dropping rows is removing the values from a multiindex dataframe but not removing the key values from the multiindex. I don't think this is intended behavior. This is pandas 15.1\n\n``` python\nIn [33]: s = pd.Series(np.random.randn(8), index=arrays)\n\nIn [34]: df = pd.DataFrame(np.random.randn(8, 4), index=arrays)\n\nIn [35]: df\nOut[35]: \n                0         1         2         3\nbar one -0.236317  0.105585 -0.611251  0.016047\n    two  0.296703 -2.415282 -0.595308  0.388648\nbaz one  1.388280  0.261497  0.717756  0.407535\n    two  0.379969 -0.592787  1.093628  0.082563\nfoo one  0.100581 -0.186784 -0.150018 -0.032548\n    two  2.009603 -0.516111  0.801641  0.693063\nqux one -0.194570  0.952694  0.913118  0.299275\n    two  1.716092  0.465544  1.453519 -0.679434\n\nIn [36]: df.index\nOut[36]: \nMultiIndex(levels=[[u'bar', u'baz', u'foo', u'qux'], [u'one', u'two']],\n           labels=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]])\n\nIn [37]: df.drop('two', level=1, axis=0,inplace=True)\n\nIn [38]: df\nOut[38]: \n                0         1         2         3\nbar one -0.236317  0.105585 -0.611251  0.016047\nbaz one  1.388280  0.261497  0.717756  0.407535\nfoo one  0.100581 -0.186784 -0.150018 -0.032548\nqux one -0.194570  0.952694  0.913118  0.299275\n\nIn [39]: df.index\nOut[39]: \nMultiIndex(levels=[[u'bar', u'baz', u'foo', u'qux'], [u'one', u'two']],\n           labels=[[0, 1, 2, 3], [0, 0, 0, 0]])\n```\n\nI noticed this because when I then try to create a panel from from the dataframe I get:\n\n``` python\nIn [40]: panel = df.to_panel()\n\nIn [41]: panel\nOut[41]: \n<class 'pandas.core.panel.Panel'>\nDimensions: 4 (items) x 4 (major_axis) x 2 (minor_axis)\nItems axis: 0 to 3\nMajor_axis axis: bar to qux\nMinor_axis axis: one to two\n```\n\nI also noticed this same behavior when I used query on a multiindex dataframe and then tried to create a panel from the result. Is there a way to get around having to drop extra array from the panel?\n"},{"labels":["api",null,null],"text":"``` python\nimport pandas as pd\na = pd.TimedeltaIndex(range(3),unit='S')\nprint (a[1])\n```\n\ngives output\n\n```\n11574 days 01:46:40\n```\n\nI would expect the same output as from\n\n``` python\nimport pandas as pd\nimport numpy as np\nnpa = np.array(range(3),dtype='timedelta64[s]')\na = pd.TimedeltaIndex(npa)\nprint(a[1])\n```\n\ngiving output\n\n```\n 0 days 00:00:01\n```\n\nThis is pandas 0.15.1 as installed by conda update pandas from a fresh python 3.4 install of anaconda.\n\n```\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: nb_NO\n\npandas: 0.15.1\nnose: 1.3.4\nCython: 0.21\nnumpy: 1.9.1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.2.0\nsphinx: 1.2.3\npatsy: 0.3.0\ndateutil: 2.1\npytz: 2014.9\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.0\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.5.7\nlxml: 3.4.0\nbs4: 4.3.2\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: None\n```\n"},{"labels":["api",null,null,null],"text":"Although it's _my_ desired behavior, I realize perhaps this is not _the_ desired behavior, but I think it's reasonable to bring up.\n\nIf I want to concat two data frames and their column names are the same, the order is preserved:\n\n```\n>>> df1 = pd.DataFrame(columns=[\"A\",\"1\"])\n>>> pd.concat([df1,df1])\nEmpty DataFrame\nColumns: [A, 1]\nIndex: []\n```\n\nHowever, if they are not, it seems they are sorted:\n\n```\n>>> df2 = pd.DataFrame(columns=[\"A\",\"2\"])\n>>> pd.concat([df1,df2])\nEmpty DataFrame\nColumns: [1, 2, A]\nIndex: []\n```\n\nWould it be reasonable to try to preserve the order? So the output would be:\n\n```\n>>> pd.concat([df1,df2])\nEmpty DataFrame\nColumns: [A, 1, 2]\nIndex: []\n```\n\nI realize that, in an ordered list, `\"A\"` doesn't come before `\"1\"` or `\"2\"`, but in both `df1` and `df2`,  `\"A\"` _does_ come before `\"1\"` and `\"2\"`.\n"},{"labels":["api",null,null],"text":"``` python\npd.DataFrame({\"A\": [1]}).squeeze()  # -> 1\npd.DataFrame({\"A\": []}).squeeze()  # Noop\n```\n\nThe second one is genuinely surprising; I would have expected it to return an empty Series with the name set to `A`. But it is consisent with numpy (`np.array([[], []]).squeeze()`) so I've marked is as a doc issue.\n"},{"labels":["api",null],"text":"The following behavior surprised me.  Of course there might be a good reason for it.  Thought I'd report, feel free to close if it's not appropriate.\n\n``` Python\nIn [1]: from pandas import DataFrame, Series\n\nIn [2]: s = Series([1, 2, 3], name='foo')\n\nIn [3]: DataFrame(s, columns=['bar'])\nOut[3]: \nEmpty DataFrame\nColumns: [bar]\nIndex: []\n```\n"},{"labels":["api",null,null],"text":"From #8946:\n\nIf `cat > scalar` is allowed and `cat == list` also because it basically is doing a comparison of each line as if it was the scalar case, then by that logic, `cat > list` should also be allowed: each row in that comparison would treat the element from the list as a scalar. \n\nOn the other hand a scalar comparison with the categorical makes only sense if the scalar can be treated as a category (for any other value, it's basically a \"not of the same type\" comparison, which would raise on python3), so the scalar must be in `categories` and this should not work:\n\n```\nIn[4]: df = pd.DataFrame({\"a\":[1,3,3,3,np.nan]})\nIn[6]: df[\"b\"] = df.a.astype(\"category\")\nIn[7]: df.b\nOut[7]: \n0     1\n1     3\n2     3\n3     3\n4   NaN\nName: b, dtype: category\nCategories (2, float64): [1 < 3]\nIn[8]: df.b > 2\nOut[8]: \n0    False\n1     True\n2     True\n3     True\n4    False\nName: b, dtype: bool\n```\n\nOh, one more thing: according to that thought, `df.b == 2` (-> The \"equality\" case) should also NOT work, because `2` is not in `categories` and therefore a \"different type\".\n\nCurrent code results in this:\n\n```\nIn [5]: df.b==2\nOut[5]: \n0    False\n1    False\n2    False\n3    False\n4    False\nName: b, dtype: bool\n```\n\nthis is actually consistent (e.g. it returns False). On a comparison it shouldn't raise so this is a reasonable result. I think this is de-facto like the following and is useful.\n\n```\nIn [7]: Series(['a','b','c'])==2\nOut[7]: \n0    False\n1    False\n2    False\ndtype: bool\n```\n"},{"labels":["api"],"text":"A student of mine ran into a confusing problem which ended up being due to an asymmetry in `DataFrame.__setattr__` and `DataFrame.__getattr__` when an attribute and a column have the same name.  Here is a short example session:\n\n``` python\nimport pandas as pd\nprint(pd.__version__)  # '0.14.1'\ndata = pd.DataFrame({'x':[1, 2, 3]})\n\n# try to create a new column, making a common mistake\ndata.y = 2 * data.x\n\n# oops! That didn't create a column.\n# we need to do it this way instead\ndata['y'] = 2 * data.x\n\n# update the attribute, and it updates the column, not the attribute\ndata.y = 0\nprint(data['y'])  # [0, 0, 0]\n\n# print the attribute, and it prints the attribute, not the column\nprint(data.y)  # [2, 4, 6]\nprint(data['y']) # [0, 0, 0]\n```\n\nThe confusion derived from the fact that in this situation, `data.__getattr__('y')` refers to the _attribute_, while `data.__setattr__('y', val)` refers to the _column_.\n\nI understand that using attributes to access columns is not recommended, but the asymmetry in this corner case led to a lot of confusion! It would be better if `__getattr__` and `__setattr__` would always refer to the same object.\n"},{"labels":["api",null],"text":"from [SO](http://stackoverflow.com/questions/27164380/merging-pandas-dataframes-on-categorical-series)\n\n```\nIn [1]: a = pd.Series(['a','b','c'],dtype=\"category\")\nIn [2]: b = pd.Series(['a','b','c'],dtype=\"object\")\nIn [3]: c = pd.Series(['a','b','cc'],dtype=\"object\")\nIn [5]: a==b\nTypeError: Cannot compare a Categorical for op <built-in function eq> with type <type 'numpy.ndarray'>. If you want to \ncompare values, use 'series <op> np.asarray(cat)'.\n```\n\n```\nIn [6]: A = pd.DataFrame({'A':a,'B':[1,2,3]})\nIn [7]: B = pd.DataFrame({'A':b,'C':[4,5,6]})\n\nIn [9]: A.merge(B,on='A') \nOut[9]: \n   A  B  C\n0  a  1  4\n1  b  2  5\n2  c  3  6\n\nIn [10]: A.merge(B,on='A').dtypes\nOut[10]: \nA    object\nB     int64\nC     int64\ndtype: object\n\nIn [11]: A.dtypes\nOut[11]: \nA    category\nB       int64\ndtype: object\n\nIn [12]: B.dtypes\nOut[12]: \nA    object\nC     int64\ndtype: object\n```\n"},{"labels":["api",null,null,null,null],"text":"Compare (on Python 2.7):\n\n```\nIn [7]: pd.Timestamp('2000-01-01') > pd.Series(range(5))\nTypeError: Cannot compare type 'Timestamp' with type 'int'\n\nIn [8]: pd.Timestamp('2000-01-01') > pd.DataFrame({'x': range(5)})\nOut[8]:\n      x\n0  True\n1  True\n2  True\n3  True\n4  True\n```\n\nProbably will fix this in the process of getting #8916 to pass, but I wanted to raise this as a separate issue in case this is intentional. Pretty sure it's not, but there was a test-case specifically introduced that relies on this behavior (see PR #4983).\n"},{"labels":["api",null,null],"text":"See also: http://stackoverflow.com/questions/26736745/indexing-a-pandas-panel-counterintuitive-or-a-bug\n\nThese are actually two related(?) issues.\nThe first is that the DataFrame is transposed, when you index the major_indexer or minor_indexer:\n\n```\nfrom pandas import Panel\nfrom numpy import arange\np = Panel(arange(24).reshape(2,3,4))\np.shape\nOut[4]: (2, 3, 4)\np.iloc[0].shape # original order\nOut[5]: (3, 4)\np.iloc[:,0].shape # I would expect (2,4), but it is transposed\nOut[6]: (4, 2)\np.iloc[:,:,0].shape # also transposed\nOut[7]: (3, 2)\np.iloc[:,0,:].shape # transposed (same as [6])\nOut[8]: (4, 2)\n```\n\nThis may be a design choice, but it seems counterintuitive to me and it is not in line with the way numpy indexing works.\nOn a related note, I would expect the following two commands to be equivalent:\n\n```\np.iloc[1:,0,:].shape # Slicing item_indexer, then transpose\nOut[9]: (4, 1)\np.iloc[1:,0].shape # Expected to get the same as [9], but slicing minor_indexer instead????\nOut[10]: (3, 2)\n```\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: nl_NL\n\npandas: 0.15.1\nnose: 1.3.3\nCython: 0.20.1\nnumpy: 1.9.1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.2.0\nsphinx: 1.2.2\npatsy: 0.2.1\ndateutil: 1.5\npytz: 2014.9\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.2\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.5\nlxml: 3.3.5\nbs4: 4.3.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\n"},{"labels":["api",null,null,null],"text":"Clearly the default is `complete=True` for back-compat\nbut allows for a match starting at the beginning (has always been like this),\nbut doesn't require the string to ONLY match the format, IOW, can be extra stuff after the match.\n\nAvoids having to do a regex replace first.\n\n```\nIn [21]: s = Series(['19MAY11','19MAY11:00:00:00']*100000)\n\nIn [22]: %timeit pd.to_datetime(s.str.replace(':\\S+$',''),format='%d%b%y')\n1 loops, best of 3: 828 ms per loop\n\nIn [23]: %timeit pd.to_datetime(s,format='%d%b%y',compete=False)\n1 loops, best of 3: 603 ms per loop\n```\n"},{"labels":["api",null,null,null],"text":"Here I expected a TimedeltaIndex with 11 elements from 0 hours through 11 hours:\n\n```\n>>> pd.timedelta_range(0, 10, freq='H')\n<class 'pandas.tseries.tdi.TimedeltaIndex'>\n['0 days']\nLength: 1, Freq: <Hour>\n```\n\nIt appears that `freq` is taken to refer only to the step size, not the start/stop, and the unit defaults to `ns` like Timedelta itself? I think we should either:\n1. add a separate `unit` argument -- but this is a subtle distinction\n2. use `freq` to also imply the `unit`\n3. raise on integer input (possibly in addition to 1?)\n\nSimilarly strange and somewhat inconsistent with the above example:\n\n```\n>>> pd.timedelta_range(0, 10, periods=100)\nValueError: Must specify two of start, end, or periods\n```\n"},{"labels":["api",null,null,null],"text":"http://stackoverflow.com/questions/27074469/memory-leak-in-python-pandas-reshuffling-index\n\n```\nIn [1]: df = DataFrame(np.random.randn(10000))\n\nIn [2]: def f(df):\n   ...:     for dfi in np.array_split(df,100):\n   ...:         shuffle = dfi.reindex(np.random.permutation(dfi.index))\n   ...:         one = shuffle.iloc[:50]\n   ...:         two = shuffle.iloc[50:]\n```\n\nso the `np.random.permutation` seems to be shuffling the index in a weird way\nthis doesn't leak with `dfi.index.values`\n"},{"labels":["api",null,null,null],"text":"Hello everyone, \n\nI stumbled upon the following behavior of groubpy with categorical, which seems at least inconsistent with the way groupby usually operates.\n\nWhen grouping on a string type column with `sort=False`, the order of the groups is the order in which the keys first appear in the column.\n\nHowever, when grouping with a categorical column, the groups seem to be always ordered by the categorical, even when `sort=False`.\n\n``` python\nimport pandas as pd\nd = {'foo': [10, 8, 5, 6, 4, 1, 7], 'bar': [10, 20, 30, 40, 50, 60, 70],\n     'baz': ['d', 'c', 'e', 'a', 'a', 'd', 'c']}\ndf = pd.DataFrame(d)\ncat = pd.cut(df['foo'], np.linspace(0, 10, 5))\ndf['range'] = cat\ngroups = df.groupby('range', sort=True)\n# Expected behaviour\nresult = groups.agg('mean')\n\n# Why are the categorical still sorted in this case ?\ngroups2 = df.groupby('range', sort=False)\nresult2 = groups2.agg('mean')\n\n# I would expect an output like this one: keep the order in which the groups\n# are first encountered\ngroups3 = df.groupby('baz', sort=False)\nresult3 = groups3.agg('mean')\n```\n\n``` python\nresult\n```\n\n|  | bar | foo |\n| --- | --- | --- |\n| range |  |  |\n| (0, 2.5] | 60 | 1.0 |\n| (2.5, 5] | 40 | 4.0 |\n| (5, 7.5] | 55 | 6.5 |\n| (7.5, 10] | 15 | CC |\n\n``` python\nresult2\n```\n\n|  | bar | foo |\n| --- | --- | --- |\n| range |  |  |\n| (0, 2.5] | 60 | 1.0 |\n| (2.5, 5] | 40 | 4.0 |\n| (5, 7.5] | 55 | 6.5 |\n| (7.5, 10] | 15 | CC |\n\n``` python\nresult3\n```\n\n|  | bar | foo |\n| --- | --- | --- |\n| baz |  |  |\n| d | 35 | 5.5 |\n| c | 45 | 7.5 |\n| e | 30 | 5.0 |\n| a | 45 | 9.0 |\n\n``` python\npd.__version__\nOut[110]: '0.15.1'\n```\n\nSetting `as_index=False` does not change the presented bahavior.\n"},{"labels":["api",null],"text":"`df.stack(level=[0,1], dropna=False)` appears to be equivalent to `df.stack(level=0, dropna=False).stack(level=0, dropna=False)`. While this makes a certain amount of sense, it results in additional rows that in many cases are probably not expected/desired. In particular, `df.stack(level=[0,1], dropna=False)` is not equivalent to `df.stack(level=[0,1], dropna=True)` even when `df` contains no missing values, which seems counterintuitive.\n\nI think that when stacking multiple levels, one may want them stacked in one go, rather than sequentially -- so that when there are no missing values, `df.stack(level=[0,1], dropna=False)` would produce the same result as `df.stack(level=[0,1], dropna=True)`.\n\nHere is an example of current behavior. Since `df` has no missing values, I would want `[6]` to produce the same results as `[5]`, not `[7]`.\n\n```\nPython 3.4.2 (v3.4.2:ab2c023a9432, Oct  6 2014, 22:16:31) [MSC v.1600 64 bit (AMD64)]\nType \"copyright\", \"credits\" or \"license\" for more information.\n\nIPython 2.3.1 -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\n\nIn [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: df = pd.DataFrame(np.zeros((2,3)), columns=pd.MultiIndex.from_tuples([('A','x'), ('A','y'), ('B','z')], names=['Upper', 'Lower']))\n\nIn [4]: df\nOut[4]:\nUpper  A     B\nLower  x  y  z\n0      0  0  0\n1      0  0  0\n\nIn [5]: df.stack(level=[0,1], dropna=True)\nOut[5]:\n   Upper  Lower\n0  A      x        0\n          y        0\n   B      z        0\n1  A      x        0\n          y        0\n   B      z        0\ndtype: float64\n\nIn [6]: df.stack(level=[0,1], dropna=False)\nOut[6]:\n   Upper  Lower\n0  A      x         0\n          y         0\n          z       NaN\n   B      x       NaN\n          y       NaN\n          z         0\n1  A      x         0\n          y         0\n          z       NaN\n   B      x       NaN\n          y       NaN\n          z         0\ndtype: float64\n\nIn [7]: df.stack(level=0, dropna=False).stack(level=0, dropna=False)\nOut[7]:\n   Upper  Lower\n0  A      x         0\n          y         0\n          z       NaN\n   B      x       NaN\n          y       NaN\n          z         0\n1  A      x         0\n          y         0\n          z       NaN\n   B      x       NaN\n          y       NaN\n          z         0\ndtype: float64\n\nIn [13]: pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.2.final.0\npython-bits: 64\nOS: Windows\nOS-release: 8\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 62 Stepping 4, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.15.1\nnose: 1.3.4\nCython: 0.21.1\nnumpy: 1.9.1\nscipy: 0.14.0\nstatsmodels: 0.6.0\nIPython: 2.3.1\nsphinx: None\npatsy: 0.3.0\ndateutil: 2.2\npytz: 2014.9\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.4.2\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: 0.6.3\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.8\npymysql: None\npsycopg2: None\n```\n"},{"labels":["api"],"text":"The behaviour below occurs in version `'0.15.1'`.\n\nWhen a series has a duplicate index, the method `reindex` will raise an exception, unless the index passed to `reindex` is identical to the series' index.\n\nI propose that when a series has a duplicate index, the method `reindex` should always raise an exception, because when a series with a duplicate index is to be conformed to a new index, the intended behaviour is _always_ ambiguous.\n\nThis issue applies to the methods `reindex_like` and `reindex_axis` too.\n\nExamples of current behaviour:\n\n(a)\n\n``` python\n>>> pd.Series([1, 2, 3], index=['a', 'b', 'b']).reindex(['a', 'b'])\nValueError: cannot reindex from a duplicate axis\n```\n\n(b)\n\n``` python\n>>> pd.Series([1, 2, 3], index=['a', 'b', 'b']).reindex(['a', 'b', 'b'])\na    1\nb    2\nb    3\ndtype: int64\n```\n\nThe exception message in (a) implies that (b) should raise; but it doesn't.\n"},{"labels":["api",null,null],"text":"methods\n- to_timestamp\n- asfreq\n\nproperties\n- start_time/end_time\n"},{"labels":["api",null],"text":"xref #3004\nxref #841\nxref #7873 \nxref #7223\nxref #8815\n\nBuilding on @immerrr's excellent refactor in #8753, I would like to propose adding a `get_nearest` method to pandas.Index that does nearest neighbor lookups. The idea is that nearest neighbor lookups are usually the desirable/sane thing to do when using inexact indexes. Eventually, we might want to add an alternative \"wrapper index\" (like IntervalIndex), e.g., `NearestNeighborIndex` which switches the default behavior; this would be an intermediate step in that direction.\n\nThe implementation would be a simple wrapper that calls `Index.get_slice_bound` twice, once to the left and once to the right. Ideally, this would this would even work for array-like arguments, though perhaps there should be separate `get_nearest_loc` and `get_nearest_indexer` methods.\n"},{"labels":["api",null,null,null],"text":"from [SO](http://stackoverflow.com/questions/26973393/between-time-not-working-on-a-pandas-panel/26974754#26974754)\r\n\r\nxref #2141 "},{"labels":["api",null,null],"text":"Hello,\n\nPandas Series provide `to_json` method but they doesn't provide `to_html` method.\nAdding `to_html` will avoid user to convert a `pandas.core.series.Series` to a `pandas.core.frame.DataFrame` in order to output HTML file using `s.to_frame().to_html(...)` but directly use `s.to_html(...)`\n\nsee also https://github.com/pydata/pandas/issues/8825\n\nKind regards\n"},{"labels":["api",null,null],"text":"Hello,\n\nIPython autocomplete (tab) shows\n\n```\nIn [1]: ts.to\nts.to_clipboard  ts.to_frame      ts.to_period     ts.to_string\nts.to_csv        ts.to_hdf        ts.to_pickle     ts.to_timestamp\nts.to_dense      ts.to_json       ts.to_sparse     ts.tolist\nts.to_dict       ts.to_msgpack    ts.to_sql\n```\n\nmaybe `tolist` should be named `to_list`\nand `tolist` should be marked as deprecated\nbecause it seems that other methods follow\nan implicit naming convention (`to_...`) . \n\nKind regards\n"},{"labels":["api",null,null],"text":"Hello,\n\nPandas Series provide `to_csv`method but they doesn't provide `to_excel` method.\nAdding `to_excel` will avoid user to convert a `pandas.core.series.Series` to a `pandas.core.frame.DataFrame` in order to output Excel file\n\nDataFrames provide \n\n```\ndf.to_clipboard  df.to_hdf        df.to_period     df.to_string\ndf.to_csv        df.to_html       df.to_pickle     df.to_timestamp\ndf.to_dense      df.to_json       df.to_records    df.to_wide\ndf.to_dict       df.to_latex      df.to_sparse\ndf.to_excel      df.to_msgpack    df.to_sql\ndf.to_gbq        df.to_panel      df.to_stata\n```\n\nSeries provide\n\n```\nts.to_clipboard  ts.to_frame      ts.to_period     ts.to_string\nts.to_csv        ts.to_hdf        ts.to_pickle     ts.to_timestamp\nts.to_dense      ts.to_json       ts.to_sparse     ts.tolist\nts.to_dict       ts.to_msgpack    ts.to_sql\n```\n\nKind regards\n"},{"labels":["api",null,null,null],"text":"from #8626 discussion\ncc @fkaufer\n\nallow `pd.factorize` to take a Dataframe and process with tuples of columns.\nfurther allow `DataFrame.factorize` with a `subset` argument which just calls `pd.factorize`\n\nimpl is below (e.g. simple fast-zipping then factorizing after dense conversion)\njust needs some tests\n\n```\nIn [41]: df = pd.DataFrame({'A':['a1','a1','a2','a2','a1'], 'B':['b1','b2','b1','b2','b1']})\n\nIn [42]: df\nOut[42]: \n    A   B\n0  a1  b1\n1  a1  b2\n2  a2  b1\n3  a2  b2\n4  a1  b1\n\nIn [43]: cols_as_tuples = pd.lib.fast_zip([df[col].get_values() for col in df.columns])\n\nIn [44]: cols_as_tuples \nOut[44]: array([('a1', 'b1'), ('a1', 'b2'), ('a2', 'b1'), ('a2', 'b2'), ('a1', 'b1')], dtype=object)\n\nIn [47]: pd.factorize(cols_as_tuples)\nOut[47]: \n(array([0, 1, 2, 3, 0]),\n array([('a1', 'b1'), ('a1', 'b2'), ('a2', 'b1'), ('a2', 'b2')], dtype=object))\n\nIn [48]: pd.Categorical(cols_as_tuples)\nOut[48]: \n[(a1, b1), (a1, b2), (a2, b1), (a2, b2), (a1, b1)]\nCategories (4, object): [(a1, b1) < (a1, b2) < (a2, b1) < (a2, b2)]\n\nIn [59]: pd.Categorical(df.to_records(index=False))\nOut[59]: \n[(a1, b1), (a1, b2), (a2, b1), (a2, b2), (a1, b1)]\nCategories (4, object): [(a1, b1) < (a1, b2) < (a2, b1) < (a2, b2)]\n```\n"},{"labels":["api",null],"text":"From http://stackoverflow.com/questions/26918045/python-pandas-typeerror-cannot-compare-type-timestamp-with-type-float\n\nThe docstring of `Index.asof` says:\n\n```\nIn [73]: pd.Index.asof?\nType:        instancemethod\nString form: <unbound method Index.asof>\nFile:        c:\\anaconda\\lib\\site-packages\\pandas\\core\\index.py\nDefinition:  pd.Index.asof(self, label)\nDocstring:\nFor a sorted index, return the most recent label up to and including\nthe passed label. Return NaN if not found\n```\n\nwhile `asof` only works for a time series.\n- Clarify the docs that `asof` only works for `DatetimeIndex` (and PeriodIndex?)\n- Why is this actually restricted to timestamps? This functions seems also possibly useful to me for other index types? Something like\n  \n  ```\n  In [74]: idx = pd.Index([1,2,3,8,9])\n  \n  In [75]: idx.asof(5)    # gives now TypeError\n  3\n  ```\n  \n  Instead of having to do:\n  \n  ```\n  In [78]: idx[idx.searchsorted(5) - 1]\n  Out[78]: 3\n  ```\n"},{"labels":["api",null,null],"text":"From http://stackoverflow.com/questions/26924904/check-if-dataframe-column-is-categorical/26925340#26925340\n\nWhat is the preferred way to check for categorical dtype?\n\nI now answered:\n\n```\nIn [42]: isinstance(df.cat_column.dtype, pd.core.common.CategoricalDtype)\nOut[42]: True\n\nIn [43]: pd.core.common.is_categorical_dtype(df.cat_column)\nOut[43]: True\n```\n\nBut:\n- this seems somewhat buried in pandas. Should there be a more top-level function to do this?\n- we should add the preferred way to the categorical docs.\n"},{"labels":["api",null,null],"text":"I recently updated Pandas and found this strange behaviour which broke some of my existing code. \nI was using a column of Datetime.date objects as a the second level in a two-level MulitIndex. \nHowever, when setting the index with the latest version, the Datetime.date objects are converted to Timestamp objects with 00:00:00 as the time component:\n\n> > > pd.**version**\n> > > '0.15.1'\n> > > df\n> > >           0  ID        date\n> > > 0  0.486567  10  2014-11-12\n> > > 1  0.214374  20  2014-11-13\n> > > df.date[0]\n> > > datetime.date(2014, 11, 12)\n> > > df.set_index(['ID', 'date']).index[0](10, Timestamp%28'2014-11-12 00:00:00'%29)\n\nThis doesn't happen with version 0.14 or older.\n\nThere is a hack to get around it, setting the dates to a single level index, adding the other level and then swapping:\n\n> > > df.set_index('date').set_index('ID', append=True).index.swaplevel(0, 1)[0](10, datetime.date%282014, 11, 12%29)\n\nThis seems strange and I wondered was it intentional.\n"},{"labels":["api",null],"text":"`Series.mask` just performs a mask operation. `Series.where` accepts substitute values. (In the meantime, workaround is simple - just invert the where selection).\n\nVersion 0.15.1 docs:\n\n```\nSeries.mask(cond)\n     Returns copy whose values are replaced with nan if the inverted condition is True\n\nSeries.where(cond, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True)\n     Return an object of same shape as self and whose corresponding entries are from self where cond is True and otherwise are from other.\n\n```\n"},{"labels":["api",null,null],"text":"From http://stackoverflow.com/questions/26882499/reset-time-part-of-a-pandas-timestamp#26882499\n\nThere is now a `DatetimeIndex.normalize()` method to set times to midnight. \nWould it be useful to also have this method available on the individual `Timestamp` object?\n\nFurther, this method could also be added to the `.dt` accessor (udpate: for this, there already is a similar issue: #5502).\n"},{"labels":["api",null],"text":"This should work:\n\n``` python\n    with pandas.HDFStore('file.h5') as f:\n        print(f)\n```\n\nas a more convenient form of \n\n``` python\n    with contextlib.closing(pandas.HDFStore('file.h5')) as f:\n         print(f)\n```\n\nAFAIK this is a easy as adding the following to `HDFStore`\n\n``` python\n    def _enter_(self): pass\n    def _exit_(self): self.close()\n```\n"},{"labels":["api",null],"text":"Hello All!\n\nMoving into Pandas from R/Matlab/Stata. One feature I'm finding I really miss: a field generated during a merge that reports, for each row, whether that row came from the left dataset, the right dataset, or was successfully merged. \n\nI do a lot of work in social science where our data is VERY dirty. For example, I'm often merging transliterated names from Pakistan, and so I just want to see after a merge how many records successfully merged, and then easily pull out the records of each time to compare. I'm including a kludge I'm using below, but an in-line option would be so nice, and I think others in my area would also appreciate it. \n\nThanks!\n\n``` python\n    df1['left'] = 1\n    df2['right'] = 1\n\n    mergedDF = pd.merge(df1,df2, how='outer', left_on='key1', right_on='key2')\n\n    def mergeVar(x):\n        if x['left'] == 1 and x['right'] == 1 :\n            return 'both'\n        elif x['left'] == 1 and x['right'] != 1:\n            return 'leftOnly'\n        else: return 'rightOnly'\n\n    mergedDF['mergeReport'] = mergedDF.apply(mergeVar, axis=1)\n    mergedDF.drop(['left', 'right'], axis = 1)\n```\n\n(I'd also be happy to help make it, but I'm relatively new to python, so would need some hand-holding to learn how to integrate a new option into an existing function...)\n"},{"labels":["api",null,null,null,null],"text":"Hello, I'm trying to apply this \n\n```\nfrom scipy.spatial import distance\ndef myrolling_apply(df):\n    print type(df)\n    print df\n    print df.dtypes\n    a = df['predicted'].values\n    b = df['actual'].values\n    #dst = distance.euclidean(a,b)\n    return(df)\n\nrrbug = pd.read_csv('bug.csv')\nrrbug = pd.rolling_apply(rrbug, 24, myrolling_apply)\n```\n\nTo this dataframe https://dl.dropboxusercontent.com/u/1118905/bug.csv. I would aspect to have a dataframe BUT df is a ndarray, as you see from the results:\n\n```\n<type 'numpy.ndarray'>\n[ 18.038866  18.81628   15.652594  19.418134  23.685213  21.636703\n  32.264257  44.563431  63.160193  46.759563  44.06377   45.75921\n  57.929419  58.121308  61.65729   59.300808  64.033246  57.590802\n  56.642796  53.321256  68.65092   57.555255  39.222144  34.622948]\n```\n"},{"labels":["api",null,null],"text":"While they have two members `labels` and `names` , single indices lack the member `levels` of multi-indices. \nIt can be useful to set it to `[df.values]` to treat the single indices in the same way as hierarchical indices thanks to duck typing. \n"},{"labels":["api",null],"text":"Would be nice to have the query method support partial string matching, so you could do the equivalence of df[df['A'].str.contains(\"abc\")] using query: df.query(\"A contains 'abc'\").\n"},{"labels":["api",null,null],"text":"What about implementing the following slice function? The df.start and df.end columns contain the start and end index required to slice the df.string.\n\n```\ndf.sliced = df.string[df.start:df.end]\n```\n\nCurrently we can slice columns with a fixed start and end index\n\n```\ndf.sliced = df.string.str.slice(1, -1)\n```\n\nHowever it would be great if we could do this using variable start and stop indices from the dataframe itself, without the need for lambda functions.\n\nPossible complications:\nI can imagine that this would be complicated by the presence of NaN values in the column. \n\nYou could either force users to clean up their data first, so they can only apply the function if the column dtypes of the start and stop are integers (basically: take your dirty boots off before stepping into the house!). \n\nOr you could be nice, and apply the slice function to anything in the target column that looks like a string, using anything in the start and end columns that looks like an integer. (not that I would have a clue how to do that!) Using this strategy, return NaN only when invalid strings, NaN or floats, or index-out-of-range are encountered?\n\nThis problem was raised along with  #8747 in a StackOverflow question. Some code and examples are given.\nhttp://stackoverflow.com/questions/26658213/how-can-i-find-the-start-and-end-of-a-regex-match-using-a-python-pandas-datafram\n\nedit: here is some example code, including a current workaround. Sorry, I'll make sure the code is included immediately next time.\n\n```\nimport pandas as pd\n#some example strings for slicing, start indices, stop indices\nh1,h2,h3 = 'MPTMGFWVYITVE','MP-NSSLVYIGLE','MPLETQDALYVAL' \ns1, s2, s3 = 1,1,2\ne1, e2, e3 = 7,7,8\n#create a pandas dataframe to hold the aligned sequences\ndf = pd.DataFrame({'hit':[h1,h2,h3],'start': [s1, s2, s3],'end': [e1, e2, e3]})\n#reorder columns\ndf = df[['hit', 'start', 'end']]\n\n#HERE is where the new syntax would be useful to slice the strings, example\n#df.sliced = df.hit.str[df.start:df.end] \n#or the equivalent df['sliced'] = df['hit'].str[df['start']:df['end']]\n\n#Current workaround using a lambda function that specifies the start & end columns.\nfn_slice_hit = lambda x : x['hit'][x['start']:x['end']]\n#apply the slice function to the dataframe\ndf['sliced'] = df.apply(fn_slice_hit, axis = 1)\n\nIn [2]: df\nOut[2]: \n             hit  start  end  sliced\n0  MPTMGFWVYITVE      1    7  PTMGFW\n1  MP-NSSLVYIGLE      1    7  P-NSSL\n2  MPLETQDALYVAL      2    8  LETQDA\n\n[3 rows x 4 columns]\n```\n"},{"labels":["api",null,null,null],"text":"What about including a method to get the start and stop after a regex search of items in a DataFrame . Perhaps using .str.extract?\n\nReturning the start as a new column would perhaps be as follows:\n\n```\ndf['start'] = df['string'].str.extract(pattern, output = 'start')\n```\n\nan alternative suggestion from jkitchen on StackOverflow was to use start_index = True, or end_index = True\n\n```\ndf['start'] = df['string'].str.extract(pattern, start_index = True)\n```\n\nFor multiple parameters (e.g. start and end) as outputs, there needs to be a way to avoid running the search twice. One solution would be to give the output as a tuple:\n\n```\ndf['regex_output_tuple'] = df['string'].str.extract(pattern, output = ('start','end'))\n```\n\nI don't use regex very often, so I don't know if there are other parameters that people want after a regex search. If there really is just the text in the groups, the start and the end, perhaps there's a way to put the output directly into new columns?\n\n```\ndf['groups'], df['start'], df['end']  = df['string'].str.extract(pattern, output = ('groups','start','end'))\n```\n\nI think it makes sense that non-matches return a NaN, just as in the regular extract function. This would mix integer and float datatypes in the  df['start'] column, but I guess we all know about that situation :)\n\nI'm not an experienced programmer, so sorry if I misunderstood some basic concepts.\n\nPlease see the question in StackOverflow for example code and comments:\nhttp://stackoverflow.com/questions/26658213/how-can-i-find-the-start-and-end-of-a-regex-match-using-a-python-pandas-datafram\n\nA block of example data and code is below, as requested by jreback.\n\n```\nimport pandas as pd\nimport re\n#some example query sequences, markup strings, hit sequences.\nq1,q2,q3 = 'MPIMGSSVYITVELAIAVLAILG','MPIMGSSVYITVELAIAVLAILG','MPI-MGSSVYITVELAIAVLAIL'\nm1,m2,m3 = '|| ||  ||||||||||||||||','||   | ||| :|| || |:: |','||:    ::|: :||||| |:: '\nh1,h2,h3 = 'MPTMGFWVYITVELAIAVLAILG','MP-NSSLVYIGLELVIACLSVAG','MPLETQDALYVALELAIAALSVA' \n#create a pandas dataframe to hold the aligned sequences\ndf = pd.DataFrame({'query':[q1,q2,q3],'markup':[m1,m2,m3],'hit':[h1,h2,h3]})\n\n#create a regex search string to find the appropriate subset in the query sequence, \ndesired_region_from_query = 'PIMGSS'\nregex_desired_region_from_query = '(P-*I-*M-*G-*S-*S-*)'\n\n#Pandas has a nice extract function to slice out the matched sequence from the query:\ndf['extracted'] = df['query'].str.extract(regex_desired_region_from_query)\n\n#However I need the start and end of the match in order to extract the equivalent regions \n#from the markup and hit columns. For a single string, this is done as follows:\nmatch = re.search(regex_desired_region_from_query, df.loc[2,'query'])\nsliced_hit = df.loc[2,'hit'][match.start():match.end()]\nprint('sliced_hit, non-vectorized example: ', sliced_hit)\n\n#HERE the new syntax is necessary\n#e.g. df['start'], df['end']  = df['string'].str.extract(pattern, output = ('start','end'))\n\n#My current workaround in pandas is as follows.\n#define function to obtain regex output (start, stop, etc) as a tuple\ndef get_regex_output(x):\n    m = re.search(regex_desired_region_from_query, x)\n    return (m.start(), m.end())\n#apply function\ndf['regex_output_tuple'] = df['query'].apply(get_regex_output)\n#convert the tuple into two separate columns\ncolumns_from_regex_output = ['start','end']      \nfor n, col in enumerate(columns_from_regex_output):\n    df[col] = df['regex_output_tuple'].apply(lambda x: x[n])\n#delete the unnecessary column\ndf = df.drop('regex_output_tuple', axis=1)\n```\n"},{"labels":["api",null,null,null,null],"text":"Motivating from [SO](http://stackoverflow.com/questions/26762100/reconstruct-a-categorical-variable-from-dummies-in-pandas)\n\nThis is the inverse of `pd.get_dummies`. So maybe `invert_dummies` is better?\nI think this name makes more sense though. \n\nThis seems a reasonable way to do it. Am I missing anything?\n\n```\nIn [46]: s = Series(list('aaabbbccddefgh')).astype('category')\n\nIn [47]: s\nOut[47]: \n0     a\n1     a\n2     a\n3     b\n4     b\n5     b\n6     c\n7     c\n8     d\n9     d\n10    e\n11    f\n12    g\n13    h\ndtype: category\nCategories (8, object): [a < b < c < d < e < f < g < h]\n\nIn [48]: df = pd.get_dummies(s)\n\nIn [49]: df\nOut[49]: \n    a  b  c  d  e  f  g  h\n0   1  0  0  0  0  0  0  0\n1   1  0  0  0  0  0  0  0\n2   1  0  0  0  0  0  0  0\n3   0  1  0  0  0  0  0  0\n4   0  1  0  0  0  0  0  0\n5   0  1  0  0  0  0  0  0\n6   0  0  1  0  0  0  0  0\n7   0  0  1  0  0  0  0  0\n8   0  0  0  1  0  0  0  0\n9   0  0  0  1  0  0  0  0\n10  0  0  0  0  1  0  0  0\n11  0  0  0  0  0  1  0  0\n12  0  0  0  0  0  0  1  0\n13  0  0  0  0  0  0  0  1\n\nIn [50]: x = df.stack()\n\n# I don't think you actually need to specify ALL of the categories here, as by definition\n# they are in the dummy matrix to start (and hence the column index)\nIn [51]: Series(pd.Categorical(x[x!=0].index.get_level_values(1)))\nOut[51]: \n0     a\n1     a\n2     a\n3     b\n4     b\n5     b\n6     c\n7     c\n8     d\n9     d\n10    e\n11    f\n12    g\n13    h\nName: level_1, dtype: category\nCategories (8, object): [a < b < c < d < e < f < g < h]\n```\n\nNB. this is buggy ATM.\n\n```\nIn [51]: Series(pd.Categorical(x[x!=0].index.get_level_values(1)),categories=df.categories)\n```\n"},{"labels":["api",null],"text":"Just had something odd come up while trying to come up with something for [this SO question](http://stackoverflow.com/questions/26747689/mapping-pandas-dataframe-rows-to-a-pandas-series).\n\nIf we use `DataFrame.apply()` to try and create dictionaries from the rows of a dataframe, it seems to return the `dict.values()` method rather than returning the dict itself.\n\n``` python\ndf = pd.DataFrame({'k': ['a', 'b', 'c'], 'v': [1, 2, 3]})\ndf.apply(lambda row: {row['k']: row['v']}, axis=1)\nOut[52]: \n0    <built-in method values of dict object at 0x07...\n1    <built-in method values of dict object at 0x03...\n2    <built-in method values of dict object at 0x07...\ndtype: object\n```\n\nLooks like it's probably something to do with trying to grab the `values` attribute when the output of the applied function is a Series or something similar.\n\nLibrary versions:\n\n```\npd.show_versions()\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.6.final.0\npython-bits: 32\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 30 Stepping 5, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.15.0\nnose: 1.3.4\nCython: 0.21\nnumpy: 1.9.0\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.3.0\nsphinx: 1.2.3\npatsy: 0.3.0\ndateutil: 1.5\npytz: 2014.7\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.2\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.7\nlxml: 3.4.0\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: None\n```\n"},{"labels":["api",null,null],"text":"from [SO](http://stackoverflow.com/questions/26701849/pandas-groupby-and-finding-maximum-in-groups-returning-value-and-count)\n\nThinking that this is a nice general idiom that might deserve a function: \n\n`df.loc[df.groupby(level='host').idxmax()['no']]`\n\nmaybe:\na) `df.groupby(level='host').idxmax(loc='no')` ? (e.g. have the idxmin/idxmin) functions take a 'loc' argument. or is this bending the api a big much?\nb) `df.groupby(level='host').loc('no').idxmax()`\nc) `df.groupby(level='hist').loc[lambda x: x.idxmax()['no']]`\nd) `df.groupby(level='hist').loc('no','idxmax')`\n\n```\ndf_logfile = pd.DataFrame({ \n    'host' : ['this.com', 'this.com', 'this.com', 'that.com', 'other.net', \n              'other.net', 'other.net'],\n    'service' : ['mail', 'mail', 'web', 'mail', 'mail', 'web', 'web' ] })\n\ndf = df_logfile.groupby(['host','service'])['service'].agg({'no':'count'})\nmask = df.groupby(level=0).agg('idxmax')\ndf_count = df.loc[mask['no']]\ndf_count = df_count.reset_index()\n\n# yields\n        host service  no\n0  other.net     web   2\n1   that.com    mail   1\n2   this.com    mail   2\n```\n"},{"labels":["api",null],"text":"```\ns = Series(list('abbc')).astype('category')\n```\n\nThis raises\n\n```\ns.hist()\n```\n\nThis works\n\n```\ns.value_counts().plot()\n```\n\nI think this should work, beyond this, anyone have ideas on what/how to plot categorical data?\n\ncc @JanSchulz\ncc @TomAugspurger \ncc @jorisvandenbossche \n"},{"labels":["api",null,null,null],"text":"mentioned in #8705 \n\nneed to be able to combine like categorical (e.g. ordered,categories are ==)\ninto a 2-d structure so can store in a single CategoricalBlock\n"},{"labels":["api",null,null,null],"text":"So this as expected. Though I its pretty straightforward to add support for this directly to a Timedelta, after the notion is the same, _though_ a `time` object can support a time-zone so not sure if this is odd (or maybe we should add an optional tz to Timedelta).\n\n```\nIn [5]: from datetime import time\n\nIn [6]: time(secIn [8]: pd.Timedelta(time(second=1))\nValueError: Value must be Timedelta, string, integer, float, timedelta or convertible\n\nIn [9]: pd.Timedelta('1s')          \nOut[9]: Timedelta('0 days 00:00:01')\n```\n"},{"labels":["api",null],"text":"This results in an API change, so holding off till later\n\nhttps://github.com/jreback/pandas/tree/milliseconds\n"},{"labels":["api",null],"text":"The  `result_type` argument of `Series.str.split` has a default value of `'series'` for compatibility purposes. This should be deprecated in favor of the default `'frame'`.\n\nSee issue #8428 , PR #8663 for introduction of the argument and discussion of default value.\n"},{"labels":["api",null],"text":"I messed up and used `=` instead of `==` in a `query`.\n\n``` python\ndf = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\ndf.query('a=1')\n```\n\nThat raises a ValueError. But `df` was modified.\n\n``` python\nIn [15]: df\nOut[15]:\n   a  b\n0  1  a\n1  1  b\n2  1  c\n```\n\nversions:\n\n```\npandas: 0.15.0-6-g403f38d\nbottleneck: None\ntables: None\nnumexpr: 2.3.1\n```\n\nCan't look right now.\n"},{"labels":["api",null,null,null,null],"text":"Catchall for `rolling_*` issues:\r\n- [x] #12537 exclude nuiscance columns\r\n- [ ] #12536 fully support timedeltas\r\n- [ ] #4130 (return type of `rolling_apply`)\r\n- [x] #3185 (stop using `np.apply_along_axis`)\r\n- [x] #5071, #12950 (pass frames, operate 2-D)\r\n- [x] #9413 `rolling_quantile`\r\n- [ ] #9481 `rolling_idxmax`\r\n- [ ] #10759 rolling_qcut \r\n- [ ] #11446, #20773  casting in `.apply`\r\n- [x] #12595 casting in all window functions (min/max are good)\r\n- [x] #10702 namespacing (though prob just create a `Rolling` object), #11603 \r\n- [x] #12535 testing of datetimelike with nans\r\n- [x] #12541 count should be integer dtype (and deal with infs)\r\n- [ ] #15095 Table-wise\r\n\r\n---\r\n\r\nHi all,\r\n\r\nI intended to apply a function that gives on each day a ranking based on means calculated from previous n-day's data. The natural way is to use pd.rolling_apply. A toy example:\r\n\r\n```\r\nIn [93]: df = pd.DataFrame(np.random.randint(10, size=20).reshape(4, 5))\r\n\r\nIn [94]: df\r\nOut[94]: \r\n   0  1  2  3  4\r\n0  2  0  0  2  0\r\n1  9  5  5  6  1\r\n2  2  3  6  8  8\r\n3  5  1  2  9  0\r\n\r\nIn [95]: import bottleneck as bn\r\n\r\nIn [96]: bn.nanrankdata(df.mean())\r\nOut[96]: array([ 4. ,  1.5,  3. ,  5. ,  1.5])\r\n```\r\n\r\nUp to now, it is cool. Then:\r\n\r\n```\r\nIn [97]: pd.rolling_apply(df, 2, lambda x: bn.nanrankdata(bn.nanmean(x, axis=0)))\r\nOut[97]: \r\n    0   1   2   3   4\r\n0 NaN NaN NaN NaN NaN\r\n1   1   1   1   1   1\r\n2   1   1   1   1   1\r\n3   1   1   1   1   1\r\n```\r\n\r\nThis is clearly wrong. Is this a bug?\r\n"},{"labels":["api",null,null],"text":"The `pandas.concat` routine's behaviour is slightly at odds with its documentation (and error messages). The documentation states that the first parameter (objs) accepts \"list or dict of Series, DataFrame, ...\" but the routine is rather more forgiving and appears to accept lists, tuples, dicts, and generator objects (very useful!); hence my impression (during usage) was that it accepted \"iterables\" generally. Unfortunately it turns out this isn't the case; attempting to concatenate a deque of DataFrame objects results in the following:\n\n``` python\nfrom collections import deque\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({'a': [1, 2, 3], 'b': [4, 5, 6]})\nd = deque((df, df, df))\npd.concat(d)\n```\n\n```\n----> 1 pd.concat(d)\n\n/home/dave/arcticenv/lib/python3.4/site-packages/pandas/tools/merge.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\n    720                        keys=keys, levels=levels, names=names,\n    721                        verify_integrity=verify_integrity,\n--> 722                        copy=copy)\n    723     return op.get_result()\n    724 \n\n/home/dave/arcticenv/lib/python3.4/site-packages/pandas/tools/merge.py in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\n    735             raise TypeError('first argument must be a list-like of pandas '\n    736                             'objects, you passed an object of type '\n--> 737                             '\"{0}\"'.format(type(objs).__name__))\n    738 \n    739         if join == 'outer':\n\nTypeError: first argument must be a list-like of pandas objects, you passed an object of type \"deque\"\n```\n\nThe error message states that the first argument \"must be a list-like of pandas objects\" (which is slightly different to the documentation, and closer to the actual implementation's behaviour). Given that a deque is iterable but not indexable (similar to a generator expression) it seems to fulfil the criteria of being \"list-like\".\n\nDigging into the implementation, the test that's failing appears to be the first line of `_Concatenator.__init__` in `pandas.tools.merge` which reads as follows (in my installation):\n\n``` python\n        if not isinstance(objs, (list,tuple,types.GeneratorType,dict,TextFileReader)):\n            raise TypeError('first argument must be a list-like of pandas '\n                            'objects, you passed an object of type '\n                            '\"{0}\"'.format(type(objs).__name__))\n```\n\nSo it appears the actual set of iterables accepted by `pandas.concat` is lists, tuples, generator expressions, dicts, and instances of `TextFileReader`. I suggest that it might be better to check for (and act upon) special cases and otherwise assume that `objs` is a suitable iterable of `DataFrame` objects. In other words, get rid of that check entirely, and add a couple of checks for \"expected\" special cases (such as a user mistakenly passing a `DataFrame` as objs; there's already a check in place for dicts a bit further on).\n\nThe conversion of `objs` to a list-comprehension later on (below `if keys is None`) should raise a `TypeError` in the case that it isn't iterable so the change shouldn't cause much impact (i.e. in the case of a non-iterable passed as `objs`, it'll raise an exception of the same class as the existing code).\n\nIf this sounds reasonable, I'm happy to provide a pull request?\n"},{"labels":["api",null,null],"text":"this should actually be relatively straightforward.\n\nIncorporating this library: https://github.com/libdydn/dynd-python\nas an optional dep, allows native Integer NA support (implemented\nvery similarly to how NaT is represented in `datetime64[ns]` dtypes ATM.\n\n`dynd-python` are the python bindings for `libdynd` and have an almost 100% numpy\ncompatibility (they use the buffer interface so should be perf-wise pretty costless to do this).\n\ntangentially related to #8350 \n\ncc @mwiebe \ncc @shoyer\ncc @jorisvandenbossche \ncc @cpcloud \n"},{"labels":["api",null,null,null,null,null],"text":"*update for 2019-10-07: We have a StringDtype extension dtype. It's memory model is the same as the old implementation, an object-dtype ndarray of strings. The next step is to store & process it natively*.\r\n\r\n---\r\n\r\nxref #8627 \r\nxref #8643, #8350\r\n\r\nSince we introduced `Categorical` in 0.15.0, I think we have found 2 main uses.\r\n\r\n1) as a 'real' Categorical/Factor type to represent a limited of subset of values that the column can take on\r\n2) as a memory saving representation for object dtypes.\r\n\r\nI could see introducting a `dtype='string'` where `String` is a slightly specialized sub-class of `Categroical`, with 2 differences compared to a 'regular' Categorical:\r\n- it allows unions of arbitrary other string types, currently `Categorical` will complain if you do this:\r\n\r\n```\r\nIn [1]: df = DataFrame({'A' : Series(list('abc'),dtype='category')})\r\nIn [2]: df2 = DataFrame({'A' : Series(list('abd'),dtype='category')})\r\nIn [3]: pd.concat([df,df2])\r\nValueError: incompatible levels in categorical block merge\r\n```\r\n\r\nNote that this works if they are `Series` (and prob should raise as well, side -issue)\r\n\r\nBut, if these were both 'string' dtypes, then its a simple matter to combine (efficiently).\r\n- you can restrict the 'sub-dtype' (e.g. the dtype of the categories) to `string/unicode` (iow, don't allow numbers / arbitrary objects), makes the constructor a bit simpler, but more importantly, you now have a 'real' non-object string dtype.\r\n\r\nI don't think this would be that complicated to do. The big change here would be to essentially convert any object dtypes that are strings to `dtype='string'` e.g. on reading/conversion/etc. might be a perf issue for some things, but I think the memory savings greatly outweigh.\r\n\r\nWe would then have a 'real' looking object dtype (and `object` would be relegated to actual python object types, so would be used much less).\r\n\r\ncc @shoyer\r\ncc @JanSchulz\r\ncc @jorisvandenbossche \r\ncc @mwiebe \r\nthoughts?\r\n"},{"labels":["api",null,null],"text":"I'm not entirely sure why (perhaps just the trouble of implementing it), but categoricals do not currently support arithmetic:\n\n> In contrast to statistical categorical variables, categorical data might have an order (e.g. ‘strongly agree’ vs ‘agree’ or ‘first observation’ vs. ‘second observation’), but numerical operations (additions, divisions, ...) are not possible.\n\nSimilarly to string operations (#8627), arithmetic with scalars could very efficient transform categoricals into new categoricals.\n\nIn contrast, array + array operations should probably just return a normal array.\n\nCC @JanSchultz @jreback \n"},{"labels":["api",null,null],"text":"There seems no direct way to return to the original dtype and the [documentation](http://pandas-docs.github.io/pandas-docs-travis/categorical.html#object-creation) recommends: _\"To get back to the original Series or numpy array, use Series.astype(original_dtype) or np.asarray(categorical)\"_\n\nThat's slow and a `decode` or `decat` method would be trivial:\n\n``` python\ndf=pd.DataFrame(np.random.choice(list(u'abcde'), 4e6).reshape(1e6, 4),\n    columns=list(u'ABCD'))                                     \nfor col in df.columns: df[col] = df[col].astype('category')   \n\n%timeit for col in df.columns: df[col].astype('unicode')      \n1 loops, best of 3: 1.06 s per loop\n\n%timeit for col in df.columns: cats=df[col].cat.categories; cats[df[col].cat.codes]    \n10 loops, best of 3: 33.2 ms per loop   \n```\n\nI was working with ~10 categories (partially longer strings) on a 20 mio rows dataset where the difference was even bigger (unfortunately can't reproduce it with dummy data) and using `astype` felt rather buggy (minutes) than only a performance issue.\n\nGiven the current limitations on exporting categorical data, having a fast `decode` method would be very convenient. Since category codes are most often strings an optional parameter for direct character set encoding would also be good to have for such a method.\n\n``` python\n%timeit for col in df.columns: df[col].astype('unicode').str.encode('latin1')  \n1 loops, best of 3: 3.95 s per loop\n%timeit for col in df.columns: cats=pd.Series(df[col].cat.categories).str.encode('latin1'); cats[df[col].cat.codes]                                                                  \n10 loops, best of 3: 74.5 ms per loop   \n```\n"},{"labels":["api",null,null,null],"text":"xref #8595\n\nsomething like: `Interval(left=(-1.5,'open),right=(0,'closed'))`\n\nreprs to `(-1.5,0]`\n\ncan be used immediately in the index values for `pd.cut` and such (as an object index)\nevenutally can form the basis for `IntervalIndex`\n\ncc @shoyer\ncc @JanSchulz\ncc @rosnfeld\n"},{"labels":["api",null,null,null],"text":"xref #8599, #8578 \n\nallow a nice UX and a more correct estimate of memory usage for object (string) columns\nprob will require a cythonized evaluation\n\nthis would replace the need for the '+' on object dtypes for mem usage\n"},{"labels":["api",null],"text":"A lot of functions in pandas ignore any additional keyword arguments in `**kwargs`. This makes it very easy for unnoticed mistakes in the code, without getting any error; I have personally came across both cases below in my code:\n\n```\n>>> def foo(df):\n...     df.drop('joe', axis=1, inplce=True)  # mis-spelling a keyword\n...     df.reindex([1, 0], inplace=True)  # specifying an unsupported keyword\n...     return df\n\n>>> df\n   jim  joe  jolie\n0   71   22     92\n1   78   55     77\n\n>>> foo(df)\n   jim  joe  jolie\n0   71   22     92\n1   78   55     77\n```\n\nAnother example that this may happen unnoticed is with legacy code, once there is an api change; for example once `cols` argument is dropped form [`.drop_duplicates`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html#pandas.DataFrame.drop_duplicates). I think, at least at the public api level, the code should check if after `.pop`ing supported key-word arguments, anything is left in `**kwargs`. [example from scipy project](https://github.com/scipy/scipy/blob/5cf30815cbf2bf9e66cea8e289fa09213e8161fc/scipy/optimize/optimize.py#L654).\n"},{"labels":["api",null,null,null],"text":"it would be nice to have number in front of all labels. put number like 00, 01, 02 in front of labels so that it would order appropriately.\n\nLib\\site-packages\\pandas\\tools\\tile.py\n\n```\ndef _format_levels(bins, prec, right=True,\n                   include_lowest=False):\n    fmt = lambda v: _format_label(v, precision=prec)\n    cnter=0\n    if right:\n        levels = []\n        for a, b in zip(bins, bins[1:]):\n            fa, fb = fmt(a), fmt(b)\n\n            if a != b and fa == fb:\n                raise ValueError('precision too low')\n\n            formatted = '%02d: (%s, %s]' % (cnter, fa, fb)\n            cnter=cnter +1\n            levels.append(formatted)\n\n        if include_lowest:\n            levels[0] = '[' + levels[0][1:]\n    else:\n        levels = ['[%s, %s)' % (fmt(a), fmt(b))\n                  for a, b in zip(bins, bins[1:])]\n\n    return levels\n```\n"},{"labels":["api",null,null,null],"text":"The behaviour below occurs in versions `'0.15.0rc1-21-g32c5016'` and `'0.14.1'`.\n\nWhen the `label` passed to the `drop` method of a `Series` is not in the axis:\n(a) if the index is not a `MultiIndex` then an exception is raised (in most cases; see issue #8530)\n(b) if the index is a `MultiIndex` then an exception is not raised; the original series is returned\n\nExamples of current behaviour:\n(a)\n\n``` python\n>>> pd.Series([10, 20, 30], [1, 2, 3]).drop(7)\n\nValueError: labels [7] not contained in axis\n```\n\n(b)\n\n``` python\n>>> pd.Series(\n    [10, 20, 30],\n    pd.MultiIndex(\n        levels=[[1, 2, 3], [4, 5, 6]],\n        labels=[[0, 1, 2], [0, 1, 2]],\n        names=['a', 'b'])\n).drop(7, level='a')\n\na  b\n1  4    10\n2  5    20\n3  6    30\ndtype: int64\n```\n\nI propose that in (b) an exception be raised. Furthermore, an exception should be raised in (b) if (for instance) the label passed to `drop` had been `4`, because although `4` is in the axis, it is not in the specified level.\n\nThe exception message in (b) would have to refer to the level, e.g.:\n\n``` python\nValueError: labels [7] not contained in level\n```\n"},{"labels":["api",null,null,null],"text":"- [ ] `pd.Summary`\n- [x] dict-of-dict #9052\n- [ ] named functions #10100 \n\nFrom [SO](http://stackoverflow.com/questions/26456125/python-pandas-is-order-preserved-when-using-groupby-and-agg/26465555?noredirect=1#comment41593401_26465555)\n\n```\nIn [100]: df = pd.DataFrame({'A': ['group1', 'group1', 'group2', 'group2', 'group3', 'group3'],\n                   'B': [10, 12, 10, 25, 10, 12],\n                   'C': [100, 102, 100, 250, 100, 102]})\n\nIn [101]: df\nOut[101]: \n        A   B    C\n0  group1  10  100\n1  group1  12  102\n2  group2  10  100\n3  group2  25  250\n4  group3  10  100\n5  group3  12  102\n\nIn [102]: df.groupby('A').agg(['mean',lambda x: x.iloc[1]])\nOut[102]: \n           B               C          \n        mean  <lambda>  mean  <lambda>\nA                                     \ngroup1  11.0        12   101       102\ngroup2  17.5        25   175       250\ngroup3  11.0        12   101       102\n```\n\nWould be nice to be able to use `lambda x: x.nth(1)` or maybe `'nth(1)'`\n`In [103]: df.groupby('A').agg(['mean','nth(1')])`\n"},{"labels":["api",null],"text":"Related  #7770\n\nUsing the example of the docs (http://pandas.pydata.org/pandas-docs/stable/reshaping.html#multiple-levels):\n\n```\ncolumns = MultiIndex.from_tuples([('A', 'cat', 'long'), ('B', 'cat', 'long'), ('A', 'dog', 'short'), ('B', 'dog', 'short')], \n                                 names=['exp', 'animal', 'hair_length'])\ndf = DataFrame(randn(4, 4), columns=columns)\n```\n\nCONTEXT: `df.stack(level=['animal', 'hair_length'])` and `df.stack(level=[1, 2])` are equivalent (feature introduced in #7770). Mixing integers location and string names (eg `df.stack(level=['animal', 2])`) gives a ValueError.\n\n**But** if you have level names of mixed types, some different (and wrong things) happen:\n- With a total different number, it still works as it should:\n  \n  ```\n  df.columns.names = ['exp', 'animal', 10]\n  df.stack(level=['animal', 10])\n  ```\n- With the number 1, it treats the 1 as a level number instead of the level name, leading to a wrong result (two times the same level unstacked):\n  \n  ```\n  In [42]: df.columns.names = ['exp', 'animal', 1]\n  \n  In [43]: df.stack(level=['animal', 1])\n  Out[43]: \n  exp                     A         B\n    animal animal                    \n  0 cat    cat    -1.006065  0.401136\n    dog    dog     0.526734 -1.753478\n  1 cat    cat    -0.718401 -0.400386\n    dog    dog    -0.951336 -1.074323\n  2 cat    cat     1.119843 -0.606982\n    dog    dog     0.371467 -1.837341\n  3 cat    cat    -1.467968  1.114524\n    dog    dog    -0.040112  0.240026\n  ```\n- With the number 0, it gives a strange error:\n  \n  ```\n  In [46]: df.columns.names = ['exp', 'animal', 0]\n  \n  In [47]: df.stack(level=['animal', 0])\n  ---------------------------------------------------------------------------\n  KeyError                                  Traceback (most recent call last)\n  <ipython-input-47-4e9507e0708f> in <module>()\n  ----> 1 df.stack(level=['animal', 0])\n  \n  /home/joris/scipy/pandas/pandas/core/frame.pyc in stack(self, level, dropna)\n  3390 \n  3391         if isinstance(level, (tuple, list)):\n  -> 3392             return stack_multiple(self, level, dropna=dropna)\n  3393         else:\n  3394             return stack(self, level, dropna=dropna)\n  \n  ....\n  \n  /home/joris/scipy/pandas/pandas/core/index.pyc in _partial_tup_index(self, tup, side)\n  3820             raise KeyError('Key length (%d) was greater than MultiIndex'\n  3821                            ' lexsort depth (%d)' %\n  -> 3822                            (len(tup), self.lexsort_depth))\n  3823 \n  3824         n = len(tup)\n  \n  KeyError: 'Key length (2) was greater than MultiIndex lexsort depth (0)'\n  ```\n"},{"labels":["api",null,null],"text":"Currently it is easy to apply functions to the data in a row or column using apply.  However, when important parameters for the analysis is contained in the multiindex for the row or column being analyzed (that is, the column index when analyzing along a column or the row index when analyzing along a row), there is currently no easy way to make use of these parameters.  \n\nI think it would be useful if users could provide a dictionary that maps index levels to function arguments to use when applying a function to a pandas Series, DataFrame, Panel, or Grouper.\n\nSome specific examples:\n1. One of the multiindex levels is sampling frequency, which varies across rows.  You want to do the fft of each row, but you need the sampling frequency for that.\n2. One of the multiindex levels is the total duration of each recording, and you want to do a count of events per unit of time.\n3. Each element is a time point.  One of the multiindex levels is the minimum allow of the recording and another is the stop time, and you want to slice the results to only include values between the start and stop.\n\nThese are all easy if the values are the same for all rows or columns, but become much harder currently if they vary.\n"},{"labels":["api",null,null],"text":"from [SO](http://stackoverflow.com/questions/26421221/propogation-of-metadata-from-dataframe-to-series/26423596#26423596)\nxref #2485 \nxref #7868\n- [ ] documentation (maybe in the cookbook) to start.\n- [ ] add _metadata to Index\n- [ ] top-level metadata control (maybe)\n\n```\ndef unit_meta(self, other, method=None):\n     # combine and return a new unit for this\n\n      if self.unit == other.unit:\n          return self.unit\n\n      # coerce unit\n      return coerced_unit\n\npd.set_metadata({ 'unit' : unit_meta })\n```\n\nThis last will require a bit of change in `__finalize__` to handle a metadata finalizer for a specific name (but straightforward)\n"},{"labels":["api",null],"text":"from [SO](http://stackoverflow.com/questions/26421221/propogation-of-metadata-from-dataframe-to-series/26423596#26423596)\n\nthis allows `copy.copy(df)` to work properly\n\nby definition 'always' deep\n\n```\n__copy__ = copy\n__deepcopy__ = copy\n```\n"},{"labels":["api",null,null],"text":"Would it make sense for IndexingError to subclass IndexError to facilitate duck-typing as well as catching IndexError exceptions?\n"},{"labels":["api",null,null],"text":"`Series.value_counts()` also shows categories with count 0.\n\nThought this would be a bug but according to [doc](http://pandas.pydata.org/pandas-docs/version/0.15.0/categorical.html#operations) it is intentional.\n\nThis makes the output of value_counts inconsistent when switching between category and non-category dtype. Apart from that it blows up the value_counts output for series with many categories.\n\nI would prefer to hide counts (i.e. zero) for non-occuring categories by default and rather consider a parameter `dropzero=True` similar to `dropna` (see also #5569).\n"},{"labels":["api",null,null,null],"text":"Subtracting a scalar datetime from a series returns timedeltas, but doesn't do the same for a dataframe.\n\n```\nimport pandas as pd \nseries = pd.Series(pd.date_range('2000-1-1', periods=100), index=range(0, 100))\ndf = pd.DataFrame(series)\ndate = pd.Timestamp('1995-1-1')\n\nseries - date\n#0    1826 days                                                                                 \n#1    1827 days                                                                                 \n#2    1828 days\n# ....   \n\ndf - date\n#             0\n#0  1975-01-01                                                                                  \n#1  1975-01-02                                                                                  \n#2  1975-01-03\n# ....\n```\n\nIt seems like both should return timedeltas.\n"},{"labels":["api",null,null],"text":"I am converting a Stata dataset to a dataframe, then multiply two columns to create a third one. The chosen datatype for the two columns is int8 and for some reason the new column blindly follows that.\n\nFor instance, this code\n\n``` Python\ndf = pd.read_stata(file)\ndf['w_age_educ'] = df['w_age'] * df['weduc']\nprint(df[['w_age', 'weduc', 'w_age_educ']].dtypes)\nprint(df[['w_age', 'weduc', 'w_age_educ']][:3])\n```\n\nwould give me\n\n``` Python\nw_age         int8\nweduc         int8\nw_age_educ    int8\ndtype: object\n   w_age  weduc  w_age_educ\n0     44     14         104\n1     34     13         -70\n2     33     18          82\n```\n\nIs this a bug or intended behavior? If the latter, how can I get my desired product column?\n"},{"labels":["api",null,null],"text":"(edit):\r\n\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime as dt\r\nimport numpy as np\r\n\r\ndatetime_start = dt.datetime(2014, 9, 1, 9, 30)\r\ndatetime_end = dt.datetime(2014, 9, 1, 16, 0)\r\n\r\ntt = pd.date_range(datetime_start, datetime_end, freq='1Min')\r\ndf = pd.DataFrame(np.arange(len(tt)), index=tt, columns=['A'])\r\n```\r\n\r\nThe first item is at 9:30, which is not divisible by 8 minutes. We'd like `df.resample('8T', base='start').first()` to be equivalent to `df.resample('8T', base=2)`.\r\n\r\n```python\r\nIn [13]: df.resample(\"8T\", base=2).first()\r\nOut[13]:\r\n                       A\r\n2014-09-01 09:30:00    0\r\n2014-09-01 09:38:00    8\r\n2014-09-01 09:46:00   16\r\n2014-09-01 09:54:00   24\r\n2014-09-01 10:02:00   32\r\n```\r\n\r\n\r\n---\r\n\r\nIt seems that for 1Min bar data, resample() with sampling frequency of any multiple of 8 has a bug. The code below illustrates the bug when resampling is done at [3, 5, 6, 8, 16] Min. For both 3 and 5 frequency, the first entry of the resampled dataframe index starts at the base timestamp (9:30 in this case) while for frequencies 8 and 16, the resampled index starts at 9:26 and 9:18 respectively.\r\n\r\n``` Python\r\nimport pandas as pd\r\nimport datetime as dt\r\nimport numpy as np\r\n\r\ndatetime_start = dt.datetime(2014, 9, 1, 9, 30)\r\ndatetime_end = dt.datetime(2014, 9, 1, 16, 0)\r\n\r\ntt = pd.date_range(datetime_start, datetime_end, freq='1Min')\r\ndf = pd.DataFrame(np.arange(len(tt)), index=tt, columns=['A'])\r\n\r\nfor freq in [3, 5, 6, 8, 16]:\r\n    print(freq)\r\n    print(df.resample(str(freq) + 'Min', how='first', base=30).head(2))\r\n```\r\n\r\nproduces the following output:\r\n\r\n```\r\n3\r\n                     A\r\n2014-09-01 09:30:00  0\r\n2014-09-01 09:33:00  3\r\n5\r\n                     A\r\n2014-09-01 09:30:00  0\r\n2014-09-01 09:35:00  5\r\n6\r\n                     A\r\n2014-09-01 09:30:00  0\r\n2014-09-01 09:36:00  6\r\n8\r\n                     A\r\n2014-09-01 09:26:00  0\r\n2014-09-01 09:34:00  4\r\n16\r\n                     A\r\n2014-09-01 09:18:00  0\r\n2014-09-01 09:34:00  4\r\n```\r\n"},{"labels":["api"],"text":"Just wondering if this is a bug, from http://stackoverflow.com/questions/26246864/restoring-the-default-display-context-in-pandas\n\nIf you use `pd.option_context` accidentally without `with`, this changes the default option values.\n"},{"labels":["api",null,null,null],"text":"Nested dictionaries are commonly emitted by web APIs that speak json. Getting this sort of data into pandas isn't very easy right now, without manual data structure munging, as the dicts reaing objects rather then converted into a flat naming hirerchy.\n\nHere's a common example of data:\n\n``` python\nIn [95]: data=[dict(user=dict(uid=123,full_name='Alice'),followers=1),\n    ...:  dict(user=dict(uid=456,full_name='Bob'),followers=2)]\n    ...: data\nOut[95]: \n[{'followers': 1, 'user': {'full_name': 'Alice', 'uid': 123}},\n {'followers': 2, 'user': {'full_name': 'Bob', 'uid': 456}}]\n```\n\nPandas keeps the dicts as objects:\n\n``` python\nIn [96]: df=pd.DataFrame(data)\n    ...: df\nOut[96]: \n   followers                                   user\n0          1  {u'uid': 123, u'full_name': u'Alice'}\n1          2    {u'uid': 456, u'full_name': u'Bob'}\n```\n\nBut I'd love to see something along the lines of:\n\n``` python\nIn [91]:  df=pd.DataFrame(data,flatten_dicts=True)\n    ...: df\nOut[94]: \n   followers  user.uid user.full_name\n0          1       123          Alice\n1          2       456            Bob\n\n```\n"},{"labels":["api",null,null],"text":"xref #9286 \n\n0 / 0 does not have a value, and should return NaN, not infinity.\n\n``` python\n>>> x = pd.DataFrame([[0, 0], [1, 0], [-1, 0]])\n>>> x[0] / x[1]\n0    inf                              <------------- here\n1    inf\n2   -inf\ndtype: float64\n```\n\nnumpy does this correctly:\n\n``` python\n>>> y = np.array([[0, 0], [1, 0], [-1, 0]])\n>>> y[:, 0] / y[:, 1]\narray([ nan,  inf, -inf])\n```\n"},{"labels":["api",null,null],"text":"In[43]: datetime(2014,10,10) + BQuarterBegin()\nOut[43]: Timestamp('2014-12-01 00:00:00')\n\nIn[45]: datetime(2014,10,10) + QuarterBegin()\nOut[45]: Timestamp('2014-12-01 00:00:00')\n\nExpected output is 2015-01-01. \n(Note QuarterEnd and BQuarterEnd _do_ produce the expected output of 2014-12-31)\n"},{"labels":["api",null,null],"text":"I find the behavior of `str.split` a bit odd, and it should by default just return a `DataFrame` (or maybe have a new function / option). Its straightforward to coerce it, but could/should be done internally.\n\n(and `str.extract` does return a DataFrame IIRC)\n\n```\nIn [22]: s\nOut[22]: \n0            apple\n1    apple, orange\n2           orange\ndtype: object\n\nIn [23]: s.str.split(',\\s+')\nOut[23]: \n0            [apple]\n1    [apple, orange]\n2           [orange]\ndtype: object\n\nIn [24]: s.str.split(',\\s+').apply(Series)\nOut[24]: \n        0       1\n0   apple     NaN\n1   apple  orange\n2  orange     NaN\n```\n"},{"labels":["api",null],"text":"This is incorrect, as this is applied column by column (as they are different dtypes)\nso `_first_compat` should first compute the mask then use it.\n\nfrom [SO](http://stackoverflow.com/questions/26108181/selecting-first-row-with-groupby-and-nan-columns)\n\n```\nIn [18]: df = pd.DataFrame([{'id':\"a\",'val':np.nan, 'val2':-1},{'id':\"a\",'val':'TREE','val2':15}])\n\nIn [19]: df\nOut[19]: \n  id   val  val2\n0  a   NaN    -1\n1  a  TREE    15\n\nIn [20]: df.groupby('id').first()\nOut[20]: \n     val  val2\nid            \na   TREE    -1\n```\n"},{"labels":["api",null,null],"text":"``` python\n>>> from pandas.tseries.frequencies import to_offset as _to_offset\n>>> _to_offset('D') < _to_offset('H')\nFalse\n>>> _to_offset('D') < _to_offset('W')\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-14-77799b586280> in <module>()\n----> 1 _to_offset('D') < _to_offset('W')\n\nC:\\Anaconda\\lib\\site-packages\\pandas\\tseries\\offsets.pyc in f(self, other)\n   1873 def _tick_comp(op):\n   1874     def f(self, other):\n-> 1875         return op(self.delta, other.delta)\n   1876 \n   1877     return f\n\nAttributeError: 'Week' object has no attribute 'delta'\n>>> pd.show_versions()\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.7.final.0\npython-bits: 64\nOS: Windows\nOS-release: 8\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.14.0\nnose: 1.3.3\nCython: 0.20.1\nnumpy: 1.8.1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.1.0\nsphinx: 1.2.2\npatsy: 0.2.1\nscikits.timeseries: None\ndateutil: 1.5\npytz: 2014.3\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.3.1\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.5\nlxml: 3.3.5\nbs4: 4.3.1\nhtml5lib: None\nbq: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\n```\n"},{"labels":["api",null],"text":"Using NumPy's nanmin on a pandas Series worked as expected, now however the Series is returned with each element set to the minimum, rather than just getting the minimum. Not sure if this is a Numpy of Pandas issue.\n\n```\nnp.nanmin(pds.Series([1,2,3,4]))\nOut[14]: \n0    1\n1    1\n2    1\n3    1\ndtype: int64\n\nnp.min(pds.Series([1,2,3,4]))\nOut[15]: 1\n```\n"},{"labels":["api",null,null],"text":"Let's take the following example:\n\n``` python\nd = {'Item1' : pandas.DataFrame(numpy.random.randn(4, 3)),\n     'Item2' : pandas.DataFrame(numpy.random.randn(4, 2))}\n\np = pandas.Panel(d)\np.clip(0,1)\n```\n\nIf I run the script with pandas 0.14.1 on both python 2.7.8 and 3.4.1 I'm getting the following error:\n\n```\nTraceback (most recent call last):\n  File \"Untitled.py\", line 11, in <module>\n    p.clip(0,1)\n  File \"/usr/local/lib/python2.7/site-packages/pandas/core/generic.py\", line 2684, in clip\n    result = result.clip_lower(lower)\n  File \"/usr/local/lib/python2.7/site-packages/pandas/core/generic.py\", line 2722, in clip_lower\n    return self.where((self >= threshold) | isnull(self), threshold)\n  File \"/usr/local/lib/python2.7/site-packages/pandas/core/ops.py\", line 934, in f\n    self._constructor.__name__)\nValueError: Simple arithmetic with Panel can only be done with scalar values\n```\n\nDigging a little bit deeper showed that the following line (the | operator to be precise) raises the error:\nhttps://github.com/pydata/pandas/blob/1d65bc89d64c71f8d36f3ca92dd57db2efad7fdb/pandas/core/generic.py#L2796\n"},{"labels":["api",null,null,null,null],"text":"Now that `Timedelta` is a full fledged type. It is quite trivial to convert a `datetime.time` to a `Timedelta`.\n\npros:\n- highly performant\n- very similar output\n- no need to create a `TimeBlock`\n\ncons:\n- no longer a `datetime.time` instance (instead its a sub-class of `datetime.timedelta`)\n- doesn't support `tz`(anyone actually use this 'feature')?\n- negative times would be allowed (eg. I think this is not allowed for `datetime.time`).\n\nSo either we go full-fledged and create a `Time` scalar and `TimeBlock` (though `TimeIndex` could work w/o these.). Its possible to also create a `TimeBlock` and NOT create a `Time` scalar.\n\nThat said creating a `Time` scalar is very easy and much simpler than `Timedelta` and shares a lot of code.\n\n```\nIn [28]: s = Series([datetime.time(12,0,0),datetime.time(14,1,2),datetime.time(2,0,1)])\n\nIn [29]: s\nOut[29]: \n0    12:00:00\n1    14:01:02\n2    02:00:01\ndtype: object\n\nIn [30]: pd.lib.infer_dtype(s)\nOut [30]: 'time'\n\nIn [31]: s.iloc[0]\nOut[31]: datetime.time(12, 0)\n\nIn [32]: s.apply(lambda x: pd.Timedelta(seconds=x.hour*3600+x.minute*60+x.second,microseconds=x.microsecond))\nOut[32]: \n0   12:00:00\n1   14:01:02\n2   02:00:01\ndtype: timedelta64[ns]\n\nIn [32]: s.apply(lambda x: pd.Timedelta(seconds=x.hour*3600+x.minute*60+x.second,microseconds=x.microsecond)).iloc[0]\nOut[32]: Timedelta('0 days 12:00:00')\n```\n\nthoughts?\n\nIs `datetime.time` really THAT different from `datetime.timedelta` that it SHOULD be a full-fledged type?\n"},{"labels":["api",null,null],"text":"http://stackoverflow.com/questions/25952790/convert-pandas-series-from-dtype-object-to-float-and-errors-to-nans/25952844#25952844\n\nmaybe remove`raise_on_error` and rename to `coerce` (more inline with the rest of pandas API)\n\n```\nIn [30]: pd.Series([1,2,3,4,'.']).convert_objects(convert_numeric=True)\nOut[30]: \n0     1\n1     2\n2     3\n3     4\n4   NaN\ndtype: float64\n```\n\nI suspect this might make a nice API as well\n\n```\ns = pd.Series([1,2,3,4,'.'])\ns.astype('float64', coerce=True)\n```\n\nmaybe `coerce=True|None|'raise'` make sense\n"},{"labels":["api",null,null],"text":"I could be missing an easier way, but a lot of times I want to apply a rolling function to something like a groupby. This isn't the easiest thing in the world since the rolling functions take 2 arguments and `groupby.apply` is expecting a function with one argument. Maybe there's some way with passing in `args` to `groupby.apply`, but I can never remember what exactly get passed where, and what needs to be in tuples.\n\nThe most intuitive way (for me) is to make the function being applied, `f`, a function of one argument (the grouped values). This is difficult/impossible with the rolling funcs since there are 2 positional arguments and we want to `partial` the second. You have go through hoops with flipping function arguments, `partial`ing, and flipping back. meh...\n\nI've got a patch that would allow\n\n``` python\nIn [1]: df = pd.DataFrame(np.random.randn(100, 4))\n\nIn [2]: df['g'] = np.random.randint(0, 3, size=(100))\n\nIn [3]: from functools import partial\n\nIn [4]: f = partial(pd.rolling_mean, window=4)\n\nIn [5]: df.groupby('g').apply(f)\n```\n\nThis is fully backwards compatible since it just raises if window is None. Any reason not to?\n"},{"labels":["api",null],"text":"``` python\nIn [1]: import pandas as pd\nIn [2]: import rpy2\nIn [7]: import numpy as np\nIn [4]: arr = rpy2.robjects.IntVector(range(10))\nIn [5]: arr\nOut[5]:\n<IntVector - Python:0x102ed3908 / R:0x105a7c178>\n[       0,        1,        2, ...,        7,        8,        9]\n\nIn [6]: pd.Series(arr) # wrong behavior\nOut[6]:\n0    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\ndtype: object\n\nIn [9]: np.array(arr)\nOut[9]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)\n\nIn [10]: pd.Series(np.array(arr)) # correct behavior\nOut[10]:\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\ndtype: int32\n```\n\nI guess this is a regression due to Series no longer being a subclass of np.ndarray. Not sure if `numpy` exposes a catchall `isarray` that does the checks.\n"},{"labels":["api",null,null],"text":"I'm getting a weird error in 0.14.1 that i didn't have in 0.10.1\n\n``` python\n>>> import numpy as np\n>>> import pandas\n>>> np.__version__\nOut[8]: '1.8.2'\n>>> pandas.__version__\nOut[9]: '0.14.1'\n>>> a = np.arange(10)\n>>> b = pandas.Series([1,2,3])\n>>> a[b]\nTraceback (most recent call last):\n  File \"C:\\Python27\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2746, in run_code\n    exec code_obj in self.user_global_ns, self.user_ns\n  File \"<ipython-input-17-a686c4bc0a37>\", line 1, in <module>\n    a[b]\nIndexError: unsupported iterator index\n```\n\nI would have expected to get something like array([1,2,3]), note this works if i use `.values`\n\n``` python\n>>> a[b.values]\nOut[20]: array([1, 2, 3])\n```\n\nThis would print out as expected in 0.10.1:\n\n``` python\nIn [28]: numpy.__version__\nOut[28]: '1.6.2'\nIn [29]: pandas.__version__\nOut[29]: '0.10.1'\nIn [30]: a = numpy.arange(10)\nIn [31]: b = pandas.Series([1,2,3])\nIn [32]: a[b]\nOut[32]: array([1, 2, 3])\n```\n\nAny ideas?\n"},{"labels":["api",null,null,null],"text":"xref #6552\n\n```\nidx = pd.date_range(..., tz='UTC')\nidx2 = idx.reindex(pd.date_range(..., tz='US/Pacific')\n```\n- [ ] subtraction of datetime like \n- [ ] intersection / union / difference\n- [ ] reindex \n"},{"labels":["api",null],"text":"It appears that Series' `s.any` and `s.all` methods miss level kwargs, unlike their statistical counterparts like `s.sum`:\n\n``` python\nIn [4]: s = pd.Series([0,1,2], index=[0,0,1])\n\nIn [5]: s.sum(level=0)\nOut[5]: \n0    1\n1    2\ndtype: int64\n\nIn [6]: s.prod(level=0)\nOut[6]: \n0    0\n1    2\ndtype: int64\n\nIn [7]: s.any(level=0)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-7-1d8c43752bc9> in <module>()\n----> 1 s.any(level=0)\n\n/home/immerrr/sources/pandas/pandas/core/series.pyc in f(self, *args, **kwargs)\n     74     @Appender(func.__doc__)\n     75     def f(self, *args, **kwargs):\n---> 76         result = func(self.values, *args, **kwargs)\n     77         if isinstance(result, (pa.Array, Series)) and result.ndim == 0:\n     78             # return NumPy type\n\nTypeError: _any() got an unexpected keyword argument 'level'\n\nIn [8]: s.all(level=0)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-8-bca0491001a6> in <module>()\n----> 1 s.all(level=0)\n\n/home/immerrr/sources/pandas/pandas/core/series.pyc in f(self, *args, **kwargs)\n     74     @Appender(func.__doc__)\n     75     def f(self, *args, **kwargs):\n---> 76         result = func(self.values, *args, **kwargs)\n     77         if isinstance(result, (pa.Array, Series)) and result.ndim == 0:\n     78             # return NumPy type\n\nTypeError: _all() got an unexpected keyword argument 'level'\n\n```\n\nFrames have those and I think so should series. Maybe, there are more reduction methods that I know not of that also miss those...\n"},{"labels":["api",null,null],"text":"http://stackoverflow.com/questions/25896241/creating-columns-dynamically-assigning-them-a-constant-row-vector/25896504#25896504\n\ne.g. \n\nInstead of \n`pd.concat([df,DataFrame([[1,2,3,4]],columns=list('ABCD'),index=df.index)],axis=1)`\n\nallow as a convience feature\n`df.append(DataFrame([[1,2,3,4]],columns=list('ABCD'),index=df.index),axis=1)`\n"},{"labels":["api",null,null],"text":"applymap function for panel, as per DataFrame\n"},{"labels":["api"],"text":"Is it accidental that the Series and DataFrame constructors accept a single number for `data`, but the Panel and Panel4D constructors do not?\n\n```\nIn [182]: from pandas import Series, DataFrame, Panel, Panel4D\n\nIn [183]: Series(1., index=range(2))\nOut[183]:\n0    1\n1    1\ndtype: float64\n\nIn [184]: DataFrame(1., index=range(2), columns=range(2))\nOut[184]:\n   0  1\n0  1  1\n1  1  1\n\nIn [185]: Panel(1., items=range(2), major_axis=range(2), minor_axis=range(2))\n---------------------------------------------------------------------------\nPandasError                               Traceback (most recent call last)\n<ipython-input-185-89f6618b848f> in <module>()\n----> 1 Panel(1., items=range(2), major_axis=range(2), minor_axis=range(2))\n\nC:\\Python34\\lib\\site-packages\\pandas\\core\\panel.py in __init__(self, data, items, major_axis, minor_axis, copy, dtype)\n    133                  copy=False, dtype=None):\n    134         self._init_data(data=data, items=items, major_axis=major_axis,\n--> 135                         minor_axis=minor_axis, copy=copy, dtype=dtype)\n    136\n    137     def _init_data(self, data, copy, dtype, **kwargs):\n\nC:\\Python34\\lib\\site-packages\\pandas\\core\\panel.py in _init_data(self, data, copy, dtype, **kwargs)\n    161             dtype = None\n    162         else:  # pragma: no cover\n--> 163             raise PandasError('Panel constructor not properly called!')\n    164\n    165         NDFrame.__init__(self, mgr, axes=axes, copy=copy, dtype=dtype)\n\nPandasError: Panel constructor not properly called!\n\nIn [186]: Panel4D(1., labels=range(2), items=range(2), major_axis=range(2), minor_axis=range(2))\n---------------------------------------------------------------------------\nPandasError                               Traceback (most recent call last)\n<ipython-input-187-8f22d9107a3e> in <module>()\n----> 1 Panel4D(1., labels=range(2), items=range(2), major_axis=range(2), minor_axis=range(2))\n\nC:\\Python34\\lib\\site-packages\\pandas\\core\\panel4d.py in panel4d_init(self, data, labels, items, major_axis, minor_axis, copy, dtype)\n     37     self._init_data(data=data, labels=labels, items=items,\n     38                     major_axis=major_axis, minor_axis=minor_axis,\n---> 39                     copy=copy, dtype=dtype)\n     40\n     41 Panel4D.__init__ = panel4d_init\n\nC:\\Python34\\lib\\site-packages\\pandas\\core\\panel.py in _init_data(self, data, copy, dtype, **kwargs)\n    161             dtype = None\n    162         else:  # pragma: no cover\n--> 163             raise PandasError('Panel constructor not properly called!')\n    164\n    165         NDFrame.__init__(self, mgr, axes=axes, copy=copy, dtype=dtype)\n\nPandasError: Panel constructor not properly called!\n```\n"},{"labels":["api",null,null,null],"text":"xref #8282\nxref #8239 (this issue won't touch the API per se)\n\nexpand extend: `core/generic.py/NDFrame.sort_index`: https://github.com/pydata/pandas/blob/master/pandas/core/generic.py#L1606\n\nwith the frame version.\nhttps://github.com/pydata/pandas/blob/master/pandas/core/frame.py#L2689\n\nneeds generic docs (e.g. look how reindex is done)\n\nas prob best to start with the frame version that handles ndim=2. Need tests for added capabilities to Series/Panel, e.g. mainly supporting sort_index on a multi-level index.\n"},{"labels":["api",null],"text":"I think `expanding/rolling_cov()` should support a `ddof` argument, the way `expanding/rolling_std/var()` do (though the latter is not documented; see #8064). So long as `ddof` defaults to `1`, it will be backwards compatible, so I don't see any downsides. This is very easy/straightforward.\n"},{"labels":["api",null,null],"text":"see https://github.com/pydata/pandas/pull/8184\nrelated #8185\n\n`Index.to_datetime` currently exists but really no need for this as `pd.to_datetime` subsumes this functionaility (and is more general). and just pollutes the interface.\n\nso remove:\n- [x] `Timestamp.to_datetime`\n- [x] `DatetimeIndex.to_datetime` #14096 \n- [x] `Index.to_datetime` #14096 \n- [x] `NaT.to_datetime`\n- [x] `PeriodIndex.to_datetime`#14113\n\n(and fix the parser to call directly try the conversion / use a helper function).\n"},{"labels":["api",null,null],"text":"originally #5190\nxref #9816\nxref #3942 \n\nThis issue is for creating a unified API to Series & DataFrame sorting methods. Panels are not addressed (yet) but a unified API should be easy to extend to them. Related are #2094, #5190, #6847, #7121, #2615.  As discussion proceeds, this post will be edited. \n\nFor reference, the 0.14.1 signatures are:\n\n``` python\nSeries.sort(axis=0, ascending=True, kind='quicksort', na_position='last', inplace=True)\nSeries.sort_index(ascending=True)\nSeries.sortlevel(level=0, ascending=True, sort_remaining=True)\n\nDataFrame.sort(columns=None, axis=0, ascending=True, inplace=False, kind='quicksort', \n               na_position='last')\nDataFrame.sort_index(axis=0, by=None, ascending=True, inplace=False, kind='quicksort', \n                     na_position='last')\nDataFrame.sortlevel(level=0, axis=0, ascending=True, inplace=False, sort_remaining=True)\n```\n\nProposed unified signature for `Series.sort` and `DataFrame.sort` (except Series version retains current inplace=True):\n\n``` python\ndef sort(self, by=None, axis=0, level=None, ascending=True, inplace=False, \n         kind='quicksort', na_position='last', sort_remaining=True):\n         \"\"\"Sort by labels (along either axis), by the values in column(s) or both.\n\n         If both, labels take precedence over columns. If neither is specified,\n         behavior is object-dependent: series = on values, dataframe = on index.\n\n         Parameters\n         ----------\n         by : column name or list of column names\n             if not None, sort on values in specified column name; perform nested\n             sort if list of column names specified. this argument ignored by series\n         axis : {0, 1}\n             sort index/rows (0) or columns (1); for Series, only 0 can be specified\n         level : int or level name or list of ints or list of column names\n             if not None, sort on values in specified index level(s)\n         ascending : bool or list of bool\n             Sort ascending vs. descending. Specify list for multiple sort orders.\n         inplace : bool\n             if True, perform operation in-place (without creating new instance)\n         kind : {‘quicksort’, ‘mergesort’, ‘heapsort’}\n             Choice of sorting algorithm. See np.sort for more information. \n             ‘mergesort’ is the only stable algorithm. For data frames, this option is \n             only applied when sorting on a single column or label.\n         na_position : {'first', 'last'}\n             ‘first’ puts NaNs at the beginning, ‘last’ puts NaNs at the end\n         sort_remaining : bool\n             if true and sorting by level and index is multilevel, sort by other levels\n             too (in order) after sorting by specified level\n         \"\"\"\n```\n\nThe `sort_index` signatures change too and `sort_columns` is created:\n\n``` python\nSeries.sort_index(level=0, ascending=True, inplace=False, kind='quicksort', \n                  na_position='last', sort_remaining=True)\nDataFrame.sort_index(level=0, axis=0, by=None, ascending=True, inplace=False, \n                     kind='quicksort', na_position='last', sort_remaining=True) \n                     # by is DEPRECATED, see change 7\n\nDataFrame.sort_columns(by=None, level=0, ascending=True, inplace=False, \n                       kind='quicksort', na_position='last', sort_remaining=True)\n                       # or maybe level=None\n```\n\nProposed changes:\n1. ~~make `inplace=False` default (changes `Series.sort`)~~ _maybe, possibly in 1.0_\n2. new `by` argument to accept column-name/list-of-column-names in first position \n   - deprecate `columns` keyword of `DataFrame.sort`, replaced with `by` (df.sort signature would need to retain columns keyword until finally removed but it's not shown in proposal)\n   - don't allow tuples to access levels of multi-index (`columns` arg of `DataFrame.sort` allows tuples); use new `level` argument instead\n   - don't swap order of `by`/`axis` in `DataFrame.sort_index` (see change 7)\n   - this argument is ignored by series but `axis` is too so for the sake of working with dataframes, it gets first position\n3. new `level` argument to accept integer/level-name/list-of-ints/list-of-level-names for sorting (multi)index by particular level(s)\n   - replaces tuple behavior of `columns` arg of `DataFrame.sort`\n   - add `level` argument to `sort_index` in first position so level(s) of multilevel index can be specified; this makes `sort_index`==`sortlevel` (see change 8)\n   - also adds `sort_remaining` arg to handle multi-level indexes\n4. new method `DataFrame.sort_columns`==`sort(axis=1)` (see syntax below)\n5. deprecate `Series.order` since change 1 makes `Series.sort` equivalent (?)\n6. add `inplace`, `kind`, and `na_position` arguments to `Series.sort_index` (to match `DataFrame.sort_index`); `by` and `axis` args are not added since they don't make sense for series\n7. deprecate and eventually remove `by` argument from `DataFrame.sort_index` since it makes `sort_index` equivalent to `sort`\n8. deprecate `sortlevel` since change 3b makes `sort_index` equivalent\n\nNotes:\n- default behavior of `sort` is still object-dependent: for series, sorts by values and for data frames, sorts by index\n- new `level` arg makes `sort_index` and `sortlevel` equivalent. if sortlevel is retained:\n  - should rename `sortlevel` to `sort_level` for naming conventions\n  - `Series.sortlevel` should have `inplace` argument added\n  - maybe don't add `level` and `sort_remaining` args to `sort_index` so it's not equivalent to `sort_level` (intentionally limiting sort_index seems like a bad idea though)\n- it's unclear if default should be `level=None` for `sort_columns`. probably not since level=None falls back to level=0 anyway \n- both `by` and `axis` arguments should be ignored by `Series.sort`\n\nSyntax: \n- dataframes\n  - `sort()` == `sort(level=0)` == `sort_index()` == `sortlevel()`\n    - without columns or level specified, defaults to current behavior of sort on index\n  - `sort(['A','B'])`\n    - since columns are specified, default index sort should not occur; sorting only happens using columns 'A' and 'B'\n  - `sort(level='spam')` == `sort_index('spam')` == `sortlevel('spam')`\n    - sort occurs on row index named 'spam' or level of multi-index named 'spam'\n  - `sort(['A','B'], level='spam')`\n    - `level` controls here even though columns are specified so sort happens along row index named 'spam' first, then nested sort occurs using columns 'A' and 'B'\n  - `sort(axis=1)` == `sort(axis=1, level=0)` == `sort_columns()` \n    - since data frames default to sort on index, leaving level=None is the same as level=0\n  - `sort(['A','B'], axis=1)` == `sort_columns(['A','B'])`\n    - as with preceding example, level=None becomes level=0 in sort_columns\n  - `sort(['A','B'], axis=1, level='spam')` == `sort_columns(['A','B'], level='spam')`\n    - `axis` controls `level` so sort will be on columns named 'A' and 'B' in column index named 'spam'\n- series:\n  - `sort()` == `order()` -- sorts on values\n  - with `level` specified, sorts on index/named index/level of multi-index:\n    - `sort(level=0)` == `sort_index()` == `sortlevel()`\n    - `sort(level='spam')` == `sort_index('spam')` == `sortlevel('spam')`\n\nComments welcome.\n"},{"labels":["api",null],"text":"see comments at the end of #8184 \n\nso this type of syntax\n\n`index1 + index2`\nwould be replaced by\n`index1.union(index2)`\n\nand \n\n`index1 - index2`\nby\n`index1.diff(index2)`\n\nTheir is some internal code to fix but not a lot.\n\nThis would go a long ways toward simplifying the user API for index ops. I think.\n"},{"labels":["api",null],"text":"I'm unsure about a corner case I encountered:\n\n``` python\n>>> pd.DataFrame([]).applymap(round)\nSeries([], dtype: float64)\n```\n\nfull example\n\n```\nIn [1]: df = pandas.DataFrame({ 'x' : [], 'y' : [], 'z' : []})\n\nIn [2]: df\nOut[2]: \nEmpty DataFrame\nColumns: [x, y, z]\nIndex: []\n\nIn [3]: df.applymap(lambda x: x)\nOut[3]: \nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nIn [4]: df = pandas.DataFrame({ 'x' : [1], 'y' : [1], 'z' : [1]})\n\nIn [5]: df.applymap(lambda x: x)\nOut[5]: \n   x  y  z\n0  1  1  1\n```\n\nI would have expected to get the empty DataFrame back.\n"},{"labels":["api",null,null,null],"text":"Hello,\n\nI'm trying to parse a file with the extension 'xls' which is not an Excel file but it's clearly an html file (open with a text editor it's clearly html code).\n\nOne of my columns uses the italian convention of using the comma instead of the dot before decimals: 5,5 instead of 5.5. I was hoping to parse it at least as string and replace commas with dots and convert the string to a float.\n\nThe problem is that the commas are completely ignored and instead of getting 5,5 or 7,04, I'm getting 55 and 704.\n\nIs this known? Any idea on how to solve it?\n"},{"labels":["api",null,null],"text":"From https://github.com/pydata/pandas/pull/8177#issuecomment-54711316, how should we handle NaNs in Pie Plots. Right now the documented behavior is to fill 0s and plot. This can result in some unattractive labeling depending on where the NaNs are at in the series.\n\n``` python\nIn [11]: series = Series(3 * np.random.rand(8), index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], name='series')\nIn [15]: series.iloc[:4] = np.nan\n\nIn [16]: series.plot(kind='pie')\nOut[16]: <matplotlib.axes._subplots.AxesSubplot at 0x11183e128>\n```\n\n![nana](https://cloud.githubusercontent.com/assets/1312546/4175208/b465dd2c-35c5-11e4-96b7-0d9cf184f897.png)\n\nThis is actually the first case where I can see a use for a NaN-handling kwarg in `*.plot()` itself. You could reasonably want to\n- Drop the NaNs (which I think should be the default)\n- Fill 0s (shows they're missing, but makes the labeling confusing)\n- Aggregate the NaNs and put those in a separate \"missing\" wedge that takes up some proportion of the pie.\n\nI've got no idea how that proportion should be calculated (counts? relative to the sum or size?).\n\nFor now let's just decide if the default should change from filling 0s to filling dropping (with a couple releases of notice.)\n\ncc @jorisvandenbossche @sinhrks\n"},{"labels":["api",null,null],"text":"```\nIn [6]: s = pd.Series(pd.date_range(start='2014-01-01', periods=30, freq='d'))\n\nIn [7]: s.dt.tz_localize('US/Central')\n```\n\nWould return a Series with the same index whose values are Timestamps with the timezone set.\n\nI haven't looked at this code too closely, so I'm not sure how difficult it would be and if it's doable for 0.15.\n\n@hayd brought it up on SO: http://stackoverflow.com/questions/25656826/speeding-up-timestamp-operations/25657249?noredirect=1#comment40118386_25657249\n"},{"labels":["api",null,null,null],"text":"I'm trying to find a home for this function: https://gist.github.com/TomAugspurger/9156696\n\nOutput is shown [here](http://tomaugspurger.github.io/blog/2014/02/22/Visualizing%20Missing%20Data/).\nBasically it highlights the locations of missing data.\n\nShould it go\n- Under `df.plot(kind='missing')`\n- As a recipe in the cookbook\n- Elsewhere (seaborn)\n\nI've been talking with Michael from seaborn and he thinks it could be in scope for pandas since it's essentially a view on the DataFrame.\n\nIt's possible that I already posted about this, and we already made a decision, but I could remember and search didn't turn up anything.\n"},{"labels":["api",null],"text":"In Pandas version 0.12, you could apply the Python built-in `divmod` function to a `Series` and an integer:\n\n```\nEnthought Canopy Python 2.7.6 | 64-bit | (default, Jun  4 2014, 16:42:26) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pandas as pd\n>>> pd.__version__\n'0.12.0'\n>>> divmod(pd.Series(range(4)), 2)\n(0    0\n1    0\n2    1\n3    1\ndtype: int64, 0    0\n1    1\n2    0\n3    1\ndtype: int64)\n>>> \n```\n\nWith version >= 0.13, it appears that this usage is no longer supported:\n\n```\nEnthought Canopy Python 2.7.6 | 64-bit | (default, Jun  4 2014, 16:42:26) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pandas as pd\n>>> pd.__version__\n'0.14.1'\n>>> divmod(pd.Series(range(4)), 2)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for divmod(): 'Series' and 'int'\n```\n\nWas this change intentional?\n\nSome context: I was using this to read a climate datafile that had a 4-digit column holding combined month and day values. The original code looked something like: `month, day = divmod(df['MODA'], 100)`, but broke after upgrading to version 0.14.\n"},{"labels":["api",null],"text":"- [ ] #13243 use df.index/df.columns names to automatically choose axis along which to broadcast\n- [ ] #11373 Request for some kind of named arguments loc\n- [ ] #4036 Partial Selection on MultiIndex: The need for empty slice support & dict indexing\n\nWhat if we allowed the index of a dataframe to be referred to in the usual ways?\n\n``` python\n\ndata = pd.read_table(\"...\", index_col=\"id\")\ndata.id  # breaks\ndata[\"id\"]  # breaks\n```\n\nI find myself setting and resetting indices very often to join to a different dataframe or to pull in the values of the index to a subselection of the dataframe, etc. I figure this is because of how the data is stored under the hood, but wouldn't this be convenient?\n"},{"labels":["api",null,null],"text":"I was trying to figure out what's the most efficient way to create a dataframe out of a large numpy record array (I initially naively thought this could be done in a zero-copy way) and ran some simple tests:\n\n---\n\n``` python\nn_columns = 2\nn_records = int(15e6)\n\ndata = np.zeros((n_columns, n_records), np.int32)\narr = np.core.records.fromarrays(data)\n\n%timeit -n1 -r1 df = pd.DataFrame(arr)\n%timeit -n1 -r1 df = pd.DataFrame(arr, copy=False)\n%timeit -n1 -r1 f0, f1 = arr['f0'].copy(), arr['f1'].copy()\nf0, f1 = arr['f0'].copy(), arr['f1'].copy()\n%timeit -n1 -r1 df = pd.DataFrame({'f0': f0, 'f1': f1})\n%timeit -n1 -r1 df = pd.DataFrame({'f0': f0, 'f1': f1}, copy=False)\n```\n\n---\n\n```\n1 loops, best of 1: 2.47 s per loop\n1 loops, best of 1: 2.42 s per loop\n1 loops, best of 1: 48.2 ms per loop\n1 loops, best of 1: 4.25 s per loop\n1 loops, best of 1: 4.57 s per loop\n```\n\n---\n\nI wonder what's the DataFrame constructor doing for several seconds straight even if it's provided with a materialized `ndarray` (note that copying both arrays takes < 50ms)? This is on v0.14.1.\n\nP.S. Is there a more efficientway to make a dataframe out of a recarray?\n"},{"labels":["api",null,null],"text":"from [SO](http://stackoverflow.com/questions/36804141/vectorized-construction-of-datetimeindex-in-pandas)\n\nI didn't find an issue about this, but it has come up some times at stackoverflow: having columns with integers for year, month, day, hour, ..., how do you convert this to a datetime column/index ?\n\nhttp://stackoverflow.com/questions/19350806/how-to-convert-columns-into-one-datetime-column-in-pandas\n\nYou have the typical solution of adding the columns: `pd.to_datetime((df['Y']*10000 + df['M']*100 + df['D']).astype('int'), format='%Y%m%d')`, and @unutbu added now a faster solution using numpy's different datetime64 resolutions to that question on SO.\n\nI personally think this would be a nice addition to pandas to have a more native solution for this. But then we need to figure out a nice API. Or we keep it as is, but try to document it more (add as example to docs?)\n"},{"labels":["api",null,null],"text":"Say you have to parse some nicely ISO formatted date strings, you can just parse this with `todatetime` very fast. But if you were 'overcautious' and provided the `format=\"%Y-%m-%d %H:%M:%S\"` for safety, this seems to be around 20 times slower. \nWould it be possible to provide a fastpath for certain provided format strings (as already exists for `%Y%m%d` I think).\n\n```\nIn [129]: s = pd.Series(pd.date_range('2000-01-01', periods=1000, freq='H'))\n\nIn [130]: s_as_dt_strings = s.apply(lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"))\n\nIn [131]: %timeit pd.to_datetime(s_as_dt_strings)\n1000 loops, best of 3: 406 µs per loop\n\nIn [132]: %timeit pd.to_datetime(s_as_dt_strings, format=\"%Y-%m-%dT%H:%M:%S.%f\")\n100 loops, best of 3: 9.73 ms per loop\n```\n\n```\nIn [133]: s_as_dt_strings = s.apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\nIn [134]: %timeit pd.to_datetime(s_as_dt_strings)\n1000 loops, best of 3: 361 µs per loop\n\nIn [135]: %timeit pd.to_datetime(s_as_dt_strings, format=\"%Y-%m-%d %H:%M:%S\")\n100 loops, best of 3: 8.36 ms per loop\n```\n\nFor non-standard formats, providing format _does_ give a big improvement:\n\n```\nIn [136]: s_as_dt_strings = s.apply(lambda x: x.strftime(\"%Y/%m/%d %H:%M:%S\"))\n\nIn [137]: %timeit pd.to_datetime(s_as_dt_strings)\n10 loops, best of 3: 92.2 ms per loop\n\nIn [138]: %timeit pd.to_datetime(s_as_dt_strings, format=\"%Y/%m/%d %H:%M:%S\")\n100 loops, best of 3: 9.08 ms per loop\n```\n"},{"labels":["api",null],"text":"add currently behaves like this:\n    def **add**(self, other):\n        if isinstance(other, Index):\n            return self.union(other)\n        else:\n            return Index(np.array(self) + other)\n\nbut subtraction falls straight through to diff.\nso if index_object is an Int64Index, the following behavior takes place:\n   index_object + 1 => add one to all indices\n   index_object - 1 => error in diff\n   index_object + -1 => subtract one from all indices\n"},{"labels":[null,"api",null],"text":"Currently `get_dummies` has both `prefix` and `prefix_sep` parameters. This is inconsistent with the rest of pandas prefix / suffix arguments that have you included the separator in the pre/suffix.\n- `read_csv`\n- `join`\n  probably others.\n\nWe should add a warning to `get_dummies` to have users change `prefix_sep` to `''` and include the separators in `prefix`.\n"},{"labels":["api",null,null],"text":"It would be nice if the join method had `lprefix` and `rprefix` parameters in analogy to `lsuffix` and `rsuffix`.\nP.S. No pull request, sorry.\n"},{"labels":["api",null],"text":"Hello everyone,\n\nI just stumbled upon an inconsistent behaviour of the groupby function which is causing me a lot of trouble. When grouping on bins, I expect the empty bins to be kept as NA values, for dimension consistency when one wants to aggregate and compare data.\n\nThis is effectively the case when grouping on a single key, but the empty bins are dropped as soon as one adds a second key to the groupby function.\n\n``` python\nimport pandas as pd\nimport numpy as np\n\nd = {'Col 1': [3, 3, 4, 5], 'Col 2': [1, 2, 3, 4], 'Col 3': [10, 100, 200, 34]}\n\ntest = pd.DataFrame(d)\n\nvalues = pd.cut(test['Col 1'], [1, 2, 3, 6])\n\n# Grouping on a single column\ngroups_single_key = test.groupby(values)\n\n# Grouping on two columns\ngroups_double_key = test.groupby([values,'Col 2'])\n\n# The empty group is kept as NA, which is the behaviour I was expecting\ngroups_single_key.describe()\n\n# The empty groups are dropped\ngroups_double_key.describe()\n\n# This is not just an artifact of the describe() method: the empty group really\n# does exist and is taken into account when performing aggregation\nprint(groups_single_key.agg('mean'))\nprint(groups_double_key.agg('mean'))\n```\n\n``` python\npd.show_versions()\n```\n\npandas: 0.14.1\nnose: 1.3.1\nCython: 0.20.1\nnumpy: 1.8.1\nscipy: 0.13.3\nstatsmodels: 0.5.0\nIPython: 2.2.0\nsphinx: 1.2.2\npatsy: 0.3.0\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2013.9\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.4.0\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.5.7\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\n"},{"labels":["api"],"text":"I was a little surprised by this\n\n```\nIn [11]: pd.Series([])\nOut[11]: Series([], dtype: float64)\n\nIn [12]: pd.Series([None])  # should be NaN / float ?\nOut[12]:\n0    None\ndtype: object\n\nIn [13]: pd.Series([None, 1])\nOut[13]:\n0   NaN\n1     1\ndtype: float64\n```\n"},{"labels":["api",null,null],"text":"`get_dummies` currently just expects a Series.\n\n``` python\nIn [17]: data\nOut[17]: \n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n\n                                                Name     Sex  Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male   22      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n\n   Parch     Ticket     Fare Cabin Embarked  \n0      0  A/5 21171   7.2500   NaN        S  \n1      0   PC 17599  71.2833   C85        C  \n```\n\nIf it took DataFrames we could change the required call from\n\n```\nfeatures = pd.concat([data.get(['Fare', 'Age']),\n                      pd.get_dummies(data.Sex, prefix='Sex'),\n                      pd.get_dummies(data.Pclass, prefix='Pclass'),\n                      pd.get_dummies(data.Embarked, prefix='Embarked')],\n                     axis=1)\n```\n\nto\n\n```\nfeatures = pd.get_dummies(data)\n```\n\nWe'll infer that things with `object` dtype need to be encoded as 0's and 1's, but also take arguments to explicitly encode a column, or not.\n\nThe column names in the output will automatically include the original column name as a prefix, which can be overridden by the `prefix` kwarg by passing a list or dictionary.\n\nSame thing with prefix separators.\n\nOn NaN handling, I think we'll have one `{prefix}_NaN` output column per original column when `dummy_na` is True.\n\nI've got some tests written already.\n"},{"labels":["api",null,null],"text":"It appears that `Series.quantile()` interpolates between the two bracketing values, whereas `expanding/rolling_quantile()` don't.\n\n```\nIn [496]: x = Series([0,1])\n\nIn [497]: x.quantile(0.3)\nOut[497]: 0.29999999999999999\n\nIn [498]: expanding_quantile(x, 0.3)\nOut[498]:\n0    0\n1    0\ndtype: float64\n\nIn [499]: rolling_quantile(x, 2, 0.3, min_periods=1)\nOut[499]:\n0    0\n1    0\ndtype: float64\n```\n"},{"labels":["api",null],"text":"Once https://github.com/pydata/pandas/pull/7736 is merged, we can add this option\n\n```\ndf.plot(subplots=True, layout=(4, -1)\n```\n\nThis will behave similar to numpy's `reshape` method, except that we won't require the number of subplots generated by layout to exactly match the number of subplots in `df`. We'll just fill in the smallest integer that' sable to hold the subplots.\n"},{"labels":["api",null,null],"text":"```\nIn [1]: idx = pd.Index(range(5))\n\nIn [2]: idx\nOut[2]: Int64Index([0, 1, 2, 3, 4], dtype='int64')\n\nIn [3]: idx.shift(1)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-3-d8362983dbf7> in <module>()\n----> 1 idx.shift(1)\n\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\index.pyc in shift(self, period\ns, freq)\n   1096             return self\n   1097\n-> 1098         offset = periods * freq\n   1099         return Index([idx + offset for idx in self], name=self.name)\n   1100\n\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\n```\n\nShould `shift` be allowed at all when it is no `DatetimeIndex`?\n"},{"labels":["api",null],"text":"Currently `get_dummies` always returns dummy values with `0` or `1`. When we want others,  have to use `replace`, `apply...` etc (and it is not easy in some cases because column name can differs depending on target value).\n\n```\ns = pd.Series(['A', 'A', 'B', 'C'])\npd.get_dummies(s)\n#    A  B  C\n#0  1  0  0\n#1  1  0  0\n#2  0  1  0\n#3  0  0  1\n```\n\nAllow to specify values to be used with `values` and `na_value` kwds corresponding `True` or `False`, accepting `scalar`, `dict`, or `Series` like `fillna` does.\n\n```\npd.get_dummies(s, value=True, na_value=False)\n#        A      B      C\n#0   True  False  False\n#1   True  False  False\n#2  False   True  False\n#3  False  False   True\n```\n"},{"labels":["api",null],"text":"The interpretation of `min_periods` in the `ewm*()` functions seems rather odd to me. For example (in 0.14.1):\n\n```\nIn [19]: x\nOut[19]:\n0     0\n1   NaN\n2   NaN\n3   NaN\n4     4\n5   NaN\n6     6\ndtype: float64\n\nIn [20]: ewma(x, com=3., min_periods=2)\nOut[20]:\n0         NaN\n1         NaN\n2    0.000000\n3    0.000000\n4    2.285714\n5    2.285714\n6    3.891892\ndtype: float64\n```\n\nThe way it works, is it finds the first non-`NaN` value (`0` in the example above) and then makes sure that the `min_periods` entries (`min_periods-1` in 0.15.0, per https://github.com/pydata/pandas/pull/7898) in the result starting at that entry are `NaN`. Does it make any sense that the result has entry `0` set to `NaN`, but entries `2` and `3` (and `1` in 0.15.0) set to `0.0`?\n\nI would have thought that the values to be explicitly `NaN`ed would be those determined by `x.notnull().cumsum() < min_periods`. This would be consistent with the meaning of `min_periods` in the `rolling_*()` and `expanding_*()` functions.\n\nCC'ing @snth and @jaimefrio, in case they have opinions.\n"},{"labels":["api",null,null,null],"text":"Instead of having a represenation of an array of `Period` objects (object dtype) in a Series/DataFrame. Allow an internal type of `PeriodBlock` to represent, similary to `DatetimeBlock`.\n\nHere is a usecase. The following would be very simple if `to_period(...)` returned a `PeriodIndex`\n\n```\nIn [59]: df = DataFrame({'date' : pd.date_range('20130101 00:01:00',periods=5,freq='61s')})\n\nIn [60]: df\nOut[60]: \n                 date\n0 2013-01-01 00:01:00\n1 2013-01-01 00:02:01\n2 2013-01-01 00:03:02\n3 2013-01-01 00:04:03\n4 2013-01-01 00:05:04\n\nIn [61]: df.date.dt.to_period('T')                   \nOut[61]: \n0    2013-01-01 00:01\n1    2013-01-01 00:02\n2    2013-01-01 00:03\n3    2013-01-01 00:04\n4    2013-01-01 00:05\ndtype: object\n\nIn [62]: pd.Series(pd.PeriodIndex(df.date.dt.to_period('T')+1).to_timestamp())\nOut[62]: \n0   2013-01-01 00:02:00\n1   2013-01-01 00:03:00\n2   2013-01-01 00:04:00\n3   2013-01-01 00:05:00\n4   2013-01-01 00:06:00\ndtype: datetime64[ns]\n```\n"},{"labels":["api",null,null],"text":"Follow-up of #6300 to discuss the object-oriented API for the sql functions.\n\nSo there are two ways to interact with SQL databases:\n- more simple, high-level functions: `read_sql_query`, `read_sql_table`, `to_sql`\n- \"OO API\" for more advanced functionality\n\nThe high-level functions are already quite polished (but there are still some issues with NaN, datetime, schema, ..), but the OO API should still be discussed and is not yet really public.\n\n**Current design**:\n- `PandasSQLAlchemy` object (with `execute`, `read_sql`, `read_table`, `to_sql`, `has_table`, `drop_table` methods)\n-  `PandasSQLTable` object to do the actual table reading/writing\n\n**Some questions remain**:\n- What is this 'more advanced' usage that we want to enable in the API?\n  - possibility to provide custom `MetaData`\n  - the fact you don't recreate the object and metadata for each query (if you do multiple queries)\n  - other sqlalchemy features people would want? Possibility to provide your own `Table` description instead of generating automatically?\n  - more like `HDF5Store`? (getitem/setitem support? eg `pandas_sql['table_name']` to read a table?)\n- What do we regard as public API?\n  - Only `PandasSQLAlchemy`? Or also `PandasSQLTable`?\n  - which methods of `PandasSQLAlchemy` should be public?\n- Naming:\n  - naming of `PandasSQLAlchemy` -> do we need a better name? ('SQLDatabase', 'SQLdb', 'SQLConnection', ...)\n  - naming of the methods (`read_sql` -> `read_query`?)\n- How do we see `PandasSQLAlchemy`?\n  - wrapper around an `Engine`? or wrapper around `Engine` + `MetaData`\n  - do we want to support reading/writing to multiple schema's? Or do you need to create seperate objects for each schema?\n"},{"labels":["api",null],"text":"I'm building a simulation tool that essentially requires iterating through some fairly large DataFrames with iterrows (ie. the tool cannot be vectorized).  I've looked into converting it to Cython but I'm just not that skilled with Cython to make it work.  Also, I don't quite get why it's such a no-no to iterate on rows (other than it's slow) but for my purpose I have to since each row is a record in time.\n\nHowever, I _did_ build my own iterator for the DataFrame using np.nditer that is a whole lot faster than iterrows and can give dict-like objects unlike itertuples.  Is there a desire for something like this?\n"},{"labels":["api",null,null],"text":"When storing datetimes with timezone information in mysql I split out the is_dst flag into a separate column.  Then when reconstructing the Timestamps I am either forced to iterate through each row and call pytz.timezone.localize on every Timestamp which is very slow or do some magic with localizing what I can and then manually dealing with the fall transition time (note that infer_dst won't work because there could be many rows that have transitions in them).  I would much rather create the DatetimeIndex from the column of dates and then call tz_localize with the is_dst column.  This would then appropriately set the offset.\n\n```\ndi = DatetimeIndex(frame['DateColumn'])\ndi = di.tz_localize(TimeZone, is_dst_flat=frame['IsDstColumn'])\n```\n\nThoughts?\n"},{"labels":["api",null],"text":"Calling `.where()` treats `None` differently depending on the value of `inplace`.\n\n```\nIn [1]: import pandas\n\nIn [2]: s1 = pandas.Series(['a', 'b', 'c'])\n\nIn [3]: s1.where(s1 != 'a', None)\nOut[3]: \n0    None\n1       b\n2       c\ndtype: object\n\nIn [4]: s2 = pandas.Series(['a', 'b', 'c'])\n\nIn [5]: s2.where(s1 != 'a', None, inplace=True)\n\nIn [6]: s2\nOut[6]: \n0    NaN # would have expected None here!\n1      b\n2      c\ndtype: object\n```\n\nThis causes surprising behaviour when setting values in a series to None.\n\n```\nIn [1]: import pandas\n\nIn [2]: s3 = pandas.Series(['a', 'b', 'c'])\n\nIn [3]: s3[0] = None\n\nIn [4]: s3[s3 == 'b'] = None\n\nIn [5]: s3\nOut[5]: \n0    None\n1     NaN # Expected None here too!\n2       c\ndtype: object\n```\n"},{"labels":["api",null,null],"text":"It would be great if a `columns` option could be added to the Stata Reader. Some `dta` files can be large and I would like to import a few specified columns. Current I am importing the entire file and returning a selection such as:\n\n``` python\ndata = pd.read_stata(fn)[['year', 'exports']]\n```\n\nI would propose `pd.read_stata(fn, columns=['year', 'exports'])`\n"},{"labels":["api",null,null],"text":"I was about to answer an SO question giving the standard \"use `lambda` or `functools.partial`\" to create a new function with bound arguments when to my surprise my example didn't work with the OP's code.  After some experimentation, it turns out to be because we don't always get the same return type.  For example:\n\n```\ndf = pd.DataFrame(np.arange(5, dtype=np.int64), \n                  index=pd.DatetimeIndex(start='2014/01/01', periods=5, freq='d'))\n\ndef f(x,a=1):\n    print(type(x))\n    return int(isinstance(x, pd.DataFrame)) + 1000*a\n```\n\nAfter which:\n\n```\n>>> df.resample(\"M\", how=f)\n<class 'pandas.core.series.Series'>\n               0\n2014-01-31  1000\n>>> df.resample(\"M\", how=lambda x: f(x,5))\n<class 'pandas.core.series.Series'>\n               0\n2014-01-31  5000\n>>> df.resample(\"M\", how=partial(f, a=9))\n<class 'pandas.core.frame.DataFrame'>\n               0\n2014-01-31  9001\n```\n\n`how` shouldn't care about how the function was constructed, only whether it's callable, and we should be getting the same result (whether Series or DataFrame) fed into `how` in every case.\n"},{"labels":["api",null,null],"text":"Currently `ewma()` has an `adjust` parameter (affecting how the weights are calculated) whereas all the other `ewm*()` functions do not. When they call `ewma()`, `adjust` is implicitly set to its default value, `True`. This is probably fine in most cases, but if `ewma()` is going to expose the `adjust` parameter, the other `ewm*()` should as well.\n\nFor clarity, here's the description of the `adjust` parameter in `_ewm_notes`:\n\n```\nWhen adjust is True (default), weighted averages are calculated using weights\n    (1-alpha)**(n-1), (1-alpha)**(n-2), ..., 1-alpha, 1.\n\nWhen adjust is False, weighted averages are calculated recursively as:\n    weighted_average[0] = arg[0];\n    weighted_average[i] = (1-alpha)*weighted_average[i-1] + alpha*arg[i].\n```\n\nOn a related note, I'm not sure what the policy/practice is for reordering parameters, but if it's not too strict I'd like to reorder the `ewm*()` parameters just a little so that (i) `freq` and `how` are next to each other, and (ii) `adjust` and `ignore_na` are next to each other.\n"},{"labels":["api",null],"text":"This means that patterns such as:\n\n``` python\nif my_frame.index:\n    # do something\n```\n\nwith `my_frame` having a `MultiIndex` will never enter the `if` block.  One can check that `bool(my_frame.index)` returns False.  A single index (`pandas.Index`) will (correctly IMO) raise a `ValueError` warning that the truth value of an array is ambiguous.  However, a `MultiIndex` fails silently.\n\nThis is such an awesome source of silent bugs that I'm surprised this hasn't come up before.  Is there any reason why we want `bool` to return `False` every time for a `MultiIndex`?\n"},{"labels":["api",null,null,null],"text":"xref #10347 \n\nI'm having trouble understand how exactly reindex is supposed to work in the case of MultiIndex.  What I would like is the exact behavior in the Index case where values can be filled or padded according to some level of the MultiIndex.  I can't understand what reindex is doing in the MultiIndex case.  The below is an example where I would like to interpolate using the prior value a month frame to an hourly frame.\n\n```\nimport pandas as pd\nd1 = pd.date_range('1/1/2012', '3/1/2012', freq='MS')\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=d1)\ndfu = df.unstack()\n\ndfu\nOut[12]: \nA  2012-01-01    1\n   2012-02-01    2\n   2012-03-01    3\nB  2012-01-01    4\n   2012-02-01    5\n   2012-03-01    6\ndtype: int64\n\nd2 = pd.date_range('1/1/2012', '3/31/2012', freq='D')\n\ndfu.reindex(d2, level=1) # What is this supposed to do?\nOut[17]: \nA  2012-01-01    1\n   2012-02-01    2\n   2012-03-01    3\nB  2012-01-01    4\n   2012-02-01    5\n   2012-03-01    6\ndtype: int64\n\ndf.reindex(d2, method='pad').unstack() #Ideally something like this, but without the trickery\nOut[19]: \nA  2012-01-01    1\n   2012-01-02    1\n   2012-01-03    1\n   2012-01-04    1\n   2012-01-05    1\n   2012-01-06    1\n   2012-01-07    1\n   2012-01-08    1\n   2012-01-09    1\n   2012-01-10    1\n   2012-01-11    1\n   2012-01-12    1\n   2012-01-13    1\n   2012-01-14    1\n   2012-01-15    1\n...\nB  2012-03-17    6\n   2012-03-18    6\n   2012-03-19    6\n   2012-03-20    6\n   2012-03-21    6\n   2012-03-22    6\n   2012-03-23    6\n   2012-03-24    6\n   2012-03-25    6\n   2012-03-26    6\n   2012-03-27    6\n   2012-03-28    6\n   2012-03-29    6\n   2012-03-30    6\n   2012-03-31    6\nLength: 182, dtype: int64\n```\n"},{"labels":["api",null,null,null],"text":"xref #7886\n\nIt would be nice to be able to write:\n\n``` python\ns[s.index.isin(['a', 'b', 'c'], level='foo')]\n```\n\ninstead of\n\n``` python\ns[s.index.get_level_values('foo').isin(['a', 'b', 'c'])]\n```\n\nThe former variant is not only shorter but is also potentially more performant: one only has to look up levels in given container and then operate on labels only avoiding instantiation of sub-index completely.\n\nDefault value should be `None`, plain `Index` objects should only accept `None`, `0`, and, `Index.name` for the sake of consistency (the last one is especially arguable).\n"},{"labels":["api",null,null],"text":"from #7867 \n\n```\nIn [1]: s = pd.Series([1,2,3])\n\nIn [2]: s\nOut[2]: \n0    1\n1    2\n2    3\ndtype: int64\n\n# this is de-facto: s.reindex([[1,2,3]])\nIn [3]: s.loc[[1,2,3]]\nOut[3]: \n1     2\n2     3\n3   NaN\ndtype: float64\n```\n\nshould this work?\n\n```\nIn [4]: s.loc[[1,2,3]] = 4\nKeyError: '[3] not in index'\n```\n\n(Doing this with only included labels works)\n"},{"labels":["api",null],"text":"In the examples below with `min_periods=2`, `expanding_mean/std` start to give values once there are two values, but `ewma/std` only once there are three values. Is this intentional? It looks like a bug to me.\n\n```\nIn [1]: from pandas import Series, expanding_mean, expanding_std, ewma, ewmstd, show_versions\n\nIn [2]: s = Series(range(4))\n\nIn [3]: expanding_mean(s, min_periods=2)\nOut[3]:\n0    NaN\n1    0.5\n2    1.0\n3    1.5\ndtype: float64\n\nIn [4]: expanding_std(s, min_periods=2)\nOut[4]:\n0         NaN\n1    0.707107\n2    1.000000\n3    1.290994\ndtype: float64\n\nIn [5]: ewma(s, min_periods=2, halflife=3.)\nOut[5]:\n0         NaN\n1         NaN\n2    1.152678\n3    1.784530\ndtype: float64\n\nIn [6]: ewmstd(s, min_periods=2, halflife=3.)\nOut[6]:\n0         NaN\n1         NaN\n2    0.856493\n3    1.162100\ndtype: float64\n\nIn [7]: show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.14.1\nnose: 1.3.3\nCython: 0.20.2\nnumpy: 1.9.0b1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.1.0\nsphinx: 1.2.2\npatsy: 0.3.0\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2014.4\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.3.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: None\n```\n"},{"labels":["api",null],"text":"As already described in issue #3690, panel.to_frame discards all minor entries with nan in the data, which can be very confusing. I believe there should be a warning, first time data is dropped or the opposite should be the default behavior.\n\nThe warning could be treated similar to a ZeroDivisionWarning in numpy only on the first occurrence.\n\nSee below for an example:\n\n``` python\ndf1 = pd.DataFrame(np.random.randn(2, 3), columns=['A', 'B', 'C'],\n                   index=['foo', 'bar'])\ndf2 = pd.DataFrame(np.random.randn(2, 3), columns=['A', 'B', 'C'],\n                   index=['foo', 'bar'])\ndf2.loc['foo', 'B'] = np.nan\nmydict = {'df1': df1, 'df2': df2}\n\npd.Panel(mydict).to_frame()\n```\n\nOutput:  \n\n| major | minor | df1 | df2 |\n| --- | --- | --- | --- |\n| foo | A | 1.9097545931480682 | -0.6710202447941566 |\n| foo | C | 1.3335254610685865 | 1.53372538551507 |\n| bar | A | 0.3145550744497975 | -1.7221352144306152 |\n| bar | B | -0.15681197178861878 | -1.2308510354641322 |\n| bar | C | -0.09598971674309852 | -0.1268630728124487 |\n\nUsing filter_observations=False, nan won't be dropped:\n\n``` python\npd.Panel(mydict).to_frame(filter_observations=False)\n```\n\nOutput:  \n\n| major | minor | df1 | df2 |\n| --- | --- | --- | --- |\n| foo | A | 1.9097545931480682 | -0.6710202447941566 |\n| foo | B | 2.092552358833253 |  |\n| foo | C | 1.333525461068586 | 1.53372538551507 |\n| bar | A | 0.3145550744497975 | -1.7221352144306152 |\n| bar | B | -0.15681197178861878 | -1.2308510354641322 |\n| bar | C | -0.09598971674309852 | -0.1268630728124487 |\n"},{"labels":["api",null,null,null],"text":"I think the following _could_ work and not raise `KeyError` because of a missing indexer. This would follow the API of loc which will be effectively a reindex as long as you have at least 1 found value.\n\n```\ns = pd.Series(np.arange(9),index=pd.MultiIndex.from_product([['A','B','C'],['foo','bar','baz']],names=['one','two'])).sortlevel()\n\ns.loc[['A','D']]\n\nidx = pd.IndexSlice\ns.loc[idx[:,['foo','bah']]]\n```\n"},{"labels":["api",null,null,null],"text":"Example below for tz_localize() (Same error for tz_convert())\n\n```\nIn [1]: import pandas as pd\n\nIn [2]: pd.__version__\nOut[2]: '0.14.1'\n\nIn [3]: cohort_date = pd.date_range('20140701', periods=5, freq='D')\n\nIn [4]: event_date  = pd.date_range('20140701', periods=5, freq='D')\n\nIn [5]: multi_index = pd.MultiIndex.from_product([cohort_date, event_date], names=['cohort_date', 'event_date'])\n\nIn [6]: df = pd.DataFrame(index=multi_index)\n\nIn [7]: df\nOut[7]:\nEmpty DataFrame\nColumns: []\nIndex: [(2014-07-01 00:00:00, 2014-07-01 00:00:00), (2014-07-01 00:00:00, 2014-07-02 00:00:00), (2014-07-01 00:00:00, 2014-07-03 00:00:00), (2014-07-01 00:00:00, 2014-07-04 00:00:00), (2014-07-01 00:00:00, 2014-07-05 00:00:00), (2014-07-02 00:00:00, 2014-07-01 00:00:00), (2014-07-02 00:00:00, 2014-07-02 00:00:00), (2014-07-02 00:00:00, 2014-07-03 00:00:00), (2014-07-02 00:00:00, 2014-07-04 00:00:00), (2014-07-02 00:00:00, 2014-07-05 00:00:00), (2014-07-03 00:00:00, 2014-07-01 00:00:00), (2014-07-03 00:00:00, 2014-07-02 00:00:00), (2014-07-03 00:00:00, 2014-07-03 00:00:00), (2014-07-03 00:00:00, 2014-07-04 00:00:00), (2014-07-03 00:00:00, 2014-07-05 00:00:00), (2014-07-04 00:00:00, 2014-07-01 00:00:00), (2014-07-04 00:00:00, 2014-07-02 00:00:00), (2014-07-04 00:00:00, 2014-07-03 00:00:00), (2014-07-04 00:00:00, 2014-07-04 00:00:00), (2014-07-04 00:00:00, 2014-07-05 00:00:00), (2014-07-05 00:00:00, 2014-07-01 00:00:00), (2014-07-05 00:00:00, 2014-07-02 00:00:00), (2014-07-05 00:00:00, 2014-07-03 00:00:00), (2014-07-05 00:00:00, 2014-07-04 00:00:00), (2014-07-05 00:00:00, 2014-07-05 00:00:00)]\n\nIn [8]: df.tz_localize('UTC')\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-8-fa0ae4582d5d> in <module>()\n----> 1 df.tz_localize('UTC')\n\n/Users/mtrbean/.virtualenvs/stats/lib/python2.7/site-packages/pandas/core/generic.pyc in tz_localize(self, tz, axis, copy, infer_dst)\n   3492                 ax_name = self._get_axis_name(axis)\n   3493                 raise TypeError('%s is not a valid DatetimeIndex or PeriodIndex' %\n-> 3494                                 ax_name)\n   3495             else:\n   3496                 ax = DatetimeIndex([],tz=tz)\n\nTypeError: index is not a valid DatetimeIndex or PeriodIndex\n```\n"},{"labels":["api",null],"text":"With pandas 0.14.1, the following code produces an empty string:\n\n```\nimport pandas\nidx = pandas.MultiIndex(levels=[['a'],[0,1,2,3,4,4]], \n                        labels=[[0,0,0,0,0],[0,1,2,3,4]],\n                        names=[0,1])\nprint idx.tostring()\n```\n\nIs this behavior correct? `Index.tostring()` appears to produce a non-empty string for a non-empty index.\n"},{"labels":["api",null,null,null],"text":"Hello,\n\nit will be nice if `to_dict` method could provide same `orient` parameter as `to_json`.\nFor example when `outtype='split'` we get same results as `outtype='series'`.\n\nI also noticed that `df.to_dict(outtype='split1234')` is understood as `df.to_dict(outtype='series')` which is quite strange but `df.to_dict(outtype='a1234')` raises `ValueError: outtype a1234 not understood` which is a correct behavior\n\nKind regards\n\nFemto\n"},{"labels":["api",null,null],"text":"This seems to not conform to the `DataFrame.groupby/Series.groupby`\n\nshould be straightforward to fix this I think, just simply removing it in favor of `core/generic/NDFrame.groupby`, but would need to fix tests, maybe. The `mapper` argument is the `by` argument.\n\nhttp://stackoverflow.com/questions/24914748/why-does-panel-groupby-not-accept-level-parameter-in-pandas-0-14/24915441#24915441\n"},{"labels":["api",null,null,null,null],"text":"**When this is merged, make the below change in Categorical as well!**\n\nHaving `np.nan` in an `Index` is not returning the position of NaN but `-1`:\n\n```\nIn[5]: from pandas.core.index import _ensure_index\nIn[6]: import numpy as np\nIn[7]: idx = _ensure_index([1,2,3,4, np.nan])\nIn[8]: np.nan in idx\nOut[8]: True\nIn[9]: idx.get_indexer([np.nan])\nOut[9]: array([-1])\n```\n\nI'm not sure if that's a bug or intended. What happens is that this (new) test for Categoricals fails:\n\n```\n        # if nan in levels, the proper code should be set!\n        cat = pd.Categorical([1,2,3, np.nan], levels=[1,2,3])\n        cat.levels = [1,2,3, np.nan]\n        cat[1] = np.nan\n        exp = np.array([0,3,2,-1])\n        self.assert_numpy_array_equal(cat.codes, exp)\n\nTraceback (most recent call last):\n  File \"C:\\data\\external\\pandas\\pandas\\tests\\test_categorical.py\", line 555, in test_set_item_nan\n    self.assert_numpy_array_equal(cat.codes, exp)\n  File \"C:\\data\\external\\pandas\\pandas\\util\\testing.py\", line 99, in assert_numpy_array_equal\n    raise AssertionError('{0} is not equal to {1}.'.format(np_array, assert_equal))\nAssertionError: [ 0 -1  2 -1] is not equal to [ 0  3  2 -1].\n```\n"},{"labels":["api",null,null],"text":"There looks no easy way to remove tz from tz-aware `DatetimeIndex` holding the timestamp represented. What required is inverse operation of `tz_localize`.\n\n```\nidx = pd.date_range('2014-01-01 09:00', '2014-01-01 20:00', freq='H', tz='Asia/Tokyo')\nidx\n# <class 'pandas.tseries.index.DatetimeIndex'>\n# [2014-01-01 09:00:00+09:00, ..., 2014-01-01 20:00:00+09:00]\n# Length: 12, Freq: H, Timezone: Asia/Tokyo\n\n# What I want to do is:\nidx.tz_reset()\n# <class 'pandas.tseries.index.DatetimeIndex'>\n# [2014-01-01 09:00:00, ..., 2014-01-01 20:00:00]\n# Length: 12, Freq: H, Timezone: None\n\n# This raises TypeError\nidx.tz_localize(None)\n# TypeError: Already tz-aware, use tz_convert to convert.\n\n# This change the timestamp to UTC\nidx.tz_convert(None)\n# <class 'pandas.tseries.index.DatetimeIndex'>\n# [2014-01-01 00:00:00, ..., 2014-01-01 11:00:00]\n# Length: 12, Freq: H, Timezone: None\n```\n\nIn my case, there are some globally distributed data sources which holds local timezones. And want to merge them and analyze based on local subjective times (business hours, etc).\n"},{"labels":["api",null],"text":"e.g.\n\n```\nset_names(self, names, inplace=False, level=None)\nset_levels(self, levels, level=None, copy=False, validate=True, verify_integrity=False\n```\n\nif `level` is not None then treat `levels/names` as a list (or list-of-lists if level is a list) and just set those levels. \n\ne.g.\n\n```\nset_names('foo',level=1)\nset_names(['foo','bar'],level=[1,2])\nset_levels(['a','b','c'],level=1)\nset_levels([['a','b','c'],[1,2,3]],level=[1,2])\n```\n\nHi all,\n\nfirst of all: I'm not sure whether this is a bug or if there is just no nice way to do this. I'd like to use tz_convert in a MultiIndex DataFrame in pandas 0.14.1.\n\n```\ndt_rng = pd.date_range(start='2014-01-01 00:00', periods = 1000, freq='1s', tz='Europe/Berlin')\ndf = pd.DataFrame({'a':np.random.randn(1000), 'b': np.random.randn(1000)},index = dt_rng)\ndf['b'] = df['b'].round()\ndf = df.groupby('b').resample('1h')\ndf.index.levels[1] = df.index.levels[1].tz_convert('UTC')\n```\n\nThis does not work: 'FrozenList' does not support mutable operations.\n\nWhat I currently do:\n\n```\ndt_rng = pd.date_range(start='2014-01-01 00:00', periods = 1000, freq='1s', tz='Europe/Berlin')\ndf = pd.DataFrame({'a':np.random.randn(1000), 'b': np.random.randn(1000)},index = dt_rng)\ndf['b'] = df['b'].round()\ndf = df.groupby('b').resample('1h')\ndf.index.set_levels([\n    df.index.levels[0],\n    df.index.levels[1].tz_convert('UTC')\n],inplace=True)\n```\n\nThis does work as expected, but is it really the way to do it?!\n\nThanks and best regards!\n## PS: INSTALLED VERSIONS\n\ncommit: None\npython: 3.4.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 8\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: DE\n\npandas: 0.14.1\nnose: 1.3.3\nCython: 0.20.1\nnumpy: 1.8.1\nscipy: 0.14.0\nstatsmodels: None\nIPython: 2.1.0\nsphinx: 1.2.2\npatsy: 0.2.1\nscikits.timeseries: None\ndateutil: 2.1\npytz: 2014.4\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.3.1\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.5.5\nlxml: 3.3.5\nbs4: 4.3.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\n"},{"labels":["api",null],"text":"One can construct DataFrame with frozensets in the index:\n\n```\n >>> import pandas\n >>> a = frozenset([1])\n >>> b = frozenset([1, 2])\n >>> c = frozenset([1, 2, 3])\n >>> \n >>> frame = pandas.DataFrame(index=[a, b], columns=[c])\n >>> frame\n        (1, 2, 3)\n (1)          NaN\n (1, 2)       NaN\n```\n\nSetting new values for existing items works:\n\n```\n >>> frame.loc[a] = 0\n >>> frame\n        (1, 2, 3)\n (1)            0\n (1, 2)       NaN\n >>> frame[c] = 1\n >>> frame\n         (1, 2, 3)\n (1)             1\n (1, 2)          1\n >>> frame[frozenset([4])] = 1\n >>> frame\n         (1, 2, 3)  (4)\n (1)             1    1\n (1, 2)          1    1\n```\n\nBut new items cannot be created:\n\n```\n >>> frame.loc[frozenset([3])] = 1\n Traceback (most recent call last):\n   File \"<stdin>\", line 1, in <module>\n   File \"venv/lib/python2.7/site-packages/pandas/core/indexing.py\", line 118, in __setitem__\n     indexer = self._convert_to_indexer(key, is_setter=True)\n   File \"venv/lib/python2.7/site-packages/pandas/core/indexing.py\", line 1085, in _convert_to_indexer\n     raise KeyError('%s not in index' % objarr[mask])\n KeyError: '[3] not in index'\n```\n"},{"labels":["api",null,null],"text":"``` python\nimport pandas as pd\nimport numpy as np\nfrom decimal import *\n\ndef make_currency(x):\n    try:\n        return Decimal(x).quantize(Decimal('.01'))\n    except (InvalidOperation, TypeError) as e:\n        return Decimal(0).quantize(Decimal('.01'))\n\ncols = {\"c1\": [1.23,2.34,3.45], \"c2\": [4,5,6], \"c3\": [1,2,3]}\nindex = [1, 1, 1]\ndf = pd.DataFrame(cols, index)\ndf['c1'] = df['c1'].apply(make_currency)\n```\n\n**Test Case 1**\nExpected - _Return all 3 columns with sum aggregation applied_\nObservations - _Passed_\n\n```\nIn [2]: df.sum()\nOut[2]:\nc1     7.02\nc2    15.00\nc3     6.00\ndtype: float64\n```\n\n**Test Case 2**\nExpected - _Return all 3 columns with sum aggregation applied_\nObservations - _Column 'c1' has not been returned by the sum function, which in this case is of Decimal datatype_\n\n```\nIn [8]: df.sum(level=0)\nOut[8]:\n   c2  c3\n1  15   6\n\nIn [10]: df.groupby(level=0).sum()\nOut[10]:\n   c2  c3\n1  15   6\n```\n\n**Workaround**\nUsing numpy.sum in-conjunction with pandas.DataFrame.apply function appears to give the expected result one would expect from Test Case 2:\n\n```\nIn [9]: df.groupby(level=0).apply(lambda x: np.sum(x))\nOut[9]:\n     c1  c2  c3\n1  7.02  15   6\n```\n"},{"labels":["api",null,null,null],"text":"Previously, for resampling with PeriodIndex, you had two conventions: `'start'` (start -> start) and `'end'` (end -> end). This would give something like this (note: the following is **not current real code output**, but from Wes' book):\n\n```\nIn [25]: s = pd.Series(np.arange(2), index=pd.period_range('2000-1', periods=2, freq='A'))\n\nIn [26]: s\nOut[26]:\n2000    0\n2001    1\nFreq: A-DEC, dtype: int32\n\nIn [27]: s.resample('Q-DEC', fill_method='ffill', convention='start')\nOut[27]:\n2000Q1    0\n2000Q2    0\n2000Q3    0\n2000Q4    0\n2001Q1    1\nFreq: Q-DEC, dtype: int32\n\nIn [28]: s.resample('Q-DEC', fill_method='ffill', convention='end')\nOut[27]:\n2000Q4    0\n2001Q1    1\n2001Q2    1\n2001Q3    1\n2001Q4    1\nFreq: Q-DEC, dtype: int32\n```\n\nFollowing Wes' book, the default argument was 'end'. However, the current behaviour is like this (this is real output):\n\n```\nIn [27]: s.resample('Q-DEC', fill_method='ffill')\nOut[27]:\n2000Q1    0\n2000Q2    0\n2000Q3    0\n2000Q4    0\n2001Q1    1\n2001Q2    1\n2001Q3    1\n2001Q4    1\nFreq: Q-DEC, dtype: int32\n```\n\nSo in fact this is a third option `'span'` (start -> end). This option is mentioned in #1635, but from the issue it seems it was never implemented ([the commit](https://github.com/changhiskhan/pandas/commit/cb81f9ad5607419a5d11ceafd2225c2fdf0e6632) was never merged. There was a test added in comments at that time, but this is still in comments: https://github.com/pydata/pandas/blob/master/pandas/tseries/tests/test_resample.py#L1134). \nIn practice, however, this is the case (the default behaviour is this mentioned 'span' behaviour). But also the option `'start'` has changed:\n\n```\nIn [28]: s.resample('Q-DEC', fill_method='ffill', convention='start')\nOut[28]:\n2000Q1    0\n2000Q2    0\n2000Q3    0\n2000Q4    0\n2001Q1    1\n2001Q2    1\n2001Q3    1\n2001Q4    1\nFreq: Q-DEC, dtype: int32\n```\n\nThis gives the same as the default (only for `'end'` it is the same as before).\n\nSome issues/questions:\n- what is the default value for `convention`? It is nowhere in the  [docs](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#up-and-downsampling), and also not in the [docstring](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html) (apart from the signature, which says 'start').\n- I don't find the issue/PR/release note where it says that the default for period resample (upsampling) has changed\n- the default now is a 'spanning' behaviour, but this is the same as 'start'. Shouldn't be this something else? So that the 'start' option has another behaviour (start -> start) than the default spanning behaviour ('start' -> 'end')?\n"},{"labels":["api",null],"text":"At the moment `read_hdf` is implemented such way that it opens HDFStore connection immediately when it is called. If the file does not exist, then it creates an HDF file and then throws `KeyError`. There are two side effects:\n- You can not distinguish situation when the file does not exist from situation when the key does not exist in the file\n- If you have a typo in the filename, empty file is created -> polluting directory \n\nI suggest to check if the file exist before and throw `IOError` if it does not.\n"},{"labels":["api",null],"text":"I think [this StackOverflow question](http://stackoverflow.com/q/24650117/1156707) illustrates both the premise and the problem quite well.  In short, it would be nice to have something like `df.window(df.somecolumn == 'ping', winsize=5)` which would return a subset of `df` such that for each row where `df.somecolumn=='ping'`, we select the 5 previous lines and five following lines.\n\nThis would be particularly useful for time-locking epochs to certain values (e.g. TTLs from some hardware).\n"},{"labels":["api",null,null],"text":"Example from Wes' book:\n\n```\nIn [1]: data = pd.DataFrame({'year':[1959,1959,1959,1959,1960,1960,1960,1960], '\nquarter':[1,2,3,4,1,2,3,4]})\nIn [2]: data\nOut[2]:\n   quarter  year\n0        1  1959\n1        2  1959\n2        3  1959\n3        4  1959\n4        1  1960\n5        2  1960\n6        3  1960\n7        4  1960\n\nIn [3]: pd.PeriodIndex(year=data.year, quarter=data.quarter)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n...\nC:\\Anaconda\\envs\\devel\\lib\\site-packages\\numpy\\core\\fromnumeric.pyc in repeat(a,\n repeats, axis)\n    342     except AttributeError:\n    343         return _wrapit(a, 'repeat', repeats, axis)\n--> 344     return repeat(repeats, axis)\n    345\n    346\n\nTypeError: repeat() takes exactly 2 arguments (3 given)\n```\n\nThe docstring of PeriodIndex says `int or array` for the `year` and `quarter` keywords, and indeed with arrays it does work:\n\n```\nIn [4]: pd.PeriodIndex(year=data.year.values, quarter=data.quarter.values)\nOut[4]:\n<class 'pandas.tseries.period.PeriodIndex'>\n[1959Q1, ..., 1960Q4]\nLength: 8, Freq: Q\n```\n\nBut it previously also worked for Series (I suppose when it was still a subclass?). So, should we keep the current behaviour or make it also work for Series? \nIn any case, a very strange error message that should be improved.\n"},{"labels":["api",null],"text":"Hi folks,\n\nIt appears that in Pandas 0.14.0, resampling on data frames with multi-index columns erases the multi-index names.  You can see an example of this at http://stackoverflow.com/questions/24442832/how-to-resample-a-pandas-multi-index-data-frame-via-methods-depending-on-the-col/24443544?noredirect=1#comment38000583_24443544.\n"},{"labels":["api",null,null],"text":"This works fine\nhttp://stackoverflow.com/questions/24631991/division-of-a-panel-by-a-series/24632381#24632381\n\n```\n[8]: p = Panel(np.arange(3*4*5).reshape(3,4,5),items=['ItemA','ItemB','ItemC'],major_axis=date_range('20130101',periods=4),minor_axis=list('ABCDE'))\n\nIn [9]: p\nOut[9]: \n<class 'pandas.core.panel.Panel'>\nDimensions: 3 (items) x 4 (major_axis) x 5 (minor_axis)\nItems axis: ItemA to ItemC\nMajor_axis axis: 2013-01-01 00:00:00 to 2013-01-04 00:00:00\nMinor_axis axis: A to E\n\nIn [10]: d = p.sum(axis=1).ix[0]\n\nIn [11]: d\nOut[11]: \nItemA     30\nItemB    110\nItemC    190\nName: A, dtype: int64\n\nIn [12]: p.iloc[0]\nOut[12]: \n             A   B   C   D   E\n2013-01-01   0   1   2   3   4\n2013-01-02   5   6   7   8   9\n2013-01-03  10  11  12  13  14\n2013-01-04  15  16  17  18  19\n\nIn [13]: p.apply(lambda x: x/d,axis=0).iloc[0]\nOut[13]: \n                   A         B         C         D         E\n2013-01-01  0.000000  0.033333  0.066667  0.100000  0.133333\n2013-01-02  0.166667  0.200000  0.233333  0.266667  0.300000\n2013-01-03  0.333333  0.366667  0.400000  0.433333  0.466667\n2013-01-04  0.500000  0.533333  0.566667  0.600000  0.633333\n```\n\n`p.div(d,axis=0)` should raise `NotImplementedError` instead of returning `None`. \n\nhttps://github.com/pydata/pandas/blob/master/pandas/core/panel.py#L655\nneeds an `else` that raises.\n\nOf course this can/should be implemented.\n"},{"labels":["api",null],"text":"In the following scenario:\n\n```\nimport pandas as pd\nimport numpy as np\n\na = np.random.rand(1000)\ndf = pd.DataFrame({'a' : a })\n```\n\nThis call doesn't work (while it is supposed to, I guess)\n\n```\ndf.eval('sqrt(a)')\n\n     NotImplementedError: 'Call' nodes are not implemented\n```\n\nwhile this one does\n\n```\ndf.eval('(a)**(.5)')\n```\n\nI guess this is a bug.\n"},{"labels":["api",null],"text":"related #7615\n\nBit of a UI problem here (although it's behaving as the docstring says it does, so it doesn't quite qualify as a bug):\n\n```\n>>> df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6]})\n>>> df.to_csv(\"tmp.csv\", sep=\";\")\n>>> !cat tmp.csv\n;A;B\n0;1;4\n1;2;5\n2;3;6\n>>> df.to_csv(\"tmp.csv\", delimiter=\";\")\n>>> !cat tmp.csv\n,A,B\n0,1,4\n1,2,5\n2,3,6\n```\n\n`read_csv` accepts both `sep` and `delimiter` but `to_csv` silently ignores `delimiter`.  Someone was recently tripped up by this on SO.  I'm fine with either teaching `to_csv` to behave the same way `read_csv` does or, alternatively, raising if `delimiter` is found as a keyword.  \n\nThat is, I'm less bothered by the inconsistency than the silent unexpected behaviour.\n"},{"labels":["api",null,null,null],"text":"related #4667, #7647\n\nAccording to the documentation, `inplace` should replace values appropriately with `other`, but this does not seem to work as expected.  This worked in 0.12.\n\n```\nIn [6]: df = pandas.DataFrame([{'A': 1, 'B': np.nan, 'C': 'Test'}, {'A': np.nan, 'B': 'Test', 'C': np.nan}])\n\nIn [7]: df\nOut[7]: \n    A     B     C\n0   1   NaN  Test\n1 NaN  Test   NaN\n\nIn [8]: df1 = df.where(~pandas.isnull(df), None)\n\nIn [9]: df1\nOut[9]: \n      A     B     C\n0     1  None  Test\n1  None  Test  None\n\nIn [10]: df.where(~pandas.isnull(df), None, inplace=True)\n\nIn [11]: df\nOut[11]: \n    A     B     C\n0   1   NaN  Test\n1 NaN  Test   NaN\n```\n"},{"labels":["api",null,null,null],"text":"http://stackoverflow.com/questions/24553921/pandas-matching-on-level-of-hierarchical-index/24554350#24554350\n\nThis should align on the columns axis. Not clear how to specify the alignment level though (e.g. broadcasting would be tricky here).\n\n```\nimport numpy as np\nimport pandas as pd\n\narrays = [np.hstack([ ['one']*3, ['two']*3]), ['Dog', 'Bird', 'Cat']*2]\ncolumns = pd.MultiIndex.from_arrays(arrays, names=['foo', 'bar'])\n\ndf = pd.DataFrame(np.zeros((3,6)),columns=columns,\n                  index=pd.date_range('20000103',periods=3))\n\ndf['one'] = pd.DataFrame({'Bird' : np.ones(3)*2,\n                          'Dog' : np.ones(3),\n                          'Cat' : np.ones(3)*3},\n                          index= pd.date_range('20000103',periods=3))\ndf['two'] = pd.DataFrame({'Dog' : np.ones(3)*4,\n                          'Bird' : np.ones(3)*5,\n                          'Cat' : np.ones(3)*6,},\n                          index= pd.date_range('20000103',periods=3))\n```\n"},{"labels":["api",null],"text":"As the next step of separation-of-concerns plan (#6744) I'd like to\npropose adding a method (or several, actually) to `Index` class that\nwould encapsulate the details of `foo.loc[l1,l2,...]` lookup.\n### Implementation Idea\n\nRoughly, the idea is to make `loc`'s getitem as simple as\n\n``` python\ndef __getitem__(self, indexer):\n    axes = self.obj.axes\n    return self.obj.iloc[axes[0].lookup_labels_nd(indexer, axes[1:], typ='loc')]\n```\n\nNot quite, but hopefully you get the point. The default `lookup_labels_nd` implementation would then look something like this:\n\n``` python\ndef lookup(self, indexer, other_axes, typ=None):\n    if not isinstance(indexer, tuple):\n        return self.lookup_labels(indexer, typ=typ)\n    else:\n        # ndim mismatch error handling is omitted intentionally\n        return (self.lookup_labels(indexer[0]),) + \\\n               tuple(ax.lookup_labels(ix, typ=typ)\n                     for ax, ix in zip(other_axes, indexer))\n\n```\n\nThe result should be an object that could be fed to an underlying\nBlockManager to perform the requested operation.  To support adding\nnew rows with \"setitem\", it is only needed to agree that `lookup_labels_nd` will\nnever return negative indices unless they reference newly appended\nitems along that axis.\n\nThis would allow to hide Index-subclass-specific lookup peculiarities\nin their respective overrides of `lookup_labels_nd` and `lookup_labels` (proposals for\nbetter names are welcome), e.g.:\n- looking up str in DatetimeIndex/PeriodIndex\n- looking up int in FloatIndex\n- looking up per-level slices in MultiIndex\n### Benefits\n- no more confusing errors due to `try .. catch` block carpet-catching a\n  logic error, because corner cases will be handled precisely where\n  they are needed and nowhere else\n- no more relying on isinstance checks and exceptions to seek for\n  alternative lookup scenarios, meaning more performance\n- the API will provide a contract that is simple to grasp, test, benchmark and,\n  eventually, cythonize (as a side effect of this point I'd like to try putting\n  up a wiki page with indexing API reference)\n"},{"labels":["api",null,null],"text":"``` python\ndf = pd.DataFrame({'grp': [0, 0, 1, 1], 'returns': [0, 0.1, -.1, .1]})\ndf.groupby('grp').returns.compound()\ndf.groupby('grp').returns.apply('compound')\n```\n\nDoesn't seem to work:\n\n```\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-193-cb3ce32c6c2e> in <module>()\n      1 df = pd.DataFrame({'grp': [0, 0, 1, 1], 'returns': [0, 0.1, -.1, .1]})\n----> 2 df.groupby('grp').returns.compound()\n      3 df.groupby('grp').returns.apply('compound')\n\n/home/wiecki/envs/zipline_p14/lib/python2.7/site-packages/pandas/core/groupby.py in __getattr__(self, attr)\n    481             return self[attr]\n    482         if hasattr(self.obj, attr):\n--> 483             return self._make_wrapper(attr)\n    484 \n    485         raise AttributeError(\"%r object has no attribute %r\" %\n\n/home/wiecki/envs/zipline_p14/lib/python2.7/site-packages/pandas/core/groupby.py in _make_wrapper(self, name)\n    496                    \"using the 'apply' method\".format(kind, name,\n    497                                                      type(self).__name__))\n--> 498             raise AttributeError(msg)\n    499 \n    500         # need to setup the selection\n\nAttributeError: Cannot access callable attribute 'compound' of 'SeriesGroupBy' objects, try using the 'apply' method\n```\n\napply also doesn't work.\n\nThis is with pandas 0.14.\n"},{"labels":["api",null,null,null],"text":"### Design\n\nThe idea is to have a natural representation of the grids that ubiquitously appear in simulations and measurements of physical systems. Instead of referencing a single value, a grid cell references a _range_ of values, based on the chosen discretization. Typically, cells boundaries would be specified by floating point numbers. In one dimension, a grid cell corresponds to an _interval_, the name we use here.\n\nThe key feature of `IntervalIndex` is that looking up an indexer should return all intervals in which the indexer's values fall. `FloatIndex` is a poor substitute, because of [floating point precision issues](https://groups.google.com/d/topic/pydata/8aTLVKrPJfs/discussion), and because I don't want to label values by a single point.\n\nA `IntervalIndex` is uniquely identified by its `intervals` and `closed` (`'left'` or `'right'`) properties, an ndarray of shape `(len(idx), 2)`, indicating each interval. Other useful properties for `IntervalIndex` would include `left`, `right` and `mid`, which should return arrays (indexes?) corresponding to the left, right or mid-points of each interval.\n\nThe constructor should allow the optional keyword argument `breaks` (an array of length `len(idx) + 1`) to specified instead of `intervals`.\n\nIt's not entirely obvious what `idx.values` should be (`idx.mid`? strings like `'(0, 1]'`? an array of tuples or `Interval` objects?). I think the most useful choice for cross compatibility would probably be to an ndarray like `idx.mid`.\n\n`IntervalIndex` _should_ support mathematical operations (e.g., `idx + 1`), which are calculated by vectorizing the operation over the breaks.\n### Examples\n\nAn example already in pandas that should be a `IntervalIndex` is the `levels` property of categorical returned by `cut`, which is currently an object array of strings:\n\n```\n>>> pd.cut([], [0, 5, 10]).levels\nIndex([u'(0, 5]', u'(5, 10]'], dtype='object')\n```\n\nExample usage:\n\n``` python\n>>> # should be equivalent to pd.cut([], [0, 1, 2]).levels\n>>> idx = IntervalIndex(intervals=[(0, 1), (1, 2)]) \n>>> idx2 = IntervalIndex(breaks=[0, 1, 2]) # equivalent\n>>> idx\nIntervalIndex([(0, 1), (1, 2)], closed='right')\n>>> idx.left\nnp.array([0, 1]) \n>>> idx.right\nnp.array([1, 2]) \n>>> idx.mid\nnp.array([0.5, 1.5]) \n>>> s = pd.Series([1, 2], idx)\n(0, 1]    1\n(1, 2]    2\ndtype: int64\n>>> s.loc[1]\n1\n>>> s.loc[0.5]\n1\n>>> s.loc[0]\nKeyError\n```\n### Implementation\n\nA `IntervalIndex` would be a monotonic and non-overlapping one-dimensional array of intervals. It is _not_ required to be contiguous. A scalar `Interval` would correspond to a contiguous interval between start and stop values (e.g., given by integers, floating point numbers or datetimes).\n\nFor index lookups, I propose to do a binary search (`np.searchsorted`) on `idx.left`. If we add the constraint that all intervals must have a fixed width, we could calculate the bin using a formula in constant time, but I'm not sure the loss in flexibility would be worth the speedup.\n\n`IntervalIndex` should play nicely when used as the levels for `Categorical` variable (#7217), but it is _not_ the same as a `CategoricalIndex` (#7629). For example, a `IntervalIndex` should not allow for redundant values. To represent redundant or non-continuous intervals, you would need to make in a `Categorical` or `CategoricalIndex` which uses a `IntervalIndex` for the levels. Calling `df.reset_index()` on an `DataFrame` with an `IntervalIndex` would create a new `Categorical` column.\n\n---\n\nNote: I'm not entirely sure if this design doc belongs here or on mailing list (I'm happy to post it there if requested).\n\nHere is the comment where I brought this up previously: https://github.com/pydata/pandas/issues/5460#issuecomment-44474502\n\nCC @hugadams -- I expect `IntervalIndex` would be very handy for your [pyuvvis](https://github.com/hugadams/pyuvvis).\n"},{"labels":["api",null,null],"text":"As pointed out here (https://stackoverflow.com/questions/24468333/set-xlim-for-pandas-matplotlib-where-index-is-string/24477138#24477138) if the index of a data frame is objects (that is not, the index obviously convertible to scalars and the data is being plotted against `range(len(df))`), the ticklabels are set to be string representations of the objects (I presume via `set_xticklabels`) but the locator is still a `AutoLocator` which means if you change the xlimits or pan/zoom the axes, the ticklabels become de-coupled from the data.\n\nA solution is to also use a fixed locator which will pin the ticks to only be on the index locations.\n\n```\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\ndf = pd.DataFrame({'Foo': pd.Series([2,3,4], index=['2002', '2003', '2004'])})\nfig, ax = plt.subplots()\n\ndf.plot(ax=ax)\n\nax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(df))))\nax.xaxis.set_major_formatter(mticker.FixedFormatter(df.index))\n```\n\nI can make changes to the code if someone points me to where the relevant section is.\n"},{"labels":["api",null,null],"text":"The `column_space` option doesn't seem to work.  \n\n```\nIn [28]: df=pd.DataFrame(np.array([np.random.randn(6), np.random.randint(1,9,6)*.1, \n                         np.zeros(6)]).T, columns=['A', 'B', 'C'], dtype='float')\n\nIn [29]: pd.set_option('column_space', 12)\n\nIn [30]: df\nOut[30]: \n          A    B  C\n0  0.865356  0.1  0\n1 -0.968568  0.2  0\n2 -0.584314  0.8  0\n3  0.347676  0.6  0\n4  0.296440  0.8  0\n5 -0.129957  0.2  0\n\nIn [31]: pd.set_option('column_space', 2)\n\nIn [32]: df\nOut[32]: \n          A    B  C\n0  0.865356  0.1  0\n1 -0.968568  0.2  0\n2 -0.584314  0.8  0\n3  0.347676  0.6  0\n4  0.296440  0.8  0\n5 -0.129957  0.2  0\n```\n\nIn `frame.py`, the option seems to be named `col_space` but the doc string wasn't updated in `config_init.py`.  Happy to PR this, but, what is the agreed upon name/behavior? I like `col_space`? Does anyone have any background/history?\n"},{"labels":["api",null,null],"text":"alias `len/size` to `groupby.transform` speedup table (as these are already implemented as `size`,`count`` in cython, just  a matter of transforming them\n\nsee here: http://stackoverflow.com/questions/24438273/aggregation-on-pandas-datetime-series-only-returns-as-datetime-series/24438366#24438366\n(`len` is suspect for `.agg`  as well)\n\nso \n\n```\ng = df.groupby(...)\n\ng.transform(len)\ng.transform('size')\ng.transform('len')\n```\n\nwill do the same\n"},{"labels":["api",null,null],"text":"I was very happy to see pr #5879, allowing disabling the auto-regex feature for functions in the .str namespace. Doing something about this has been on my todo list for a while. Automatic promotion of strings to regular expressions is one of the two big points of difficulty for beginners using pandas (in my experience anyway). The most obvious problem comes up when you have literal dollar signs you wish to operate on.\n\nI'd like to argue that the faster, less confusing `regex=False` be the default.\n\nThis is a trivial thing to do, and if folks are willing to accept it, I'm happy to submit a pull request.\n\nI also ran into this problem when using unicode delimiters with the read_\\* family of functions:\n\n```\nParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators; you can avoid this warning by specifying engine='python'.\n```\n\nThis is a another case where we're losing performance and increasing confusion. So, I'd like to expand this option elsewhere, and could submit this all as one pull request, or two (if it seems reasonable).\n"},{"labels":["api",null,null,null],"text":"I used to do convert a dict with tuple keys to DataFrame like\n\n```\nDataFrame.from_dict(a_dict, 'index')\n```\n\nbut it produces \"IndexError: index out of bounds\" from pandas 0.14.  Is there a way that I still can use tuple as Dataframe label?  My old codes do not work with 0.14.\n"},{"labels":["api",null,null],"text":"Currently, `Index.unique` returns `ndarray` but `DatetimeIndex.unique` returns `DatetimeIndex` to preserve datetime information. I think `PeriodIndex.unique` should also return `PeriodIndex` (currently it returns `ndarray`).\n\nI'd like to ask which is better to fix all `Index.unique` to return `Index` for consistency, or only fix `PeriodIndex`. \n\n```\ndidx = pd.DatetimeIndex([datetime.datetime(2014, 1, 1), datetime.datetime(2014, 2, 1)])\ndidx.unique()\n# <class 'pandas.tseries.index.DatetimeIndex'>\n# [2014-01-01, 2014-02-01]\n\npidx = pd.PeriodIndex(['2014-01', '2014-02'], freq='M')\npidx.unique()\n# [528 529]\ntype(pidx.unique())\n# <type 'numpy.ndarray'>\n```\n"},{"labels":["api",null,null],"text":"Related to #7485.\n\nCurrently, `DatetimeIndex.asobject` raises `ValueError` when it contains `NaT`, otherwise `PeriodIndex` doesn't inconsistently.\n\nI think these should work the same way, and would like to confirm `PeriodIndex` should raise `ValueError`, or `DatetimeIndex.asobject` allows `NaT`. There may be historical reason, but I'm not sure why `DatetimeIndex` raises `ValueError` even though we can create object-dtype `Index` containning `NaT`\n\n```\ndidx = pd.DatetimeIndex([datetime.datetime(2014, 1, 1), pd.NaT])\ndidx.asobject\n# ValueError: DatetimeIndex with NaT cannot be converted to object\n\npidx = pd.PeriodIndex(['2014-01', 'NaT'], freq='M')\npidx.asobject\n# Index([2014-01, NaT], dtype='object')\n```\n"},{"labels":["api",null,null],"text":"The head of my Pandas dataframe , `df`, is shown below:\n\n```\n      count1  count2  totalcount  season\n    0       3      13          16       1\n    1       8      32          40       1\n    2       5      27          32       1\n    3       3      10          13       1\n    4       0       1           1       1\n```\n\nI'd like to make boxplots of `count1`, `count2`, and `totalcount`, grouped by `season` (there are 4 seasons) and have each set of box plots show up on their own subplot in a single figure.\n\nWhen I do this with only two of the columns, say `count1` and `count2`, everything looks great.\n\n```\ndf.boxplot(['count1', 'count2'], by='season')\n```\n\n![enter image description here](http://i.stack.imgur.com/1Jbav.png)\n\nBut when I add `totalcount` to the mix, the axis limits go haywire.\n\n```\ndf.boxplot(['count1', 'count2', 'totalcount'], by='season')\n```\n\n![enter image description here](http://i.stack.imgur.com/Yv74U.png)\n\nThis happens irregardless of the order of the columns. I realize there are several ways around this problem, but it would be much more convenient if this worked properly. I believe this worked as expected in Pandas 0.13.1.\n\nOutput from `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.7.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 13.2.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.14.0\nnose: 1.3.3\nCython: 0.20.1\nnumpy: 1.8.1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.1.0\nsphinx: None\npatsy: 0.2.1\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2014.4\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.3.1\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.3.2\nhtml5lib: None\nbq: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: 2.5.3 (dt dec pq3 ext)\n```\n"},{"labels":["api",null,null],"text":"```\nPython 2.7.6 (default, Mar 22 2014, 22:59:56) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pandas\n>>> a = pandas.Series()\n>>> a.loc[1] = 1\n>>> a.loc['a'] = 2\n>>> a.loc[[-1, -2]]\n1    1\na    2\ndtype: int64\n```\n\nThis is ok:\n\n```\n>>> a.loc[-1]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py\", line 1129, in __getitem__\n    return self._getitem_axis(key, axis=0)\n  File \"/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py\", line 1261, in _getitem_axis\n    self._has_valid_type(key, axis)\n  File \"/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py\", line 1234, in _has_valid_type\n    error()\n  File \"/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py\", line 1221, in error\n    (key, self.obj._get_axis_name(axis)))\nKeyError: 'the label [-1] is not in the [index]'\n```\n\nThis is ok too:\n\n```\n>>> a.loc[['W']]\nW   NaN\ndtype: float64\n```\n\nBut this is not:\n\n```\n>>> a.loc[-1] = 3\n>>> a.loc[[-1, -2]]\n-1    3\n1    1\ndtype: int64\n```\n\nAnd this is not good at all:\n\n```\n>>> a\n1    1\n-1    3\ndtype: int64\n>>> a['a'] = 2\n>>> a\n1     1\n-1    3\na     2\ndtype: int64\n>>> a.loc[[-2]] = 0\n>>> a\n1     0\n-1    3\na     2\ndtype: int64\n```\n\nWithout `'a'` string in the index it raises while I expect new item (`{-2: 0}`) to be added\n\n```\n>>> del a['a']\n>>> a.loc[[-2]] = 0\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py\", line 117, in __setitem__\n    indexer = self._convert_to_indexer(key, is_setter=True)\n  File \"/tmp/venv/local/lib/python2.7/site-packages/pandas/core/indexing.py\", line 1068, in _convert_to_indexer\n    raise KeyError('%s not in index' % objarr[mask])\nKeyError: '[-2] not in index'\n```\n"},{"labels":["api",null,null,null],"text":"In pandas, unlike SQL, the rows seemed to be joining on null values. Is this a bug?\nrelated SO: http://stackoverflow.com/questions/23940181/pandas-merging-with-missing-values/23940686#23940686\n\nCode snippet\n\n``` python\nimport pandas as pd \nimport numpy as np\n\ndf1 = pd.DataFrame(\n    [[1, None],\n    [2, 'y']],\n    columns = ['A', 'B']\n)\nprint df1\n\ndf2 = pd.DataFrame(\n    [['y', 'Y'],\n    [None, 'None1'],\n    [None, 'None2']],\n    columns = ['B', 'C']\n)\nprint df2\n\nprint df1.merge(df2, on='B', how='outer')\n```\n\nOutput\n\n```\n   A     B\n0  1  None\n1  2     y\n\n      B      C\n0     y      Y\n1  None  None1\n2  None  None2\n\n   A     B      C\n0  1  None  None1\n1  1  None  None2\n2  2     y      Y\n```\n\nYou can see row 0 in df1 unexpectedly joins to both rows in df2.\n\nI would expect the correct answer to be\n\n```\n   A       B      C\n0  1      None  None\n1  2       y      Y\n2 None None None1\n3 None None None2\n```\n"},{"labels":["api",null,null],"text":"I encountered a problem with doing any arythmetic from index, in other words, when a index is time (datetime64) and i would like to count something by it, i have no other option than to assign it to some column in dataframe object.\nimport pandas as pd\n\n```\nimport pandas as pd\n\nrng = pd.date_range('1/1/2011', periods=4, freq='H')\nts = pd.Series(rng, index=rng)\n\nprint \"Data:\"\nprint ts\n\nprint \"\\nSubstraction from column\"\nprint ts-ts[0]\n\nprint \"\\nIndex to column\"\nts['lol']=ts.index\nprint ts['lol']-ts['lol'][0]\n\nprint \"\\nSubstraction by index\"\ndf = ts.index\nprint df-df[0]\n```\n\nresult:\n\n```\nData:\n2011-01-01 00:00:00   2011-01-01 00:00:00\n2011-01-01 01:00:00   2011-01-01 01:00:00\n2011-01-01 02:00:00   2011-01-01 02:00:00\n2011-01-01 03:00:00   2011-01-01 03:00:00\nFreq: H, dtype: datetime64[ns]\n\nSubstraction from column\n2011-01-01 00:00:00   00:00:00\n2011-01-01 01:00:00   01:00:00\n2011-01-01 02:00:00   02:00:00\n2011-01-01 03:00:00   03:00:00\nFreq: H, dtype: timedelta64[ns]\n\nIndex to column\nlol   00:00:00\nlol   01:00:00\nlol   02:00:00\nlol   03:00:00\ndtype: timedelta64[ns]\n\nSubstraction by index\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-146-5a8539747b5a> in <module>()\n     13 print \"\\nSubstraction by index\"\n     14 df = ts.index\n---> 15 print df-df[0]\n     16 \n\nC:\\winpy\\WinPython-64bit-2.7.6.4\\python-2.7.6.amd64\\lib\\site-packages\\pandas\\core\\index.pyc in __sub__(self, other)\n    853 \n    854     def __sub__(self, other):\n--> 855         return self.diff(other)\n    856 \n    857     def __and__(self, other):\n\nC:\\winpy\\WinPython-64bit-2.7.6.4\\python-2.7.6.amd64\\lib\\site-packages\\pandas\\core\\index.pyc in diff(self, other)\n    981 \n    982         if not hasattr(other, '__iter__'):\n--> 983             raise TypeError('Input must be iterable!')\n    984 \n    985         if self.equals(other):\n\nTypeError: Input must be iterable!\n```\n\nMaybe it's just conceptional problem, but if i want to make something with date index i have to keep additional column (with the same values as index!).\nWhen it comes to huge datasets this can be a problem, because i have to store the same thing twice, or make additional column for calculations, which is not better.\n\nP.S. pd.show_versions():\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 37 Stepping 2, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.13.1\nCython: 0.20.1\nnumpy: 1.8.1\nscipy: 0.13.3\nstatsmodels: 0.5.0\nIPython: 2.0.0\nsphinx: 1.2.2\npatsy: 0.2.1\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2013.9\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.3.1\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: None\nsqlalchemy: 0.9.4\nlxml: None\nbs4: None\nhtml5lib: None\nbq: None\napiclient: None\n```\n"},{"labels":["api",null],"text":"The `if_exists` argument of the `to_sql` function doesn't check all schema for the table while checking if it exists. Furthermore, it inserts to the default schema, causing somewhat contradictory behavior.\n\nFor example, while using SQL Server with my default schema set to `test`, `to_sql` inserts the table into `test.table_name`. However, trying this again, with `if_exists='replace'`, `to_sql` finds no table of the name `dbo.table_name`, and then tries to create `test.table_name`, causing an error.\n\nDetails :\n\nhttp://stackoverflow.com/questions/24126883/pandas-dataframe-to-sql-function-if-exists-parameter-not-working\n"},{"labels":["api",null,null],"text":"discovered in #5292\n\nThis seems odd to me, documented anywhere?\n\n```\nIn [2]: t = pd.Timestamp('2014-06-10 8am')\n\nIn [3]: t + pd.DateOffset(hour=9)\nOut[3]: Timestamp('2014-06-10 09:00:00', tz=None)\n\nIn [4]: t + pd.DateOffset(hours=9)\nOut[4]: Timestamp('2014-06-10 17:00:00', tz=None)\n```\n\nnote that\n\n```\nt + pd.offsets.Hour(9) == t + timedelta(hours=9)\n```\n\nso this definitly seems suspect\n\n(also problematic for `second/seconds` and `minute/minutes`)\n\nthis is only an issue in `DateOffset` constructions.\n"},{"labels":["api",null],"text":"When using `df.as_matrix()` method, rows and columns do not render as 1xN or Nx1 matricies, rather as 1xN arrays.\n\n```\nIn [4]: df.ix['foo']=[5,3]\n\nIn [5]: df.ix['bar']=[2,6]\n\nIn [6]: df\nOut[6]:\n     A  B\nfoo  5  3\nbar  2  6\n\n[2 rows x 2 columns]\n\nIn [7]: df['A'].as_matrix()\nOut[7]: array([ 5.,  2.])\n\nIn [8]: df.ix['foo'].as_matrix()\nOut[8]: array([ 5.,  3.])\n```\n\nExpected :\n\n```\nIn [9]: np.matrix('5; 2')\nOut[9]:\nmatrix([[5],\n        [2]])\n```\n"},{"labels":["api",null],"text":"Hello!\n\nI just heard from a colleague that they're looking for the analogue of STATA's merge command (http://www.stata.com/help.cgi?merge) which generates a `_merge` column that includes a code which specifies in an outer join whether the row existed in the right table, the left table or both. I know you can hack your way around this by doing set operations on the join columns / indices or creating new columns, but there could be an argument for having this be included functionality if it could be done simultaneously during the merge or just for sheer convenience.\n\nThe use case specified was that after they merged, they were checking over the data to find inconsistencies and rows that should have been merged but somehow didn't.\n\nLet me know if there would be any interest in this, and I could maybe have a first shot at implementing it.\n"},{"labels":["api",null,null],"text":"As pointed out by @dsm054, there are multiple lurking split/partition API requests. Here are the issues and a short summary of what they would do (there are some duplicates here, I've checked off those issues/PRs that have been closed in favor of a related issue):\n- [x] #414: (i think) original issue for these ideas going back 3 years\n- [x] #936: windowing with time-length windows like `pd.rolling_mean(ts, window='30min')` and possibly even arbitrary windows using another column\n- [x] #3066: `split` method on pandas objects, playing around with ideas\n- [x] #3101: a closed PR by @y-p to use the args of lambda to group a frame into views of a sliding window\n- [x] #3685: resampling using the first `n` samples of a bin.\n- [ ] #4059: `np.array_split` style API where you can split a pandas object into a list of `k` groups of possibly unequal size (could be a thin wrapper around `np.array_split`, or more integrated into the pandas DSL). IMO, this issue provides the best starting point for an API. [SO usage](http://stackoverflow.com/questions/24461906/how-can-i-divide-up-a-pandas-dataframe/24462529#24462529)\n- [x] #5494: an API for to allow pandas' `groupby` to have `itertools.groupby` semantics (i.e., preserve the order of duplicated group keys), i.e., `'aabbaa'` would yield groups `['aa', 'bb', 'aa']` rather than `['aaaa', 'bb']`. There'd have to be some changes to the use of `dict` in the groupby backend as noted by @y-p here https://github.com/pydata/pandas/issues/4059#issuecomment-29061036.\n- [ ] #6675: Ability to select ranged groups via another column, like \"select all rows between the values X and Y from column C\", e.g., an \"events\" column where you have a start and end markers and you want to get the data in between the markers. There are a couple of ways you can do this, but it would be nice to have an API for this. This is very similar to #936.\n\nThe [`toolz`](https://github.com/pytoolz/toolz) library has a `partitionby` function that provides a nice way to do some of the splitting on sequences and might provide us with some insight on how to approach the API.\n\ncc @jreback @jorisvandenbossche @hayd @danielballan\n"},{"labels":["api",null,null],"text":"Derived from #7299. `numpy.insert` can accept multiple elements / indices as of v1.8.\nhttp://docs.scipy.org/doc/numpy/reference/generated/numpy.insert.html\n\nAllow `Index` to accept multiple elements / indices. Currently, it raises `IndexError`.\n\n```\nimport pandas as pd\nidx1 = pd.Index([1, 2, 3, 4, 5])\nidx1.insert([1, 2], 0)\n# IndexError: invalid slice\n```\n"},{"labels":["api",null],"text":"xref: #7356\n\nAPI like:\n\n``` python\ndf.nlargest('column_name', 3)\n```\n\nimpl like:\n\n``` python\ndef nlargest(self, col, n):\n    return self.loc[self[col].nlargest(n).index]\n```\n\ncc @hayd \n"},{"labels":["api",null,null],"text":"related #7308\nrelated #7314\n\nmaybe something like this\n\n```\ndf.get_for_dtypes(types)\n```\n\ntypes could be a list of string/dtypes or a dtype/string\n\ne.g. \n\n```\n'float64'\n['float']\n['float32','float64','object']\n['float',np.dtype('bool')]\n['numeric']\n['datetime','timedelta']\n```\n\nare possible values\n\npretty easy to implement (just translate the types and filter on the blocks, ala `_get_numeric_data`)\n"},{"labels":["api",null,null],"text":"```\nIn [10]: a=TS.DataFrame(np.random.random((10,3)))\n\nIn [11]: a\nOut[11]: \n         0        1        2\n0 0.293227 0.489837 0.389012\n1 0.070556 0.230420 0.259219\n2 0.105651 0.026292 0.737140\n3 0.859360 0.092215 0.306204\n4 0.377702 0.452027 0.409900\n5 0.801594 0.123576 0.138083\n6 0.810958 0.827973 0.363331\n7 0.325093 0.527616 0.226507\n8 0.903319 0.611720 0.636183\n9 0.951649 0.663440 0.251934\n\nIn [12]: a.quantile(0.5,axis=0)\nOut[12]: \n0   0.589648\n1   0.470932\n2   0.334767\ndtype: float64\n\nIn [13]: a.quantile(0.5,axis=1)\nOut[13]: \n0   0.589648\n1   0.470932\n2   0.334767\ndtype: float64\n```\n"},{"labels":["api",null],"text":"This might not necessarily a new thing since 0.14, but I find the output of group.head() not appropriate for a grouping:\n\n``` python\ndf = pd.DataFrame(np.random.randn(6,2))\ndf['A'] = [1,2,2,1,1,2]\ndf\n```\n\n```\n          0         1  A\n0 -0.047101  0.828542  1\n1  1.617815  0.362700  2\n2  1.366453 -1.116897  2\n3  0.086743 -0.611371  1\n4  1.918440 -1.230909  1\n5 -1.003828 -0.592541  2\n```\n\n``` python\ng = df.groupby('A')\ng.head(2)\n```\n\n```\n          0         1  A\n0 -0.047101  0.828542  1\n1  1.617815  0.362700  2\n2  1.366453 -1.116897  2\n3  0.086743 -0.611371  1\n```\n\nMy expectation of the previous output would be:\n\n```\n          0         1  A\n0 -0.047101  0.828542  1\n3  0.086743 -0.611371  1\n1  1.617815  0.362700  2\n2  1.366453 -1.116897  2\n```\n\nbecause, after all, this is the result of a grouping, so things should be displayed grouped, shouldn't they?\n"},{"labels":["api"],"text":"`Series.eq` should have same signature as `DataFrame.eq`\n\nincluding `axis,level` args\n\nadd docs for these as well\n"},{"labels":["api",null,null],"text":"Here is the idea. I have predefined index and I'm grouping by timestamps in it, aggregating preceding values of the interval:\n\n``` python\nindex = DatetimeIndex([datetime(2014, 1, 11, 11, 30), datetime(2014, 1, 12, 15), datetime(2014, 1, 15, 22)])\n\ns = Series({\n    datetime(2014, 1, 10, 0): 1,\n    datetime(2014, 1, 11, 10): 2,\n    datetime(2014, 1, 11, 11): 3,\n    datetime(2014, 1, 12, 12): 3,\n    datetime(2014, 1, 15, 16): 4,\n    datetime(2014, 1, 15, 22): 5\n})\n\nprint s.groupby(lambda x: index[(index >= x).argmax()]).sum()\n```\n\n---\n\n```\n2014-01-11 11:30:00    6\n2014-01-12 15:00:00    3\n2014-01-15 22:00:00    9\n```\n\nI don't know how effecient this code, but I think it might be a very useful feature as long as \"resample\" accepts only offsets or deltas. And \"reindex\" method doesn't have 'how' argument.\n"},{"labels":["api",null,null],"text":"Seeing this SO question: http://stackoverflow.com/questions/23896088/dropping-rows-in-dataframe-if-found-in-another-one/, I was wondering if this is something that could be provided as functionality to the `.isin()` method.\n\nThe problem at the moment to use `isin` to check for the occurence of a full row in a DataFrame is that a) `isin` checks for the values in each column seperately, indepenently of the values in other columns (so you cannot check if the values occur together in the same row) and b) `isin` also checks if the index label matches.\n\nOr are there better ways to check for the occurence of a full row in a DataFrame?\n"},{"labels":["api",null,null],"text":"Pandas specifies an Index API in the docs: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-internal-details\n\nHowever, it's not currently possible (as far as I can tell) to create an object that can be used as a pandas Index unless it's an actual `pandas.Index` subclass.\n\nFrom the perspective of someone who would like to be able to create custom Index-like objects, I would prefer not to need to subclass `pandas.Index` so the implementation details of my index can be decoupled from pandas. I might still use a pandas.Index internally for speed, but in my experience composition is generally easier to reason about than inheritance.\n"},{"labels":["api",null],"text":"related is #7146, #7206 \n\nmaybe provide a `to_index()` function and/or a class-like accessor\n\n`.values_to_index` (too long though)\n"},{"labels":["api",null,null],"text":"```\nn = 5\ndf = DataFrame({'a': randint(3, size=n),\n                'b': randn(n)})\ndf2 = DataFrame({'a': randint(4, size=n),\n                 'b': df.b + (rand(n) > 0.5).astype(float)})\ndf.isin(df2)\n```\n\nyields\n\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td> False</td>\n      <td>  True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td> False</td>\n      <td>  True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td> False</td>\n      <td> False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td> False</td>\n      <td> False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td> False</td>\n      <td>  True</td>\n    </tr>\n  </tbody>\n</table>\n\n\nI might expect it to go column by column calling `isin` on the intersection of the columns. I'm not sure if this was ever defined when `DataFrame.isin()` was implemented.\n"},{"labels":["api",null],"text":"http://stackoverflow.com/questions/23709811/best-way-to-sum-group-value-counts-in-pandas/23712433?noredirect=1#comment36441762_23712433\nany of these look palatable?\n\n`df.groupby('c',as_index='exclude').apply(....)`\n`df.groupby('c')['~c'].apply(...)`\n`df.groupby('c',filter='c')`\n`df.groupby('c',filter=True)`\n\nrather than doing a negated selection (`df.columns-['c']`)\n\n```\ndf = pd.DataFrame.from_dict({'b1':['aa','ac','ac','ad'],\\\n                             'b2':['bb','bc','ad','cd'],\\\n                             'b3':['cc','cd','cc','ae'],\\\n                             'c' :['a','b','a','b']})\n\ndf.groupby('c')[df.columns - ['c']].apply(lambda x: x.unstack().value_counts())\nOut[92]: \nc    \na  cc    2\n   bb    1\n   ad    1\n   ac    1\n   aa    1\nb  cd    2\n   ad    1\n   ae    1\n   ac    1\n   bc    1\ndtype: int64\n```\n"},{"labels":["api",null],"text":"see #7143 for an example.\n"},{"labels":["api",null],"text":"`df.iloc[[0,1],[0,1]]` means return the square 0-1 rows and 0-1 columns\n\nmight be nice to have a way to mean return the _coordinates_ `(0,0),(0,1)`\n\nrelated #7522\n\nhttp://stackoverflow.com/questions/23686561/slice-a-pandas-dataframe-by-an-array-of-indices-and-column-names/23686855#23686855\n"},{"labels":["api",null,null],"text":"From https://github.com/pydata/pandas/issues/6939, @jorisvandenbossche made the suggestion that\n\n> we could add a keyword argument like show_counts (or another name) to specify if you want to show the non-null counts (to be able to override the max_info_rows option for a specific info call)\n\nscheduling for .14.1 unless someone gets to it before then.\n"},{"labels":["api",null],"text":"rather than `display.max_rows` directly\n\nfor considency with the new 'truncate' option\n"},{"labels":["api",null,null,null,null],"text":"Hi\n\nSomething is wrong when trying to save a dataframe with tz-aware timestamps to xlsx, using pd.Dataframe.to_excel:\n\n``` Python\ndf = pd.DataFrame([1], index=[pd.Timestamp('2014-05-02', tz='CET')])\ndf.to_excel('test.xlsx')\n```\n\nyields an exception on my system:\n\n```\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-2-4e18a4be2a71> in <module>()\n----> 1 df.to_excel('test.xlsx')\n\n/home/silvio/prod34/lib/python3.4/site-packages/pandas/core/frame.py in to_excel(self, excel_writer, sheet_name, na_rep, float_format, cols, header, index, index_label, startrow, startcol, engine, merge_cells)\n   1202         formatted_cells = formatter.get_formatted_cells()\n   1203         excel_writer.write_cells(formatted_cells, sheet_name,\n-> 1204                                  startrow=startrow, startcol=startcol)\n   1205         if need_save:\n   1206             excel_writer.save()\n\n/home/silvio/prod34/lib/python3.4/site-packages/pandas/io/excel.py in write_cells(self, cells, sheet_name, startrow, startcol)\n    771                 wks.write(startrow + cell.row,\n    772                           startcol + cell.col,\n--> 773                           cell.val, style)\n    774 \n    775     def _convert_to_style(self, style_dict, num_format_str=None):\n\n/home/silvio/prod34/lib/python3.4/site-packages/xlsxwriter/worksheet.py in cell_wrapper(self, *args, **kwargs)\n     55             if len(args):\n     56                 int(args[0])\n---> 57             return method(self, *args, **kwargs)\n     58         except ValueError:\n     59             # First arg isn't an int, convert to A1 notation.\n\n/home/silvio/prod34/lib/python3.4/site-packages/xlsxwriter/worksheet.py in write(self, row, col, *args)\n    374         # Write datetime objects.\n    375         if isinstance(token, date_types):\n--> 376             return self.write_datetime(row, col, *args)\n    377 \n    378         # Write number types.\n\n/home/silvio/prod34/lib/python3.4/site-packages/xlsxwriter/worksheet.py in cell_wrapper(self, *args, **kwargs)\n     55             if len(args):\n     56                 int(args[0])\n---> 57             return method(self, *args, **kwargs)\n     58         except ValueError:\n     59             # First arg isn't an int, convert to A1 notation.\n\n/home/silvio/prod34/lib/python3.4/site-packages/xlsxwriter/worksheet.py in write_datetime(self, row, col, date, cell_format)\n    666 \n    667         # Convert datetime to an Excel date.\n--> 668         number = self._convert_date_time(date)\n    669 \n    670         # Add the default date format.\n\n/home/silvio/prod34/lib/python3.4/site-packages/xlsxwriter/worksheet.py in _convert_date_time(self, dt_obj)\n   3265     def _convert_date_time(self, dt_obj):\n   3266         # Convert a datetime object to an Excel serial date and time.\n-> 3267         return datetime_to_excel_datetime(dt_obj, self.date_1904)\n   3268 \n   3269     def _options_changed(self):\n\n/home/silvio/prod34/lib/python3.4/site-packages/xlsxwriter/utility.py in datetime_to_excel_datetime(dt_obj, date_1904)\n    577 \n    578     # Convert a Python datetime.datetime value to an Excel date number.\n--> 579     delta = dt_obj - epoch\n    580     excel_time = (delta.days\n    581                   + (float(delta.seconds)\n\n/home/silvio/prod34/lib/python3.4/site-packages/pandas/tslib.cpython-34m.so in pandas.tslib._Timestamp.__sub__ (pandas/tslib.c:11918)()\n\nTypeError: can't subtract offset-naive and offset-aware datetimes\n```\n\nUsing Python 3.4.0 on Arch Linux, list of installed packages:\n\n```\n~% pip freeze\nCython==0.20.1\nJinja2==2.7.2\nMarkupSafe==0.21\nPygments==1.6\nSQLAlchemy==0.9.4\nXlsxWriter==0.5.3\necdsa==0.11\nipdb==0.8\nipython==2.0.0\nipython-sql==0.3.1\nlxml==3.3.5\nmatplotlib==1.3.1\nmysql-connector-python==1.1.6\nnose==1.3.1\nnumpy==1.8.1\nopenpyxl==1.8.5\npandas==0.13.1\nparamiko==1.13.0\npatsy==0.2.1\nprettytable==0.7.2\npsycopg2==2.5.2\npycrypto==2.6.1\npyodbc==3.0.7\npyparsing==2.0.2\npython-dateutil==2.2\npytz==2014.2\nrequests==2.2.1\nscipy==0.13.3\nsix==1.6.1\nsqlparse==0.1.11\nstatsmodels==0.6.0\ntornado==3.2\nxlrd==0.9.3\n```\n"},{"labels":["api",null,null,null,null],"text":"There are a number of offset aliases listed here:\nhttp://pandas.pydata.org/pandas-docs/dev/timeseries.html#offset-aliases\n\nI'm currently trying to resample a timeseries where the buckets are return with the beginning and/or end of the week.  There are a number of Start Month/End Month frequency aliases.  Am I missing something or should we include Week End/Week Start resampling aliases as well?\n"},{"labels":["api",null,null],"text":"xref: #7032 \n\nreally just remove the parameter's effect (no signature change) ... but give a _full_ removal warning for 0.15\n"},{"labels":["api"],"text":"Right now, as per the gotchas section, pandas raises a value error on an empty frame\nhttp://pandas.pydata.org/pandas-docs/dev/gotchas.html#gotchas-truth\n\nWhile documentation describes ambiguity, in the python world there is none.\nPEP8 (http://legacy.python.org/dev/peps/pep-0008/) recommends: \"For sequences, (strings, lists, tuples), use the fact that empty sequences are false.\"\n\nThe check returns False for None and empty values for all iterables. Further, there are any (https://docs.python.org/2/library/functions.html#any) and all(https://docs.python.org/2/library/functions.html#all) functions for the proposed ambiguity checks.\n\nAs currently it's just raising a ValueError, fixing the problem to correctly evaluate representing DataFrame.empty would be backwards compatible.\n\nThe check becomes important when working in a mixed environment with pandas and other structures - a simple `if foo` can become as convoluted as \n`empty = foo.empty() if (foo is not None and isinstance(foo, pd.DataFrame)) else not foo` \n"},{"labels":["api",null],"text":"I ran this on master current as of 2014-04-20 or so and was surprised as to the result:\n\n``` python\nIn [17]: pd.date_range('2014', '2015', freq='M')\nOut[17]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2014-04-30, ..., 2015-03-31]\nLength: 12, Freq: M, Timezone: None\n```\n\nI would have expected that to give me the month-ends in 2014, not a year's worth of month-ends starting from today.\n\nSeems to be related to #3944?\n"},{"labels":["api",null,null],"text":"Related to #6913.\n\n`DataFrame.pivot` function uses separate logic and has less keyword options than `pandas.tools.pivot_table`. How about making `DataFrame.pivot` to call `pivot_table` internally to support same keywords as `pivot_table`?\n"},{"labels":["api",null,null],"text":"See below.  Dates from the last week of December 2013 are interpreted as being from the first week of the year.  Is this due to the fact December 30th was a Monday?\n\n``` python\nimport pandas as pd\n\ndt = '2013-12-30'\npd.to_datetime(dt, coerce=True).year, pd.to_datetime(dt, coerce=True).weekofyear\n# I expected: (2013, 53) since this is day 364 and the start of the 53rd week\n# Instead it returned: (2013, 1)\n```\n\nMy current work-around is:\n\n``` python\nimport pandas as pd\nimport numpy as np\n\ndt = '2013-12-30'\ndoy = pd.to_datetime(dt, coerce=True).dayofyear\n(np.floor(doy / 7) + 1).astype('int')\n# Returns 53\n```\n\n``` python\npd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.5.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.11.0-15-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.13.1\nCython: 0.20.1\nnumpy: 1.8.1\nscipy: 0.12.0\nstatsmodels: 0.4.2\nIPython: 2.0.0\nsphinx: 1.2.2\npatsy: None\nscikits.timeseries: None\ndateutil: 1.5\npytz: 2012c\nbottleneck: None\ntables: 2.4.0\nnumexpr: 2.0.1\nmatplotlib: 1.2.1\nopenpyxl: 1.5.8\nxlrd: 0.9.2\nxlwt: 0.7.4\nxlsxwriter: None\nsqlalchemy: None\nlxml: 3.2.0\nbs4: 4.2.0\nhtml5lib: 0.95-dev\nbq: None\napiclient: None\n```\n"},{"labels":["api",null],"text":"``` python\ndef factorize(values, sort=False, order=None, na_sentinel=-1):\n    \"\"\"\n    Encode input values as an enumerated type or categorical variable\n\n    Parameters\n    ----------\n    values : ndarray (1-d)\n        Sequence\n    sort : boolean, default False\n        Sort by values\n    order :\n    na_sentinel: int, default -1\n        Value to mark \"not found\"\n\n    Returns\n    -------\n    labels : the indexer to the original array\n    uniques : the unique values\n\n    note: an array of Periods will ignore sort as it returns an always sorted PeriodIndex\n    \"\"\"\n```\n\nBut it isn't used anywhere. Technically this is an API change, if people aren't using kwargs. So I'll add it to the deprecation list.\n"},{"labels":["api",null,null],"text":"This surfaced in the Panel `pct_change` implementation at https://github.com/pydata/pandas/pull/6909\n\n`generic`: `def shift(self, periods=1, freq=None, axis=0, **kwds):`\n`Panel`:    `def shift(self, lags, freq=None, axis='major'):`\n\nI had to adjust the call to `shift` in `generic.pct_change` to not use a kwarg for `lag`/`period`:\n\nBefore: `rs = data / data.shift(periods=periods, freq=freq, **kwds) - 1`\nAfter: `rs = data.div(data.shift(periods, freq=freq, **kwds)) - 1`\n\nWe'll need to keep the `.div` change, but we can change back the kwarg once the signatures are aligned.\n- [x] Add kwarg deprecation decorator\n- [x] change `pct_change` back to kwarg\n"},{"labels":["api",null,null,null,null],"text":"```\n  File \"z:\\code\\m.py\", line 420, in get_panel\n    self.source_panel.pct_change()\n  File \"c:\\anaconda\\lib\\site-packages\\pandas\\core\\generic.py\", line 3418, in pct_change\n    rs = data / data.shift(periods=periods, freq=freq, **kwds) - 1\nTypeError: shift() got an unexpected keyword argument 'periods'\n```\n\n> pandas.version.version\n> Out[5]: '0.13.1'\n"},{"labels":["api",null],"text":"A very common operation when trying to work with data is to find out the error range for the data.  In scientific research, including error ranges is required.\n\nThere are two main ways to do this: standard deviation and standard error of the mean.  Pandas has an optimized std aggregation method for both dataframe and groupby.  However, it does not have an optimized standard error method, meaning users who want to compute error ranges have to rely on the unoptimized scipy method.  \n\nSince computing error ranges is such a common operation, I think it would be very useful if there was an optimized `sem` method like there is for `std`.\n"},{"labels":["api",null],"text":"I have python 2.7.6 and pandas 0.13.1 on Windows 7.\n\n```\n>>>portfolio_data_all.index.names\nFrozenList([u'date', u'stock_id'])\n>>>portfolio_data_all.xs(cusip_etf, level=field_stock_id, copy=False)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-69-5300763cc7b7> in <module>()\n----> 1 portfolio_data_all.xs(cusip_etf, level=field_stock_id, copy=False)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in xs(self, key, axis, level, copy, drop_level)\n   1261 \n   1262             if not copy and not isinstance(loc, slice):\n-> 1263                 raise ValueError('Cannot retrieve view (copy=False)')\n   1264 \n   1265             # level = 0\n\nValueError: Cannot retrieve view (copy=False)\n```\n"},{"labels":["api",null],"text":"An idea: to be able to specify the a name when resetting the index, which would override the index name of the default value when index name is None.\n\nEg something like this:\n\n```\nIn [10]: df = pd.DataFrame(np.random.rand(4,3), columns=list('ABC'), index=pd.date_range('2013-01-01', periods=4))\nIn [11]: df\nOut[11]: \n                   A         B         C\n2013-01-01  0.313664  0.606445  0.048081\n2013-01-02  0.907785  0.004429  0.374456\n2013-01-03  0.916584  0.067639  0.467712\n2013-01-04  0.712824  0.687895  0.397960\n\nIn [12]: df.reset_index()\nOut[12]: \n       index         A         B         C\n0 2013-01-01  0.313664  0.606445  0.048081\n1 2013-01-02  0.907785  0.004429  0.374456\n2 2013-01-03  0.916584  0.067639  0.467712\n3 2013-01-04  0.712824  0.687895  0.397960\n\nIn [12]: df.reset_index(name='datetime')\nOut[12]: \n    datetime         A         B         C\n0 2013-01-01  0.313664  0.606445  0.048081\n1 2013-01-02  0.907785  0.004429  0.374456\n2 2013-01-03  0.916584  0.067639  0.467712\n3 2013-01-04  0.712824  0.687895  0.397960\n```\n\nFor multi-index, you should provide a list. \n"},{"labels":["api"],"text":"see #6848\n\nSeries.sort defaults to `inplace=True`\nSeries.order default to `inplace=False`\n\nas they are inverses of each other\n"},{"labels":["api",null],"text":"current `Series.sort` defaults to `kind='quicksort'` prob because `np.sort` does.\nbut `Series.order` defaults to `mergesort` (and this is present in DataFrame.sort) as well.\n\nproposal is to make `Series.order` default to quicksort for some conformity\n\nany objections?\n"},{"labels":["api",null,null],"text":"From this (and other questions), related #4808\n\nhttp://stackoverflow.com/questions/22934996/pandas-pytables-append-performance-and-increase-in-file-size\n\nA convenience function to perform an on-disk concat from a list-like to a single table.\n\n```\ndef concat(self, key, generator, index=True, append=False):\n    \"\"\"\n    Parameters\n    ---------------\n    key : the resulting group for the table\n    generator : a list-like / iterator / generator to source objects\n          if string-like then read these from the same file name (and same key name)\n    index : boolean, default True,\n       create an index on the resulting table\n    append : boolean, default False\n        if True, append to the resulting table, otherwise remove first\n    \"\"\"\n\n    # remove the group if it exists (and not appending)\n    if key in self and not append:\n          self.remove(key)\n\n    for obj in generator:\n\n         # a string - treat as a file-name\n         if isinstance(obj, compat.string_types):\n             with get_store(obj, mode='r') as s:\n                  obj = s.get(key)             \n\n         self.append(key, obj, index=False)\n\n    if index:\n          self.create_index(columns=index)\n\n```\n\nHDF files\n\n```\nwith get_store('concat.hdf',mode='w') as store:\n    store.concat('data',['file1.hdf','file2','hdf'])\n```\n\nObjects\n\n```\nwith get_store('concat.hdf',mode='w') as store:\n    store.concat('data',[DataFrame(...), DataFrame(....)])\n```\n\nGenerator\n\n```\ndef f():\n      for f in files:\n            df = read_csv(f)\n            yield df\n\nwith get_store('concat.hdf',mode='w') as store:\n    store.concat('data',f)\n```\n"},{"labels":["api",null,null],"text":"See this [question in SO](http://stackoverflow.com/questions/22897597/read-entire-group-in-an-hdf5-file-using-a-pandas-hdfstore).\n\nI guess that something like `store.get_group('df')` might do the job:\n\n```\n>>> import pandas as pd\n>>> store = pd.HDFStore('test.h5',mode='w')\n>>> store.append('df/foo1',DataFrame(np.random.randn(10,2)))\n>>> store.append('df/foo2',DataFrame(np.random.randn(10,2)))\n>>> store.get_group('df')\n...           0         1\n... 0 -0.495847 -1.449251\n... 1 -0.494721  1.572560\n... 2  1.219985  0.280878\n... 3 -0.419651  1.975562\n... 4 -0.489689 -2.712342\n... 5 -0.022466 -0.238129\n... 6 -1.195269 -0.028390\n... 7 -0.192648  1.220730\n... 8  1.331892  0.950508\n... 9 -0.790354 -0.743006\n... 0 -0.761820  0.847983\n... 1 -0.126829  1.304889\n... 2  0.667949 -1.481652\n... 3  0.030162 -0.111911\n... 4 -0.433762 -0.596412\n... 5 -1.110968  0.411241\n... 6 -0.428930  0.086527\n... 7 -0.866701 -1.286884\n... 8 -0.649420  0.227999\n... 9 -0.100669 -0.205232\n... \n... [20 rows x 2 columns]\n```\n\nI never did a pull request before, nor tried to dive into pandas code, but I guess that a possible implementation (inspired in [this answer](http://stackoverflow.com/a/22897898/630598)) would be something like:\n\n```\nclass HDFStore():\n    ...\n    def get_group(self, group_name):\n        return pd.concat([ self.select(node._v_pathname) for node in self.get_node(group_name) ])\n    ...\n```\n"},{"labels":["api",null,null],"text":"I have some frames that have 44 ish columns with names I can't really remember. I think it might be useful to be able to do something like \n\n``` python\nstore.get_index('hdf5/table', axis=1)\n```\n\nwhich would return the columns of `hdf5/table`. @jreback thoughts?\n"},{"labels":["api",null],"text":"atm we're checking dict-like by doing isinstance(x, dict), I think we should be using collections.MutableMapping, slightly more general.\n"},{"labels":["api",null,null],"text":"This works\n\n```\nfrom pandas.tseries.frequencies import to_offset\npd.Timestamp('1990-9-30') + 2*to_offset('Q')\n```\n\nThis doesn't. Shouldn't it? Workaround is to convert with `to_timestamp`.\n\n```\npd.Period('1990q1') + 2*to_offset('Q')\n```\n"},{"labels":["api",null,null],"text":"Dear developers,\n\nI think that it is confusing how DataFrame.replace interprets the to_replace values as a pure string or a regexp. \n\nWhen you pass a dictionary of from and to values, these are interpreted as pure string literals and work as expected. If you pass a nested dictionary with the same values, it is interpreted as a regex even if regex = False is specifically added. This causes a problem if you want to replace values that have special characters in them.\n\nLet's see an example:\n\n``` python\n\n@buddha[J:T26]|19> df = pd.DataFrame({'a' : ['()', 'something else']})\n@buddha[J:T26]|20> df\n              <20>\n                a\n0              ()\n1  something else\n\n[2 rows x 1 columns]\n@buddha[J:T26]|21> df.replace({'()' : 'parantheses'})\n              <21>\n                a\n0     parantheses\n1  something else\n\n[2 rows x 1 columns]\n@buddha[J:T26]|22> df.replace({'a' : {'()' : 'parantheses'}})\n              <22>\n                                                   a\n0                parantheses(parantheses)parantheses\n1  paranthesessparanthesesoparanthesesmparanthese...\n\n[2 rows x 1 columns]\n```\n\nAs you can see, in the first case the () got replaced as expected. In the second case, even though the only thing I changed was to specify the column in which the replace should occur, the parentheses got treated as a regex. The same happens if I add regex = False to the options.\n\nThe current workaround for me is to make sure that the from/to values are regexes.\n\nCheers,\nAdam\n\nPs.: Pandas is awesome, thanks a lot for all the effort!\n\n@buddha[J:T26]|24> pd.show_versions()\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 32\nOS: Windows\nOS-release: 7\nmachine: x86\nprocessor: x86 Family 6 Model 23 Stepping 10, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.13.1\nCython: None\nnumpy: 1.8.0\nscipy: 0.13.3\nstatsmodels: 0.5.0\nIPython: 1.2.0\nsphinx: 1.2.1\npatsy: 0.2.1\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2013.9\nbottleneck: 0.8.0\ntables: 3.1.0\nnumexpr: 2.3\nmatplotlib: 1.3.1\nopenpyxl: None\nxlrd: 0.9.2\nxlwt: 0.7.5\nxlsxwriter: None\nsqlalchemy: 0.8.4\nlxml: 3.3.1\nbs4: 4.3.2\nhtml5lib: 0.999\nbq: None\napiclient: None\n"},{"labels":["api",null],"text":"I do this\n\n```\n>>> df = pd.DataFrame([[1,np.nan,0],[1,1,1],[2,2,2],[2,3,3]], columns=list('abc'))\n>>> print(df)\n   a   b  c\n0  1 NaN  0\n1  1   1  1\n2  2   2  2\n3  2   3  3\n\n[4 rows x 3 columns]\n>>> print(df.groupby('a').first())\n   b  c\na      \n1  1  0\n2  2  2\n\n[2 rows x 2 columns]\n```\n\nbut I expected this\n\n```\n>>> print(df.groupby('a').first())\n     b  c\na      \n1  NaN  0\n2    2  2\n\n[2 rows x 2 columns]\n```\n\nIs it possible to achieve my expected output? I get the same output in master and 0.13.1.\n"},{"labels":["api",null],"text":"related #5971\n"},{"labels":["api",null,null,null],"text":"http://stackoverflow.com/questions/22670904/python-pandas-turn-absolute-periods-into-relative-periods/22671904?noredirect=1#comment34537319_22671904\n\nthis works, but I think could be more efficient on a whole frame basis\n\n```\nIn [37]: def f(x):\n   ....:     y = x.dropna()\n   ....:     return Series(y.values,x.index[len(x)-len(y):])\n   ....: \n\nIn [40]: roller = pd.rolling_sum(df,3).reset_index(drop=True)\n\nIn [41]: roller\nOut[41]: \n    1   2   3\n0 NaN NaN NaN\n1 NaN NaN NaN\n2  61  56  56\n3   9   9   9\n4  12  12  12\n5 NaN  15  15\n6 NaN NaN  11\n7 NaN NaN NaN\n\n[8 rows x 3 columns]\n\nIn [43]: roller.apply(f).reindex_like(roller)\nOut[43]: \n    1   2   3\n0 NaN NaN NaN\n1 NaN NaN NaN\n2 NaN NaN NaN\n3 NaN NaN  56\n4 NaN  56   9\n5  61   9  12\n6   9  12  15\n7  12  15  11\n\n[8 rows x 3 columns]\n```\n"},{"labels":["api",null,null],"text":"http://stackoverflow.com/questions/22669208/attributeerror-series-object-has-no-attribute-searchsorted-pandas\n\nessentially a pass thru to `np.searchsorted`\n- should return the index (rather than indices)\n- can better handle `datetime64[ns]` (e.g. coerce input to/fro)\n- could check if sorted internally (e.g. `is_monotonic` on the index), other pass the `sorter` argument)\n"},{"labels":["api",null,null],"text":"The following code:\n\n```\ndf=DataFrame(data=[1,2,True,4.5], columns=[\"A\"])\ndf[\"B\"]=[1,1,1,2]\nprint df.groupby(df[\"B\"]).sum()\nprint df.groupby(df[\"B\"]).max()\n```\n\ngives as ouput\n\n```\n   B\nB   \n1  3\n2  2\n\n[2 rows x 1 columns]\n     A  B\nB        \n1  2.0  1\n2  4.5  2\n\n[2 rows x 2 columns]\n```\n\nAs we can see, the sum() operation has removed the column A (probably because the dtype is object/mixed). Is it expected ? The max() operation has no trouble with the column A. \nMoreover\n\n```\ndf[\"A\"].sum()\n```\n\ngives\n\n```\n8.5\n```\n\nwhich shows sum() is capable of handling mixed dtype.\nUsing aggregate(numpy.sum) gives the same result than sum().\n"},{"labels":["api"],"text":"Current behavior is to use the index over the columns. I think it makes more sense to prefer the columns.\n\nCurrent behavior:\n\n```\nIn [7]: n = 10\n\nIn [8]: df = DataFrame(randint(40, 50, size=(n, 2)), columns=list('ab'))\n\nIn [9]: df.index.name = 'a'\n\nIn [10]: df\nOut[10]:\n    a   b\na\n0  43  40\n1  43  46\n2  41  42\n3  40  45\n4  41  41\n5  40  45\n6  47  44\n7  49  41\n8  40  40\n9  47  40\n\n[10 rows x 2 columns]\n\nIn [11]: df.query('a > 5')\nOut[11]:\n    a   b\na\n6  47  44\n7  49  41\n8  40  40\n9  47  40\n\n[4 rows x 2 columns]\n```\n\nNew behavior:\n\n```\nIn [10]: df\nOut[10]:\n    a   b\na\n0  45  47\n1  44  41\n2  44  48\n3  47  40\n4  48  48\n5  47  40\n6  41  47\n7  44  44\n8  41  41\n9  40  47\n\n[10 rows x 2 columns]\n\nIn [11]: df.query('a > 43')\nOut[11]:\n    a   b\na\n0  45  47\n1  44  41\n2  44  48\n3  47  40\n4  48  48\n5  47  40\n7  44  44\n\n[7 rows x 2 columns]\n```\n"},{"labels":["api",null],"text":"I often find myself writing code that looks like this:\n\n``` python\nstart_group = df[df.str_col == 'start'].index.values + 1\nend_group = df[df.str_col == 'end'].index.values - 1\nfor s, e in zip(start_group, end_group):\n    some_function(df[s:e])\n```\n\nto get frame subsets between repeated start and end indicators from a particular column.\n\nPossible API:\n\n```\ndf.grange(marker_col='str_col', start='start', end='end')\n```\n\nwhich would return an iterator over the the subsets.\n\nI'm not sure if this is general enough to include in library code. I'll happily close the issue if it's not. There also might be a way to do this already.\n"},{"labels":["api",null,null,null,null],"text":"#5637 has some examples as well\n\nWhen the end date in `date_range` is not a valid business day it appears that `date_range` first sets it to a valid date _before_ deciding whether to return the last date depending on whether the interval is closed or open on the right. This results in the date range being incorrectly truncated when the interval is open-right - e.g.\n\n``` python\nIn [22]: for d in pd.date_range('20-Mar-2014', '23-Mar-2014', freq='D'):\n    ...:     print d.strftime('%a %d-%b-%Y')\n    ...:     \nThu 20-Mar-2014\nFri 21-Mar-2014\nSat 22-Mar-2014\nSun 23-Mar-2014\n\nIn [23]: pd.date_range('20-Mar-2014', '23-Mar-2014', freq='B')\nOut[23]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2014-03-20, 2014-03-21]\nLength: 2, Freq: B, Timezone: None\n\nIn [24]: # should return all valid business days <= '23-Mar-2014'\n    ...: pd.date_range('20-Mar-2014', '23-Mar-2014', freq='B', closed='left')\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2014-03-20]\nLength: 1, Freq: B, Timezone: None\n```\n"},{"labels":["api",null,null],"text":"http://stackoverflow.com/questions/22517167/slicing-pandas-dataframe-by-column-numbers-i-dont-want/22517737?noredirect=1#comment34263807_22517737\n\n```\nic = lambda x: pd.Int64Index(np.arange(x))\n\nIn [11]: df1.iloc[:,~ic(len(df1.columns)).isin(np.r_[0,3:5])]\n```\n\ncould borrow from `query` and support parsing I think\n\n```\ndf1.iloc[:,'~np.r_[0,3:5]`]\n```\n\ncan't parse this normally as `~` makes the indexer negative which means you index off the rhs of frame, which is not right\n"},{"labels":["api",null,null,null],"text":"maybe some more args similar to read_csv would be good too?\n"},{"labels":["api",null,null],"text":"I wrote this so my fault. This probably means some stuff with break when passing stuff with unicode.\n\nWill add example.\n\nThe reason is to include integers as strings before getting dummies, maybe should just drop that functionality?\n"},{"labels":["api",null,null],"text":"URL linked in the cookbook: http://stackoverflow.com/questions/11622652/large-persistent-dataframe-in-pandas/12193309#12193309\n\nTraceback:\n\n``` python\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py\", line 929, in concat\n    verify_integrity=verify_integrity)\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py\", line 944, in __init__\n    '\"{0}\"'.format(type(objs).__name__))\nAssertionError: first argument must be a list-like of pandas objects, you passed an object of type \"TextFileReader\"\n```\n\nTo be precise: Concatenation does not work in this example.\n"},{"labels":["api",null],"text":"Excel workbooks have 'Sheet1' as the default sheet. Can `pd.read_excel` have this as a default? `pandas.io.excel.read_excel(io, sheetname='Sheet1', **kwds)`\n\nThis makes pandas more accessible to the user transitioning from Excel by avoiding the common error below:\n\n```\nIn [9]: test = pd.read_excel('/Users/clark/Desktop/test.xlsx')\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-9-14880f6f18e4> in <module>()\n----> 1 test = pd.read_excel('/Users/clark/Desktop/test.xlsx')\n\nTypeError: read_excel() takes exactly 2 arguments (1 given)\n```\n"},{"labels":["api",null],"text":"In v0.13.1 the default representation for pd.DataFrame changed. I would like to be able to restore the previous default (see http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#dataframe-repr-changes)\n\nThe option display.large_repr only allows to set info(verbose=True) as default. The default in previous version of pandas was info(verbose=False) however. The effect is that displaying a DataFrame with a large number of columns creates a lot of output. \n\nAm I missing an easy way to change this behavior? Otherwise I suggest adding an option, e.g. `truncate`, `info_short`, `info_long`. \n\ndisplay.large_repr: [default: truncate] [currently: info]\n: 'truncate'/'info'\n\n```\n    For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can\n    show a truncated table (the default from 0.13), or switch to the view from\n    df.info() (the behaviour in earlier versions of pandas).\n```\n\npd.show_versions()\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Windows\nOS-release: 8\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.13.1\nCython: None\nnumpy: 1.8.0\nscipy: 0.13.3\nstatsmodels: 0.5.0\nIPython: 1.2.0\nsphinx: 1.2.1\npatsy: 0.2.1\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2013.9\nbottleneck: 0.8.0\ntables: 3.1.0\nnumexpr: 2.3\nmatplotlib: 1.3.1\nopenpyxl: 1.8.3\nxlrd: 0.9.2\nxlwt: 0.7.5\nxlsxwriter: 0.5.2\nsqlalchemy: None\nlxml: 3.3.1\nbs4: 4.3.2\nhtml5lib: 0.999\nbq: None\napiclient: None\n"},{"labels":["api",null,null],"text":"@dsm054 posted ridiculous fast version for this, perhaps we can wrap it in an API with filter?\n\nhttp://stackoverflow.com/questions/22208562/replace-rarely-occurring-values-in-a-pandas-dataframe#comment33717682_22208838\n\ncc @danielballan\n"},{"labels":["api",null,null],"text":"The fall through value_counts (for Series) is a bit strange, I think better result would be (with the standard options):\n\nCan put together if people think it's good.\n\n```\nIn [132]: df = pd.DataFrame([['a_link', 'dofollow'], ['a_link', 'dofollow'], ['a_link', 'nofollow'], ['b_link', 'javascript']], columns=['link', 'type'])\n\nIn [133]: g = df.groupby(['link', 'type'])\n\nIn [134]: g.value_counts()\nAttributeError: 'DataFrameGroupBy' object has no attribute 'value_counts'\n\nIn [135]: g.link.value_counts()   # redundant level\nOut[135]: \nlink    type              \na_link  dofollow    a_link    2\n        nofollow    a_link    1\nb_link  javascript  b_link    1\ndtype: int64\n\n\nFollowing would make sense for DataFrameGroupby to:\nIn [136]: pd.Series([len(g.groups[i]) for i in g.grouper.result_index], g.grouper.result_index)\nOut[136]: \nlink    type      \na_link  dofollow      2\n        nofollow      1\nb_link  javascript    1\ndtype: int64\n```\n\nNote: as_index doesn't make sense here so would be ignored.\n"},{"labels":["api",null],"text":"This seems a little inconsistent to me. IMHO, it would be good to be able to rely on the the fact that if you specify a numpy array of _any_ datetime64 type the result will still be a datetime64 type and not cast to a different type dependant on the exact dtype passed in.\n\n``` python\nIn [1]: dates = pd.date_range('01-Jan-2015', '01-Dec-2015', freq='M')\n   ...: values1 = dates.view(np.ndarray).astype('M8[D]')\n   ...: values2 = dates.view(np.ndarray).astype('datetime64[ns]')\n   ...: series1 = pd.TimeSeries(values1, dates)\n   ...: series2 = pd.TimeSeries(values2, dates)\n   ...: \n\nIn [2]: series1\nOut[2]: \n2015-01-31    2015-01-31\n2015-02-28    2015-02-28\n2015-03-31    2015-03-31\n2015-04-30    2015-04-30\n2015-05-31    2015-05-31\n2015-06-30    2015-06-30\n2015-07-31    2015-07-31\n2015-08-31    2015-08-31\n2015-09-30    2015-09-30\n2015-10-31    2015-10-31\n2015-11-30    2015-11-30\nFreq: M, dtype: object\n\nIn [3]: series2\nOut[3]: \n2015-01-31   2015-01-31\n2015-02-28   2015-02-28\n2015-03-31   2015-03-31\n2015-04-30   2015-04-30\n2015-05-31   2015-05-31\n2015-06-30   2015-06-30\n2015-07-31   2015-07-31\n2015-08-31   2015-08-31\n2015-09-30   2015-09-30\n2015-10-31   2015-10-31\n2015-11-30   2015-11-30\nFreq: M, dtype: datetime64[ns]\n\nIn [4]: series1.values\nOut[4]: array([datetime.date(2015, 1, 31), datetime.date(2015, 2, 28), datetime.date(2015, 3, 31), datetime.date(2015, 4, 30), datetime.date(2015, 5, 31), datetime.date(2015, 6, 30), datetime.date(2015, 7, 31), datetime.date(2015, 8, 31), datetime.date(2015, 9, 30), datetime.date(2015, 10, 31), datetime.date(2015, 11, 30)], dtype=object)\n\nIn [5]: series2.values\nOut[5]: array(['2015-01-31T00:00:00.000000000+0000', '2015-02-28T00:00:00.000000000+0000', '2015-03-31T01:00:00.000000000+0100', '2015-04-30T01:00:00.000000000+0100', '2015-05-31T01:00:00.000000000+0100', '2015-06-30T01:00:00.000000000+0100', '2015-07-31T01:00:00.000000000+0100', '2015-08-31T01:00:00.000000000+0100', '2015-09-30T01:00:00.000000000+0100', '2015-10-31T00:00:00.000000000+0000', '2015-11-30T00:00:00.000000000+0000'], dtype='datetime64[ns]')\n\nIn [6]: pd.__version__\nOut[6]: '0.13.1-339-g6c3755b'\n```\n"},{"labels":["api",null,null],"text":"Able to do something like this would be nice\n\n```\ndf.query('[col with space] < col')\n```\n\nI came across many external data files which have spaces in the column names. It would be nice to be able to do quick analysis on the data without first renaming the columns.\n"},{"labels":["api",null],"text":"I would expect these two return the same:\n\n```\nIn [10]: pd.Timestamp.now()\n\nOut[10]:\nTimestamp('2014-02-24 17:54:18.370128', tz=None)\n\nIn [11]: pd.Timestamp(\"now\")\n\nOut[11]:\nTimestamp('2014-02-24 22:54:18', tz=None)\n```\n"},{"labels":["api",null,null],"text":"after #6380, ability to pretty easily move ops over to the mix-in\n- [x] `date/time` (added in #6380)\n- [ ] more periods ops? #5202 \n- [x] additional field accessor (`is_beg_year`), #4823\n- [x] validate `argmin/max`, (PR #7904)\n- [x] StringMethods, e.g. `str` for `Index` (not for sub-classes though), #9068 \n\n(this turns out to be quite tricky, as sometimes construction depends on the existence of a `tz` attribute)\n- [ ] `freq,frestr,inferred_freq`\n- [x] `is_monotonic` #13336 \n- [x] `pd.infer_freq()`, related #6407,#6408\n- [ ] `is_unique,lexsort_depth,nlevels`\n- [x] `nunique()`,`unique()`,`value_counts()`, #6734\n- [x] `factorize()` #7090\n- [ ] `get_loc/find` #4616\n- [ ] `__invert__` #8875\n\nThese are related in that they are generic.py functions as they have to compute something then set the axis with the result.\n- [ ] `normalize` #5502\n- [ ] `tz_convert/tz_localize` (though this requires a bit of wrapping as they have an axis parm,\n  but should be possible), see also here: https://github.com/pydata/pandas/pull/7915 (currently implemented only on `DatetimeIndex`, `PeriodIndex` has these as not implemented (and delegating from `NDFrame` is in `core/generic`)\n- [x] `duplicated/drop_duplicates` #4060, (PR #7979)\n- [ ] `has_duplicates` #9077 \n"},{"labels":["api",null],"text":"Starting a new issue to discuss this. See merged PR #5950 and follow-up issue #6292.\n\nSummary of situation:\n\n**Current (0.13)**:\n- top-level `read_sql`, `read_frame`, `write_frame` and `DataFrame.to_sql`\n- in sql module: `uquery`, `tquery`, `has_table` (+ `read_sql`)\n\n**New after #5950**\n- top-level `read_sql` and `DataFrame.to_sql` (but not yet used in the docs)\n- in sql module: `read_sql`, `read_table`, `to_sql`, `has_table`, `execute`\n- `read_frame`, `write_frame`, `uquery`, `tquery` are deprecated\n\n**Points to dicsuss**\n- docs: use top-level functions `read_sql` and `DataFrame.to_sql` instead of functions from `sql`\n- `sql.read_table` vs existing `pd.read_table`. Is this a problem?\n- merge functionality of `read_table` and `read_sql` or not (see this comment of @mangecoeur to not do that: https://github.com/pydata/pandas/pull/5950#issuecomment-33174797)\n"},{"labels":["api",null,null,null,null],"text":"rolling_max on a TimeSeries with freq='D' appears to actually compute the rolling mean, and not the rolling max.\n\n``` python\nIn [118]: import pandas\n\nIn [119]: indices = [datetime.datetime(1975, 1, i, 12, 0) for i in range(1, 6)]\n\nIn [120]: indices.append(datetime.datetime(1975, 1, 3, 6, 0))  # So that we can have 2 datapoints on one of the days\n\nIn [121]: series = pandas.Series(range(1, 7), index=indices)\n\nIn [122]: series = series.map(lambda x: float(x))  # Use floats instead of ints as values\n\nIn [123]: series = series.sort_index()  # Sort chronologically\n\nIn [124]: expected_result = pandas.Series([1.0, 2.0, 6.0, 4.0, 5.0], index=[datetime.datetime(1975, 1, i, 12, 0) for i in range(1, 6)])\n\nIn [125]: actual_result = pandas.rolling_max(series, window=1, freq='D')\n\nIn [126]: assert((actual_result==expected_result).all())\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n<ipython-input-126-cc436c4798a7> in <module>()\n----> 1 assert((actual_result==expected_result).all())\n\nAssertionError: \n\nIn [127]: expected_result\nOut[127]: \n1975-01-01 12:00:00    1\n1975-01-02 12:00:00    2\n1975-01-03 12:00:00    6\n1975-01-04 12:00:00    4\n1975-01-05 12:00:00    5\ndtype: float64\n\nIn [128]: actual_result\nOut[128]: \n1975-01-01    1.0\n1975-01-02    2.0\n1975-01-03    4.5\n1975-01-04    4.0\n1975-01-05    5.0\nFreq: D, dtype: float64\n```\n\nWith a window of size 2 days, it still looks like the rolling mean:\n\n``` python\nIn [130]: pandas.rolling_max(series, window=2, freq='D')\nOut[130]: \n1975-01-01    NaN\n1975-01-02    2.0\n1975-01-03    4.5\n1975-01-04    4.5\n1975-01-05    5.0\nFreq: D, dtype: float64\n```\n"},{"labels":["api",null],"text":"IMO this should work\n\n```\nx = pd.DataFrame(np.random.rand(5,3))\nx.iloc[:6]\n```\n\nConsider \n\n```\nnp.arange(5)[:7]\n```\n\nAnd \n\n```\nrange(5)[:7]\n```\n\nI'm on\n\n```\n[~/]\n[14]: pd.version.version\n[14]: '0.13.1-63-gcc6ee40'\n```\n"},{"labels":["api",null,null],"text":"Hello, I recently stumbled across this:\n\n``` python\ndf1 = pd.DataFrame({'a':[1.,2.,3.,4.,nan,6,7,8]}, dtype='<f4')\ndf1.a.interpolate(inplace=True, downcast=None)\n\n\n---------------------------------------------------------------------------\nUnboundLocalError                         Traceback (most recent call last)\n<ipython-input-9-bf0994f72664> in <module>()\n----> 1 df1.a.interpolate(inplace=True, downcast=None)\n\nC:\\Program Files\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in interpolate(self, method, axis, limit, inplace, downcast, **kwargs)\n   2304         if axis == 1:\n   2305             res = res.T\n-> 2306         return res\n   2307 \n   2308     #----------------------------------------------------------------------\n\nUnboundLocalError: local variable 'res' referenced before assignment\n```\n\nThe problem arises with setting inplace to True regardless of 'downcast'. \n\nBut speaking of which: running interpolate() without any options, I would excpect to keep data types by default! I often chunk through datasets and glue them together with to_hdf. It took me a while to figure out that a float column in one chunk had just zeros in it so interpolate downcasted it to int => to_hdf raised on appending.\n\nThx in advance\n"},{"labels":["api",null],"text":"after #6134 we can drop xs and implement directly. Thus we should deprecate it.\n\n`.xs(key, level=n)` \n\nis roughly equivalent to this\n\n```\nindexer = tuple([slice(None)]*(n-1) + key)]\naxis_indexer = [ slice(None) ] * self.ndim\naxis_indexer[axis] = indexer\nself.loc[tuple(axis_indexer)]\n```\n\nroughly because needs to handle `.xs` full argument set\n- drop_level (need to surround the indexer tuple with a list or not)\n- n could be a level name \n- n = 1 (needs a test)\n"},{"labels":["api",null,null,null],"text":"``` python\nAttributeError: 'Float64Index' object has no attribute 'dropna'\n```\n\nAny reason why it could not have a dropna()?\n"},{"labels":["api",null,null,null],"text":"Shouldn't this work?\n\n```\n>>> pd.version.version\n'0.13.0-472-g9109812'\n\n>>> df = pd.DataFrame(np.random.random((10,4)))\n\n>>> df = df.drop(df[0] > .5)\n\n>>> df.drop(df[0] < .2)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-23-5df7f56de272> in <module>()\n----> 1 df.drop(df[0] > .5)\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0_472_g9109812-py2.7-linux-x86_64.egg/pandas/core/generic.pyc in drop(self, labels, axis, level, inplace, **kwargs)\n   1397                 new_axis = axis.drop(labels, level=level)\n   1398             else:\n-> 1399                 new_axis = axis.drop(labels)\n   1400             dropped = self.reindex(**{axis_name: new_axis})\n   1401             try:\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0_472_g9109812-py2.7-linux-x86_64.egg/pandas/core/index.pyc in drop(self, labels)\n   1621         mask = indexer == -1\n   1622         if mask.any():\n-> 1623             raise ValueError('labels %s not contained in axis' % labels[mask])\n   1624         return self.delete(indexer)\n   1625 \n\nValueError: labels [False False  True  True  True  True  True False] not contained in axis\n```\n"},{"labels":["api",null,null],"text":"Previously (version 0.11) it was possible to generate a boolean Series, and use that to index a numpy array. Version 0.13.0 breaks this behaviour and raises \"IndexError: unsupported iterator index\"\n\n``` python\n    import numpy as np\n    import pandas as pd    \n\n    rng = np.arange(5)\n\n    rng[rng > 2]                       # works as expected\n    >>> array([3, 4])\n\n    b = pd.Series(rng > 2)\n    rng[b]                               # doesn't work anymore\n    >>> IndexError: unsupported iterator index\n```\n"},{"labels":["api",null,null],"text":"I would expect to be able to perform an `ewma` over each minor axis within each major axis:\n\n```\nIn [52]:\n\nwp = pd.Panel(np.random.randn(2, 4, 5), items=['Item1', 'Item2'],\n               minor_axis=pd.date_range('1/1/2000', periods=5),\n               major_axis=['A', 'B', 'C', 'D'])\nIn [57]:\n\nwp\nOut[57]:\n<class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 4 (major_axis) x 5 (minor_axis)\nItems axis: Item1 to Item2\nMajor_axis axis: A to D\nMinor_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00\nIn [56]:\n\npd.ewma(wp, com=2)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-56-bb436def494f> in <module>()\n----> 1 pd.ewma(wp, com=2)\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0_285_gfcfaa7d-py2.7-linux-x86_64.egg/pandas/stats/moments.pyc in ewma(arg, com, span, halflife, min_periods, freq, time_rule, adjust)\n    373         return result\n    374 \n--> 375     return_hook, values = _process_data_structure(arg)\n    376     output = np.apply_along_axis(_ewma, 0, values)\n    377     return return_hook(output)\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0_285_gfcfaa7d-py2.7-linux-x86_64.egg/pandas/stats/moments.pyc in _process_data_structure(arg, kill_inf)\n    329         values = arg\n    330 \n--> 331     if not issubclass(values.dtype.type, float):\n    332         values = values.astype(float)\n    333 \n\n/usr/local/lib/python2.7/dist-packages/pandas-0.13.0_285_gfcfaa7d-py2.7-linux-x86_64.egg/pandas/core/generic.pyc in __getattr__(self, name)\n   1802                 return self[name]\n   1803             raise AttributeError(\"'%s' object has no attribute '%s'\" %\n-> 1804                                  (type(self).__name__, name))\n   1805 \n   1806     def __setattr__(self, name, value):\n\nAttributeError: 'Panel' object has no attribute 'dtype'\n```\n"},{"labels":["api",null],"text":"When assigning to a column in a DataFrame, If I assign an existing column, the assignment works:\n\n```\nIn [36]: df = pd.DataFrame({'a':[1,2,12], 'b':['a','b','a']})\nIn [37]: df.b = df.a\nIn [38]: print df\n    a   b\n0   1   1\n1   2   2\n2  12  12\n\n[3 rows x 2 columns]\n```\n\nbut assigning to a new column fails silently:\n\n```\nIn [40]:df = pd.DataFrame({'a':[1,2,12], 'b':['a','b','a']})\nIn [41]:df.c = df.a\nIn [42]:print df\n    a  b\n0   1  a\n1   2  b\n2  12  a\n\n[3 rows x 2 columns]\n```\n"},{"labels":["api",null,null],"text":"see also: http://stackoverflow.com/questions/21415432/pandas-v0-13-0-setting-dataframe-values-of-type-datetime64ns\n\n~~I believe this changed from 0.12.~~ was the same in 0.12\n\nBoils down to if the row indexer is a null-slice (IOW all rows are selected), should\ndtype conversion be done or not (as it current)\n\n```\nIn [45]: df = pd.DataFrame({'date':pd.date_range('2000-01-01','2000-01-5'),'val' : np.arange(5)})\n\nIn [46]: df\nOut[46]: \n        date  val\n0 2000-01-01    0\n1 2000-01-02    1\n2 2000-01-03    2\n3 2000-01-04    3\n4 2000-01-05    4\n\n[5 rows x 2 columns]\n\nIn [47]: df['date'] = 0\n\nIn [48]: df\nOut[48]: \n   date  val\n0     0    0\n1     0    1\n2     0    2\n3     0    3\n4     0    4\n\n[5 rows x 2 columns]\n\nIn [49]: df = pd.DataFrame({'date':pd.date_range('2000-01-01','2000-01-5'),'val' : np.arange(5)})\n\nIn [53]: df.loc[:,'date'] = np.array([0])\n\nIn [54]: df\nOut[54]: \n        date  val\n0 1970-01-01    0\n1 1970-01-01    1\n2 1970-01-01    2\n3 1970-01-01    3\n4 1970-01-01    4\n\n[5 rows x 2 columns]\n```\n"},{"labels":["api",null],"text":"The `unique` and `nunique` attributes are very useful in conjunction with series groupby operations.  I used these extensively in previous versions of Pandas whenever I needed to get a list of unique values for each subgroup (or the number of unique values).  This can be used, for example, to count the number of subjects in each treatment group (or get a list of the subject IDs for reporting):\n\n```\ndata = pandas.DataFrame({\n    'subject_id': ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'),\n    'treatment': (0, 0, 0, 0, 0, 1, 1, 1, 0, 0),\n})\nprint data.groupby('treatment').subject_id.apply(lambda x: x.nunique())\nprint data.groupby('treatment').subject_id.apply(lambda x: x.unique())\n```\n\nWe'd get the following output:\n\n```\ntreatment\n0            7\n1            3\ndtype: int64\n\ntreatment\n0            [A, B, C, D, E, I, J]\n1                        [F, G, H]\ndtype: object\n```\n\nThis is super-useful for generating summary statistics (e.g. N's) and debugging (e.g. tracking down which subjects are in which groups.  In previous versions of Pandas, we could simply do:\n\n```\nprint data.groupby('treatment').subject_id.nunique()\nprint data.groupby('treatment').subject_id.unique()\n```\n\nIt would be nice to continue this.  Is there a reason why `nunique` and `unique` can't be added to the whitelist?\n"},{"labels":["api",null],"text":"For read_table, Would it be possible to make \"header=None\" and \"header=False\" have the same behavior? Interpreting \"False\" as saying that the first line is the header has tricked me a couple times...\n"},{"labels":["api"],"text":"https://github.com/pydata/pandas/issues/6087\n\nI have to investigate more fully, but this warning\n\n`/home/joris/scipy/pandas/pandas/core/frame.py:2805: FutureWarning: TimeSeries\nbroadcasting along DataFrame index by default is deprecated. Please use\nDataFrame.<op> to explicitly broadcast arithmetic operations along the index\nFutureWarning)`\n\ncan be removed for cases where the index are equal (e.g. no broadcasting)\n"},{"labels":["api",null],"text":"related #5884\n- [ ] OLS https://github.com/statsmodels/statsmodels/blob/master/statsmodels/regression/linear_model.py\n- [ ] plm (regression for panels)\n- [ ] fama_macbeth\n- [ ] VAR https://github.com/statsmodels/statsmodels/tree/master/statsmodels/tsa/vector_ar\n"},{"labels":["api",null,null],"text":"The original `DatetimeIndex` has the `tz`:\n\n```\nIn [1]: pd.DatetimeIndex(\n   ...:     pd.tseries.tools.to_datetime(['2013-1-1 13:00'], errors=\"raise\")).tz_localize('US/Pacific')[0]\nOut[1]: Timestamp('2013-01-01 13:00:00-0800', tz='US/Pacific')\n```\n\nBut when converted to a `Series`, the `tz` info is lost:\n\n```\nIn [2]: pd.Series(pd.DatetimeIndex(\n   ...:     pd.tseries.tools.to_datetime(['2013-1-1 13:00'], errors=\"raise\")).tz_localize('US/Pacific'))[0]\nOut[2]: Timestamp('2013-01-01 21:00:00', tz=None)\n```\n\nIf this is by design, then a warning would be nice. \n\nThis issue came up when I tried to add a list of timezone aware dates to a DataFrame.\n"},{"labels":["api",null],"text":"Currently, with `query` and `eval` you can use local variables a la the `@` symbol. It's a bit confusing since you're not allowed to have a local variable and a column name with the same name, but it will try to pull the local if possible.\n\nCurrent API:\n\nFails with a `NameError`:\n\n``` python\na = 1\ndf  = DataFrame({'a': randn(10), 'b': randn(10)})\ndf.query('a > b')\n```\n\nBut this works:\n\n``` python\ndf.query('@a > b')\n```\n\nAnd so does this, which is confusing:\n\n``` python\na = 1\ndf = DataFrame({'b': randn(10), 'c': randn(10)})\ndf.query('a < b < c')\n```\n\nAs suggested by @y-p and @jreback, the following API is less confusing IMO. \n\nFrom now on, all local variables will need an explicit reference and if there is a column name and a local with the same name then the column will be used. Thus you can always be sure that you're referring to a column, or it doesn't exist, in which case you'll get an error. And if you use `@` then you can be sure that you're referring to local, and likewise get an error if it doesn't exist. As a bonus ( :wolf: in :sheep: 's clothing), this allows you to use both a local _and_ a column name with the same name.\n\nExamples:\n\n``` python\na = 1\ndf = DataFrame({'a': randn(10), 'b': randn(10)})\n\n# uses the column 'a'\ndf.query('a > b')\n\n# uses the local\ndf.query('@a > b')\n\n# fails because I didn't reference the local and there's no 'c' column\nc = 1\ndf.query('a > c')\n\n# local and a column name\ndf.query('b < @a < a')\n```\n"},{"labels":["api",null],"text":"From the example at http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.query.html:\n\n``` python\nfrom numpy.random import randn\nfrom pandas import DataFrame\ndf = DataFrame(randn(10, 2), columns=list('ab'))\ndf.query('a > b')\n```\n\ngives me:\n\n```\n---------------------------------------------------------------------------\nNameResolutionError                       Traceback (most recent call last)\n<ipython-input-19-47040e53b0e7> in <module>()\n      2 from pandas import DataFrame\n      3 df = DataFrame(randn(10, 2), columns=list('ab'))\n----> 4 df.query('a > b')\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc in query(self, expr, **kwargs)\n   1778                              \"query expression\")\n   1779 \n-> 1780         res = self.eval(expr, **kwargs)\n   1781 \n   1782         try:\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc in eval(self, expr, **kwargs)\n   1829         kwargs['local_dict'] = _ensure_scope(resolvers=resolvers, **kwargs)\n   1830         kwargs['target'] = self\n-> 1831         return _eval(expr, **kwargs)\n   1832 \n   1833     def _slice(self, slobj, axis=0, raise_on_error=False, typ=None):\n\n/usr/local/lib/python2.7/dist-packages/pandas/computation/eval.pyc in eval(expr, parser, engine, truediv, local_dict, global_dict, resolvers, level, target)\n    206     eng = _engines[engine]\n    207     eng_inst = eng(parsed_expr)\n--> 208     ret = eng_inst.evaluate()\n    209 \n    210     # assign if needed\n\n/usr/local/lib/python2.7/dist-packages/pandas/computation/engines.pyc in evaluate(self)\n     48 \n     49         # make sure no names in resolvers and locals/globals clash\n---> 50         self.pre_evaluate()\n     51         res = self._evaluate()\n     52         return _reconstruct_object(self.result_type, res, self.aligned_axes,\n\n/usr/local/lib/python2.7/dist-packages/pandas/computation/engines.pyc in pre_evaluate(self)\n     31 \n     32     def pre_evaluate(self):\n---> 33         self.expr.check_name_clashes()\n     34 \n     35     def evaluate(self):\n\n/usr/local/lib/python2.7/dist-packages/pandas/computation/expr.pyc in check_name_clashes(self)\n    797         lcl_keys = frozenset(env.locals.keys()) & names\n    798         gbl_keys = frozenset(env.globals.keys()) & names\n--> 799         _check_disjoint_resolver_names(res_keys, lcl_keys, gbl_keys)\n    800 \n    801     def add_resolvers_to_locals(self):\n\n/usr/local/lib/python2.7/dist-packages/pandas/computation/expr.pyc in _check_disjoint_resolver_names(resolver_keys, local_keys, global_keys)\n     39     if res_locals:\n     40         msg = \"resolvers and locals overlap on names {0}\".format(res_locals)\n---> 41         raise NameResolutionError(msg)\n     42 \n     43     res_globals = list(com.intersection(resolver_keys, global_keys))\n\nNameResolutionError: resolvers and locals overlap on names ['a']\n\n```\n\nThis is with `0.13`.\n"},{"labels":["api",null,null],"text":"The value \"-nan\" (yes, this is a valid type of NaN!) in a CSV input file causes that column to be treated as 'object' instead of float64.\n\n```\npd.read_csv(StringIO.StringIO('a,b\\n1,2.0\\n2,nan\\n3,-nan')).b\nOut[15]: \n0     2.0\n1     NaN\n2    -nan\nName: b, dtype: object\n\npd.read_csv(StringIO.StringIO('a,b\\n1,2.0\\n2,nan\\n')).b\nOut[16]: \n0     2\n1   NaN\nName: b, dtype: float64\n```\n\nWhen the file is sufficiently large, the following error is generated:\n\n```\nIn [3]: pd.read_csv('big.bad.csv')\n/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py:1033: DtypeWarning: Columns (58,64) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = self._reader.read(nrows)\n```\n\nIf the string \"-nan\" is replaced with \"nan\" all is well.  I don't really need to distinguish negative NaN from NaN but would like to be able to read my data files w/o having to pre-process them to scrub all the '-nan's.\n"},{"labels":["api"],"text":"related #5661\nmaybe `io.pickle.compat = 'global/local``\n\nwhere we only allow local changes to the UnPickler.\n\nThis is not destructive, just 'bad practive', but weighing against more usability\n"},{"labels":["api",null,null],"text":"```\n/home/user1/src/pandas/pandas/io/pytables.py:658: FutureWarning: unique(key,column) is deprecated\nuse select_column(key,column).unique() instead\n  FutureWarning)\n```\n\n@jreback, any issues with making the change? \n"},{"labels":["api",null,null],"text":"Unless I'm missing something, there's no DataFrame method analogous to set_index() for setting column values. One can directly manipulate the .columns attribute of the DF, but it's often convenient to be able to alter columns in-line after some other operation--e.g.,\n\n`data = pd.concat([df_a, df_b], axis=1).set_columns(['a', 'b', 'c'])`\n\nEven if there's already some equivalent longer way to do this, it might be nice to add set_columns for convenience; I've now caught myself trying to call set_columns() on a DF several times.\n"},{"labels":["api",null],"text":"Would it be costly to issue a warning when one adds a column to a DataFrame that has the same name as a built-in DataFrame method?\nAs you know, in that case I can not do Dataframe.name to access the column but instead I get a method pointer. \nBut if I'm unaware (or it's late and I forgot.. ;) ) that a method with that name exists, I could be confused for quite a while before I find out what's going on.\n\nExample I just had:\n\ns = Series(somedata)\nInvestigating the difference with\ns.diff()\nLooks interesting, so for easier management I want to put things in a dataframe now.\ndf = DataFrame(s)\ndf['diff'] = s.diff()\n\nAnd then I tried to filter the df for some values in diff and got this:\n\ndf[df.diff > 10]\nKeyError: u'no item named True'\n\nThat somehow didn't make me at all think about a name clash. In this case I eventually guessed what was wrong, but if I wouldn't know that a fancy method with a fancy name exists and I picked it as well, I guess I never would find out, what's going on.\n\nSo, if it isn't too much costs in terms of performance a name check against core methods would be really nice, I think.\n"},{"labels":["api",null,null],"text":"This would allow chaining operations like:\n\n``` python\npd.read_csv('imdb.txt')\n  .sort(columns='year')\n  .filter(lambda x: x['year']>1990)   # <---this is missing in Pandas\n  .to_csv('filtered.csv')\n```\n\nFor current alternatives see:\n\nhttp://stackoverflow.com/questions/11869910/pandas-filter-rows-of-dataframe-with-operator-chaining\n"},{"labels":["api",null,null],"text":"related #4130\n\nI haven't looked at the code at how easy this is to do, but it looks like things like expanding_apply (moving_apply too I suspect) expects a float returned from the function. It might make sense to have an optional no_return flag (or just be smart about it, though perhaps it makes sense to have a flag as a sanity check for the user.) \n\nUse case, I'm using expanding_apply to run some code in a subprocess and write to disk. I'm able to get around the expectation by having my function return 0.0 or something, but it's a bit of a hack.\n"},{"labels":["api",null,null,null],"text":"For some reason, with 0.13, an empty object Series is interpreted as a boolean one, when using to fetch elements in another Series:\n\n```\npandas.Series(['A', 'B'])[pandas.Series([], dtype=object)]\n```\n\nThe following works though, so the dtype confusion seems to be confined to empty Series:\n\n```\npandas.Series(['A', 'B'])[pandas.Series(['C'], dtype=object)]\n```\n"},{"labels":["api",null,null],"text":"I'm really salivating at the chance to use the `._metadata` and `__finalize__` mechanisms in pandas-0.13-dev to create my own subclass of DataFrame and to have metadata propagate after calling functions inherited from DataFrame like `dropna()`, `resample()` etc.\n\nUsing the latest 0.13-dev version of Pandas, I think I might have bumped into a small bug (although I'm not sure if this is a bug or not??):\n\n`._metadata` propagates after calling `.resample(rule='D')` and `dropna()` but not after calling `.resample(rule='D', how='max')`.\n\nMore details, including the full code of my subclass, are given under the \"experiments\" heading of this issue: https://github.com/nilmtk/nilmtk/issues/83\n"},{"labels":["api"],"text":"See reverted PR in #5853 and discussion on synergy with changes to pandas\nre SetttingWithCopy.\n"},{"labels":["api",null],"text":""},{"labels":["api",null,null],"text":"```\npd.version.version\n'0.12.0-1149-g141e93a'\n```\n\nMaybe already fixed, but I'm afraid to upgrade in the middle of working on a project given past experiences.\n\nAs I recall there were several ways proposed to join a series to a DataFrame with join being the preferred way. \n\n```\ndf = pd.DataFrame(np.random.random((30,2)), columns=[\"A\",\"B\"])\nseries = pd.Series(np.random.random(30), name=\"C\")\n```\n\nI know I can do \n\n```\ndf[\"C\"] = series\n```\n\nor \n\n```\ndf = df.join(series)\n```\n\nBut it'd be nice if the following also worked given that I'd like to write general code.\n\n```\npd.concat((df, series), axis=1) # ok error message at least\ndf.merge(series) # just call join under the hood? bad error message\n```\n\nThis is \"unexpected\" that it somehow tries to append the rows. Looks like a bug to me. Should it take an axis keyword?\n\n```\ndf.append(series)\n```\n"},{"labels":["api",null],"text":"Inspired by this [SO question](http://stackoverflow.com/questions/20875140/apply-function-to-sets-of-columns-in-pandas-looping-over-entire-data-frame-co), I was wondering if the possibility of being able to give an axis keyword to `GroupBy.apply` would be interesting.\n\nAn example. Considering this dataframe:\n\n```\nIn [1]: idx = pd.MultiIndex.from_arrays([list('AAABBB'), list('xyzxyz')])\nIn [2]: df = pd.DataFrame(np.random.randn(5,6), columns=idx)\nIn [3]: df\nOut[3]:\n          A                             B\n          x         y         z         x         y         z\n0  1.085834 -1.247589  0.174311  1.415633  0.788986  0.443246\n1  1.036490  0.223306 -0.920102  0.068396  1.577690 -0.757387\n2 -0.587843 -1.223218 -0.153409  1.863440  2.169254  0.282485\n3  1.485550  0.494427 -0.257822  1.360076 -0.049386 -0.264166\n4  0.232604 -0.972644  0.877186 -0.875215  1.198826 -0.370832\n```\n\nand then groupby the first level of the columns (`A` and `B`): \n\n```\nIn [4]: g = df.groupby(axis=1, level=0)\n```\n\nOn the individual groups of `g`, you could do a following apply (to calculate the length of the vector (x,y,z)):\n\n```\nIn [5]: df['A'].apply(lambda x: np.sqrt(x**2).sum(), axis=1)\nOut[5]:\n0    2.507734\n1    2.179897\n2    1.964471\n3    2.237799\n4    2.082434\ndtype: float64\n```\n\nHowever, to do this on all groups, you need a second apply inside the apply to provide the axis keyword:\n\n```\nIn [6]: g.apply(lambda g: g.apply(lambda x: np.sqrt(x**2).sum(), axis=1))\nOut[6]:\n          A         B\n0  2.507734  2.647866\n1  2.179897  2.403474\n2  1.964471  4.315179\n3  2.237799  1.673628\n4  2.082434  2.444873\n```\n\nWould it be a interesting enhancement that you could directly provide this keyword like this?\n\n```\ng.apply(lambda x: np.sqrt(x**2).sum(), axis=1)\n```\n\nOr Is there another way I am missing to simply do this?\n"},{"labels":["api",null],"text":"Hi, I'm testing out some code on the 0.13 release candidate and I've run into problems with a fairly common (for me) pattern. It's no longer possible to use a boolean Series to index a numpy array. E.g.:\n\n```\nimport numpy as np\nimport pandas as pd\nx = np.random.randn(30)\nmask = pd.Series(np.random.rand(30) > .5)\nx[mask].mean()\n```\n\nRaises:\n\n```\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-27-73e40e756b4e> in <module>()\n      3 x = np.random.randn(30)\n      4 mask = pd.Series(np.random.rand(30) > .5)\n----> 5 x[mask].mean()\n\nIndexError: unsupported iterator index\n```\n\nPerhaps this is not a good idiom, but this change breaks quite a bit of existing code.\n"},{"labels":["api"],"text":"If I understand this right, the syntax is:\n\n``` python\ndf.sort_index(..., inplace=False)\n```\n\nbut is \n\n``` python\ndf.reindex_axis(..., copy=True)\n```\n\nAny reason why we shouldn't just use the same keyword (either `copy` or `inplace`) in all functions to determine whether to modify the original object or make a new one?\n"},{"labels":["api",null,null],"text":"I am using Pandas 0.13.0rc1-23-g286811a, Python 2.7 and Windows XP.\n\nThe space in column names is apparently causing a problem. On IPython,\n\n```\ndf = pd.DataFrame(columns=['a', 'b', 'c c'])\ndf['d'] = 3\ndf['c c']\n```\n\nError\n\n```\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\frame.pyc in __getitem__(self, key)\n   1626             return self._getitem_multilevel(key)\n   1627         else:\n-> 1628             return self._getitem_column(key)\n   1629\n   1630     def _getitem_column(self, key):\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\frame.pyc in _getitem_column(self, key)\n   1633         # get column\n   1634         if self.columns.is_unique:\n-> 1635             return self._get_item_cache(key)\n   1636\n   1637         # duplicate columns & possible reduce dimensionaility\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\generic.pyc in _get_item_cache(self, item)\n    977         if res is None:\n    978             values = self._data.get(item)\n--> 979             res = self._box_item_values(item, values)\n    980             cache[item] = res\n    981             res._cacher = (item, weakref.ref(self))\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\frame.pyc in _box_item_values(self, key, values)\n   1834             return self._constructor(values.T, columns=items, index=self.index)\n   1835         else:\n-> 1836             return self._box_col_values(values, items)\n   1837\n   1838     def _box_col_values(self, values, items):\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\frame.pyc in _box_col_values(self, values, items)\n   1839         \"\"\" provide boxed values for a column \"\"\"\n   1840         return self._constructor_sliced.from_array(values, index=self.index,\n-> 1841                                                    name=items, fastpath=True)\n   1842\n   1843     def __setitem__(self, key, value):\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\series.pyc in from_array(cls, arr, index, name, copy, fastpath)\n    233             cls = SparseSeries\n    234\n--> 235         return cls(arr, index=index, name=name, copy=copy, fastpath=fastpath)\n    236\n    237     @property\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\series.pyc in __init__(self, data, index, dtype, name, copy, fastpath\n)\n    130             # data is an ndarray, index is defined\n    131             if not isinstance(data, SingleBlockManager):\n--> 132                 data = SingleBlockManager(data, index, fastpath=True)\n    133             if copy:\n    134                 data = data.copy()\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, block, axis, do_integrity_check, fast\npath)\n   3413                 block = block[0]\n   3414             if not isinstance(block, Block):\n-> 3415                 block = make_block(block, axis, axis, ndim=1, fastpath=True)\n   3416\n   3417         else:\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\internals.pyc in make_block(values, items, ref_items, klass, ndim, dt\nype, fastpath, placement)\n   1893\n   1894     return klass(values, items, ref_items, ndim=ndim, fastpath=fastpath,\n-> 1895                  placement=placement)\n   1896\n   1897\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, values, items, ref_items, ndim, fastp\nath, placement)\n   1297         super(ObjectBlock, self).__init__(values, items, ref_items, ndim=ndim,\n   1298                                           fastpath=fastpath,\n-> 1299                                           placement=placement)\n   1300\n   1301     @property\n\nC:\\python_envs\\prod2\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, values, items, ref_items, ndim, fastp\nath, placement)\n     63         if len(items) != len(values):\n     64             raise ValueError('Wrong number of items passed %d, indices imply '\n---> 65                              '%d' % (len(items), len(values)))\n     66\n     67         self.set_ref_locs(placement)\n\nValueError: Wrong number of items passed 1, indices imply 0\n```\n"},{"labels":["api",null,null,null],"text":"I am on `0.13.0rc1-64-gceec8bf` (master). I get the `IndexError` when trying to print large dataframe with MultiIndex full of NaNs. Following script raises the error\n\n``` python\nimport pandas as pd\nimport numpy as np\n\nn = 3500000\narrays = [range(n), np.empty(n)]\nindex = pd.MultiIndex.from_tuples(zip(*arrays))\ns = pd.Series(np.zeros(n), index=index)\n\nprint(s)\n```\n\nAnd here is a Traceback\n\n```\nTraceback (most recent call last):\n  File \"/home/mvinkler/Documents/analysis/models/test.py\", line 12, in <module>\n    print(s)\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/base.py\", line 35, in __str__\n    return self.__bytes__()\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/base.py\", line 47, in __bytes__\n    return self.__unicode__().encode(encoding, 'replace')\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/series.py\", line 857, in __unicode__\n    result = self._tidy_repr(min(30, max_rows - 4))\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/series.py\", line 876, in _tidy_repr\n    head = self.iloc[:num]._get_repr(print_header=True, length=False,\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/generic.py\", line 946, in _indexer\n    setattr(self, iname, indexer(self, name))\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/generic.py\", line 1607, in __setattr__\n    elif name in self._info_axis:\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/index.py\", line 2498, in __contains__\n    self.get_loc(key)\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/index.py\", line 2986, in get_loc\n    return self._get_level_indexer(key, level=0)\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/index.py\", line 3123, in _get_level_indexer\n    loc = level_index.get_loc(key)\n  File \"/home/mvinkler/Downloads/pandas/pandas/core/index.py\", line 1017, in get_loc\n    return self._engine.get_loc(_values_from_object(key))\n  File \"index.pyx\", line 129, in pandas.index.IndexEngine.get_loc (pandas/index.c:3560)\n  File \"index.pyx\", line 138, in pandas.index.IndexEngine.get_loc (pandas/index.c:3314)\n  File \"util.pxd\", line 40, in util.get_value_at (pandas/index.c:13101)\nIndexError: index out of bounds\n```\n"},{"labels":["api",null,null],"text":"Noticed the following difference in behavior between 0.12 and 0.13rc1 when adding a column to an empty dataframe.  Obviously, a weird case, and can be worked around easily.\n\n```\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [1.2, 4.2, 5.2]})\ny = df[df.A > 5]\ny['New'] = np.nan\nprint y\nprint y.values\n```\n\n&nbsp;\n\n```\n(pandas-0.12)$ python test.py \nEmpty DataFrame\nColumns: [A, B, New]\nIndex: []\n[]\n```\n\n&nbsp;\n\n```\n(pandas-master)$ python test.py \n     A    B  New\n0  NaN  NaN  NaN\n\n[1 rows x 3 columns]\nTraceback (most recent call last):\n  File \"do_fail.py\", line 8, in <module>\n    print y.values\n  File \"/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py\", line 1705, in values\n    return self.as_matrix()\n  File \"/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py\", line 1697, in as_matrix\n    self._consolidate_inplace()\n  File \"/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py\", line 1622, in _consolidate_inplace\n    self._data = self._protect_consolidate(f)\n  File \"/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py\", line 1660, in _protect_consolidate\n    result = f()\n  File \"/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/generic.py\", line 1621, in <lambda>\n    f = lambda: self._data.consolidate()\n  File \"/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/internals.py\", line 2727, in consolidate\n    bm = self.__class__(self.blocks, self.axes)\n  File \"/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/internals.py\", line 1945, in __init__\n    self._verify_integrity()\n  File \"/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/internals.py\", line 2227, in _verify_integrity\n    tot_items, block.values.shape[1:], self.axes)\n  File \"/home/gmd/ENV/pandas-master-2/lib/python2.7/site-packages/pandas-0.13.0rc1_82_g66934c2-py2.7-linux-i686.egg/pandas/core/internals.py\", line 3561, in construction_error\n    tuple(map(int, [len(ax) for ax in axes]))))\nValueError: Shape of passed values is (3, 0), indices imply (3, 1)\n```\n"},{"labels":["api",null],"text":"It seems that the changes to to Series break the data conversion to R: running this Notebook doesn't work anymore with some dev version from last week:\nhttps://gist.github.com/kevindavenport/7771325/raw/87ab5603f406729c6a3866f95af9a1ebfedcf619/Mahalanobis_Outliers.ipynb\n\nThe resulting error is this:\n\n``` python\n#xydata=pandas.DataFrame(...)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-38-74fcaa767ca0> in <module>()\n----> 1 get_ipython().run_cell_magic(u'R', u'-i xydata,xycols # list object to be transferred to python here', u'install.packages(\"ggplot2\") # Had to add this for some reason, shouldn\\'t be necessary\\nlibrary(ggplot2)\\ndf = data.frame(xydata)\\nnames(df) <- c(xycols)\\nprint(head(df))\\nplot = ggplot(df, aes(x = X, y = Y)) + \\ngeom_point(alpha = .8, color = \\'dodgerblue\\',size = 5) +\\ngeom_point(data=subset(df, Y >= 6.7 | X >= 4), color = \\'red\\',size = 6) +\\ntheme(axis.text.x = element_text(size= rel(1.5),angle=90, hjust=1)) +\\nggtitle(\\'Distance Pairs with outliers highlighted in red\\')\\nprint(plot)')\n\nC:\\portabel\\Python27\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc in run_cell_magic(self, magic_name, line, cell)\n   2141             magic_arg_s = self.var_expand(line, stack_depth)\n   2142             with self.builtin_trap:\n-> 2143                 result = fn(magic_arg_s, cell)\n   2144             return result\n   2145 \n\nC:\\portabel\\Python27\\lib\\site-packages\\IPython\\extensions\\rmagic.py in R(self, line, cell, local_ns)\n\nC:\\portabel\\Python27\\lib\\site-packages\\IPython\\core\\magic.pyc in <lambda>(f, *a, **k)\n    191     # but it's overkill for just that one bit of state.\n    192     def magic_deco(arg):\n--> 193         call = lambda f, *a, **k: f(*a, **k)\n    194 \n    195         if callable(arg):\n\nC:\\portabel\\Python27\\lib\\site-packages\\IPython\\extensions\\rmagic.py in R(self, line, cell, local_ns)\n    585                     except KeyError:\n    586                         raise NameError(\"name '%s' is not defined\" % input)\n--> 587                 self.r.assign(input, self.pyconverter(val))\n    588 \n    589         if getattr(args, 'units') is not None:\n\nC:\\portabel\\Python27\\lib\\site-packages\\rpy2\\robjects\\functions.pyc in __call__(self, *args, **kwargs)\n     84                 v = kwargs.pop(k)\n     85                 kwargs[r_k] = v\n---> 86         return super(SignatureTranslatedFunction, self).__call__(*args, **kwargs)\n\nC:\\portabel\\Python27\\lib\\site-packages\\rpy2\\robjects\\functions.pyc in __call__(self, *args, **kwargs)\n     29 \n     30     def __call__(self, *args, **kwargs):\n---> 31         new_args = [conversion.py2ri(a) for a in args]\n     32         new_kwargs = {}\n     33         for k, v in kwargs.iteritems():\n\nC:\\portabel\\Python27\\lib\\site-packages\\rpy2\\robjects\\pandas2ri.pyc in pandas2ri(obj)\n     26                 od[name] = StrVector(values)\n     27             else:\n---> 28                 od[name] = ro.conversion.py2ri(values)\n     29         return DataFrame(od)\n     30     elif isinstance(obj, PandasIndex):\n\nC:\\portabel\\Python27\\lib\\site-packages\\rpy2\\robjects\\pandas2ri.pyc in pandas2ri(obj)\n     49         else:\n     50             # converted as a numpy array\n---> 51             res = original_conversion(obj)\n     52         # \"index\" is equivalent to \"names\" in R\n     53         if obj.ndim == 1:\n\nC:\\portabel\\Python27\\lib\\site-packages\\rpy2\\robjects\\numpy2ri.pyc in numpy2ri(o)\n     56             raise(ValueError(\"Unknown numpy array type.\"))\n     57     else:\n---> 58         res = ro.default_py2ri(o)\n     59     return res\n     60 \n\nC:\\portabel\\Python27\\lib\\site-packages\\rpy2\\robjects\\__init__.pyc in default_py2ri(o)\n    146         res = rinterface.SexpVector([o, ], rinterface.CPLXSXP)\n    147     else:\n--> 148         raise(ValueError(\"Nothing can be done for the type %s at the moment.\" %(type(o))))\n    149     return res\n    150 \n\nValueError: Nothing can be done for the type <class 'pandas.core.series.Series'> at the moment.\n```\n\nI'm not sure if this is something pandas cares, but even if not it would be nice to mention it in the release notes.\n"},{"labels":["api",null,null,null,null],"text":"related #5172\n\nGiven this DataFrame, with an index containing the moment when DST changes (October 27th in the case of the \"Europe/Paris\" timezone):\n\n```\nindex = pandas.date_range('2013-09-30', '2013-11-02', freq = '30Min', tz = 'UTC').tz_convert('Europe/Paris')\ncolumn_a = pandas.np.random.random(index.size)\ncolumn_b = pandas.np.random.random(index.size)\ndf = pandas.DataFrame({ \"a\": column_a, \"b\": column_b }, index = index)\n```\n\nLet's say I want to find the \"min\" and \"max\" values for each month:\n\n```\ndf.resample(\"MS\", how = { \"a\": \"min\", \"b\": \"max\" })\n```\n\nHere's the incorrect result:\n\n```\n                                  a         b\n2013-09-01 00:00:00+02:00  0.015856  0.979541\n2013-10-01 00:00:00+02:00  0.002039  0.999960\n2013-10-31 23:00:00+01:00       NaN       NaN\n```\n\nSame problem with a \"W-MON\" frequency:\n\n```\n                                  a         b\n2013-09-30 00:00:00+02:00  0.015856  0.979541\n2013-10-07 00:00:00+02:00  0.007961  0.999734\n2013-10-14 00:00:00+02:00  0.002614  0.993354\n2013-10-21 00:00:00+02:00  0.005655  0.999960\n2013-10-27 23:00:00+01:00       NaN       NaN\n2013-11-03 23:00:00+01:00       NaN       NaN\n```\n\nWhereas it works fine with a \"D\" frequency.\n\n```\n                                  a         b\n...\n2013-10-26 00:00:00+02:00  0.004645  0.983281\n2013-10-27 00:00:00+02:00  0.030151  0.986827\n2013-10-28 00:00:00+01:00  0.015891  0.981455\n2013-10-29 00:00:00+01:00  0.024176  0.999306\n...\n```\n\nShould I resample only the \"a\" column, it also works fine:\n\n```\ndf[\"a\"].resample(\"MS\", how = \"min\")\n```\n\n```\n2013-09-01 00:00:00+02:00    0.015856\n2013-10-01 00:00:00+02:00    0.002039\n2013-11-01 00:00:00+01:00    0.000747\nFreq: MS, dtype: float64\n```\n\nTested with latest pandas from GIT master.\n"},{"labels":["api",null,null],"text":"In master, `NDFrame.fillna`'s `inplace` parameter's docs claim:\n\n> If True, fill in place. Note: this will modify any\n> other views on this object, (e.g. a no-copy slice for a column in a\n> DataFrame).  **Still returns the object.**\n\nBut this is not consistently true. If `inplace=True`, `self` is only returned on a couple of paths. In the typical case of `s = Series(); s.fillna(-1, inplace=True)`, `None` is returned.\n\nIn 0.12.0 (and judging by a warning in an earlier version, from 0.11), `Series.fillna` consistently (and very deliberately) returns `None` if `inplace=True`, despite its docs claiming something similar:\n\n> If True, fill the Series in place. Note: this will modify any other\n> views on this Series, for example a column in a DataFrame. Returns\n> a reference to the filled object, **which is self if inplace=True**\n\nSo I believe the intended behaviour is to consistently return `None` if `inplace=True`, and the docs are simply out of date.\n"},{"labels":["api",null,null,null],"text":"related #5690\n\nThis works:\n\n```\npd.Series(np.array([np.timedelta64(300000000)]))\n```\n\n```\n0   00:00:00.300000\ndtype: timedelta64\n```\n\nbut this does not:\n\n```\npd.Series([np.timedelta64(300000000)])\n```\n\non Numpy 1.7:\n\n```\nTypeError: don't know how to convert scalar number to float\n```\n\nand on Numpy 1.8:\n\n```\n0    300000000\ndtype: int64\n```\n"},{"labels":["api",null,null,null],"text":"See below for discussion\n\n---\n\n```\nIn [19]: import pandas as pd\n\nIn [20]: import numpy as np\n\nIn [21]: import random\n\nIn [22]: df = pd.DataFrame(np.random.random_sample((20,5)), index=[random.choice('ABCDE') for x in range(20)])\n\nIn [23]: df.loc[:,0].ix['A'].median()\nOut[23]: 0.57704085832236685\n\nIn [24]: pd.version.version\nOut[24]: '0.12.0'\n\n\nIn [1]: import pandas as pd\n\nIn [2]: import numpy as np\n\nIn [3]: import random\n\nIn [4]: df = pd.DataFrame(np.random.random_sample((20,5)), index=[random.choice('ABCDE') for x in range(20)])\n\nIn [5]: df.loc[:,0].ix['A'].median()\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-5-57f9fc9d1583> in <module>()\n----> 1 df.loc[:,0].ix['A'].median()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'median'\n\nIn [6]: pd.version.version\nOut[6]: '0.13.0rc1-43-g4f9fefc'\n```\n"},{"labels":["api",null,null,null],"text":"Referenced briefly in the OP at #3275\n\n``` python\nIn [11]: idx = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('a', 3), ('b', 1), ('b', 2), ('b', 3)])\n\nIn [12]: idx.names = ['outer', 'inner']\n\nIn [13]: df = pd.DataFrame({\"A\": np.arange(6), 'B': ['one', 'one', 'two', 'two', 'one', 'one']}, index=idx)\n```\n\nSo the idea is to be able to call\n\n``` python\ndf.groupby('B', level='inner')\n```\n\ninstead of\n\n``` python\nIn [15]: df.reset_index().groupby(['B', 'inner']).mean()\nOut[15]: \n             A\nB   inner     \none 1      0.0\n    2      2.5\n    3      5.0\ntwo 1      3.0\n    3      2.0\n\n[5 rows x 1 columns]\n```\n\nCurrently this raises `TypeError: 'numpy.ndarray' object is not callable`.  Mostly just syntactic sugar, but I've been having to do a lot of this lately and all the `reset_index`es are getting annoying. Thoughts?\n"},{"labels":["api",null],"text":"Since the output of the string methods is primarily boolean, do we want to preserve NaNs? Use case. I load in some data, empty strings (i.e., missing data in a column that's string data) gets automatically converted to NaNs. I have to do fillna everywhere first, or I run into situations like this. \n\n```\ndta = pd.DataFrame([[\"blah\"],[np.nan],[\"foobar\"]], columns=['vari'])\ndta.vari.str.startswith('f')\ndta.ix[dta.vari.str.startswith('f')]\n```\n\nSince presumably the primary use case of a boolean series is indexing, do we want to just return False in place of NaNs? Just a thought.\n"},{"labels":["api",null,null,null],"text":"`read_csv` requires the multi-index column labels to be non-sparse in order to properly parse them. I am not sure if this can be done automatically, though maybe a keyword could trigger this.\n\n```\nIn [40]: def unsparsify_labels(index):\n   ....:     new_labels = []\n   ....:     for label in index.values:\n   ....:         if label[0].startswith('Unnamed'):\n   ....:             label = list(label)\n   ....:             label[0] = ll\n   ....:             label = tuple(label)\n   ....:         else:\n   ....:             ll = label[0]\n   ....:         new_labels.append(label)\n   ....:     return MultiIndex.from_tuples(new_labels)\n   ....: \n\nIn [41]: unsparsify_labels(x)\nOut[41]: \nMultiIndex(levels=[[u'A', u'B'], [u'1', u'2']],\n           labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n\nIn [42]: data = \"\"\"A,,B,\n   ....: 1,2,1,2\n   ....: 1,2,3,4\n   ....: 5,6,7,8\n   ....: \"\"\"\n\nIn [43]: df = read_csv(StringIO(data),header=[0,1],index_col=None)\n\nIn [44]: df\nOut[44]: \n   A  Unnamed: 1_level_0  B  Unnamed: 3_level_0\n   1                   2  1                   2\n0  1                   2  3                   4\n1  5                   6  7                   8\n\n[2 rows x 4 columns]\n\nIn [45]: df.columns = unsparsify_labels(df.columns)\n\nIn [46]: df\nOut[46]: \n   A     B   \n   1  2  1  2\n0  1  2  3  4\n1  5  6  7  8\n\n[2 rows x 4 columns]\n```\n\nhttp://stackoverflow.com/questions/20473271/importing-multilevel-indexed-cvs-data-with-pandas-0-13/20474982#20474982\n"},{"labels":["api",null,null],"text":"``` python\n\nIn [12]: pd.read_json(\"No Such File\")\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-12-f479cae2f65a> in <module>()\n----> 1 pd.read_json(\"No Such File\")\n\n/home/user1/src/pandas/pandas/io/json.pyc in read_json(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit)\n    188         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n    189                           keep_default_dates, numpy, precise_float,\n--> 190                           date_unit).parse()\n    191 \n    192     if typ == 'series' or obj is None:\n\n/home/user1/src/pandas/pandas/io/json.pyc in parse(self)\n    256 \n    257         else:\n--> 258             self._parse_no_numpy()\n    259 \n    260         if self.obj is None:\n\n/home/user1/src/pandas/pandas/io/json.pyc in _parse_no_numpy(self)\n    473         if orient == \"columns\":\n    474             self.obj = DataFrame(\n--> 475                 loads(json, precise_float=self.precise_float), dtype=None)\n    476         elif orient == \"split\":\n    477             decoded = dict((str(k), v)\n\nValueError: Expected object or value\n```\n\nread_json interprets strings which are not filenames as json data, then fails to parse them\nif the filename names a path that doesn't exist (due to typo, or being in wrong directory for example).\n\nThat overloading makes it <del>impossible</del> nasty to distinguish two distinct error cases, e.g. \nmissing file and malformed json. Dubious API choice to my tastes.\n\nin any case, catch both errors and return a saner message \"missing file or malformed input\" etc.\n"},{"labels":["api"],"text":"I am really happy about the SettingWithCopyWarning but is this situation the right place to pop up? Because what I want to do seems to work, maybe I don't see the danger of a possible case?\n\n``` python\nIn [19]: df.convert_objects().dtypes\nOut[19]: \nhour      float64\nminute    float64\nsecond    float64\ntb        float64\ndtype: object\n\nIn [20]: df['hour'] = df.hour.astype('int')\n-c:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n\nIn [21]: df.dtypes\nOut[21]: \nhour        int64\nminute    float64\nsecond    float64\ntb        float64\ndtype: object\n```\n\nusing master.\n"},{"labels":["api",null,null,null,null],"text":"related #6360\n\nBased on [this SO question](http://stackoverflow.com/questions/20383972/binary-operation-broadcasting-across-multiindex?noredirect=1#comment30444368_20383972)\n\nConsider the following two Series:\n\n``` python\nx = pd.DataFrame({'year':[1,1,1,1,2,2,2,2],\n                  'country':['A','A','B','B','A','A','B','B'],\n                  'prod':[1,2,1,2,1,2,1,2],\n                  'val':[10,20,15,25,20,30,25,35]})\nx = x.set_index(['year','country','prod']).squeeze()\n\ny = pd.DataFrame({'year':[1,1,2,2],'prod':[1,2,1,2],\n                  'mul':[10,0.1,20,0.2]})\ny = y.set_index(['year','prod']).squeeze()\n```\n\nwhich look like:\n\n``` python\n    year  country  prod\n    1     A        1       10\n                   2       20\n          B        1       15\n                   2       25\n    2     A        1       20\n                   2       30\n          B        1       25\n                   2       35\n\nyear  prod\n1     1       10.0\n      2        0.1\n2     1       20.0\n      2        0.2\n```\n\nI find it to be an _extremely_ common task, to perform binary operations by distributing the values of `y` over a particular level of `x`. For example, I'd like to multiply all values of product 1 in year 1 by 10.0, regardless of `country`.\n\nThe required result is therefore as follows:\n\n``` python\n    year  country  prod\n    1     A        1       100.0\n                   2       2.0\n          B        1       150.0\n                   2       2.5\n    2     A        1       400.0\n                   2       6.0\n          B        1       500.0\n                   2       7.0\n```\n\nThe binary operation .mul() doesn't work as expected:\n\n``` python\n>>> x.mul(y, level=['year','prod'])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/series.py\", line 334, in f\n    return self._binop(other, op, level=level, fill_value=fill_value)\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/series.py\", line 2075, in _binop\n    this, other = self.align(other, level=level, join='outer')\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/series.py\", line 2570, in align\n    return_indexers=True)\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/index.py\", line 954, in join\n    return_indexers=return_indexers)\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/index.py\", line 1058, in _join_level\n    raise Exception('Join on level between two MultiIndex objects '\nException: Join on level between two MultiIndex objects is ambiguous\n```\n\nTo create the required result, the user currently has to do this:\n\n``` python\nx = x.reset_index('country').sort_index()\nx.val = x.val * y\nx = x.reset_index().set_index(['year',\n                               'country',\n                               'prod']).sortlevel()\n```\n"},{"labels":["api",null,null,null],"text":"With:\n\n```\ncsv=\"\"\"A,B,C\n1,2,2003-11-1\n\"\"\"\n```\n\nThese all work as expected:\n\n```\nIn [40]: pd.read_csv(StringIO(csv), parse_dates=\"C\",index_col=\"C\").index[0]\n\nOut[40]: Timestamp('2003-11-01 00:00:00', tz=None)\n\nIn [41]: pd.read_csv(StringIO(csv), parse_dates=[\"C\"],index_col=\"C\").index[0]\n\nOut[41]: Timestamp('2003-11-01 00:00:00', tz=None)\n\nIn [42]: pd.read_csv(StringIO(csv), parse_dates=[\"C\"]).C[0]\n\nOut[42]: Timestamp('2003-11-01 00:00:00', tz=None)\n```\n\nbut this does not parse the string:\n\n```\nIn [39]: pd.read_csv(StringIO(csv), parse_dates=\"C\",).C[0]\n\nOut[39]: '2003-11-1'\n```\n"},{"labels":["api",null,null],"text":"This issue appears in 0.13.0-rc1 and did not appear in 0.12.0-854-gde63e00\n\nLet us say we have an empty DataFrame. Then the following calls (which worked in pandas 0.12) no longer work:\n\n``` python\ndf = pd.DataFrame()\ndf['foo'] = []\n\ndf = pd.DataFrame()\ndf['foo'] = df.index\n\ndf = pd.DataFrame()\ndf['foo'] = range(len(df))\n```\n\nThey all throw an exception:\n\n```\nFile \"/usr/local/lib/python3.2/dist-packages/pandas-0.13.0rc1-py3.2-linux-x86_64.egg/pandas/core/frame.py\", line 1855, in __setitem__\n    self._set_item(key, value)\n  File \"/usr/local/lib/python3.2/dist-packages/pandas-0.13.0rc1-py3.2-linux-x86_64.egg/pandas/core/frame.py\", line 1915, in _set_item\n    self._ensure_valid_index(value)\n\nFile \"/usr/local/lib/python3.2/dist-packages/pandas-0.13.0rc1-py3.2-linux-x86_64.egg/pandas/core/frame.py\", line 1899, in _ensure_valid_index\n    raise ValueError('Cannot set a frame with no defined index '\nValueError: Cannot set a frame with no defined index and a non-series\n```\n\nThe following will work:\n\n``` python\ndf = pd.DataFrame()\ndf['foo'] = pd.Series([])\n\ndf = pd.DataFrame()\ndf['foo'] = pd.Series(df.index)\n\ndf = pd.DataFrame()\ndf['foo'] = pd.Series(range(len(df)))\n```\n\nThe issue appears to be on lines 1897-1899 of pandas.core.frame:\n\n``` python\nif not len(self.index):\n    if not isinstance(value, Series):\n        raise ValueError('Cannot set a frame with no defined index '\n```\n\nPerhaps it could be changed to:\n\n``` python\nif not len(self.index) and len(value) > 0:\n    if not isinstance(value, Series):\n        raise ValueError('Cannot set a frame with no defined index '\n```\n"},{"labels":["api",null,null],"text":"If you try following piece of code, my python process crashes:\n\n```\ndf = pd.DataFrame({'A':[1,1,2], 'B':[1,2,1], 'C':[1,2,3]})\ndf = df.set_index(['A', 'B'])\ndf.index.groupby(df.index)\n```\n\nI know this code is as such probably bullshit (I also did it by accident, the first index shouldn't have been there :-)), but still it should then generate a Error Message instead of causing python to stop?\n\nBTW `pd.Index.groupby` is a publicly available, but not really clear what is does or is meant for.\n"},{"labels":["api",null,null],"text":"`Index.to_datetime` has no `format` keyword (and more generally, it lacks all keyword arguments from `pd.to_datetime` except for `dayfirst`). \n\nMaybe it could be made more similar to `pd.datetime`? (or even just use it under the hood?)\n\nCode: https://github.com/pydata/pandas/blob/master/pandas/core/index.py#L278 \nDocs: http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Index.to_datetime.html#pandas.Index.to_datetime\n\nThanks to @stijnvanhoey for reporting\n"},{"labels":["api",null,null],"text":"I'm getting spurious warnings on some old code that I'm running with new pandas. You can replicate by doing something like this (you have to take a subset of the data first, that's the key)\n\n```\nimport pandas as pd\nfrom pandas.core.common import SettingWithCopyWarning\nfrom string import letters\nimport warnings\nwarnings.simplefilter('error', SettingWithCopyWarning)\n\ndef random_text(nobs=100):\n    df = []\n    for i in range(nobs):\n        idx= np.random.randint(len(letters), size=2)\n        idx.sort()\n        df.append([letters[idx[0]:idx[1]]])\n\n    return pd.DataFrame(df, columns=['letters'])\n\ndf = random_text(100000)\n\ndf = df.ix[df.letters.apply(lambda x : len(x) > 10)]\ndf['letters'] = df['letters'].apply(str.lower)\n```\n"},{"labels":["api",null,null],"text":"http://stackoverflow.com/questions/20224564/how-does-pandas-grouped-apply-decide-on-output-and-why-does-this-depend-on-w/20225276#20225276\n"},{"labels":["api",null,null],"text":"`__contains__` should validate the key to be of a correctly castable type, e.g. 0.1 should raise a `TypeError` with an `Int64Index`\n\n---\n\nI may well be missing something, but the following behavior seems unexpected:\n\n\"\"\"\nIn [58]: 5 in pd.Series([1.5, 2.5, 3.5])\nOut[58]: False\n\nIn [59]: 5.0 in pd.Series([1.5, 2.5, 3.5])\nOut[59]: False\n\nIn [60]: 0.1 in pd.Series([1.5, 2.5, 3.5])\nOut[60]: True\n\nIn [61]: 0 in pd.Series([1.5, 2.5, 3.5])\nOut[61]: True\n\nIn [62]: 3.5 in pd.Series([1.5, 2.5, 3.5])\nOut[62]: False\n\nIn [63]: 3.4 in pd.Series([1.5, 2.5, 3.5])\nOut[63]: False\n\nIn [64]: 0.2 in pd.Series([1.5, 2.5, 3.5])\nOut[64]: True\n\nIn [65]: 1.6 in pd.Series([1.5, 2.5, 3.5])\nOut[65]: True\n\nIn [66]: 2.6 in pd.Series([1.5, 2.5, 3.5])\nOut[66]: True\n\nIn [67]: 3.0 in pd.Series([1.5, 2.5, 3.5])\nOut[67]: False\n\nIn [68]: 2.5 in pd.Series([1.5, 2.5, 3.5])\nOut[68]: True\n\nIn [69]: 0.1 in pd.Series([1.5, 2.5, 3.5])\nOut[69]: True\n\"\"\"\n"},{"labels":["api",null],"text":"The nth groupby method on a Series takes the nth **non-NaN** value in the Series.\n\nThis means on  a DataFrame it's not going to be well defined...\n\nShould we make this a Series only method?\n\n```\nIn [1]: g = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],columns=['A']).groupby('A')\n\nIn [2]: g.nth(0)\nOut[2]: \nEmpty DataFrame\nColumns: []\nIndex: []\n\nIn [3]: g.A.nth(0)\nOut[3]: \nA\na    a\nb    b\nName: A, dtype: object\n```\n"},{"labels":["api",null,null],"text":"I've had to use this a few times, maybe some other people could use it too.\n\nAt least for unique indices, the symmetric difference should be equivalent to `(df1.index - df2.index).union(df2.index - df1.index)`. Example:\n\n``` python\nIn [54]: df1 = pd.DataFrame([1,2,3,4], index=['a', 'b', 'c', 'd'])\n\nIn [55]: df1\nOut[55]: \n   0\na  1\nb  2\nc  3\nd  4\n\nIn [56]: df2 = pd.DataFrame([5, 6, 7, 8], index=['a', 'd', 'e', 'f'])\n\nIn [57]: df2\nOut[57]: \n   0\na  5\nd  6\ne  7\nf  8\n\nIn [58]: (df1.index - df2.index).union(df2.index - df1.index)\nOut[58]: Index([u'b', u'c', u'e', u'f'], dtype='object')\n```\n\nI'll need to look into how would work for\n- Indicies with duplicates\n- MultiIndexes\n"},{"labels":["api",null,null],"text":"I'm having trouble figuring out how DataFrame.replace() is supposed to work. I'm not sure if this is a bug or a documentation issue.\n\n```\nIn [1]: import pandas\n\nIn [2]: df = pandas.DataFrame({\"col1\":range(5), \"col2\":[0.5]*3+[1.0]*2})\n\nIn [3]: df\nOut[3]: \n   col1  col2\n0     0   0.5\n1     1   0.5\n2     2   0.5\n3     3   1.0\n4     4   1.0\n\nIn [4]: df.replace(1.0, \"a\")\nOut[4]: \n  col1 col2\n0    0  0.5\n1    a  0.5\n2    2  0.5\n3    3    a\n4    4    a\n\nIn [5]: df.replace(1.0, \"a\").replace(0.5, \"b\")\nOut[5]: \n  col1 col2\n0    0    b\n1    a    b\n2    2    b\n3    3    a\n4    4    a\n```\n\nSo far, so good, everything makes sense. But I would have expected this to accomplish the same as above:\n\n```\n\nIn [6]: df.replace({1.0:\"a\", 0.5:\"b\"})\nOut[6]: \n  col1 col2\n0    b    b\n1    a    a\n2    2    b\n3    3    a\n4    4    b\n```\n\nAs you can see, I'm getting alternating \"b\" and \"a\". From a quick browse of the source code, it seems that the dictionary-replacement option should result in the same outcome as the following (which gives what I would have expected):\n\n```\nIn [15]: df.replace([1.0, 0.5], [\"a\", \"b\"])\nOut[15]: \n  col1 col2\na    0    b\nb    a    b\nc    2    b\nd    3    a\ne    4    a\n```\n\nI'm not sure what the to_replace=dict option is supposed to be doing but (at least for pandas v 0.12.0) it isn't doing what I would have expected.\n\nWhether this is a bug or not, the df.replace() method needs better documentation. It's not enough to include a disclaimer that \"This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works.\"\n"},{"labels":["api",null,null,null,null,null],"text":"This used to work (October 2012), but doesn't anymore:\n\n``` python\nfrom pandas import DataFrame\nimport numpy as np\nimport StringIO\na = [\"a\",\"b\",\"a\",\"b\",\"a\",\"b\",\"a\",\"b\",\"a\",\"b\",\"a\",\"b\"]\nb = [\"c\",\"d\",\"e\",\"c\",\"d\",\"e\",\"c\",\"d\",\"e\",\"c\",\"d\",\"e\"]\nc = [1,2,3,4,5,6,7,8,9,10,11,12]\nd = list(reversed(c))\ndf = DataFrame({\"a\":a, \"b\":b, \"c\":c, \"d\":d})\n_agg_funs = [np.mean, np.std, np.min, np.max]\ngroupby_variables = [\"a\",\"b\"]\ndf_grouped = df.groupby(groupby_variables, as_index=True).agg(_agg_funs)\noutput = StringIO.StringIO()\ndf_grouped.to_csv(output, header=[var + \"_\" + agg for (var, agg) in df_grouped.columns])\nindex = output.getvalue().split(\"\\n\")[0].split(\",\")\nexpected_index = groupby_variables + [var + \"_\" + agg for (var, agg) in df_grouped.columns]\nprint(index == expected_index) # This was true in October 2012!\nprint(index)\nprint(expected_index) \n\nFalse\n['', '', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'd']\n['a', 'b', 'c_mean', 'c_std', 'c_amin', 'c_amax', 'd_mean', 'd_std', 'd_amin', 'd_amax']\n```\n\nProbably related to #3575\n"},{"labels":["api",null,null,null],"text":"```\nlen(alphas.index.levels)\n```\n\n> 2\n\n```\n%time alphas.index.lexsort_depth\n```\n\n> CPU times: user 0.03 s, sys: 0.00 s, total: 0.03 s\n> Wall time: 0.02 s\n> 1\n\n```\n%time alphas.index.is_monotonic\n```\n\n> CPU times: user 10.22 s, sys: 0.00 s, total: 10.22 s\n> Wall time: 10.22 s\n> False\n"},{"labels":["api",null,null],"text":"Note for  0.14.1: need to add back `#N/A N/A` to na_values defaults\n\nin pandas 0.12\n\ni found a little bug:\n\nthis:\n_NA_VALUES = set(['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',\n                 '#N/A N/A', 'NA', '#NA', 'NULL', 'NaN',\n                 'nan', ''])\n\nmust be:\n_NA_VALUES = set(['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',\n                 '#N/A','N/A', 'NA', '#NA', 'NULL', 'NaN',\n                 'nan', ''])\n\nthank you\nGlauco\n"},{"labels":["api",null,null],"text":"Currently `normalize` does not work on a `Series` or a `DataFrame`:\n\n```\npd.Series(range(5), index=pd.date_range('2013-1-1', periods=5,freq='D')).normalize()\nAttributeError: 'Series' object has no attribute 'normalize'\n```\n\nHowever, changing the timezone, which I consider to be a similar type of operation, does work:\n\n```\nIn [50]: pd.Series(range(5), index=pd.date_range('2013-1-1', periods=5,freq='D')).tz_localize('America/New_York')\nOut[50]: \n2013-01-01 00:00:00-05:00    0\n2013-01-02 00:00:00-05:00    1\n2013-01-03 00:00:00-05:00    2\n2013-01-04 00:00:00-05:00    3\n2013-01-05 00:00:00-05:00    4\nFreq: D, dtype: int64\n```\n\nMy current workaround:\n\n```\ns = pd.Series(range(5), index=pd.date_range('2013-1-1', periods=5,freq='D'))\ns.index = s.index.normalize()\n```\n"},{"labels":["api",null],"text":"http://stackoverflow.com/questions/19938734/is-there-a-simple-way-to-reference-the-current-object-after-applying-a-chain-of\n\n```\ndf.where(\"A>0\",1)\ndf.loc[\"A>0\"]\n```\n\nso could process the condition via an 'eval' type of operation\n"},{"labels":["api",null,null,null],"text":"`itertools.groupby` groups things contiguously-- great for run length encoding, not so great for partitioning.  This necessitates the `groupby(sorted(items,key=keyfn), keyfn)` dance if you want to separate it.  That's not always what you want either, so you wind up writing\n\n```\ndef partition(seq, keyfn):\n    d = {}\n    for x in seq:\n        d.setdefault(keyfn(x), []).append(x)\n    return d\n```\n\nand so on. \n\n`DataFrame.groupby` is great for data partitioning, but merges discontiguous groups.  Wanting to cluster timeseries -- first x since the last y, etc. -- is a common task.  With some `cumsum` hacks you can do it, but \"get a boolean series, see when it's equal to its shifted value to find the transitions, take advantage of the fact that False == 0 and True == 1 to `cumsum` that to get something which grows for each cluster, and then `groupby` on that\"  is maybe a little more than I'd expect a beginner to have to do to get back what `itertools.groupby` does naturally.  And if there's an easier way, then we at least should at least make it more obvious.\n\nI'm not sure what the best way to proceed is,  but I've answered variants of this several times on SO, and people wanting a cumsum/cumprod-with-reset is a pretty common numpy request.\n"},{"labels":["api",null],"text":"The following groupby fails if key1 contains only a single value.  I am using pandas 12 and python 3.x\n\nThis works:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'key1': ['a','a','n','a', 'a'],\n                    'data1': np.random.randn(5),\n                   'data2': np.random.randn(5)})\n\ndef x(df):\n    return df['data2']\n\ndf['test'] = df.groupby(df['key1']).apply(x)\nprint (df)\n\nThis fails. Note the key1 values which are all the same.\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'key1': ['a','a','a','a', 'a'],\n                   'data1': np.random.randn(5),\n                   'data2': np.random.randn(5)})\n\ndef x(df):\n    return df['data2']\n\ndf['test'] = df.groupby(df['key1']).apply(x)\nprint (df)\n"},{"labels":["api",null],"text":"I had a series containing strings like these:\n\n\"November 1, 2013\"\n\nSeries length was about 500,000\n\nA)\nrunning pd.to_datetime(s) takes just over a minute.\nB)\nrunning pd.to_datetime(s, format=\"%B %d, %Y\") takes about 7 seconds!\n\nMy suggestion is a way to make case A (where user doesn't specify the format type) take about as long as case B (user does specify).\n\nBasically it looks like the code is always using date_util parser for case A.\n\nMy suggestion is based upon the idea that it's highly likely that the date strings are all in a consistent format (it's highly unlikely in this case that they would be in 500K separate formats!).\n\nIn a nutshell:\n- figure out the date format of the first entry.\n- try to use that against the entire series, using the speedy code in tslib.array_strptime\n- if that works, we've saved heaps of time, if not fall back to the current slower behaviour of using dateutil parse each time.\n\nHere's some pseudo-code::\n\n```\ndatestr1 = s[0]\n# I'm assuming dateutil has something like this, that can tell you what the format is for a given date string.\ndate_format = figure_out_datetime_format(datestr1)\n\ntry:\n    # use the super speed code that pandas uses when you tell it what the format is.\n    dt_series = tslib.array_strptime(s, format=datestr1, *, ...)\nexcept:\n    # date strings aren't consistent after all. Let's do it the old slow way.\n    dt_series = tslib.array_to_datetime(s, format=None)\n```\n\nreturn dt_series\n"},{"labels":["api",null,null,null],"text":"I'd like to propose `read_netcdf` ([netCDF (Network Common Data Format)](http://www.unidata.ucar.edu/software/netcdf/)) a new Pandas I/O api feature with a similar top level interface as the other reader functions.  This format is widely used in the scientific community.   Furthermore, netCDF4 is implemented on top of the HDF5 library, making this a natural extension to functionality already in the api.  \n\nMost likely this would sit on top of the exiting [Python/numpy interface to netCDF](https://code.google.com/p/netcdf4-python/), and because each variables metadata is stored in the file header, no complicated parsing would necessary. Multidimensional variables could be handled in a similar manner as hdf.\n\nThis may have been brought up in the past but my search here and on the google didn't bring anything up.\n"},{"labels":["api",null,null],"text":"Copying comment from #4887 to a new issue for discussion purposes:\n\nWhile testing pandas-master, hit a method not in the new groupby dispatch whitelist: `value_counts` (on a `SeriesGroupBy` object).  Another possible addition: `shift`, as used in this [SO answer](http://stackoverflow.com/a/17406083/243434).\n\nTried to generate a list of blacklisted methods for DataFrame and Series, see below -- needs further filtering, but may reveal useful blocked methods.  any thoughts on the remaining methods?\n\n```\nIn [18]: sorted([x for x in set(dir(pd.DataFrame)) - pd.core.groupby._apply_whitelist - set(dir(pd.core.groupby.GroupBy)) if not x.startswith(('_', 'T', 'to', 'from', 'as'))])\nOut[18]: ['abs', 'add', 'add_prefix', 'add_suffix', 'align', 'all', 'any', 'append', 'applymap', 'at', 'at_time', 'axes', 'between_time', 'bfill', 'blocks', 'bool', 'clip', 'clip_lower', 'clip_upper', 'columns', 'combine', 'combineAdd', 'combineMult', 'combine_first', 'compound', 'consolidate', 'convert_objects', 'copy', 'corr', 'corrwith', 'cov', 'delevel', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'dropna', 'dtypes', 'duplicated', 'empty', 'eq', 'eval', 'ffill', 'filter', 'first_valid_index', 'floordiv', 'ftypes', 'ge', 'get', 'get_dtype_counts', 'get_ftype_counts', 'get_value', 'get_values', 'groupby', 'gt', 'iat', 'icol', 'idxmax', 'idxmin', 'iget_value', 'iloc', 'index', 'info', 'insert', 'interpolate', 'irow', 'isin', 'isnull', 'iteritems', 'iterkv', 'iterrows', 'itertuples', 'ix', 'join', 'keys', 'kurt', 'kurtosis', 'last_valid_index', 'le', 'load', 'loc', 'lookup', 'lt', 'mad', 'mask', 'merge', 'mod', 'mode', 'mul', 'multiply', 'ndim', 'ne', 'notnull', 'pct_change', 'pivot', 'pivot_table', 'pop', 'pow', 'product', 'query', 'radd', 'rdiv', 'reindex', 'reindex_axis', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rpow', 'rsub', 'rtruediv', 'save', 'select', 'set_index', 'set_value', 'shape', 'shift', 'skew', 'sort', 'sort_index', 'sortlevel', 'squeeze', 'stack', 'sub', 'subtract', 'swapaxes', 'swaplevel', 'take', 'transpose', 'truediv', 'truncate', 'tshift', 'tz_convert', 'tz_localize', 'unstack', 'update', 'values', 'where', 'xs']\n\nIn [19]: sorted([x for x in set(dir(pd.Series)) - pd.core.groupby._apply_whitelist - set(dir(pd.core.groupby.GroupBy)) if not x.startswith(('_', 'T', 'to', 'from', 'as'))]) \nOut[19]: ['abs', 'add', 'add_prefix', 'add_suffix', 'align', 'all', 'any', 'append', 'argmax', 'argmin', 'argsort', 'at', 'at_time', 'autocorr', 'axes', 'base', 'between', 'between_time', 'bfill', 'blocks', 'bool', 'clip', 'clip_lower', 'clip_upper', 'combine', 'combine_first', 'compound', 'consolidate', 'convert_objects', 'copy', 'corr', 'cov', 'data', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'dropna', 'duplicated', 'empty', 'eq', 'ffill', 'filter', 'first_valid_index', 'flags', 'floordiv', 'ftype', 'ge', 'get', 'get_dtype_counts', 'get_ftype_counts', 'get_value', 'get_values', 'groupby', 'gt', 'iat', 'idxmax', 'idxmin', 'iget', 'iget_value', 'iloc', 'imag', 'index', 'interpolate', 'irow', 'is_time_series', 'isin', 'isnull', 'item', 'iteritems', 'iterkv', 'ix', 'keys', 'kurt', 'kurtosis', 'last_valid_index', 'le', 'load', 'loc', 'lt', 'mad', 'map', 'mask', 'mod', 'mode', 'mul', 'multiply', 'ndim', 'ne', 'nonzero', 'notnull', 'nunique', 'order', 'pct_change', 'pop', 'pow', 'product', 'ptp', 'put', 'radd', 'ravel', 'rdiv', 'real', 'reindex', 'reindex_axis', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'repeat', 'replace', 'reset_index', 'reshape', 'rfloordiv', 'rmod', 'rmul', 'round', 'rpow', 'rsub', 'rtruediv', 'save', 'select', 'set_value', 'shape', 'shift', 'skew', 'sort', 'sort_index', 'sortlevel', 'squeeze', 'str', 'strides', 'sub', 'subtract', 'swapaxes', 'swaplevel', 'take', 'transpose', 'truediv', 'truncate', 'tshift', 'tz_convert', 'tz_localize', 'unique', 'unstack', 'update', 'valid', 'value_counts', 'values', 'view', 'weekday', 'where', 'xs']\n```\n"},{"labels":["api",null],"text":"I've been contributing a little bit to GeoPandas / working on some of my own custom code using pandas that I want to share with others and I realized that I wasn't sure what is and is not part of the public API for pandas.\n\nHere's what I have in my head:\n\nDefinitely public:\n- anything in pandas toplevel namespace (except for modules imported into that namespace)\n- non-`_` classmethods on NDFrame\n- DateOffsets (in that they have a set interface), but not necessarily direct use of get_offset and friends\n\nSomewhat public:\n- Subset of Index methods:\n  - Definitely `union`, `intersection`, `difference`, `levels`, `labels`, `names` (and the set methods for them)\n  - Somewhat: `get_indexer`, `get_indexer_non_unique`, `groupby`, `get_loc`, `slice_locs`, `equals`, `identical`, `values` property\n  - Not public: `is_`, `is_unique`, lexsort properties on MI\n- `get_offset` and `DateOffset` subclasses\n\nNice to make public right now:\n- `compat` module (useful to provide this for modules that depend on pandas...at least in terms of guaranteeing that whatever's in the namespace now will go through a deprecation period).\n- test utils (otherwise have to reinvent the wheel)\n- `cache_readonly` decorator\n\nInternal methods/properties that subclasses may rely on/manipulate:\n- `__finalize__`\n- `_metadata` property\n- `_constructor`, `_constructor_sliced`\n- `_internal_names` (maybe - weird behavior with `__setattr__` if you don't do this)\n- `_reset_cache`, `_update_inplace`, possibly `_maybe_update_cacher`\n\nThings we could consider making public in the future:\n- many of the `is_*` methods in core/common.\n\nAm I missing anything here? Does this make sense?\n"},{"labels":["api",null,null],"text":"have to return timedelta64[ns] scalars\n\n```\ntype: timedelta64[ns]\n\nIn [23]: s = pd.to_timedelta(np.arange(5),unit='d')\n\nIn [24]: s\nOut[24]: \n0           00:00:00\n1   1 days, 00:00:00\n2   2 days, 00:00:00\n3   3 days, 00:00:00\n4   4 days, 00:00:00\ndtype: timedelta64[ns]\n\nIn [25]: s + timedelta(hours=1)\nOut[25]: \n0           01:00:00\n1   1 days, 01:00:00\n2   2 days, 01:00:00\n3   3 days, 01:00:00\n4   4 days, 01:00:00\ndtype: timedelta64[ns]\n\nIn [26]: s + pd.offsets.Hour(1)\nOut[26]: \n0           01:00:00\n1   1 days, 01:00:00\n2   2 days, 01:00:00\n3   3 days, 01:00:00\n4   4 days, 01:00:00\ndtype: timedelta64[ns]\n\nIn [27]: s.iloc[2] + pd.offsets.Hour(1)\nTypeError: ufunc add cannot use operands with types dtype('<m8[ns]') and dtype('O')\n\nIn [29]: s.iloc[2] + timedelta(hours=1)\nTypeError: ufunc add cannot use operands with types dtype('<m8[ns]') and dtype('O')\n```\n\nThis works (wrapping in a np.timedeltat64\n\n```\nIn [31]: s.iloc[2] += np.timedelta64(timedelta(hours=1))\n\nIn [32]: s\nOut[32]: \n0           00:00:00\n1   1 days, 00:00:00\n2   2 days, 01:00:00\n3   3 days, 00:00:00\n4   4 days, 00:00:00\ndtype: timedelta64[ns]\n```\n\nReevaluate whether summary ops should just return the np.timdelta64 scalar\n(not as pretty though)\n\n```\nIn [33]: s.mean()\nOut[33]: \n0   2 days, 00:12:00\ndtype: timedelta64[ns]\n\nIn [34]: s.mean().iloc[0]\nOut[34]: numpy.timedelta64(173520000000000,'ns')\n```\n"},{"labels":["api",null,null],"text":"This is an issue that I introduced when fixing #5235.\n\nThe `cols` option in `to_excel` no longer works correctly after the fix for the above issue.  For example:\n\n``` python\nimport pandas as pd\n\ndf = pd.DataFrame({'A': ['a', 'a', 'a'],\n                   'B': ['b', 'b', 'b']})\n\ndf.to_excel('frame.xlsx', sheet_name='Sheet1', cols=['B', 'A'])\n\n```\n\nGives:\n\n![screenshot](https://f.cloud.github.com/assets/94267/1462277/a4f1cd20-44ed-11e3-96be-993c49f342a6.png)\n\nNote, the headers are changed but not the column data.\n\nI have a proposed fix and test for this. Should I create a new branch/PR or merge it into the Excel MultiIndex PR: #5423\n"},{"labels":["api",null,null,null],"text":"related #5420\n\nML issue: https://groups.google.com/forum/#!topic/pydata/Ke8kntiPHqw\n\nexisting tests here (and tests/test_frame.py, test_multilevel.py)\nhttps://github.com/pydata/pandas/blob/master/pandas/tests/test_indexing.py\n- create new top-level advanced indexing tab (move hierarchical and sections below here)\n- add an indexer matrix (generated from a function), which can document a cross of the indexers with the index types and accessors, e.g. something like this (the values should be filled in with maybe a code of somesort, e.g. `Y` for supported, `NI` for not-implemented, `nan` for not-applicable, `KeyError/IndexError` if raises\n\n```\nIn [31]: from itertools import product\n\nIn [35]: index = MultiIndex.from_tuples([ (typ,indexer) for typ,indexer in product(['MultiIndex','DateTimeIndex','FloatIndex','Int64Index','mixed_labels'],['iloc','loc','ix','[]','xs']) ])\n\nIn [36]: columns = ['scalar_label_in_bounds','oob_scalar_label','scalar_position','oob_scalar_position','list_of_positions','list_of_positions_some_oob','list_of_labels','list_of_labels_some_oob','slice_of_positions','slice_of_labels','boolean_indexer']\n\nIn [37]: DataFrame(np.nan,index=index,columns=columns)\nOut[37]: \n                    scalar_label_in_bounds  oob_scalar_label  scalar_position  oob_scalar_position  list_of_positions  list_of_positions_some_oob  list_of_labels  list_of_labels_some_oob  slice_of_positions  slice_of_labels  boolean_indexer\nMultiIndex    iloc                     NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              loc                      NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              ix                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              []                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              xs                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\nDateTimeIndex iloc                     NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              loc                      NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              ix                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              []                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              xs                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\nFloatIndex    iloc                     NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              loc                      NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              ix                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              []                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              xs                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\nInt64Index    iloc                     NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              loc                      NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              ix                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              []                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              xs                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\nmixed_labels  iloc                     NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              loc                      NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              ix                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              []                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n              xs                       NaN               NaN              NaN                  NaN                NaN                         NaN             NaN                      NaN                 NaN              NaN              NaN\n\n```\n"},{"labels":["api",null,null,null,null,null],"text":"Should this be doable?\n\n``` python\nIn [39]: df = pd.DataFrame({'A': [1, 2], 'B': pd.to_datetime(['a', 'b'])},\n                  index=pd.MultiIndex.from_tuples([(1, 'one'), (1, 'two')]))\n\nIn [40]: df\nOut[40]: \n       A  B\n1 one  1  a\n  two  2  b\n\nIn [41]: wp = pd.Panel({'i1': df, 'i2': df})\n\nIn [42]: wp.to_frame()\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-42-e49d9f2f9609> in <module>()\n----> 1 wp.to_frame()\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/core/panel.pyc in to_frame(self, filter_observations)\n    846         index = MultiIndex(levels=[self.major_axis, self.minor_axis],\n    847                            labels=[major_labels, minor_labels],\n--> 848                            names=[maj_name, min_name], verify_integrity=False)\n    849 \n    850         return DataFrame(data, index=index, columns=self.items)\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.pyc in __new__(cls, levels, labels, sortorder, names, copy, verify_integrity)\n   1880         if names is not None:\n   1881             # handles name validation\n-> 1882             subarr._set_names(names)\n   1883 \n   1884         if sortorder is not None:\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.pyc in _set_names(self, values, validate)\n   2150         # set the name\n   2151         for name, level in zip(values, self.levels):\n-> 2152             level.rename(name, inplace=True)\n   2153 \n   2154     names = property(\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/core/index.pyc in set_names(self, names, inplace)\n    333         \"\"\"\n    334         if not com.is_list_like(names):\n--> 335             raise TypeError(\"Must pass list-like as `names`.\")\n    336         if inplace:\n    337             idx = self\n\nTypeError: Must pass list-like as `names`.\n```\n\nI think the issue comes when the index of the lower dimensional DataFrame (`df` in this case) is already a MultiIndex. These two work:\n\n``` python\nIn [45]: wp.transpose(1, 0, 2).to_frame()\nOut[45]: \n              1    \n            one two\nmajor minor        \ni1    A       1   2\n      B       a   b\ni2    A       1   2\n      B       a   b\n\nIn [46]: wp.transpose(1, 2, 0).to_frame()\nOut[46]: \n              1    \n            one two\nmajor minor        \nA     i1      1   2\n      i2      1   2\nB     i1      a   b\n      i2      a   b\n```\n\nI was expecting that `wp.to_frame()` would create a new MultiIndex with 3 levels:\n\n``` python\nIn [63]: df = pd.DataFrame({'A': [1, 2, 1, 2], 'B': pd.to_datetime(['a', 'b', 'a', 'b'])},\n                  index=pd.MultiIndex.from_tuples([('i1', 1, 'one'), ('i1', 1, 'two'), ('i2', 1, 'one'), ('i2', 1, 'two')]))\n\nIn [64]: df\nOut[64]: \n          A  B\ni1 1 one  1  a\n     two  2  b\ni2 1 one  1  a\n     two  2  b\n```\n\nThe ordering of the new MultiIndex (with `wp.items` inserted) is ambiguous... But something like that. You could always swaplevels later.\n\n(side note to myself: check on if `verify_integrity` is `validate` in MultiIndex land. It doesn't get passed to `_set_names`).\n"},{"labels":["api",null,null,null,null],"text":"Just on panels, works fine for DataFrames.\nGives a `TypeError` when reading:\n\n``` python\nIn [7]: df = pd.DataFrame({'A': [1, 2], 'B': pd.to_datetime(['2010-01-01', np.nan])})\n\nIn [8]: df\nOut[8]:\n   A                   B\n0  1 2010-01-01 00:00:00\n1  2                 NaT\n\nIn [10]: tst = pd.HDFStore('tst.h5')\n\nIn [12]: df.to_hdf('tst.h5', 'df')\n\nIn [13]: tst.select('df')\nOut[13]:\n   A                   B\n0  1 2010-01-01 00:00:00\n1  2                 NaT\n\nIn [14]: df2 = pd.DataFrame({'A': [1, 2], 'B': pd.to_datetime(['2010-01-01', np.nan])})\n\nIn [17]: wp = pd.Panel({'i1': df, 'i2': df2})\n\nIn [18]: wp\nOut[18]:\n<class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 2 (major_axis) x 2 (minor_axis)\nItems axis: i1 to i2\nMajor_axis axis: 0 to 1\nMinor_axis axis: A to B\n\nIn [19]: wp.to_hdf(tst, key='wp')\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/io/pytables.py:2310: PerformanceWarning:\nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->['i1', 'i2']]\n\n  warnings.warn(ws, PerformanceWarning)\n\nIn [20]: tst.select('wp')\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-20-da42baeaf1c6> in <module>()\n----> 1 tst.select('wp')\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/io/pytables.pyc in select(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\n    595\n    596         return TableIterator(self, func, nrows=s.nrows, start=start, stop=stop,\n--> 597                              auto_close=auto_close).get_values()\n    598\n    599     def select_as_coordinates(\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/io/pytables.pyc in get_values(self)\n   1225\n   1226     def get_values(self):\n-> 1227         results = self.func(self.start, self.stop)\n   1228         self.close()\n   1229         return results\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/io/pytables.pyc in func(_start, _stop)\n    584         def func(_start, _stop):\n    585             return s.read(where=where, start=_start, stop=_stop,\n--> 586                           columns=columns, **kwargs)\n    587\n    588         if iterator or chunksize is not None:\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/io/pytables.pyc in read(self, **kwargs)\n   2517         for i in range(self.nblocks):\n   2518             blk_items = self.read_index('block%d_items' % i)\n-> 2519             values = self.read_array('block%d_values' % i)\n   2520             blk = make_block(values, blk_items, items)\n   2521             blocks.append(blk)\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/pandas-0.12.0_993_gda89834-py2.7-macosx-10.8-x86_64.egg/pandas/io/pytables.pyc in read_array(self, key)\n   2075         import tables\n   2076         node = getattr(self.group, key)\n-> 2077         data = node[:]\n   2078         attrs = node._v_attrs\n   2079\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/tables-3.0.0-py2.7-macosx-10.8-x86_64.egg/tables/vlarray.pyc in __getitem__(self, key)\n    659             start, stop, step = self._process_range(\n    660                 key.start, key.stop, key.step)\n--> 661             return self.read(start, stop, step)\n    662         # Try with a boolean or point selection\n    663         elif type(key) in (list, tuple) or isinstance(key, numpy.ndarray):\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/tables-3.0.0-py2.7-macosx-10.8-x86_64.egg/tables/vlarray.pyc in read(self, start, stop, step)\n    799         atom = self.atom\n    800         if not hasattr(atom, 'size'):  # it is a pseudo-atom\n--> 801             outlistarr = [atom.fromarray(arr) for arr in listarr]\n    802         else:\n    803             # Convert the list to the right flavor\n\n/Users/tom/Envs/pandas-dev/lib/python2.7/site-packages/tables-3.0.0-py2.7-macosx-10.8-x86_64.egg/tables/atom.pyc in fromarray(self, array)\n   1149         if array.size == 0:\n   1150             return None\n-> 1151         return cPickle.loads(array.tostring())\n   1152\n   1153\n\nTypeError: ('__new__() takes exactly one argument (2 given)', <class 'pandas.tslib.NaTType'>, ('\\x00\\x01\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00',))\n```\n\nHaven't had a chance to look at what's going on.\n"},{"labels":["api",null,null],"text":"Enable value_counts on DataFrame by delegating along axis (ie columns/items) to value_counts as convenience, boiling down to same Series value_counts (Panel is more complicated so not going to attempt for now).\n\nGets weird with heterogeneous dtypes (b/c union of all unique values) \n\n@rockg - can you make up some test cases? I already have the implementation. I can put together panel.\n"},{"labels":[null,"api"],"text":"Now that we have all the arithmetic methods, there's no reason to keep it as before. Not having this means that operating on an int column with missing values switches operation result from floordiv to truediv.\n\n@wesm You had, at one point, said that you would consider this.  I believe this change would iron over a long-standing Python wart and make pandas easier to use:\n\nExisting behavior (note how the future import doesn't affect div at all because it's defined in a separate file, but does affect division because Python will call `__truediv__` instead of `__div__`):\n\n``` python\nIn [22]: from __future__ import division\n\nIn [23]: ser1 / ser2\nOut[23]:\n0    0.0\n1    0.5\n2    1.0\n3    1.5\ndtype: float64\n\nIn [24]: ser1.div(ser2)\nOut[24]:\n0    0\n1    0\n2    1\n3    1\ndtype: int64\n```\n\nAnd missing values immediately causes this behavior to change (because coerces to float)\n\n``` python\nIn [11]: from pandas import Series\n\nIn [12]: ser1 = Series([0, 1, 2, 3])\n\nIn [13]: ser2 = Series([2, 2, 2, 2])\n\nIn [14]: ser1.div(ser2)\nOut[14]:\n0    0\n1    0\n2    1\n3    1\ndtype: int64\nIn [15]: ser1[0] = np.nan\n\nIn [16]: ser1.div(ser2)\nOut[18]:\n0    NaN\n1    0.5\n2    1.0\n3    1.5\ndtype: float64\n```\n\nAnd this distinction is totally eliminated in Python 3:\n\n``` python\nIn [11]: from pandas import Series\n\nIn [12]: ser1 = Series([0, 1, 2, 3])\n\nIn [13]: ser2 = Series([2, 2, 2, 2])\n\nIn [14]: ser1.div(ser2)\nOut[14]:\n0    0.0\n1    0.5\n2    1.0\n3    1.5\ndtype: float64\nIn [15]: ser1[0] = np.nan\n\nIn [16]: ser1.div(ser2)\nOut[18]:\n0    NaN\n1    0.5\n2    1.0\n3    1.5\ndtype: float64\n```\n\nTrivial to fix and, while slightly backwards incompatible, would mostly just make things simpler.  We should still leave `a / b` to be dependent upon the future import.\n"},{"labels":["api",null],"text":"Hi,\nI posted this on stackoverflow http://stackoverflow.com/questions/19602864/how-to-plot-in-a-specific-axis-with-dataframe-histby-in-pandas?noredirect=1#comment29118427_19602864 and was suggested to file a bug report here.\n\nI am trying plot several histogram groups in the same figure. Each group contains two conditions and I am therefore using the 'by=' argument from pandas histogram options. However, this does not work as I expected and pandas creates a new figure instead of plotting in the axis I am passing. I tried to pass four axes as well, but still no go. Sample code:\n\n``` python\nimport pandas as pd\ndf = pd.DataFrame({'color': ['blue','blue','yellow','blue','yellow'], 'area': [2,2,3,4,4]})\nfig, (ax1, ax2) = plt.subplots(1,2)\ndf.area.hist(by=df.color, ax=ax1)\n```\n\nI'm using pandas 0.12.0, matplotlib 1.3.0, numpy 1.7.1 and python 2.7.5. Any suggestion that leads to a way of combining/stitching multiple 'hist(by=)-plots' in the same subplot grid is welcome.\n"},{"labels":["api",null,null],"text":"See #5342\n"},{"labels":["api",null],"text":"Not sure if that is expected, but if it is expected it would be nice if the error message and the docsring of `Timestamp.now()` could be improved.\n\n```\nfrom pandas.tslib import Timestamp\nTimestamp.now()\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-45-634034a47269> in <module>()\n----> 1 Timestamp.now()\n\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\tslib.pyd in pandas.tslib.Timestamp.__new__ (pandas\\tslib.c:5167)()\n\nTypeError: __new__() takes at most 5 positional arguments (9 given)\n```\n"},{"labels":["api",null,null],"text":"``` python\nimport pandas as pd\ndf = pd.DataFrame({\"color\":[1,2,3,4]})\nprint df\n   color\n0      1\n1      2\n2      3\n3      4\nprint df.replace({\"color\":{\"1\":\"2\",\"3\":\"4\",}}) # works but shouldn't?\n  color\n0     2\n1     2\n2     4\n3     4\nprint df.replace({\"color\":{\"1\":\"2\",\"2\":\"3\",\"3\":\"4\",\"4\":\"5\"}}) # strange\n  color\n0     2\n1     4\n2     3\n3     5\nprint df.replace({\"color\":{1:\"2\",2:\"3\",3:\"4\",4:\"5\"}}) # works by replacing each cell once\n  color\n0     2\n1     3\n2     4\n3     5\n\ndf = pd.DataFrame({\"color\":[\"1\",\"2\",\"3\",\"4\"]})\nprint df\n  color\n0     1\n1     2\n2     3\n3     4\nprint df.replace({\"color\":{\"1\":\"2\",\"3\":\"4\",}}) # works\n  color\n0     2\n1     2\n2     4\n3     4\nprint df.replace({\"color\":{\"1\":\"2\",\"2\":\"3\",\"3\":\"4\",\"4\":\"5\"}}) # works not\n  color\n0     3\n1     3\n2     5\n3     5\nprint df.replace({\"color\":{1:\"2\",2:\"3\",3:\"4\",4:\"5\"}}) # works as expected: shouldn't replace anything!\n  color\n0     1\n1     2\n2     3\n3     4\n```\n\nSo, my expected behaviour would be:\n- don't replace a cell if the type of the cell does not match the key (as it is the case when a string cell is replaced by a int key)\n- if a value of a cell is replaced, the cell shouldn't be replaced a second time in the same replace call\n\nI found the problem when I tried to match string values to colors and got blown up color values: like `{\"3\":\"#123456\",\"4\":\"#000000\"}` wouldn't convert `\"3\"` into `\"#123#00000056\"`\n\n_Edit_: insert string cell cases and my expected behaviour and deleted the intial comments which had these examples\n"},{"labels":["api",null,null],"text":"I recall discussing this before but came up on the mailing list again. Sorting of cut output is always wrong / ugly. I propose creating a str-like that compares with others of the same type as if they were tuples (possibly with nice logic around ranges) - would be facilitated by intro of CategoricalBlock.\n\nSimilar to replacing qcut output with this:\n\n``` python\ndef convert_bins_to_tuple(iterable):\n    return [(left, right) for elem in iterable for left, right in elem.strip('()[]').split(',')]\n```\n"},{"labels":["api",null,null,null],"text":"tl;dr - add true support for Categoricals in NDFrame. \n\nThere was an issue on the mailing list about using cut and sorting the results that brought this to mind. The issue is both that (I believe) a categorical loses its representation when you put it in a DataFrame and so the output of cut has to just be strings. I propose the following:\n1. Add a `CategoricalBlock` (or `FactorBlock`) internally that can handle categoricals like those produced from cut that could share most of MI's internals, as a 2D int ndarray with an associated list of indexes for each column (again, nearly the same as MI except most ops would be working on just one 'level' and underlying could/would be 2D rather than list of Int64Index). Probably also would mean abstracting common operations to a separate mixin class.\n2. Change `Categorical` to be a Series subclass with a SingleBlockManager that's a CategoricalBlock. This would not change its API, but it would gain Series methods.\n3. Add a `to_categorical` method to Series (bonus points if we change convert_objects to detect if there are < Some_Max number of labels and convert object dtypes to categoricals).\n4. Add a registration method to make_block so it iterates over a set of functions that either return a klass or None before falling back to ObjectBlock (so abstract existing else clause into a function and make the list of functions semi-public).\n\nI'm going to work on this and I don't think it will be that difficult to implement, but it would make pandas more useful for representing level sets and other normalized data.\n"},{"labels":["api",null,null],"text":"`startingMonth` is the month the quarter ends. This is confusing.\n"},{"labels":["api"],"text":"- [x] Add 'rows' as synonym for 'index' for DataFrame and Series - #5309\n- [x] Allow Series to accept axis kwarg for flex ops (and only allow axis=0) - #5352\n- [x] Fix Panel functions that aren't getting axis correctly - #5354\n\nI think (2) may already be there, but I'm not sure and I want to check that.\n\nThis should be really trivial to do but it should make interactive playing around with frame and series a bit more intuitive. Don't think it makes sense for panel.\n"},{"labels":["api",null],"text":"Current dataframe.drop will raise error for below code for 'non_exist_in_df_col' :\n\n```\ndf = df.drop(['col_1', 'col_2', 'non_exist_in_df_col'], axis=1)\n```\n\nBut below is better for it can accept it.\n\n```\ndef drop_cols(df, del_cols):   \n    for col in (set(del_cols) & set(df.columns)):\n        df = df.drop([col], axis=1)\n    return df\nDataFrame.drop_cols = drop_cols\n```\n\nAnd it will be better to add an 'inplace' option to speed up the repeatly df self-assignment.\n"},{"labels":["api",null,null,null],"text":"AssertionError bubbling up to the user - ruroh:\n\n``` python\nfrom io import StringIO\ndata = StringIO(u\"\"\"\n121301234\n121300123\n121300012\n\"\"\")\npd.read_fwf(data, colspecs=[4, 5])\n```\n\nEnds up with an AssertionError:\n\n```\n/opt/anaconda/envs/np17py27-1.5/lib/python2.7/site-packages/pandas/io/parsers.pyc in __init__(self, f, colspecs, filler, thousands)\n   1771         assert isinstance(colspecs, (tuple, list))\n   1772         for colspec in colspecs:\n-> 1773             assert isinstance(colspec, (tuple, list))\n   1774             assert len(colspec) == 2\n   1775             assert isinstance(colspec[0], int)\n\nAssertionError: \n```\n\nLow priority but we should fix at some point.  Really could be as simple as this in `read_fwf`. `if not all(isinstance(colspec, (tuple, list) and len(colspec) == 2 and isinstance(colspec[0], int) for colspec in colspecs): raise ValueError(\"colspecs must be 2-tuples of integers\")`\n"},{"labels":["api",null,null,null],"text":"related similar operation\n\nhttp://stackoverflow.com/questions/19484344/how-do-i-use-a-specific-columns-value-in-a-pandas-dataframe-where-clause/19494873#19494873\n\nhttp://stackoverflow.com/questions/19507088/filtering-a-pandas-dataframe-without-removing-rows/19516869#19516869\n\nhttp://stackoverflow.com/q/21627926/190597\n\nThis should be a bit more intuitive\n\n```\nIn [59]: data = \"\"\"      A    B    C    D\n1/1   0    1    0    1\n1/2   2    1    1    1\n1/3   3    0    1    0 \n1/4   1    0    1    2\n1/5   1    0    1    1\n1/6   2    0    2    1\n1/7   3    5    2    3\"\"\"\n\nIn [60]: df = read_csv(StringIO(data),sep='\\s+')\n\nIn [61]: df\nOut[61]: \n     A  B  C  D\n1/1  0  1  0  1\n1/2  2  1  1  1\n1/3  3  0  1  0\n1/4  1  0  1  2\n1/5  1  0  1  1\n1/6  2  0  2  1\n1/7  3  5  2  3\n\nIn [62]: df.where((df>df.shift(1)).values & DataFrame(df.D==1).values)\nOut[62]: \n      A   B   C   D\n1/1 NaN NaN NaN NaN\n1/2   2 NaN   1 NaN\n1/3 NaN NaN NaN NaN\n1/4 NaN NaN NaN NaN\n1/5 NaN NaN NaN NaN\n1/6   2 NaN   2 NaN\n1/7 NaN NaN NaN NaN\n```\n\nGiven that normal binary operators like addition or logical `and` work well between a pair of Series objects, or between a pair of DataFrame objects (returning a element-wise addition/conjuction), I found it surprising that I cannot do the same between a Series object and a DataFrame object.\n\nHere's a demonstration of what doesn't work now and what would be the expected result: http://nbviewer.ipython.org/urls/dl.dropboxusercontent.com/u/52886258/000-qdoqud/Untitled0.ipynb\n"},{"labels":["api",null,null],"text":"Is there a reset_index equivalent for the column headings? In other words, if the column names are an MultiIndex, how would I drop one of the levels?\n"},{"labels":["api",null,null],"text":"related #6512, #6524, #6346\n\n_Update from @hayd I think these should reference `_selected_obj` rather than `obj`._\n\nLooking through some others, looks these also ignore the selection\n- [x] count #6570\n- [x] corr #6570\n- [x] cummax #6570\n- [x] cummin #6570 \n- [x] cumsum #6570 \n- [x] cumprod  #6570 \n- [x] describe #6570\n- [x] fillna #6570\n- [x] quantile #6570\n- [x] head #6533\n- [x] hist? the output is ok but the plots have all\n- [ ] ohlc? possibly fixed with #6570 (resample with ohlc is tested), should this method exist? see #6594\n- [x] plot\n- [x] rank #6570\n- [x] tail #6533 \n- [x] filter, #6570 (tested in #6593)\n- [x] resample, #6570\n- [x] nth #6569\n- [x] diff/shift, #6570\n- [x] all/any #6570\n- [x] ffill, #6570\n- [x] pct_change  #6570\n- [x] idxmin/idxmax, #6570\n- [x] dtypes #6570 \n- [ ] apply  #6570 (could be tested more / different paths?)\n\nAggregation functions like (they already _kind_ of do, but they allow bad selections ie column names not in columns, may be sep issue?):\n- [x] sum/max/min/median/mean/var/std/.. (not tested)\n- [x] agg (not tested)\n  (these \"work\" with the described bug)\n\nAtm selecting a column not in df doesn't raise:\n- [ ]  it should raise a Key Error, #6578\n\nwhat about `iloc/loc/ix` (current all disabled)?\n- [x] iloc (very similar to head/tail)\n- [ ] loc/ix (maybe push off for now, this is pretty tricky)\n- [x] iterate over all (whitelisted) functions to check they adhere to this\n\nThe column selection on a groupby object is being ignored when `.quantile()` is called. So it computes the quantile on all the (numeric) columns and returns the full DataFrame.\n\n``` python\nIn [92]: t = pd.DataFrame(np.random.randn(10, 4)); t[0] = np.hstack([np.ones(5), np.zeros(5)])\n\nIn [95]: t.groupby(0)[[1, 2]].quantile()  # shows other cols\nOut[95]: \n   0         1         2         3\n0                                 \n0  0  0.127152  0.108908  0.369601\n1  1 -0.321279  0.265550 -0.382398\n\nIn [96]: t[[1, 2]].groupby(t[0]).quantile()  # Should be equivalent to:\nOut[96]: \n          1         2\n0                    \n0  0.127152  0.108908\n1 -0.321279  0.265550\n```\n\nSeeing all these, I'm wondering if this is a bug or just how some of the methods are implementer. The [docs](http://pandas.pydata.org/pandas-docs/dev/groupby.html#dataframe-column-selection-in-groupby) don't mention anything about only supporting some methods though.\n\nversion: '0.12.0-883-g988d4be'\n"},{"labels":["api",null],"text":"related #4264\n\nThe normal matplotlib boxplot command in Python returns a dictionary with keys for the boxes, median, whiskers, fliers, and caps. This makes styling really easy. Pandas.groupby boxplots, however, return an AxesSubplot object. This makes styling the plots more difficult.\n\nI posted this question recently on [Stack Overflow](http://stackoverflow.com/questions/19453994/styling-of-pandas-groupby-boxplots) and eventually came to this solution. I hope it will be useful to others.\n\n```\nfrom numpy.random import rand\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#2 columns produces an array of 2 matplotlib.axes.AxesSubplot objects\ndf2 = pd.DataFrame(rand(10,2), columns=['Col1', 'Col2'] )\ndf2['X'] = pd.Series(['A','B','A','B','A','B','A','B','A','B'])\n\n#1 column produces a single matplotlib.axes.AxesSubplot object\ndf1 = pd.DataFrame(rand(10), columns=['Col1'] )\ndf1['X'] = pd.Series(['A','B','A','B','A','B','A','B','A','B'])\n\ndef stylable_groupby_boxplot(df, by):\n    '''\n    If you plot only one column, boxplot returns a single AxesSubplot object.\n    If there are several columns, boxplot returns an array of several AxesSubplot objects.\n    '''\n    bp = df.boxplot(by=by, grid=False)\n    bptype = str(type(bp))\n    if bptype == \"<class 'matplotlib.axes.AxesSubplot'>\":\n        cl = bp.get_children()\n        cl=[item for item in cl if isinstance(item, matplotlib.lines.Line2D)]\n        bpdict = {}\n        groups = df.groupby(by).groups.keys()\n        for i in range(len(groups)):\n            bpdict[groups[i]] = {'boxes': [], 'caps': [], 'fliers': [], 'medians': [], 'whiskers': []}\n            bpdict[groups[i]]['boxes'] = [cl[4+8*i]]\n            bpdict[groups[i]]['caps'] = [cl[2+8*i], cl[3+8*i]]\n            bpdict[groups[i]]['fliers'] = [cl[6+8*i], cl[7+8*i]]\n            bpdict[groups[i]]['medians'] = [cl[5+8*i]]\n            bpdict[groups[i]]['whiskers'] = [cl[0+8*i], cl[1+8*i]]\n    else:\n        bpdict = {}\n        groups = df.groupby(by).groups.keys()\n        keys = range(len(bp))\n        for i in keys:\n            bpdict[keys[i]] = {}\n            cl = bp[i].get_children()\n            cl=[item for item in cl if isinstance(item, matplotlib.lines.Line2D)]\n            for j in range(len(groups)):\n                bpdict[keys[i]][groups[j]] = {'boxes': [], 'caps': [], 'fliers': [], 'medians': [], 'whiskers': []}\n                bpdict[keys[i]][groups[j]]['boxes'] = [cl[4+8*j]]\n                bpdict[keys[i]][groups[j]]['caps'] = [cl[2+8*j], cl[3+8*j]]\n                bpdict[keys[i]][groups[j]]['fliers'] = [cl[6+8*j], cl[7+8*j]]\n                bpdict[keys[i]][groups[j]]['medians'] = [cl[5+8*j]]\n                bpdict[keys[i]][groups[j]]['whiskers'] = [cl[0+8*j], cl[1+8*j]]\n    return bpdict\n\nbp2 = stylable_groupby_boxplot(df2, by=\"X\")\nbp1 = stylable_groupby_boxplot(df1, by=\"X\")\n\n\n#2 column styling\nplt.suptitle(\"\")\nplt.setp(bp2[0]['A']['boxes'], color='blue')\nplt.setp(bp2[0]['A']['medians'], color='red')\nplt.setp(bp2[0]['A']['whiskers'], color='blue')\nplt.setp(bp2[0]['A']['fliers'], color='blue')\nplt.setp(bp2[0]['A']['caps'], color='blue')\nplt.setp(bp2[0]['B']['boxes'], color='red')\nplt.setp(bp2[0]['B']['medians'], color='blue')\nplt.setp(bp2[0]['B']['whiskers'], color='red')\nplt.setp(bp2[0]['B']['fliers'], color='red')\nplt.setp(bp2[0]['B']['caps'], color='red')\nplt.setp(bp2[1]['A']['boxes'], color='green')\nplt.setp(bp2[1]['A']['medians'], color='purple')\nplt.setp(bp2[1]['A']['whiskers'], color='green')\nplt.setp(bp2[1]['A']['fliers'], color='green')\nplt.setp(bp2[1]['A']['caps'], color='green')\nplt.setp(bp2[1]['B']['boxes'], color='purple')\nplt.setp(bp2[1]['B']['medians'], color='green')\nplt.setp(bp2[1]['B']['whiskers'], color='purple')\nplt.setp(bp2[1]['B']['fliers'], color='purple')\nplt.setp(bp2[1]['B']['caps'], color='purple')\n\n#1 column styling\nplt.suptitle(\"\")\nplt.setp(bp1['A']['boxes'], color='blue')\nplt.setp(bp1['A']['medians'], color='red')\nplt.setp(bp1['A']['whiskers'], color='blue')\nplt.setp(bp1['A']['fliers'], color='blue')\nplt.setp(bp1['A']['caps'], color='blue')\nplt.setp(bp1['B']['boxes'], color='red')\nplt.setp(bp1['B']['medians'], color='blue')\nplt.setp(bp1['B']['whiskers'], color='red')\nplt.setp(bp1['B']['fliers'], color='red')\nplt.setp(bp1['B']['caps'], color='red')\n```\n"},{"labels":["api"],"text":"This can at least be done for Series and DataFrame (for dropna, thresh and how would just have no effect and axis must always be 0). Need to see whether it's possible for Panel.\n\ncf the recent issue with OLS.predict and issues with how. #5233\n"},{"labels":["api",null,null],"text":"http://stackoverflow.com/questions/19346033/how-to-collectively-set-the-values-of-multiple-columns-for-certain-selected-rows/19346925#19346925\n\nThis does not work as we are using a rhs of a series which is not broadcast\nto the assignment\n\n```\ndf.loc[df['A'],['A','B']] = df['C']\n```\n\nThis works, but a tad cumbersome\n\n```\nIn [17]: df = DataFrame(dict(A = [1,2,0,0,0],B=[0,0,0,10,11],C=[3,4,5,6,7]))\n\nIn [18]: df\nOut[18]: \n   A   B  C\n0  1   0  3\n1  2   0  4\n2  0   0  5\n3  0  10  6\n4  0  11  7\n\nIn [19]: mask = df['A'] == 0\n\nIn [20]: for col in ['A','B']:\n   ....:     df.loc[mask,col] = df['C']\n   ....:     \n\nIn [21]: df\nOut[21]: \n   A  B  C\n0  1  0  3\n1  2  0  4\n2  5  5  5\n3  6  6  6\n4  7  7  7\n```\n"},{"labels":["api",null,null,null,null],"text":"see related https://github.com/pydata/pandas/issues/5200, #4551, #4056\n\nseem that a direct subtraction of PeriodIndexes would accomplish this,\nbut don't seem to support those types of ops...\n\nThis is currently an intersection (with a '-'), but want to do a timedelta like op\n\n```\nIn [40]: pd.PeriodIndex(start='20130101',end='20130501',freq='M').sub(\npd.PeriodIndex(start='20130201',end='20130401',freq='M'))\n```\n\nPartially supported actuallly\n\n```\nIn [5]:  pd.PeriodIndex(start='20130101',end='20130501',freq='M').to_series()-pd.PeriodIndex(start='20130205',end='20130405',freq='M').to_series()\nOut[5]: \n2013-01   NaN\n2013-02     0\n2013-03     0\n2013-04     0\n2013-05   NaN\nFreq: M, dtype: float64\n\nIn [6]:  pd.PeriodIndex(start='20130101',end='20130201',freq='B').to_series()-pd.Period('20130101',freq='B')\n\nTypeError: unsupported operand type(s) for -: 'int' and 'Period'\n```\n"},{"labels":["api",null],"text":"related is #2094\nrelated is #6847 (fixes kind and some arg ordering)\nrelated is #7121 (make `sortlevel` a part of `sort_index` by adding level arg)\n\nthe sorting API is currently inconsistent and confusing. here is what exists:\n\nSeries:\n- `sort`: calls `Series.order`, in-place, defaults `quicksort`\n- `order`: do the sort on values, return a new object, defaults `mergesort`\n- `sort_index`: sort by labels, returns new object\n\nFrame:\n- `sort`: calls `sort_index`\n- `sort_index`: sorts by the index with no args, otherwise a nested sort of the passed columns\n\nThe semantics are different between `Series` and `DataFrame`. In `Series`, `sort` mean in-place, `order` returns a new object. `sort/order` sort on the values, while `sort_index` sorts on the index. For a `DataFrame`, `sort` and `sort_index` are the same and sort on a column/list of columns; `inplace` is a keyword.\n\nProposed signature of combined methods. We need to break a `Series` API here. because `sort` is an in-place method which is quite inconsistent with everything else.\n\n```\ndef sort(self, by=None, axis=0, level=None, ascending=True, inplace=False,\n                   kind='quicksort', na_last=True):\n```\n\nThis is what I think we should do:\n- make `Series.sort/order` be the same.\n- by can take a column/list of columns (as it can now), or an index name / `index` to provide index sorting (which means sort by the specifiied axis)\n- default is `inplace=False` (which is the same as now, except for `Series.sort`).\n- `Series.sort_index` does `s.sort('index')`\n- `DataFrame.sort_index` does `df.sort('index')`\n- eventually deprecate `Series.order`\n- add `DataFrame.sort_columns` to perform axis=1 sorting\n\nThis does switch the argument to the current `sort_index`, (e.g. axis is currently first), but I think then allows more natural syntax\n- `df.sort()` or `df.sort_index()` or `df.sort_index('index')` sort on the index labels\n- `df.sort(['A','B'],axis=1)` sort on these columns (allow 'index' here as well to sort on the index too)\n- `df.sort_columns()` or `df.sort('columns')` sort on the column labels\n- `df.sort_columns()` defaults `axis=1`, so `df.sort_columns(['A','B'])` is equiv of - - `df.sort(['A','B'],axis=1)`\n- `s.sort()` sort on the values\n- `s.sort('index')` or `s.sort_index()` sort on the series index\n"},{"labels":["api",null,null,null],"text":"this might be interesting as it allows jit optimizations but w/o major deps (like blaze/numba).\n\ncould be useful as a back end for eval\n\npotentially could be used in apply as well\n\nhttp://www.parakeetpython.com/\n\nhttp://www.phi-node.com/2013/01/just-in-time-compilers-for-number.html\n"},{"labels":["api",null],"text":"seems we need a public version of `tm.assert_frame_equal`\n\nmaybe `equals` on generic NDFrame objects\n\nhttp://stackoverflow.com/questions/19322506/pandas-nan-comparison\n\nseems good on the values comparison (should compare axes/types as well of course)\n\n```\n((df1 == df2) | ((df != df) & (df2 != df2))).values.all()\n```\n"},{"labels":["api",null,null,null,null],"text":"- [x] #5694\n\n```\ndf = pd.DataFrame([0], index=[datetime(2012, 11, 4, 23, 0, 0)])\ndf = df.tz_localize('America/New_York')\ndf.resample(rule='D', how='sum')\n```\n\nraises a AmbiguousTimeError even though datetime(2012, 11, 4, 23, 0, 0) is not ambiguous.\n- [x] #8601 \n\n```\nidx = date_range('2014-10-08 00:00','2014-10-09 00:00', freq='D', tz='Europe/Berlin')\npd.Series(5, idx).resample('MS')\n```\n- [ ] #8744\n\n```\nindex = pd.to_datetime(pd.Series([\n'2014-10-26 07:35:49',\n'2014-10-26 07:45:08',\n'2014-10-26 08:04:58'\n]))\n\ndf = pd.DataFrame(np.arange(len(index)), index=index)\ndf = df.tz_localize('Asia/Krasnoyarsk', ambiguous='NaT')\ndf.resample('D')\n\nAmbiguousTimeError: Cannot infer dst time from Timestamp('2014-10-26 01:00:00'), try using the 'ambiguous' argument\n```\n- [x]  #8653 (might be separate issue)\n\n```\nimport datetime\nimport pytz as tz\nimport pandas as pd\n\nrome = tz.timezone('Europe/Rome')\n\ndr = []\nfor i in range(2):\n    dp = datetime.datetime(2014, 10, 25) + datetime.timedelta(days=i)\n    dr.append(rome.localize(dp))\n\nseries = {}\nfor i, ddr in enumerate(dr):\n    series[ddr] = i * 10\n\ns1 = pd.Series(series)\ns1 = s1.resample('D', how='mean')\n```\n- [x] #9119 \n- [ ] #9173\n- [x] #9468 \n"},{"labels":["api"],"text":""},{"labels":["api",null],"text":"Right now sometimes getting config options iterates over a list filtering on regular expressions, which seems like overdoing it when internal code knows exactly what it wants. Makes sense to add getitem and set item to bypass the checks (and then add an update method to set up everything).\n\nhttps://github.com/pydata/pandas/commit/7aa7880f#diff-1e746a348e7db9596f3d053134d00e74R515\n"},{"labels":["api",null],"text":"Right now sometimes getting config options iterates over a list filtering on regular expressions, which seems like overdoing it when internal code knows exactly what it wants. Makes sense to add getitem and set item to bypass the checks (and then add an update method to set up everything).\n"},{"labels":["api",null],"text":"I ran into some unexpected behavior with `replace` today.\n\nI want to replace the answer with a numeric value according to the dictionary `weights`\n\n``` python\nIn [33]: df.answer.head()\nOut[33]: \n0       Strongly Agree\n1                Agree\n2              Neutral\n3             Disagree\n4    Strongly Disagree\nName: answer, dtype: object\n\nIn [34]: weights\nOut[34]: \n{'Agree': 4,\n 'Disagree': 2,\n 'Neutral': 3,\n 'Strongly Agree': 5,\n 'Strongly Disagree': 1}\n\nIn [35]: df.answer.replace(weights).head()\nOut[35]: \n0    4\n1    4\n2    3\n3    2\n4    2\ndtype: int64\n```\n\nIt looks like replace matches on the first part, `Agree`, or `Disagree` and doesn't make it through the dict to the `Strongly`s.  Am I just being a noob with how regexes work, or is this a bug?\n"},{"labels":["api",null,null],"text":"```\nIn [67]: df\nOut[67]:\n          counter\ncreated\n05-16-13        3\n05-17-13        1\n05-18-13        1\n\nIn [68]: print df.to_json('asdfasdf')\nNone\n\nIn [69]: print df.to_dict('series')\n{'counter': created\n05-16-13    3\n05-17-13    1\n05-18-13    1\nName: counter, dtype: int64}\n\nIn [70]: print df.counter.to_json('asdfasdf')\nNone\n\nIn [71]: print df.counter.to_json('index')\nNone\n```\n\nThe \"default\" value of `'index'` (whose default is actually `None`) doesn't do what it says.\n"},{"labels":["api",null,null],"text":"Allow creating a `_LiberalLxmlFrameParser` (see conversation on: https://github.com/pydata/pandas/issues/4469#issuecomment-22142824)\n\nFor now I will factor out:\n\n```\nparser = HTMLParser(recover=False)\n```\n\nso that subclass can change the behavior. \n\nWould there be an issue with adding `_LiberalLxmlFrameParser` to the Pandas codebase?\n"},{"labels":["api",null],"text":""},{"labels":["api",null],"text":"Previously, in `0.12` and earlier, I could quickly visualize `groupby` objects with a call to `.plot()`. Currently in master, the `.plot` method on a `groupby` object raises, with following error:\n\n``` python\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-63-3760a47a721b> in <module>()\n----> 1 grp.kWh.plot()\n\n/pandas/pandas/core/groupby.pyc in __getattr__(self, attr)\n    250 \n    251         if hasattr(self.obj, attr) and attr != '_cache':\n--> 252             return self._make_wrapper(attr)\n    253 \n    254         raise AttributeError(\"%r object has no attribute %r\" %\n\n/pandas/pandas/core/groupby.pyc in _make_wrapper(self, name)\n    265                    \"using the 'apply' method\".format(kind, name,\n    266                                                      type(self).__name__))\n--> 267             raise AttributeError(msg)\n    268 \n    269         f = getattr(self.obj, name)\n\nAttributeError: Cannot access callable attribute 'plot' of 'SeriesGroupBy' objects, try using the 'apply' method\n```\n"},{"labels":["api",null,null],"text":"xref #9252 \n"},{"labels":["api",null,null],"text":"Looking at these three methods on DatetimeIndex:\n\n``` python\n    # alias to offset\n    @property\n    def freq(self):\n        return self.offset\n\n    @cache_readonly\n    def inferred_freq(self):\n        try:\n            return infer_freq(self)\n        except ValueError:\n            return None\n\n    @property\n    def freqstr(self):\n        return self.offset.freqstr\n```\n\nyou would think that `freqstr` returns a string and `inferred_freq` and `freq` return \"frequencies\" (i.e. offsets). In reality `inferred_freq` is a string as well.\n\nI am not sure of the best way to deal with this issue. At the very least I would suggest adding a `inferred_freq_offset` property. Perhaps also deprecate `inferred_freq` and create a `inferred_freqstr`.\n\nTracing this method, you find `_FrequencyInferer::get_freq` which returns a string rather than an Offset.\n"},{"labels":["api",null,null],"text":"Changing to not be an ndarray subclass should make a number of things simpler (especially the API unification discussed in #3268). Plus, it will clarify the required interface for an Index object. It avoids all the workarounds to make Index immutable (instead, will define `__array__` and `__array_wrap__` and choose which other functions should be allowed).\n- [  ] Move Index underlying data to `_data` to mirror other objects. Keep metadata on object itself. (MI should work with only minor tweaks, given that it was always a fake ndarray)\n- [  ] Reimplement fastpath. (figure out how to do this inference again).\n- [  ] Move MI underlying data to `_data` (potentially turn levels into 2d int64 ndarray and keep labels as list of lists or as ordered-dict-like. not sure whether row-major or column-major makes more sense. I'm guessing row-major since that would speed key lookups) \n\n---\n\nHow much backwards compatibility do we need to support? (pickle compat should be simple). E.g. do we want to do something like this:\n\n``` python\ndef __getattr__(self, attr): \n    res = getattr(self._data, attr) \n    warnings.warn(\"Accessing %s on %s is deprecated. Convert to an ndarray first.\" %  \n                 (attr, self.__class__.__name__))\n    return res\n```\n"},{"labels":["api",null],"text":"This is the whatsnew example. I think for compat, the 3rd entry should be `(np.nan, 3)` ?\n\n```\nIn [1]: Series(['a1', 'b2', '3']).str.match('(?P<letter>[ab])?(?P<digit>\\d)')\nOut[1]: \n0       (a, 1)\n1       (b, 2)\n2    (None, 3)\ndtype: object\n```\n"},{"labels":["api",null,null,null,null],"text":"Returning a Series: http://stackoverflow.com/questions/19121854/using-rolling-apply-on-a-dataframe-object\n\nReturning a Scalar: http://stackoverflow.com/questions/21040766/python-pandas-rolling-apply-two-column-input-into-function/21045831#21045831\n"},{"labels":["api",null,null],"text":"post #4324\n- [ ] Fillna column wise, #4514\n- [x] interpolation, consolidate Series/DataFrame interpolation to internals.Block.interpolatte, #4434, cc @TomAugspurger\n- [ ] update, #3025 \n- [ ] shift, #4865, #4994\n- [ ] sort, #5190\n- [ ] dropna API for Series, see #5234, #5250\n- [ ] in place dropna , #1960 \n"},{"labels":["api",null,null],"text":"```\nfrom pandas import Series, DataFrame\ns = Series([2, 3, 4, 5, 6, 7, 8, 9, 10])\nd = DataFrame({'a': s})\ns + d\nd + s\nprint(d & s)\n```\n\nproduces Traceback:\n\n```\nraceback (most recent call last):\n  File \"testcase2.py\", line 6, in <module>\n    print(d & s)\n  File \"../pandas/core/frame.py\", line 217, in f\n    return self._combine_series(other, na_op, fill_value, axis, level)\n  File \"../pandas/core/frame.py\", line 3040, in _combine_series\n    return self._combine_match_columns(other, func, fill_value)\n  File \"../pandas/core/frame.py\", line 3079, in _combine_match_columns\n    func, right, axes=[left.columns, self.index])\n  File \"../pandas/core/internals.py\", line 2048, in eval\n    return self.apply('eval', *args, **kwargs)\n  File \"../pandas/core/internals.py\", line 2033, in apply\n    applied = getattr(blk, f)(*args, **kwargs)\n  File \"../pandas/core/internals.py\", line 850, in eval\n    result = get_result(other)\n  File \"../pandas/core/internals.py\", line 834, in get_result\n    return self._try_coerce_result(func(values, other))\n  File \"../pandas/core/frame.py\", line 201, in na_op\n    mask = notnull(xrav) & notnull(yrav)\nValueError: operands could not be broadcast together with shapes (81) (9)\n```\n\n`s & d` currently raises `NotImplemented` instead. (because there's no `__rand__`)\n"},{"labels":["api",null,null],"text":"`read_csv` uses the `usecols` parameter, and the `read_excel` function uses `parse_cols` parameter. Both parameter do basically the same thing:\n\n```\nusecols : array-like\n    Return a subset of the columns.\n    Results in much faster parsing time and lower memory usage.\n```\n\n```\nparse_cols : int or list, default None\n    * If None then parse all columns,\n    * If int then indicates last column to be parsed\n    * If list of ints then indicates list of column numbers to be parsed\n    * If string then indicates comma separated list of column names and\n      column ranges (e.g. \"A:E\" or \"A,C,E:F\")\n```\n\nAlso maybe supporting the same thing that `parse_cols` does in `usecol` wouldn't be so bad?\n"},{"labels":["api",null,null],"text":"If I have a Series of `timedelta64[ns]` (the result of a `diff()` on a Timestamp Series in my case), doing mean, median, and quantile operations on that series returns inconsistent values. Example:\n\n```\nIn [68]: s\nOut[68]: \n0   00:00:14.020705\n1   00:00:14.020705\n2   00:00:14.020705\n3   00:00:28.041410\n4   00:00:28.041410\n5   00:00:42.062116\n6   00:00:42.062116\n7   00:00:42.062116\n8   00:00:56.082821\n9   00:02:20.207052\ndtype: timedelta64[ns]\n\nIn [69]: s.mean()\nOut[69]: 42062115618.0\n\nIn [70]: type(s.mean())\nOut[70]: numpy.float64\n\nIn [71]: s.median()\nOut[71]: 35051763015.0\n\nIn [72]: type(s.median())\nOut[72]: float\n\nIn [73]: s.quantile(.95)\nOut[73]: numpy.timedelta64(102351148003,'ns')\n\nIn [74]: type(s.quantile(.95))\nOut[74]: numpy.timedelta64\n```\n\nAs you can see, `mean` and `median` return a float (although the printed Out is a little different), but `quantile` returns a single `timedelta64[ns]`.\n\nThis is with versions pandas==0.12.0 and numpy==1.7.1\n"},{"labels":["api",null,null],"text":"Decide what to do with each of these\r\n- [x] deprecate `from_csv` ~~/ change to be exactly like `read_csv`~~ -> https://github.com/pandas-dev/pandas/pull/17812\r\n- [ ] from_dict\r\n- [ ] from_records (see #8161)\r\n- [x] from_items (#18529)\r\n\r\nThis is just a suggestion: `DataFrame` constructor already supports the things that `from_dict` and `from_records` do (`from_items` is not supported by the constructor, but maybe it should be?). Also `from_csv` does the same thing as `pd.read_csv`.\r\n\r\nSince `There should be one-- and preferably only one --obvious way to do it.` maybe these methods should be removed/deprecated?\r\n\r\nI know that this would cause backward incompatibility, so maybe just a deprecation warning could be displayed until a future major release that could remove them.\r\n"},{"labels":["api",null],"text":"With 0.12 and development version it's not possible fo feed a generator to Dataframe.from_records if nrows = None, it leds to a TypeError exception see: http://stackoverflow.com/questions/18915941/create-a-pandas-dataframe-from-generator\n\nIt's no clear whats nrows do, it's not documented.\n\nI've create a PR with a simple fix without touch nrows, and two tests for generator data input.\n"},{"labels":["api"],"text":"Just using the existing configuration framework, but with a file format like matplotlib uses... See how they do it here: http://matplotlib.org/users/customizing.html.\n\nPlus, we can document all the config options in a single file.\n\nrelated #2452, #3046\n"},{"labels":["api",null],"text":"``` python\nIn [1]: import pandas as pd\n\nIn [2]: panel = pd.util.testing.makePa\npd.util.testing.makePanel    pd.util.testing.makePanel4D\n\nIn [2]: panel = pd.util.testing.makePanel()\n\nIn [3]: panel.resample(\"W\")\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-3-b71dd2bc83c6> in <module>()\n----> 1 panel.resample(\"W\")\n\n/Users/datacaliber/Dropbox/Projects/trading/python/externals/pandas/pandas/core/generic.py in resample(self, rule, how, axis, fill_method, closed, label, convention, kind, loffset, limit, base)\n   2138                               fill_method=fill_method, convention=convention,\n   2139                               limit=limit, base=base)\n-> 2140         return sampler.resample(self)\n   2141\n   2142     def first(self, offset):\n\n/Users/datacaliber/Dropbox/Projects/trading/python/externals/pandas/pandas/tseries/resample.pyc in resample(self, obj)\n     99             return obj\n    100         else:  # pragma: no cover\n--> 101             raise TypeError('Only valid with DatetimeIndex or PeriodIndex')\n    102\n    103         rs_axis = rs._get_axis(self.axis)\n\nTypeError: Only valid with DatetimeIndex or PeriodIndex\n```\n\nWould expect this to default to the major axis. \n"},{"labels":["api",null,null],"text":"df = P.DataFrame({'a':[1,1,2,2,3,3]})\ndf.eval(\"a.mean()\")\n\nraise:\nNotImplementedError: 'Call' nodes are not implemented\n\ni hope this will be possible\nthank's \nGla\n"},{"labels":["api",null,null,null,null],"text":"see #4850 \n\n`Series(range(5))[3.0]`\n\nis accepted in 0.13, but ought to be deprecated (and warned)\n\nideally raise a `TypeError` after some transition period\n\nnote this does not really affect positional indexing fallback per se, \n"},{"labels":["api",null,null],"text":"`datetime.time` objects cause `to_json` to fail, presumably because they are attempting to be converted in the same way as `datetime.datetime` objects. \n\nFor example:\n\n```\n>>> pd.DataFrame(data=dict(a = [datetime.time()])).to_json()\n...\nOverflowError: Maximum recursion level reached\n```\n\nIn contrast: \n\n```\n >>> pd.DataFrame(data=dict(a = [datetime.datetime.now()])).to_json()\n'{\"a\":{\"0\":1379498870147}}'\n```\n\nTested on 0.12 and HEAD (b039c1d33722bfdac8c665f9b1f3af44782e0e58)\n"},{"labels":["api",null],"text":"Currently `Panel.shift` has its own code-path that is 4 years old. Change it to use the generic `shift` code found in `generic.py`/`internals.py`.\n"},{"labels":["api",null,null],"text":"`tshift` is broken for `Panel` because `Panel.shift` does not handle `freq`.\n"},{"labels":["api",null,null],"text":"At the moment you have to use MI.from_arrays or MI.from_tuples\n\nI think it would be nicer to be able to call MultiIndex directly (and have an orient argument, similar to some other functions). Perhaps even with a little guess work to decide what the user wants similar to other contructors.\n"},{"labels":["api",null,null],"text":"The `Series` class in 0.10 had a `real` and  a `imag` attribute but in 0.12dev these attrs are missing. This probably happend during the refactoring of the Series class for 0.12.\n\n``` python\nIn [1]: pd.__version__\nOut[1]: '0.10.0'\n\nIn [2]: a = np.arange(5)\n\nIn [3]: b = pd.Series(a + 4j*a)\n\nIn [4]: b.real\nOut[4]: \n0    0\n1    1\n2    2\n3    3\n4    4\n```\n\n``` python\nIn [1]: pd.__version__\nOut[1]: '0.12.0-381-g15d5f0a'\n\nIn [2]: a = np.arange(5)\n\nIn [3]: b = pd.Series(a + 4j*a)\n\nIn [4]: b.real\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-4-ec5ddb077f9e> in <module>()\n----> 1 b.real\n\n/usr/lib64/python2.7/site-packages/pandas-0.12.0_381_g15d5f0a-py2.7-linux-x86_64.egg/pandas/core/generic.pyc in __getattr__(self, name)\n   1287             return self[name]\n   1288         raise AttributeError(\"'%s' object has no attribute '%s'\" %\n-> 1289                              (type(self).__name__, name))\n   1290 \n   1291     def __setattr__(self, name, value):\n\nAttributeError: 'Series' object has no attribute 'real'\n```\n"},{"labels":["api",null],"text":"Don't have time to implement this, but I wanted to float the idea and park it. It's pretty trivial and you can achieve the same thing with filter, but it might be nice if drop had a regex keyword. E.g., these would be equivalent\n\n```\ndf = df.filter(regex=\"^(?!var_start)\")\ndf = df.drop(regex=\"^var_start\", axis=1)\n```\n"},{"labels":["api",null,null,null,null,null],"text":"discussed in #4698\n\n`convert_data` would return an appropriate Converter object to do various types of conversions. Mainly useful when you have to chunk both sides of these.\n\n```\ncsv -> hdf\nsql -> hdf\nhdf -> csv\n```\n\n```\nc = pd.convert_data('csv','hdf', chunksize=....)\nc.read_csv(input_path. ......)\nc.to_hdf(output_path, key.....)\nc.execute()\n```\n\npretty simple under the hood...\nbiggest issue is MAY need to read the input file twice (for read_csv) (if ints are detected in the first input chunk), because the dtype MAY change (e.g. you get a NaN later in later chunks)\n\nuseful?\n"},{"labels":["api",null,null],"text":"I think expected result of this is with multiindex column...\n\n```\ndf = pd.DataFrame({'PRICE': {pd.Timestamp('2011-01-06 10:59:05', tz=None): 24990,\n  pd.Timestamp('2011-01-06 12:43:33', tz=None): 25499,\n  pd.Timestamp('2011-01-06 12:54:09', tz=None): 25499},\n 'VOLUME': {pd.Timestamp('2011-01-06 10:59:05', tz=None): 1500000000,\n  pd.Timestamp('2011-01-06 12:43:33', tz=None): 5000000000,\n  pd.Timestamp('2011-01-06 12:54:09', tz=None): 100000000}})\n\n\nIn [2]: g = df.groupby('PRICE')\n\nIn [3]: df.groupby('PRICE').describe()  # expected .unstack(1)\nOut[3]: \n             PRICE        VOLUME\nPRICE                           \n24990 count      1  1.000000e+00\n      mean   24990  1.500000e+09\n      std      NaN           NaN\n      min    24990  1.500000e+09\n      25%    24990  1.500000e+09\n      50%    24990  1.500000e+09\n      75%    24990  1.500000e+09\n      max    24990  1.500000e+09\n25499 count      2  2.000000e+00\n      mean   25499  2.550000e+09\n      std        0  3.464823e+09\n      min    25499  1.000000e+08\n      25%    25499  1.325000e+09\n      50%    25499  2.550000e+09\n      75%    25499  3.775000e+09\n      max    25499  5.000000e+09\n\n```\n\ncc #4740 @jreback \n\nI guess it just applies method/attribute from df:\n\n```\nIn [4]: pd.DataFrame.foo = 1\n\nIn [5]: g.foo\nOut[5]: \nPRICE\n24990    1\n25499    1\ndtype: int64\n\nIn [6]: pd.DataFrame.bar = lambda x: 1\n\nIn [7]: g.bar()\nOut[7]: \nPRICE\n24990    1\n25499    1\ndtype: int64\n```\n\nshould we just special case describe? :s\n"},{"labels":["api",null,null],"text":"for:\n\n`min,max,sum,var,std,kurt,median,mad,prod,skew`\n\nThe big reason to do this is a performance hit when you happen to have an object column. and you don't realize it.\n\nSee: http://stackoverflow.com/questions/18701569/pandas-dataframe-mean-very-slow-how-can-i-calculate-means-of-columns-faster\n\n`_reduce` in Frame already supports this (and Series will ignore it), so should be trvial to implement\n\nalternatively could just pass `**kwds` thru (I think we had an issue about this earlier)\n"},{"labels":["api"],"text":"```\nIn [5]: s = Series(list('abc'))\n\nIn [6]: s.isin('a')\nOut[6]:\n0     True\n1    False\n2    False\ndtype: bool\n\nIn [5]: s = Series(list('abc'))\n\nIn [6]: s.isin('a')\nOut[6]:\n0     True\n1    False\n2    False\ndtype: bool\n\nIn [7]: s = Series(['aaa', 'b', 'c'])\n\nIn [8]: s.isin('aaa')\nOut[8]:\n0    False\n1    False\n2    False\ndtype: bool\n```\n\nI would expect:\n\n```\nIn [7]: s = Series(['aaa', 'b', 'c'])\n\nIn [8]: s.isin('aaa')\nOut[8]:\n0    True\n1    False\n2    False\ndtype: bool\n```\n\nor a raise....but raising seems inconsistent with the fact that single strings work....\n\nthis is happening bc the iterable passed in is turned into a set which is why a single string works\n"},{"labels":["api",null],"text":"http://stackoverflow.com/questions/18558429/create-a-new-dataframe-column-from-an-existing-one\n"},{"labels":["api",null],"text":"E.g. for reindex on panel\n\n```\nelp on method reindex in module pandas.core.generic:\n\nreindex(self, *args, **kwargs) method of pandas.core.panel.Panel instance\n    Conform DataFrame to new index with optional filling logic, placing\n    NA/NaN in locations having no value in the previous index. A new object\n    is produced unless the new index is equivalent to the current one and\n    copy=False\n\n    Parameters\n    ----------\n    axes : array-like, optional (can be specified in order, or as keywords)\n        New labels / index to conform to. Preferably an Index object to\n        avoid duplicating data\n    method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n        Method to use for filling holes in reindexed DataFrame\n        pad / ffill: propagate last valid observation forward to next valid\n        backfill / bfill: use NEXT valid observation to fill gap\n    copy : boolean, default True\n        Return a new object, even if the passed indexes are the same\n    level : int or name\n        Broadcast across a level, matching Index values on the\n        passed MultiIndex level\n    fill_value : scalar, default np.NaN\n        Value to use for missing values. Defaults to NaN, but can be any\n        \"compatible\" value\n    limit : int, default None\n        Maximum size gap to forward or backward fill\n    takeable : boolean, default False\n        treat the passed as positional values\n\n    Examples\n    --------\n```\n\nFunctions that need parameters list/explanation:\n- [x] swapaxes\n- [x] pop [what does 'items' mean?]\n\nFunctions that only need list of axes:\n- [x] filter [need to check if it should be defined for Series at all]\n- [x] fillna [Series, Frame]\n- [x] reindex_axis [needs class name + axes]\n- [x] rename [might need a bit more logic to do docstring + note that args interpreted like axes??]\n- [x] transpose\n\nFunctions needing a little work:\n- [x] reindex\n  - needs to have panel check that both major and major_axis aren't passed, same with minor and minor_axis (like `SparsePanel` does)\n  - would be better if there were individual functions for Series Frame and Panel that explicitly enumerate axes in type signature.\n\nFunctions that may not need to be changed:\n- [x] align\n\nFunctions that could use slight wording tweaks:\n- [x] truncate\n- [x] where\n- [x] mask\n"},{"labels":["api",null],"text":"closes #4563\n\nWhy does read_excel take a `kind` parameter at all? It gets passed to the `ExcelFile` constructor, set on the object and then (as far as I can tell) never gets used for anything.\n\nThis should either be removed from the signature (which doesn't affect anything, because it will be accepted into kwds anyways) or added into the docs and then actually used for something.\n\nif we remove it, just need to add a:\n\n``` python\n# Remove this branch in 0.14\nif 'kind' in kwargs:\n   warnings.warn(\"Kind is deprecated and will be removed in a future version of pandas\")\n   kwargs.pop('kind')\n```\n\nWas not previously being used for anything anyways.\n"},{"labels":["api",null],"text":"Seems to lose a row when passing a list as header, check it out:\n\n```\n>>> from StringIO import StringIO\n>>> import pandas\n>>> text = \"\"\"\na  a  a  b  c  c\nq r s t u v\n1 2 3 4 5 6\n1 2 3 4 5 6\n\"\"\"\n>>> # expected behavior without header\n>>> pandas.read_csv(StringIO(text.strip()), sep='\\s+')\n   a a.1 a.2  b  c c.1\n0  q   r   s  t  u   v\n1  1   2   3  4  5   6\n2  1   2   3  4  5   6\n>>> #drops row  with specified header\n>>> pandas.read_csv(StringIO(text.strip()), sep='\\s+', header=[0, 1])\n   (a, q)  (a, r)  (a, s)  (b, t)  (c, u)  (c, v)\n0       1       2       3       4       5       6\n>>> pandas.read_csv(StringIO(text.strip()), sep='\\s+', header=[0, 1], tupleize_cols=False)\n   a        b  c\n   q  r  s  t  u  v\n0  1  2  3  4  5  6\n```\n"},{"labels":["api",null],"text":"see https://github.com/pydata/pandas/issues/4679#issuecomment-23410759\n"},{"labels":["api",null,null],"text":"The following code\n\n```\nimport numpy as np\nimport pandas as pd\nhor= [str(k) for k in range(3)]\ncolumns=[ 'A','B','C', 'D' ]\nind1=np.repeat(hor, len(columns))\nind2=np.tile(columns,4)\ndf=pd.DataFrame(index=[ind1,ind2],columns=columns)        \nprint(df)\nprint(df.xs('A',level=1))\n```\n\ngenerates an error when trying to trying to slice the dataframe. The slicing syntax is correct, but the issue appears because the dataframe is invalid:\nlen(ind1)= 12 and len(ind2)=16.\nI think an error should be generated upon creation of the dataframe.\n"},{"labels":["api",null,null],"text":"See the table here: http://www.sec.gov/Archives/edgar/data/47217/000104746913006802/a2215416z10-q.htm#CCSE\n\nthe trailing \")\" and the leading \"$\" are in different columns (aka td's / cells) from the number..\n\nThere should be an option to merge all column under a given heading (#4679).\n"},{"labels":["api",null,null,null],"text":"related #4468\n\nAdd keyword argument to ExcelFile parser to take an integer / list for the rows to interpret as header rows. If more than one, interpret as hierarchical columns / MultiIndex.\n\nPresumably this would also allow you to round-trip  Data Frame with hierarchical columns.\n\nBasically, if you have something spanning two columns, just converts to two cells with the data in the original cell, ending up just like what you need for csv reader.\n"},{"labels":["api",null,null,null],"text":"http://stackoverflow.com/questions/18411110/python-pandas-timeseries-resample-giving-unexpected-results/18411516?noredirect=1#comment27046758_18411516\n"},{"labels":["api",null,null],"text":"similar to `parse_dates`\n"},{"labels":["api",null,null,null],"text":"if passing a `dtype= { 'column_that_is_a_timedelta' : lambda x: the_td64_parser_of_doom(x) }`\n\nthis could provide a specific dtype parser for a column\n\nalternatively, maybe provide a `parse_timedelta` option?\n"},{"labels":["api",null,null],"text":"I'm not sure what a good word for this is (count is taken, order means sort)!\n\nBut it's quite an often used thing to create a column which enumerates the items in each group / counts their occurrences.\n\nYou can hack it:\n\n```\nIn [1]: df = pd.DataFrame([[1, 2], [2, 3], [1, 4], [1, 5], [2, 6]])\n\nIn [2]: g = df.groupby(0)\n\nIn [3]: g.apply(lambda x: pd.Series(np.arange(len(x)), x.index))\nOut[3]:\n0    0\n1    0\n2    1\n3    2\n4    1\ndtype: int64\n\nIn [5]: df['order'] = _\n\nIn [6]: df\nOut[6]:\n   0  1  order\n0  1  2      0\n1  2  3      0\n2  1  4      1\n3  1  5      2\n4  2  6      1\n```\n\n_I've seen this in a few SO questions, here's just [one](http://stackoverflow.com/questions/18392283/pandas-return-first-item-in-dataframe-grouped-by-user)._\n\ncc @cpcloud (and I've seen @jreback answer a question with this)\n"},{"labels":["api",null,null],"text":"Not sure if this will cause weirdness but haveing the arithmetic operations able to accept an index (when appropriate type), e.g. a datecolumn should be able to interact with a DatetimeIndex, but prob should prohibit an int column from interacting with a IntIndex?\n\ne.g. in below `df['date']-df.index` should work\n\nThis actually DOES work now! (and I take it back, does make some sense)\nshould `df[1]-df.reset_index().index` work????\n\nhttp://stackoverflow.com/questions/18368442/how-to-directly-use-pandas-date-time-index-in-calculations/18368793?noredirect=1#comment26971689_18368793\n\n```\nIn [1]: df = DataFrame(randn(5,2),index=date_range('20130101',periods=5))\n\nIn [3]: df['date'] = Timestamp('20130102')\n\nIn [4]: df\nOut[4]: \n                   0         1                date\n2013-01-01  2.406932 -0.313473 2013-01-02 00:00:00\n2013-01-02  0.034162 -0.708450 2013-01-02 00:00:00\n2013-01-03 -1.585031  0.587243 2013-01-02 00:00:00\n2013-01-04  1.454818  1.089208 2013-01-02 00:00:00\n2013-01-05 -0.778016 -0.994242 2013-01-02 00:00:00\n\nIn [5]: df['td'] = df['date']-df.index.to_series()\n\nIn [6]: df\nOut[6]: \n                   0         1                date                td\n2013-01-01  2.406932 -0.313473 2013-01-02 00:00:00  1 days, 00:00:00\n2013-01-02  0.034162 -0.708450 2013-01-02 00:00:00          00:00:00\n2013-01-03 -1.585031  0.587243 2013-01-02 00:00:00 -1 days, 00:00:00\n2013-01-04  1.454818  1.089208 2013-01-02 00:00:00 -2 days, 00:00:00\n2013-01-05 -0.778016 -0.994242 2013-01-02 00:00:00 -3 days, 00:00:00\n```\n"},{"labels":["api"],"text":"Change up `MultiIndex.rename` to be an alias for `MultiIndex.set_names`.\n- `Index.rename` : takes single object \n- `MultiIndex.set_names`, `Index.set_names` - same type signature: take list of names [Index.set_names can only take iterable of length 1)\n- `MultiIndex.rename` - synonym for `set_names`\n\nSo if you want to do something generically, you should use `set_names` (just like you would with other attributes). If you expect a non-MI index, then you could use `rename` with a string, etc.\n\ncc @jreback \n\nI personally think that this behavior is how it should work  for Index. Everyone okay with this?  I'd prefer not to do type sniffing if at all possible.\n\n``` python\nind = Index(range(10))\nind.rename([\"apple\"], inplace=True)\nprint(ind.name) # ['apple']\nprint(ind.names) # [['apple']]\nind.set_names('apple') # Fails with error that it's too long\n```\n"},{"labels":["api",null],"text":"maybe `dropna='` as default (current behavior), and allow values: `None`,`all`,False, where `all` is equivalent of `None` (current behavior), maybe settable by a `set_option` as well\n\nrelated: https://github.com/pydata/pandas/pull/3013\n"},{"labels":["api",null,null],"text":"I've thrown in a few functions which I think are similar (but I've seen a few times asked about). e.g. this [SO question](http://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series).\n\nUser wants to get the index where a criteria is true:\n\n```\nimport pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\nprint myseries.find(7) # should output 3\n```\n\nI guess the usual way is to use:\n\n```\ns[msk].index\ns[msk].index[0]\n```\n\nOr as @jreback suggests (in this simple example):\n\n```\npd.index(s).get_loc(7)\n```\n\nPerhaps there should be some methods to grab these out efficiently (i.e. short-circuit in cython)... maybe it makes more sense to have a crit function rather than a mask.\n\ncc @cpcloud \n\n_also similar to idxmax and idxmin._\n"},{"labels":["api",null,null],"text":"If I try to rename a MultiIndexed DataFrame, everything works correctly:\n\n``` python\nIn [3]: pd.DataFrame([11,21,31],\n            index=pd.MultiIndex.from_tuples([(\"A\",x) for x in [\"a\",\"B\",\"c\"]])).rename(str.lower)\nOut[3]: \n      0\na a  11\n  b  21\n  c  31\n```\n\nBut if I do the same thing with the Series, it doesn't work:\n\n``` python\nIn [4]: pd.Series([11,21,31],\n            index=pd.MultiIndex.from_tuples([(\"A\",x) for x in [\"a\",\"B\",\"c\"]])).rename(str.lower)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-4-aa25b537c1df> in <module>()\n----> 1 pd.Series([11,21,31],index=pd.MultiIndex.from_tuples([(\"A\",x) for x in [\"a\",\"B\",\"c\"]])).rename(str.lower)\n\n/home/viktor/pandas/pandas/core/series.py in rename(self, mapper, inplace)\n   3187         result = self if inplace else self.copy()\n   3188         result.index = Index([mapper_f(x)\n-> 3189                              for x in self.index], name=self.index.name)\n   3190 \n   3191         if not inplace:\n\nTypeError: descriptor 'lower' requires a 'str' object but received a 'tuple'\n```\n\nThis behavior is observed in 0.12 and in the master branch.\n"},{"labels":["api",null],"text":"Currently the `flavor` argument to `read_html` must either be `None` or a string or a container of strings. This make is difficult for the user to write his own instance of `_HtmlFrameParser` and to use that when parsing. I suggest also allowing the user to pass in a subclass of `_HtmlFrameParser` in place of a string.\n\nImplementation wise, this should be easy. `_parser_dispatch` would be modified to for this case. Likewise `_validate_parser_flavor` would need to allow this.\n"},{"labels":["api",null],"text":"When concat'ing DataFrames, the column names get alphanumerically sorted *if* there are any differences between them. If they're identical across DataFrames, they don't get sorted.\r\nThis sort is undocumented and unwanted. Certainly the default behavior should be no-sort. EDIT: the standard order as in SQL would be: columns from df1 (same order as in df1), columns (uniquely) from df2 (less the common columns) (same order as in df2). Example:\r\n\r\n``` python\r\ndf4a = DataFrame(columns=['C','B','D','A'], data=np.random.randn(3,4))\r\ndf4b = DataFrame(columns=['C','B','D','A'], data=np.random.randn(3,4))\r\ndf5  = DataFrame(columns=['C','B','E','D','A'], data=np.random.randn(3,5))\r\n\r\nprint \"Cols unsorted:\", concat([df4a,df4b])\r\n# Cols unsorted:           C         B         D         A\r\n\r\nprint \"Cols sorted\", concat([df4a,df5])\r\n# Cols sorted           A         B         C         D         E\r\n``'\r\n```\r\n"},{"labels":["api",null,null],"text":"`to_hdf(....., append=True,table=True)` raising, should accept this\n\nIs there a way to use DataFrame.to_hdf() so that data can be appened the data? Somehow to_hdf(filename, table_name, mode='a', table=True) overwrites the data while to_hdf(filename, table_name, table=True, append=True) gives the following error:\n\n> Traceback (most recent call last):\n>   File \"M:/Projects/PortfolioAnalytics/Code/Python/attribution/TopContributors.py\", line 1035, in <module>\n>     portfolio_data.to_hdf(filename, OUTPUT_NAME_PORTFOLIO, table=True, append=Settings.TC_OUTPUT_APPEND)\n>   File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\generic.py\", line 521, in to_hdf\n>     return pytables.to_hdf(path_or_buf, key, self, *_kwargs)\n>   File \"C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py\", line 194, in to_hdf\n>     f(store)\n>   File \"C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py\", line 188, in <lambda>\n>     f = lambda store: store.append(key, value, *_kwargs)\n>   File \"C:\\Python27\\lib\\site-packages\\pandas\\io\\pytables.py\", line 658, in append\n>     self._write_to_group(key, value, table=True, append=True, **kwargs)\n> TypeError: _write_to_group() got multiple values for keyword argument 'table'\n"},{"labels":["api",null,null],"text":"Would prob need to be done in cython for perf\n\n```\ndef cross_apply(func, is_symmetric=True, **kwargs):\n   l = len(df.columns)\n   results = np.zeros((l,l))\n\n   if is_symmetric:\n      for i in range(l):\n          for j in range(i + 1):\n                 results[i,j] = results[j,i] = func(ac,bc, **kwargs)\n   else:\n      for i, ac in enumerate(df):\n          for j, bc in enumerate(df):\n                 if is_symmetric:\n                 results[j,i] = func(ac,bc, **kwargs)\n    return DataFrame(results,index=df.columns,columns=df.columns)\n```\n\nhttp://stackoverflow.com/questions/18234440/pairwise-correlation-of-pandas-dataframe-columns-with-custom-function\n"},{"labels":["api",null,null,null],"text":"I think an implementation of a general recurrent relation might be interesting. These\nare not able to evaluated in numpy/numexpr directly\n\nsee here:\nhttp://stackoverflow.com/questions/9984373/calculate-two-coupled-equation-with-numpy\nhttp://stackoverflow.com/questions/21336794/python-recursive-vectorization-with-timeseries/21338665\nhttp://stackoverflow.com/questions/23996260/python-pandas-using-the-previous-value-in-dataframe/23997030#23997030\n\n```\nR[i<=k] = start(i) \nR[i] = R[i-k]*a(i) + b(i) \n```\n\nin the case where a(i) and b(i) are constants then this can be completely cythonized \nif a and/or b are passed functions then the shell can still be cythonized (with the function called at each iteration) \n\nExample\n\n```\nvalues = df['input']\n\nIn [62]: result = np.empty(len(values))\n\nIn [65]: result[0] = values[0]\n\nIn [66]: for i in xrange(len(result)):\n   ....:     if i > 0:\n   ....:         result[i] = result[i-1]*.8\n   ....:         \n\nIn [67]: result\nOut[67]: array([ 5.    ,  4.    ,  3.2   ,  2.56  ,  2.048 ,  1.6384])\n```\n\nI propose that pandas have a `recurrence` wrapper (that can then call a pure constant cython version and a cython version that takes functions)\nProbably just for a 1-dim array atm\n\nIn theory this can also be done by a `rolling_apply` that has memory (or I guess that is sort of the same thing)\n\nrelated #4546\nhttps://groups.google.com/forum/#!topic/pydata/0MCWhwurOWs\n"},{"labels":["api",null,null],"text":"```\nIn [110]: x = Timestamp('20130101')\n```\n\nLast day of month\n\n```\nIn [111]: pd.tslib.monthrange(x.year,x.month)[1]==x.day\nOut[111]: False\n```\n\nFirst day of month\n\n```\nIn [112]: x.day==1\nOut[112]: True\n```\n\nFirst day of year\n\n```\nIn [113]: x.day==1 & x.month==1\nOut[113]: True\n```\n\nhttp://stackoverflow.com/questions/18233107/pandas-convert-datetime-to-end-of-month/18233876?noredirect=1#comment26732170_18233876\n"},{"labels":["api",null,null],"text":"The following code exporting a dataframe with a timeseries index with np.datetime64 type\n\n``` python\nimport numpy as np\nimport pandas as pd\n\ntimestamps = pd.date_range(np.datetime64('2013-01-01 11:00:00.123456'), periods=5, freq='123U')\ndataframe = pd.DataFrame(np.random.randn(len(timestamps)), index=timestamps)\ndataframe.to_stata('out')\n```\n\nthrows a\n\n```\nValueError: Data type datetime64[ns] not currently understood. Please report an error to the developers.\n```\n\nIt would be very nice if np.datetime64 (and thus pd.Timestamp as well) would be supported here.\n"},{"labels":["api"],"text":"The corr()  method attached to DataFrames is a great way to get a matrix of correlation coefficients. But for the Kendall and Spearman options, the p-value is discarded:\n\nhttps://github.com/pydata/pandas/blob/8ac0e11b59b65f0ac898dca2beebde5f87836649/pandas/core/nanops.py#L510-L517\n\nFor a function called corr() this would certainly be expected behavior, but the problem remains that there is no other method (as far as I've been able to find) to get those significance values without going back to scipy and doing it myself.\n"},{"labels":["api",null,null],"text":"related #3281\n\nCreate a read_pdf method in IO tools for reading tables from PDF documents. Many data sets are released in PDF form.\n\nFor example:\n- Walmart's Historical Unit Count and Square Footage: http://az204679.vo.msecnd.net/media/documents/unit-counts-q1-fy14_130131488115936836.pdf\n- Las Vegas Visitor Statistics: http://www.lvcva.com/includes/content/images/media/docs/ES-YTD20128.pdf\n- Coffee export data: http://www.ico.org/historical/2010-19/PDF/EXPCALY.pdf\n\nThere are a number of standalone tools, projects for this:\n- http://ieg.ifs.tuwien.ac.at/projects/pdf2table/ (an academic project /paper; Java)\n- https://pypi.python.org/pypi/pdfquery (Python lib)\n- https://pypi.python.org/pypi/pdftable (Python lib)\n- http://www.unixuser.org/~euske/python/pdfminer/  (Python lib)\n- https://github.com/ashima/pdf-table-extract (Python lib)\n\nThere are also a number of site / projects to convert PDF to HTML:\n- https://github.com/coolwanglu/pdf2htmlEX/wiki (open source)\n"},{"labels":["api"],"text":"Using Pandas 0.12.0\n\nIt's hard to think of why a DataFrame would allow a `regex=True` replace method, and a Series wouldn't. This must just be an oversight, right?\n\n```\n>>> test = pd.DataFrame(['hello','goodbye','thankyou'],columns=['to_replace'])\n>>> test\n  to_replace\n0      hello\n1    goodbye\n2   thankyou\n>>> test.replace(\"he\", \"je\")\n  to_replace\n0      hello\n1    goodbye\n2   thankyou\n>>> test.replace(\"he\", \"je\", regex=True)\n  to_replace\n0      jello\n1    goodbye\n2   thankyou\n>>> test['to_replace'].replace(\"he\", \"je\", regex=True)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: replace() got an unexpected keyword argument 'regex'\n```\n"},{"labels":["api",null],"text":"This may be seen like a weird function call but it is actually triggered when plotting with development versions of matplotlib and the case is explicitly (and wrongly as it seems) by pandas:\n\n```\n>>> import pandas as pd\n>>> x = pd.Series(range(5))\n>>> x.reshape(x.shape)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/pymodules/python2.7/pandas/core/series.py\", line 981, in reshape\n    return ndarray.reshape(self, newshape, order)\nTypeError: an integer is required\n```\n\nIt is easily fixed just replacing the offending line by this one:\n\n`return ndarray.reshape(self, newshape, order=order)`\n"},{"labels":["api",null,null],"text":"It would work similar to drop_duplicates (same interface), but would only remove consecutive duplicates (like Unix's uniq does).  So to in order to remove all duplicates with this function, the DataFrame would need to be sorted by the same criteria.  This function can be useful when order is important and non-consecutive duplicates convey useful information.\n"},{"labels":["api",null,null],"text":"For 0.14 or later, after #4039 is merged this becomes a todo item for the future.\n"},{"labels":["api",null,null,null],"text":"```\nIn [10]: df = DataFrame(dict(time = [Timestamp('20130101 9:01'),Timestamp('20130101 9:02')],value=[1.0,2.0]))\n\nIn [11]: df\nOut[11]: \n                 time  value\n0 2013-01-01 09:01:00      1\n1 2013-01-01 09:02:00      2\n```\n\nworks on series\n\n```\nIn [12]: df.time.diff()\nOut[12]: \n0        NaT\n1   00:01:00\nName: time, dtype: timedelta64[ns]\n```\n\nBroken with a datelike dtype inclded\n\n```\nIn [13]: df.diff()\nValueError: cannot include dtype 'm' in a buffer\n```\n"},{"labels":["api",null,null,null],"text":"Trying to print a data frame as plain, strict tsv (i.e., no quoting and no escaping, because I know none the fields will contain tabs), I wanted to use the \"quoting\" option, which is documented in pandas and is passed through to csv, as well as the \"quotechar\" option, not documented in pandas but also a csv option. But it doesn't work:\n\n``` python\nIn [1]: import sys, csv\n\nIn [2]: from pandas import DataFrame\n\nIn [3]: data = {'col1': ['contents of col1 row1', 'contents \" of col1 row2'], 'col2': ['contents of col2 row1', 'contents \" of col2 row2'] }\n\nIn [4]: df = DataFrame(data)\n\nIn [5]: df.to_csv(sys.stdout, sep='\\t', quoting=csv.QUOTE_NONE, quotechar=None)\n        col1    col2\n0       contents of col1 row1   contents of col2 row1\n---------------------------------------------------------------------------\nError                                     Traceback (most recent call last)\n<ipython-input-5-a30d32266fb4> in <module>()\n----> 1 df.to_csv(sys.stdout, sep='\\t', quoting=csv.QUOTE_NONE, quotechar=None)\n\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/core/frame.pyc in to_csv(self, path_or_buf, sep, na_rep, float_format, cols, header, index, index_label, mode, nanRep, encoding, quoting, line_terminator, chunksize, tupleize_cols, **kwds)\n   1409                                      tupleize_cols=tupleize_cols,\n   1410                                      )\n-> 1411         formatter.save()\n   1412\n   1413     def to_excel(self, excel_writer, sheet_name='sheet1', na_rep='',\n\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/core/format.pyc in save(self)\n    974\n    975             else:\n--> 976                 self._save()\n    977\n    978\n\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/core/format.pyc in _save(self)\n   1080                 break\n   1081\n-> 1082             self._save_chunk(start_i, end_i)\n   1083\n   1084     def _save_chunk(self, start_i, end_i):\n\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/core/format.pyc in _save_chunk(self, start_i, end_i)\n   1098         ix = data_index.to_native_types(slicer=slicer, na_rep=self.na_rep, float_format=self.float_format)\n   1099\n-> 1100         lib.write_csv_rows(self.data, ix, self.nlevels, self.cols, self.writer)\n   1101\n   1102 # from collections import namedtuple\n\n/home/brechea/.local/lib/python2.6/site-packages/pandas-0.12.0-py2.6-linux-x86_64.egg/pandas/lib.so in pandas.lib.write_csv_rows (pandas/lib.c:13871)()\n\nError: need to escape, but no escapechar set\n```\n\nAdding the parameter\n\nquotechar=kwds.get(\"quotechar\")\n\nto the\n\nformatter = fmt.CSVFormatter(...\n\ncall in to_csv(), and doing corresponding changes to format.CSVFormatter()'s **init**() and save(), produces the expected output:\n\n``` python\nIn [1]: import sys, csv\n\nIn [2]: from pandas import DataFrame\n\nIn [3]: data = {'col1': ['contents of col1 row1', 'contents \" of col1 row2'], 'col2': ['contents of col2 row1', 'contents \" of col2 row2'] }\n\nIn [4]: df = DataFrame(data)\n\nIn [5]: df.to_csv(sys.stdout, sep='\\t', quoting=csv.QUOTE_NONE, quotechar=None)\n        col1    col2\n0       contents of col1 row1   contents of col2 row1\n1       contents \" of col1 row2 contents \" of col2 row2\n```\n\ni.e., unescaped, unquoted tsv.\n\nMore generally, there could be many reasons to want more control of the underlying csv writer, so a generic mechanism (as opposed to adding each param one by one) might be called for (e.g., allowign for a csv dialect object or at least a dictionary holding dialect attributes).\n"},{"labels":["api",null,null],"text":"See for example: http://www.datazoa.com/publish/export.asp?hash=vg6p5mBl1v&uid=dzadmin&a=exportcsv, http://www.datazoa.com/publish/export.asp?hash=yjPceG6fHL&uid=dzadmin&a=exportcsv or the http://pastebin.com/S2NqhFq0.\n\nIn all cases, it is important to set `skipfooter` in order to correctly parse the CSV.\n\nI current set `skipfooter` in the above files using:\n\n```\n    data_raw = pd.io.parsers.read_csv(text,skiprows=range(1,13+1))\n    skipfooter = data_raw[\"Series title\"].index[-1]-data_raw[data_raw[\"Series title\"].isnull()].index[0]+1\n\n    ret = pd.io.parsers.read_csv(text,skiprows=range(1,13+1),skipfooter=skipfooter,parse_dates=[0],index_col=0)\n```\n\nThe issue is that the value of skipfooter is not static, but rather varies based on how much meta data the provider is placing at the end of the file.\n"},{"labels":["api",null,null,null],"text":"created by @prossahl\n\nReversing an index and forward filling succeeds but produces misleading data:\n\n```\n>>> dr = pd.date_range('2013-08-01', periods=6, freq='B')\n>>> df = pd.DataFrame(np.random.randn(6,1), index=dr, columns=list('A'))\n>>> df['A'][3] = np.nan\n>>> df.reindex(df.index[::-1], method='ffill')\n                   A\n2013-08-08  2.127302\n2013-08-07       NaN\n2013-08-06       NaN\n2013-08-05       NaN\n2013-08-02       NaN\n2013-08-01       NaN\n```\n\nThis change enforces \"monotocity\" (well actually weakly increasing) and causes a ValueError to be raised.\n"},{"labels":["api",null],"text":"from [the SO question](http://stackoverflow.com/questions/18082398/pandas-convert-upper-triangular-dataframe-by-shifting-rows-to-the-left)\n"},{"labels":["api",null,null],"text":"from [this SO question](http://stackoverflow.com/questions/18079563/finding-the-intersection-between-two-series-in-pandas)\n"},{"labels":["api",null,null,null],"text":"Currently `pandas.io.html.read_html` returns a list of DataFrames. This offers no context as to where in the source HTML the table was found. For example, a user might be interested in the title or caption of the table.\n\nFor example in an SEC 10-Q filing (see for example: http://apps.shareholder.com/sec/viewerContent.aspx?companyid=GMCR&docid=9277772#A13-6685_110Q_HTM_UNAUDITEDCONSOLIDATEDSTATEMENTSOF_223103), there are many tables. The user might be interested in one or more of them. They are not returned in a \"standard\" order such that without the title provided in the HTML document, finding the desired table in the return list is difficult. \n\nMy suggestion is to offer some way of linking the returned table to the context in which it was found.\n"},{"labels":["api",null,null,null],"text":"related #4438 #4679 \n\nFor example see the \"INCOME_STATEMENT\" sheet on http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=F7D_689jk-BEFvN&ID=9277772\n\nThere are four columns in the desired DataFrame:\n\n```\n1           2               3           4\nThirteen weeks ended        Twenty-six weeks ended  \nMarch 30,   March 24,       March 30,   March 24,\n```\n\nCurrently the `ExcelFile` parser only takes an int for the header parameter, whereas the html parser takes \"The row (or rows for a MultiIndex) to use to make the columns headers.\"\n"},{"labels":["api",null],"text":"From the PyTables ML\n\nSelect where month=5 from the index\n(this could be done internally maybe)\n\nbig issues is that Coordinates is sort of 'private' here,\nmake where take a boolean array / coordinates\n\n```\n# create a frame\nIn [45]: df = DataFrame(randn(1000,2),index=date_range('20000101',periods=1000))\n\nIn [53]: df\nOut[53]: \n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 1000 entries, 2000-01-01 00:00:00 to 2002-09-26 00:00:00\nFreq: D\nData columns (total 2 columns):\n0    1000  non-null values\n1    1000  non-null values\ndtypes: float64(2)\n\n# store it as a table\nIn [46]: store = pd.HDFStore('test.h5',mode='w')\n\nIn [47]: store.append('df',df)\n\n# select out the index (a datetimeindex in this case)\nIn [48]: c = store.select_column('df','index')\n\n# get the coordinates of matching index\nIn [49]: coords = c[pd.DatetimeIndex(c).month==5]\n\n# select those rows\nIn [51]: from pandas.io.pytables import Coordinates\n\nIn [50]: store.select('df',where=Coordinates(coords.index,None,None))\nOut[50]: \n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 93 entries, 2000-05-01 00:00:00 to 2002-05-31 00:00:00\nData columns (total 2 columns):\n0    93  non-null values\n1    93  non-null values\ndtypes: float64(2)\n```\n"},{"labels":["api",null,null],"text":"renamed `carray` package: https://github.com/Blosc/bcolz\n\nSoliciting any comments on this proposal to create a columnar access table in `HDFStore`.\n\nThis is actually very straightforward to do.\n\nneed a new kw argument to describe the type of format for\nstorage: ``format='s|t|c'` (also allows expansion in the future to other formats)\n- `s` is the `Storer` format (e.g. `store['df'] = value`), implied currently\n  with `put``\n- `t` is the `Table` format (e.g. `store.append('df',value)`, created\n  with `table=True` when using `put` or using `append`\n- `c` is a `CTable` format (new), which is a column oriented table\n\nso will essentially deprecate `append=,table=` keywords (or just translate them)\nto a `format=` kw.\n\n```\ndf.to_hdf('test.h5','df',format='c')\n```\n\nWill have a master node which holds the structure.\nWill store a format with a single column from a `DataFrame` in a sub-node of the\nmaster.\n\nadvantages:\n- index(s) are kept in their own columns (this is true with `Table` now)\n- allows easy delete/add of columns (somewhat tricky in the `Table` format)\n- allows appends (interesting twist is that have to keep the indices in sync)\n- selection is straightforward as everything is indexed the same\n- selecting a small number of columns relative to the total should be faster than an equivalent `Table`\n- API will be the same as current. This is essentially an extension of the `append_as_multiple / select_as_multiple` multi-table accssors.\n- can be included/coexist alongside existing `Table/Storer`s\n\ndisadvantages:\n- selecting lots of columns will be somewhat slower that an equivalent `Table`\n- requires syncing of all the indices (the coordinates of all rows)\n- delete operations will be somewhat slower than an equivalent `Table`\n\nThere are actually 2 different formats that could be used here, I propose just the single-file for now. However, The sub-nodes could be spread out in a directory and stored as separate files. This allows concurrent access with some concurrent reads allowed (this is pretty tricky, so hold off on this for now).\n\nThis `CTable` format will use the existing `PyTables` infrastructure under the hood; it is possible to use the `ctable` module however http://carray.pytables.org/docs/manual/ (this is basically what BLAZE uses under the hood for its storage backend)\n"},{"labels":["api",null,null],"text":"I will submit PR\n"},{"labels":["api",null,null],"text":"This is and API change in  numpy >= 1.7, but need to work around. Numpy now\nlooks at the structure for a p`rod` method (if the function is not operating on a numpy array), if it has one, then it passes the arguments\n`axis,dtype,out`, currently these types of methods only aceept `axis`, need to accept `dtype,out` as dummy arguments for compat\n\naffects `mean,sum,prod,std`...prob others\n\n```\nIn [1]: df = DataFrame(randn(10,2))\n\nIn [2]: np.prod(df)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-2-99feb18ea783> in <module>()\n----> 1 np.prod(df)\n\n/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc in prod(a, axis, dtype, out, keepdims)\n   2111             return _methods._prod(a, axis=axis, dtype=dtype,\n   2112                                 out=out, keepdims=keepdims)\n-> 2113         return prod(axis=axis, dtype=dtype, out=out)\n   2114     else:\n   2115         return _methods._prod(a, axis=axis, dtype=dtype,\n\nTypeError: prod() got an unexpected keyword argument 'dtype'\n\nIn [3]: np.prod(df.values)\nOut[3]: -2.2566177751865827e-07\n\nIn [4]: np.prod(df.values,axis=1)\nOut[4]: \narray([-0.00912739,  0.00569453,  0.58508784,  0.87462665, -0.32556754,\n        0.12015118,  1.01860104, -0.15380337, -0.54518287, -2.5393832 ])\n```\n"},{"labels":["api",null,null,null],"text":"related #1892, #1479\n\nIs there any interest in giving interpolate and resample (to higher frequency) some additional methods?\n\nFor example:\n\n``` python\nfrom scipy import interpolate\ndf = pd.DataFrame({'A': np.arange(10), 'B': np.exp(np.arange(10) + np.random.randn())})\nxnew = np.arange(10) + .5\n\nIn [46]: df.interpolate(xnew, method='spline')\n```\n\nCould return something like\n\n``` python\nIn [47]: pd.DataFrame(interpolate.spline(df.A, df.B, xnew, order=4), index=xnew)\nOut[47]: \n               0\n0.5     1.044413\n1.5     0.798392\n2.5     3.341909\n3.5     8.000314\n4.5    22.822819\n5.5    60.957659\n6.5   166.844351\n7.5   451.760621\n8.5  1235.969910\n9.5     0.000000  # falls outside the original range so interpolate.spline sets it to 0.\n```\n\nI have never used the DataFrame's interpolate, but a quick glance says that something like the above wouldn't be backwards compatible with the current calling convention.  Maybe a different name?  This may be confusing two issues: interpolating over missing values and interpolating / predicting non-existent values.  Or are they similar enought that they can be treated the same.  I would think so.\n\nThese are just some quick thoughts before I forget.  I haven't spent much time thinking a design through yet. I'd be happy to work on this in a month or so.\n\nAlso does this fall in the realm of statsmodels?  \n"},{"labels":["api",null],"text":"Has the json module always been here (I thought it was only np and datetime)?\n\n```\npd.json\n```\n\nShould it be? confusingly I can't actually see where it's coming from...\n"},{"labels":["api"],"text":"From [this](http://stackoverflow.com/questions/17979327/drop-rows-with-multiple-keys-in-pandas/17979614?noredirect=1#comment26286280_17979614) SO question.\n\nThe user had two dataframes and wanted to use the second as the values argument to `isin`. \n\nWe'd need to think about how to handle the other index.  In this case the user only cared about the columns, not the index labels.\n\nPrevious issues/PRs: #4258 and #4211 \n"},{"labels":["api",null],"text":"Testing and playing with HDFStores a lot, I have found several times that a store.close() leaves things hanging, like this:\n\n``` python\nstore.close()\n# no output\nstore\nClosedFileError: the file object is closed\ndf.to_hdf('test.h5','df',data_columns=['A','B'],mode='w',table=True)\nValueError: The file 'test.h5' is already opened.  Please close it before reopening in write mode.\n# The store was opened with 'test.h5' as filename.\n```\n\npandas version 0.12.0\n"},{"labels":["api",null,null,null,null],"text":"see #4337\n"},{"labels":["api",null],"text":"A new module `core.align` would hold a couple of generic alignment functions, one of which will be `align_multiple` or possibly `align2` (taking 2 pandas objects) and then `align` would take a sequence of pandas objects to align.\n\nNeed to think about the API a bit more.\n\nPsuedocode-ish:\n\n``` python\ndf = DataFrame(...)\ndf2 = DataFrame(...) # possibly different index and columns than df\ns = Series(...)\nresult = pd.align([df, df2, s], how='outer') # compute the union of the indexes\n\n# smoke test 1\nresult.columns == df.columns.join(df2.columns, how='outer').join(s.index, how='outer')\nresult.index == df.index.join(df2.index, how='outer')\n\nresult2 = pd.align([df, df2, s], how='inner') # intersection\n\n# smoke test 2\nresult2.columns == df.columns.join(df2.columns).join(s.index)\nresult2.index == df.index.join(df2.index)\n```\n\ninput with `Index` types would return the union/intersection of the objects. not clear what would happen if aligning an `Index` and another non-`Index` pandas object\n"},{"labels":["api",null,null],"text":"```\nIn [11]: s = True\n\nIn [12]: df = DataFrame(rand(10, 5)> 0.5)\n\nIn [13]: df\nOut[13]:\n       0      1      2      3      4\n0   True  False  False   True  False\n1   True  False  False   True  False\n2  False  False   True  False  False\n3   True  False  False   True  False\n4  False  False   True   True  False\n5  False  False  False   True   True\n6   True   True  False   True   True\n7   True   True  False  False   True\n8   True   True   True   True  False\n9  False  False   True  False  False\n\nIn [14]: s & df\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-14-cab092ff8433> in <module>()\n----> 1 s & df\n\nTypeError: unsupported operand type(s) for &: 'bool' and 'DataFrame'\n\nIn [15]: df & s\nOut[15]:\n       0      1      2      3      4\n0   True  False  False   True  False\n1   True  False  False   True  False\n2  False  False   True  False  False\n3   True  False  False   True  False\n4  False  False   True   True  False\n5  False  False  False   True   True\n6   True   True  False   True   True\n7   True   True  False  False   True\n8   True   True   True   True  False\n9  False  False   True  False  False\n```\n"},{"labels":["api",null,null],"text":"After #3482 is merged, some additional methods to fixup\n- [x] Series.replace (use generic/replace) via f4fe5fd03816deab1dc71e958ebbb4d9c1b9d35d\n- [x] Series align (use generic/align), PR #4800\n- [x] Move Arithmetic/Comparison methods to generic, #4051, #5034\n- [ ] Fillna column wise, #4514\n- [x] Verify Copy for Series (deep=True), #2721, PR #4627\n- [x] Verify that `deep=True` is implemented in `core/internals/copy`, related to #4039, PR #4627\n- [ ] consolidate tests as appropriate to `tests/test_generic.py` (ones that should have a conforming API, e.g. `replace,filter`) see #4118, (test_generic is created via #4627)\n- [x] add `AXIS_LEN` check in `filter`, PR #4855\n- [x] rename consistency between series/frame, see #4605, PR #4627\n- [x] clean up reindex / fix method= issues #4604, PR is #4610\n- [x] interpolation, consolidate `Series/DataFrame` interpolation to \n    `internals.Block.interpolatte`,  #4434, cc @TomAugspurger \n- [x] change setitem_with_indexer to use `Block.setitem` (core/indexing), PR #4624\n- [x] move _get_numeric_data/_get_bool_data to generic.py PR #4675\n- [x] fix tools/plotting for series sub of NDFrame PR #4675\n- [x] ~~remove use of common/_maybe_upcast_putmask and possibly_cast_item~~\n- [x] clip ( see #2747), need to move from frame.py to generic.py (series is ok), PR #4798\n- [x] take ( PR #4757 )\n- [ ] update, #3025\n"},{"labels":["api",null,null,null],"text":"related #3190\n\nCurrently it is only possible to specify justify settings for column headers.  This would extremely useful when used in conjunction with IPython Notebook.\n"},{"labels":["api",null,null,null,null],"text":"xref #8184\n\nto say round by minute\nhttp://stackoverflow.com/questions/17781159/round-pandas-datetime-index\n\nby hour\nhttps://github.com/pydata/pandas/issues/4314\n\nmight be simpler to use numpy astype here\nhttp://stackoverflow.com/questions/23315163/merging-two-pandas-timeseries-shifted-by-1-second\n\n```\nIn [75]: index = pd.DatetimeIndex([ Timestamp('20120827 12:05:00.002'), Timestamp('20130101 12:05:01'), Timestamp('20130712 15:10:00'), Timestamp('20130712 15:10:00.000004') ])\n\nIn [79]: index.values\nOut[79]: \narray(['2012-08-27T08:05:00.002000000-0400',\n       '2013-01-01T07:05:01.000000000-0500',\n       '2013-07-12T11:10:00.000000000-0400',\n       '2013-07-12T11:10:00.000004000-0400'], dtype='datetime64[ns]')\n\nIn [78]: pd.DatetimeIndex(((index.asi8/(1e9*60)).round()*1e9*60).astype(np.int64)).values\nOut[78]: \narray(['2012-08-27T08:05:00.000000000-0400',\n       '2013-01-01T07:05:00.000000000-0500',\n       '2013-07-12T11:10:00.000000000-0400',\n       '2013-07-12T11:10:00.000000000-0400'], dtype='datetime64[ns]')\n```\n"},{"labels":["api",null,null,null],"text":"This has been suggested in the pydata mailing list. seems like a simple fix and would be a nice API improvement\n"},{"labels":["api",null],"text":"It'd be really cool when plotting kde's to be able to set the bandwidth used for the kde and the indices at which it is evaluated. This is done with the bw_method kwarg for scipy.stats.gaussian_kde for scipy >= 0.11, a couple years old now, for the former; a variable ind is created in the pandas class KdePlot's _make_plot method and passed to gkde.evaluate. \n\nHere's a patch which implements this functionality: https://github.com/khgitting/pandas/compare/master...kde_kwargs\n\nI'm having trouble passing the Travis CI tests here: https://travis-ci.org/khgitting/pandas/jobs/9284591\n\nIt appears the tests use an older version of scipy where gaussian_kde does not take bw_method kwarg. I'd love to be able to resolve this, but am not sure what the right course of action is. Any advice? \n"},{"labels":["api",null],"text":"``` python\nimport pandas\nimport array\n\nx=array.array('i', range(10))\n\npdf = pandas.DataFrame.from_items([('a', x)])\n# fails with:\n# ValueError: If use all scalar values, must pass index\n```\n\nAlso, since x has a `__len__` may be the error message should not refer to a scalar.\n\nExplicit casting to numpy arrays is a workaround, but it would seem convenient to have it done within the constructor for `DataFrame`:\n\n``` python\nimport numpy\npdf = pandas.DataFrame.from_items([('a', numpy.asarray(x))])\n```\n"},{"labels":["api"],"text":"Numpy 1.8 API change in mean?\n\n```\n/home/data/flash_supression/yarik/scripts/utils.pyc in paired_t_anal(d, value, testcol, rows, cols, print_pt, plots, aggfunc)\n    162     pvalues = np.array(pvalues, ndmin=1)\n    163 \n--> 164     pt_paired_t = DataFrame(np.array([np.mean(v0, axis=0),\n    165                                       np.mean(v1, axis=0),\n    166                                       np.mean(v10, axis=0),\n\n/usr/lib/pymodules/python2.7/numpy/core/fromnumeric.pyc in mean(a, axis, dtype, out, keepdims)\n   2481         try:\n   2482             mean = a.mean\n-> 2483             return mean(axis=axis, dtype=dtype, out=out)\n   2484         except AttributeError:\n   2485             pass\n\nTypeError: mean() got an unexpected keyword argument 'dtype'\n```\n\ncomplete \"line\" calling this is\n\n```\n    pt_paired_t = DataFrame(np.array([np.mean(v0, axis=0),\n                                      np.mean(v1, axis=0),\n                                      np.mean(v10, axis=0),\n                                      tvalues, pvalues]),\n                            index=Index(['mean(%s)' % lvalues[0],\n                                         'mean(%s)' % lvalues[1],\n                                         'mean effect',\n                                         't-score', 'p-value'],\n                                        name=contrast_s),\n                            columns = columns)\n\n```\n\nit used to work before ... did not check for sure yet though if it is pandas or recent numpy upgrade (if I did any)\n\nmore information:\n\n```\nipdb> print a.mean\n<bound method DataFrame.mean of cond                full   profile\nsubject                           \n01jul10sc.dat   1.572131  1.569658\n01oct10cs.dat   1.491370  1.678300\n...\n```\n\nand if to provide ndarray compatibility here then interface should match numpy's where dtype is a valid argument to mean\n"},{"labels":["api",null,null],"text":"- `numeric_only=True` flag to `min,max,idxmax,idxmin` to select out numeric only\n- fix `idxmax` on object dtypes (related to #4147)\n\nrelated is #6287\n\nJust happened to come across this. Related to #2982\n\n```\ndf = pd.DataFrame([[\"zimm\", 2.5],[\"biff\", 1.],[\"bid\", 12.]])\n```\n\nThis is fine (I was surprised it worked for objects)\n\n```\ndf.max()\n```\n\nThis falls down\n\n```\ndf.idxmax()\n```\n\nI'd be fine if there's a strong performance reason not to support this, though it would be nice to not return an error at least. Maybe a stop-gap solution is to only return a solution for numeric types similar to the quantiles method.\n"},{"labels":["api",null,null],"text":"```\nIn [1]: df = DataFrame(columns=list('xyz'))\n\nIn [2]: df\nOut[2]: \nEmpty DataFrame\nColumns: [x, y, z]\nIndex: []\n\nIn [3]: df.dtypes\nOut[3]: \nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nIn [5]: df['x'].dtype\nOut[5]: dtype('O')\n```\n\nexpected\n\n```\nIn [11]: Series(dict([ (s,np.dtype('O')) for s in list('xyz')]))\nOut[11]: \nx    object\ny    object\nz    object\ndtype: object\n```\n"},{"labels":["api",null],"text":"The docstring for `DataFRame.boxplot` claims that the method returns matplotlib axes:\n\n```\nReturns\n-------\nax : matplotlib.axes.AxesSubplot\n```\n\nIn fact, it instead returns a dict:\n\n```\nIn [71]: data.boxplot()\nOut[71]:\n{'boxes': [<matplotlib.lines.Line2D at 0x10e376110>,\n  <matplotlib.lines.Line2D at 0x10e491350>,\n  <matplotlib.lines.Line2D at 0x10e477590>,\n  <matplotlib.lines.Line2D at 0x10eb0e7d0>,\n  <matplotlib.lines.Line2D at 0x10eb3da10>,\n  <matplotlib.lines.Line2D at 0x10eb7ec50>,\n  <matplotlib.lines.Line2D at 0x10eb9be90>],\n 'caps': [<matplotlib.lines.Line2D at 0x10e3ab5d0>,\n  <matplotlib.lines.Line2D at 0x10ebbc4d0>,\n  <matplotlib.lines.Line2D at 0x10e357690>,\n  ...\n  <matplotlib.lines.Line2D at 0x10eb020d0>,\n  <matplotlib.lines.Line2D at 0x10eb02710>,\n  <matplotlib.lines.Line2D at 0x10eb0b310>,\n  <matplotlib.lines.Line2D at 0x10eb0b950>,\n  <matplotlib.lines.Line2D at 0x10eb9a550>,\n  <matplotlib.lines.Line2D at 0x10eb9ab90>]}\n```\n\nAside from being inconsistent with the documentation, this is inconvenient because I am no longer able to manipulate the axes of the plot directly.\n\nUsing Pandas 0.12.0.dev-af85719\n"},{"labels":["api"],"text":"The following currently are exceptions in master(the corresponding operations on DataFrames work though)\n\n```\nIn [4]: s = Series([1,2,3,4],['foo','bar','foo','bah'])\n\nIn [5]: s.ix[['foo','bar','bah','bam']]\nNotImplementedError: cannot handle non-unique indexing for non-DataFrame (yet)\n\nIn [6]: s[['foo','bar','bah','bam']]\nAssertionError: Index length did not match values\n```\n"},{"labels":["api",null],"text":"Shouldn't it carry all possible properties instead?\n\nSteps to reproduce:\n\n``` python\n\nimport pandas as pd\n\n\nst = pd.Timestamp('2013-07-01 00:00:00', tz='America/Los_Angeles')\net = pd.Timestamp('2013-07-02 00:00:00', tz='America/Los_Angeles')\n\ndr = pd.date_range(st, et, freq='H', name='timebucket')\nassert dr.name == 'timebucket'\ndr = dr[1:]\nassert dr.name == None\n```\n"},{"labels":["api",null],"text":"entries with \"Inf\" (beginning with upper case I) will assign that column a dtype of object while \"inf\" is parsed as infinity.\n\nI believe that in R, the function write.table uses \"Inf\" (with the uppercase) which would make importing tables exported from R more difficult.\n"},{"labels":["api",null,null,null],"text":"From stackoverflow: http://stackoverflow.com/questions/17579932/statsmodel-arma-function-is-incompatiable-with-pandas/17581170\n\nI'm looking only quickly at this, but I'm wondering if this should be recognized and return an offset.\n\n```\nfrom pandas.tseries.frequencies import get_offset\nget_offset('30s')\n```\n\nor if in statsmodels we should accept custom offsets in place of frequency strings.\n"},{"labels":["api",null],"text":"http://stackoverflow.com/questions/17591104/in-pandas-can-i-deeply-copy-a-dataframe-including-its-index-and-column/17591423#17591423\n\nThis means if we change the index in place, e.g. with name:\n\n```\ndf1 = df.copy()\ndf1.index.name = 'ffg'\n```\n\nThis changes the name of the df.index.\n"},{"labels":["api",null],"text":"This of course can be done directly, but adding the keyword `quantiles` that accepts a number or list (to create multiple fields), seems like a nice idea\n\nand maybe deprecate `percentile_width` / replace with `quantiles=[50]`\n\nlike this:\nhttp://stackoverflow.com/questions/17578115/pass-percentiles-to-pandas-agg-function\n"},{"labels":["api",null,null,null],"text":"just too confusion and not consistent\n\nrelated: #3418, #3171\n"},{"labels":["api",null,null],"text":"Currently in 0.12.\n\n```\nIn [3]: DataFrame(randn(10,2)).to_hdf('test_storer.h5','df')\n\nIn [4]: DataFrame(randn(10,2)).to_hdf('test_table.h5','df',table=True)\n```\n\nSelecting from a table is good with or without a where\n\n```\nIn [6]: pd.read_hdf('test_table.h5','df',where='index>5')\nOut[6]: \n          0         1\n6  0.296947 -1.876055\n7 -0.200516 -0.641670\n8 -0.177828  0.877286\n9  0.836903 -1.626247\n```\n\nThis raises if where is not None, the theory being that passing a non-None where was a mistake.\n\nHowever, then you have to a-prior know if you are dealing with a `Storer` or a `Table`\n\n```\nIn [7]: pd.read_hdf('test_storer.h5','df',where='index>5')\nTypeError: cannot pass a where specification when reading a Storer\n```\n\nmake this a warning instead?\n"},{"labels":["api",null],"text":"```\nIn [3]: m = pd.MultiIndex.from_arrays([[1,2], [3,4]])\n\nIn [4]: m\nOut[4]:\nMultiIndex\n[(1, 3), (2, 4)]\n\nIn [5]: pd.Series(m)\n[1]    786 segmentation fault  ipython\n\npd.Series(m.values) has same fault (list of tuples)\n```\n\nProbably this should just be disallowed.\n"},{"labels":["api",null],"text":"There's a duplicate plot created when calling `Series.hist` as well as a duplicate plot when calling `groupby.plot` that I think would be fixed by resolving this issue.\n#4113 #413 #2168\n"},{"labels":["api",null,null,null],"text":"cumsum seems to require skipna=False otherwise it sulks here. (Not investigate which others are also affected, cumprod is though).\n\n```\nIn [10]: b = pd.Series([False, False, False, True, True, False, False])\n\nIn [11]: b\nOut[11]:\n0    False\n1    False\n2    False\n3     True\n4     True\n5    False\n6    False\ndtype: bool\n\nIn [12]: b.cumsum()\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-12-f3f684a93525> in <module>()\n----> 1 b.cumsum()\n\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/series.pyc in cumsum(self, axis, dtype, out, skipna)\n   1626\n   1627         if do_mask:\n-> 1628             np.putmask(result, mask, pa.NA)\n   1629\n   1630         return Series(result, index=self.index)\n\nValueError: cannot convert float NaN to integer\n\nIn [13]: b.cumsum(skipna=False)\nOut[13]:\n0    0\n1    0\n2    0\n3    1\n4    2\n5    2\n6    2\ndtype: int64\n```\n\nIf it has nans or you int or object it works as expected:\n\n```\nIn [21]: b.astype(int).cumsum()\nIn [22]: b.astype(object).cumsum()  # False at the beginning is expected\nIn [23]: b.astype(int).astype(object).cumsum()\n```\n\nAlso, if you try and inset an nan it doesn't work nor raise (!):\n\n```\nIn [31]: b.loc[0] = np.nan\n\nIn [32]: b\nOut[32]:\n0     True\n1    False\n2    False\n3     True\n4     True\n5    False\n6    False\ndtype: bool\n```\n"},{"labels":["api",null,null],"text":"xref #14139 for empty MI\n\nHi everybody,\n\nin the current version renaming of MultiIndex DataFrames does not work. Lets take the following example:\n\n```\nimport datetime as DT\nimport pandas as pd\ndf = pd.DataFrame({\n'Branch' : 'A A A A A B'.split(),\n'Buyer': 'Carl Mark Carl Joe Mark Carl'.split(),\n'Quantity': [1,3,5,8,9,3],\n'Date' : [\n    DT.datetime(2013,9,1,13,0),\n    DT.datetime(2013,9,1,13,5),\n    DT.datetime(2013,10,1,20,0),\n    DT.datetime(2013,10,3,10,0),\n    DT.datetime(2013,12,2,12,0),                                      \n    DT.datetime(2013,12,2,14,0),\n    ]})\n```\n\nand the following query:\n\n```\ntest_df = df[df['Buyer'].isin(['Carl', 'Mark'])].set_index('Buyer', append=True)[['Date']].unstack(['Buyer'])\n```\n\nNow, the following renaming does not work\n\n```\ntest_df.rename(columns={('Date', 'Carl'): 'Carl'}, inplace=True)\n```\n\nThanks in advance\n\nAndy\n"},{"labels":["api",null,null,null],"text":"from stack: http://stackoverflow.com/questions/17491446/wrong-result-from-timedelta-operation/17492653#17492653\n\n```\nIn [101]: rng = pd.date_range('2013', '2014')\n\nIn [102]: rng\nOut[102]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-07-05 00:00:00, ..., 2014-07-05 00:00:00]\nLength: 366, Freq: D, Timezone: None\n\nIn [103]: s = pd.Series(rng)\n```\n\nThese work\n\n```\nIn [105]: rng - pd.offsets.Hour(1)\nOut[105]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-07-04 23:00:00, ..., 2014-07-04 23:00:00]\nLength: 366, Freq: D, Timezone: None\n\nIn [106]: s - np.timedelta64(100000000)\n```\n\nbut the other way around doesn't.\n\n```\nIn [107]: rng - np.timedelta64(100000000)\nNotImplementedError:\n\nIn [108]: s - pd.offsets.Hour(1)\nValueError: cannot operate on a series with out a rhs of a series/ndarray of type datetime64[ns] or a timedelta\n```\n\ncc #3009.\n"},{"labels":["api",null],"text":"Currently `DataFrame` and `Series` do slightly different things when their respective `replace` methods are called and accept slightly different arguments. This should be rectified in 0.13.\n"},{"labels":["api",null,null,null],"text":"The numpy way\n\n```\nIn [183]: x\nOut[183]: \n   one two\n0    1   a\n2    2   c\n3    3   d\n4    4   d\n5    5   d\n\nIn [184]: np.unique(x['two'].values,return_inverse=True)\nOut[184]: (array(['a', 'c', 'd'], dtype=object), array([0, 1, 2, 2, 2]))\n```\n\nThe pandas way - maybe provide a better API to this\nmaybe: `uniques, indexer = Index(x['two']).get_uniques()` ??\n\n```\nIn [186]: uniques = x['two'].unique()\n\nIn [187]: uniques\nOut[187]: array(['a', 'c', 'd'], dtype=object)\n\nIn [188]: Index(uniques).get_indexer_non_unique(x['two'])\nOut[188]: (Int64Index([0, 1, 2, 2, 2], dtype=int64), array([], dtype=int64))\n```\n"},{"labels":["api",null],"text":"The following should work, but since it doesn't should raise an error...\n\nIn [116]: pd.Timestamp(np.datetime64('1677-01-01'),'D')\nOut[116]: Timestamp('2261-07-22 23:34:33.709551616', tz=None)\n\nIn [117]: pd.Timestamp(np.datetime64('1678-01-01'),'D')\nOut[117]: Timestamp('1678-01-01 00:00:00', tz=None)\n\nIn [121]: pd.**version**\nOut[121]: '0.11.1.dev-06cd915'\n\nIn [125]: sys.version\nOut[125]: '3.3.2 (default, May 21 2013, 15:40:45) \\n[GCC 4.8.0 20130502 (prerelease)]'\n"},{"labels":["api",null,null],"text":"related #4551\n"},{"labels":["api",null,null],"text":"http://stackoverflow.com/questions/17315737/split-a-large-pandas-dataframe\n\nrelated #414\n"},{"labels":["api",null,null],"text":"related #5202\n\nYou can do it but its akward\n\n```\nIn [142]: pd.period_range('2011-07','2012-05',freq='M').values.min()\nOut[142]: 498\n\nIn [143]: pd.period_range('2011-07','2012-05',freq='M').values.max()\nOut[143]: 508\n\nIn [146]: pd.period_range(pd.Period(ordinal=498,freq='M'),pd.Period(ordinal=508,freq='M'),freq='M')\nOut[146]: \n<class 'pandas.tseries.period.PeriodIndex'>\nfreq: M\n[2011-07, ..., 2012-05]\nlength: 11\n```\n\nThis fails\n\n```\nIn [150]: pd.period_range('2011-07','2012-05',freq='M').min()\nAttributeError: 'PeriodIndex' object has no attribute 'freq'\n```\n"},{"labels":["api",null,null,null],"text":"related #4036, #4116\nfrom SO with 0.14.0 multi-index slicers: http://stackoverflow.com/questions/24126542/pandas-multi-index-slices-for-level-names/24126676#24126676\n\nHere's the example from there:\n\n```\nIn [11]: midx = pd.MultiIndex.from_product([list(range(3)),['a','b','c'],pd.date_range('20130101',periods=3)],names=['numbers','letters','dates'])\n\nIn [12]: midx.names.index('letters')\nOut[12]: 1\n\nIn [13]: midx.names.index('dates')\nOut[13]: 2\n```\n\nHere's a complete example\n\n```\nIn [18]: df = DataFrame(np.random.randn(len(midx),1),index=midx)\n\nIn [19]: df\nOut[19]: \n                                   0\nnumbers letters dates               \n0       a       2013-01-01  0.261092\n                2013-01-02 -1.267770\n                2013-01-03  0.008230\n        b       2013-01-01 -1.515866\n                2013-01-02  0.351942\n                2013-01-03 -0.245463\n        c       2013-01-01 -0.253103\n                2013-01-02 -0.385411\n                2013-01-03 -1.740821\n1       a       2013-01-01 -0.108325\n                2013-01-02 -0.212350\n                2013-01-03  0.021097\n        b       2013-01-01 -1.922214\n                2013-01-02 -1.769003\n                2013-01-03 -0.594216\n        c       2013-01-01 -0.419775\n                2013-01-02  1.511700\n                2013-01-03  0.994332\n2       a       2013-01-01 -0.020299\n                2013-01-02 -0.749474\n                2013-01-03 -1.478558\n        b       2013-01-01 -1.357671\n                2013-01-02  0.161185\n                2013-01-03 -0.658246\n        c       2013-01-01 -0.564796\n                2013-01-02 -0.333106\n                2013-01-03 -2.814611\n```\n\nThis is your dict of level names -> slices\n\n```\nIn [20]: slicers = { 'numbers' : slice(0,1), 'dates' : slice('20130102','20130103') }\n```\n\nThis creates an indexer that is empty (selects everything)\n\n```\nIn [21]: indexer = [ slice(None) ] * len(df.index.levels)\n```\n\nAdd in your slicers\n\n```\nIn [22]: for n, idx in slicers.items():\n              indexer[df.index.names.index(n)] = idx\n```\n\nAnd select (this has to be a tuple, but was a list to start as we had to modify it)\n\n```\nIn [23]: df.loc[tuple(indexer),:]\nOut[23]: \n                                   0\nnumbers letters dates               \n0       a       2013-01-02 -1.267770\n                2013-01-03  0.008230\n        b       2013-01-02  0.351942\n                2013-01-03 -0.245463\n        c       2013-01-02 -0.385411\n                2013-01-03 -1.740821\n1       a       2013-01-02 -0.212350\n                2013-01-03  0.021097\n        b       2013-01-02 -1.769003\n                2013-01-03 -0.594216\n        c       2013-01-02  1.511700\n                2013-01-03  0.994332\n```\n\nI use hierarchical indices regularly with pandas DataFrames and Series objects. It is invaluable to be able to partially select subsets of rows based on a set of arbitrary index values, and retain the index information for subsequent groupby operations etc.\n\nI am looking for an elegant way to pass an ordered tuple (with possibly empty slices) or an arbitrary dict of {index_level_name:value,...} pairs to select rows matching the passed index:value pairs. Note: I am aware that with Boolean indexing on data columns and nested np.logical_and() statements you can construct such a Boolean select index. I'm looking for an elegant solution using indexes & levels to avoid repeatedly using df.reset_index and building Boolean arrays. Also, df.xs() does not work in every situation (see below) and does not exist for Series with MultiIndex.\n\nTo explain this lets create a DataFrame with 5 index levels:\n\n``` python\nIn [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: print pd.__version__\n0.10.0\n\nIn [4]: # Generate Test DataFrame\n   ...: NUM_ROWS = 100000\n   ...: \n\nIn [5]: NUM_COLS = 10\n\nIn [6]: col_names = ['A'+num for num in map(str,np.arange(NUM_COLS).tolist())]\n\nIn [7]: index_cols = col_names[:5]\n\nIn [8]: # Set DataFrame to have 5 level Hierarchical Index.\n   ...: # The dtype does not matter try str or np.int64 same results.\n   ...: # Sort the index!\n   ...: df = pd.DataFrame(np.random.randint(5, size=(NUM_ROWS,NUM_COLS)), dtype=np.int64, columns=col_names)\n   ...: \n\nIn [9]: df = df.set_index(index_cols).sort_index()\n\nIn [10]: df\nOut[10]: <class 'pandas.core.frame.DataFrame'>\nMultiIndex: 100000 entries, (0, 0, 0, 0, 0) to (4, 4, 4, 4, 4)\nData columns:\nA5    100000  non-null values\nA6    100000  non-null values\nA7    100000  non-null values\nA8    100000  non-null values\nA9    100000  non-null values\ndtypes: int64(5)\n\nIn [11]: df.index.names\nOut[11]: ['A0', 'A1', 'A2', 'A3', 'A4']\n```\n\nNow index on every level and we get back the rows we want :) I love that I get back the complete index too because it may be useful later.\n\n``` python\nIn [12]: df.ix[(0,1,2,3,4)]\nOut[12]:                 A5  A6  A7  A8  A9\nA0 A1 A2 A3 A4                    \n0  1  2  3  4    1   3   1   1   3\n            4    4   3   4   2   4\n            4    0   2   3   1   3\n      ...\n            4    1   1   3   4   3\n            4    0   1   2   4   1\n```\n\nNow if we index on the first 4 levels we get back something different, a data frame with the first 4 index levels dropped. It would be nice to have the option to keep all index levels even though they are repetitive (like above).\n\n``` python\nIn [13]: df.ix[(0,1,2,3)]\nOut[13]: <class 'pandas.core.frame.DataFrame'>\nInt64Index: 144 entries, 0 to 4\nData columns:\nA5    144  non-null values\nA6    144  non-null values\nA7    144  non-null values\nA8    144  non-null values\nA9    144  non-null values\ndtypes: int64(5)\n```\n\nNow comes the tricky part. What if I only want to index on the first and last 2 index levels, and want everything from the 3rd level? Empty slicing is not supported.\n\n``` python\nIn [14]: df.ix[(0,1,:,3,4)]\n  File \"<ipython-input-14-7e44e59fa5b1>\", line 1\n    df.ix[(0,1,:,3,4)]\n               ^\nSyntaxError: invalid syntax\n\nIn [15]: df.ix[(0,1,slice(None),3,4)]\n---------------------------------------------------------------------------\nIndexingError                             Traceback (most recent call last)\n<ipython-input-15-0e4517ae7fc5> in <module>()\n----> 1 df.ix[(0,1,slice(None),3,4)]\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\indexing.pyc in __getitem__(self, key)\n     30                 pass\n     31 \n---> 32             return self._getitem_tuple(key)\n     33         else:\n     34             return self._getitem_axis(key, axis=0)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\indexing.pyc in _getitem_tuple(self, tup)\n    212         for i, key in enumerate(tup):\n    213             if i >= self.obj.ndim:\n--> 214                 raise IndexingError('Too many indexers')\n    215 \n    216             if _is_null_slice(key):\n\nIndexingError: Too many indexers\n```\n\ndf.xs can somewhat help here but its useless for MultiIndex on a series. And it drops the indexed levels leaving you unsure to what fixed index levels you have drilled to. :( \n\n``` python\n\nIn [16]: df.xs((0,2,3),level=df.index.names[::2])\nOut[16]: <class 'pandas.core.frame.DataFrame'>\nMultiIndex: 805 entries, (0, 0) to (4, 4)\nData columns:\nA5    805  non-null values\nA6    805  non-null values\nA7    805  non-null values\nA8    805  non-null values\nA9    805  non-null values\ndtypes: int64(5)\n```\n\nInterestingly df.xs() is not consistant, because you cannot explicitly index on every level giving it the list of all level names:\n\n``` python\nIn [17]: df.xs((0,1,2,3,4), level=df.index.names)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-19-d0e373dfcd5f> in <module>()\n----> 1 df.xs((0,1,2,3,4), level=df.index.names)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in xs(self, key, axis, level, copy)\n   2233         labels = self._get_axis(axis)\n   2234         if level is not None:\n-> 2235             loc, new_ax = labels.get_loc_level(key, level=level)\n   2236 \n   2237             if not copy and not isinstance(loc, slice):\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in get_loc_level(self, key, level)\n   2193 \n   2194                 result = loc if result is None else result & loc\n-> 2195             return result, _drop_levels(result, level)\n   2196 \n   2197         level = self._get_level_number(level)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in _drop_levels(indexer, levels)\n   2177             levels = [self._get_level_number(i) for i in levels]\n   2178             for i in sorted(levels, reverse=True):\n-> 2179                 new_index = new_index.droplevel(i)\n   2180             return new_index\n   2181 \n\nAttributeError: 'Int64Index' object has no attribute 'droplevel'\n```\n\nHowever df.xs without the level attribute on all index levels works as expected...\n\n``` ptthon\nIn [18]: df.xs((0,1,2,3,4))\nOut[18]:                 A5  A6  A7  A8  A9\nA0 A1 A2 A3 A4                    \n0  1  2  3  4    1   3   1   1   3\n            4    4   3   4   2   4\n            4    0   2   3   1   3\n           ...\n            4    1   1   3   4   3\n            4    0   1   2   4   1\n```\n\nThoughts:\nOne (somewhat limiting) solution could be allowing df.ix[(0,1,3,:,4)] to take an empty slice for an index level and return the data frame indexed on only the the passed index levels that are known. Today this capability does not exist, although an ordered partial list of index levels works.\n\nThe next and more general approach could be to pass a dict of df.ix[{index_level:value}] pairs and return the rows where the specified index levels equal the passed values. Unspecified levels are not filtered down and we have the option to return all index levels.\n"},{"labels":["api",null],"text":"most plotting functions return an `Artist` instance whereas a few return something else. for example, `boxplot` returns a `dict` and `scatter` returns a `tuple`. they should at least return a `matplotlib.artist.Artist` subclass (usually either a `Figure` or `Axes`) or an array of `Artist`s.\n\nthis has been brought up before but it would be nice to make a decision here.\n\n[mailing list thread](https://groups.google.com/d/topic/pystatsmodels/biNlCvJPNNY/discussion) and #3474.\n"},{"labels":["api",null],"text":"At the moment, you can use `sortlevel` to select one level to sort by and then \"Data will be lexicographically sorted by the chosen level followed by the other levels (in order)\"\n\nShould this be the case?\nShould we be able to specify multiple levels to sort by?\n\nsee http://stackoverflow.com/questions/17242970/multi-index-sorting-in-pandas\n"},{"labels":["api",null],"text":"Bug? Did this used to work?\n\n```\nnp.array(pandas.DataFrame.from_dict(dict(a=np.random.rand(50), b=np.random.randint(2, size=50).astype(bool))))\n```\n\nWe would expect that this casts to a plain float ndarray.\n\nSee https://github.com/statsmodels/statsmodels/issues/880\n"},{"labels":["api",null,null],"text":"Creating a pivot table from a DataFrame is easy.\n\n``` python\nIn [17]: df\nOut[17]: \n                 date variable     value\n0 2000-01-03 00:00:00        A  0.528219\n1 2000-01-04 00:00:00        A -0.135071\n2 2000-01-05 00:00:00        A -0.343018\n3 2000-01-03 00:00:00        B -0.097701\n4 2000-01-04 00:00:00        B -1.383248\n\nIn [18]: df.pivot('date', 'variable', 'value')\nOut[18]: \nvariable           A         B\ndate                          \n2000-01-03  0.528219 -0.097701\n2000-01-04 -0.135071 -1.383248\n2000-01-05 -0.343018       NaN\n```\n\nHowever if the index has been set, and there is no column to be set as index, pivot fails. From api point of view, the `index` argument is optional, but in fact it is not.\n\n``` python\nIn [19]: df.set_index('date', inplace=True)\n\nIn [20]: df\nOut[20]: \n           variable     value\ndate                         \n2000-01-03        A  0.528219\n2000-01-04        A -0.135071\n2000-01-05        A -0.343018\n2000-01-03        B -0.097701\n2000-01-04        B -1.383248\n\nIn [21]: df.pivot(columns='variable', values='value')\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n...\nKeyError: u'no item named None'\n```\n\nOf course one can reset the index before using pivot.\n\n``` python\nIn [22]: df.reset_index().pivot('date', 'variable', 'value')\nOut[22]: \nvariable           A         B\ndate                          \n2000-01-03  0.528219 -0.097701\n2000-01-04 -0.135071 -1.383248\n2000-01-05 -0.343018       NaN\n```\n"},{"labels":["api",null,null,null],"text":"I don't _think_ there is a way to get the nlargest elements in a DataFrame without sorting.\n\nIn ordinary python you'd use heapq's nlargest (and we can hack a bit to use it for a DataFrame):\n\n```\nIn [10]: df\nOut[10]:\n                IP                                              Agent  Count\n0    74.86.158.106  Mozilla/5.0+(compatible; UptimeRobot/2.0; http...    369\n1   203.81.107.103  Mozilla/5.0 (Windows NT 6.1; rv:21.0) Gecko/20...    388\n2  173.199.120.155  Mozilla/5.0 (compatible; AhrefsBot/4.0; +http:...    417\n3    124.43.84.242  Mozilla/5.0 (Windows NT 6.2) AppleWebKit/537.3...    448\n4  112.135.196.223  Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.3...    454\n5   124.43.155.138  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:21.0) G...    461\n6   124.43.104.198  Mozilla/5.0 (Windows NT 5.1; rv:21.0) Gecko/20...    467\n\nIn [11]: df.sort('Count', ascending=False).head(3)\nOut[11]:\n                IP                                              Agent  Count\n6   124.43.104.198  Mozilla/5.0 (Windows NT 5.1; rv:21.0) Gecko/20...    467\n5   124.43.155.138  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:21.0) G...    461\n4  112.135.196.223  Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.3...    454\n```\n\n```\nIn [21]: from heapq import nlargest\n\nIn [22]: top_3 = nlargest(3, df.iterrows(), key=lambda x: x[1]['Count'])\n\nIn [23]: pd.DataFrame.from_items(top_3).T\nOut[23]:\n                IP                                              Agent Count\n6   124.43.104.198  Mozilla/5.0 (Windows NT 5.1; rv:21.0) Gecko/20...   467\n5   124.43.155.138  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:21.0) G...   461\n4  112.135.196.223  Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.3...   454\n```\n\nThis is much slower than sorting, presumbly from the overhead, I thought I'd throw this as a feature idea anyway.\n\nsee http://stackoverflow.com/a/17194717/1240268\n"},{"labels":["api",null],"text":"see #3944\n\ncurrently the default is 'ignore', which returns the input argument (rather than raise, say a `ValueError`); various routines will try a conversion (which will result in a conversion or no effect), then move on; rather than raising (and having to catch it)\n\nso investigate....\n"},{"labels":["api",null,null],"text":"after #3890, still unresolved / inconsistent\nnon-trivial to fix, so pushing to 0.12\n\n```\nIn [14]: Timestamp('2012')\nOut[14]: Timestamp('2012-06-18 00:00:00', tz=None)\n\nIn [15]: to_datetime('2012')\nOut[15]: Timestamp('2012-01-01 00:00:00', tz=None)\n```\n\n```\nIn [17]: pd.date_range('2014', '2015', freq='M')\nOut[17]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2014-04-30, ..., 2015-03-31]\nLength: 12, Freq: M, Timezone: None\n```\n"},{"labels":["api",null,null],"text":"another example:\n\nhttp://stackoverflow.com/questions/23814368/sorting-pandas-categorical-labels-after-groupby\n\nAt the moment bins / buckets are simply strings, so you can't sort them properly.\n\n```\nA=pd.qcut(X['x'], [0,0.25,0.5,0.75,1.0])\n\nIn [28]: A[0]\nOut[28]: '(0.635, 2.4]'\n\nIn [29]: type(A[0])\nOut[29]: str\n```\n\n The dtype of the Index is object (because these are strings), maybe we could just have these as a subclass of strings where:\n\n```\ndef __lt__(self, other):\n    from ast import literal_eval\n    return literal_eval(self) < literal_eval(other)\n```\n\nCan put together pr if seems reasonable.\n"},{"labels":["api",null,null],"text":"Many python functions  (sorting, max/min) accept a key argument, perhaps they could in pandas too.\n\n.\n\n_The terrible motivating example was this awful hack from [this question](http://stackoverflow.com/a/17157110/1240268).... for which maybe one could do_\n\n```\ndf.sort_index(key=lambda t: literal_eval(t[1:-1]))\n```\n\n_This would still be an awful awful hack, but a slightly less awful one_.\n"},{"labels":["api",null],"text":"...when the hour is not zero.\n\nExample below:\n\n``` python\nIn [7]: series = pd.TimeSeries(numpy.arange(5), pd.date_range('2014-01-01 09:00', periods=5, freq='D'))\n   ...: series\n   ...: \nOut[7]: \n2014-01-01 09:00:00    0\n2014-01-02 09:00:00    1\n2014-01-03 09:00:00    2\n2014-01-04 09:00:00    3\n2014-01-05 09:00:00    4\nFreq: D, dtype: int32\n\nIn [8]: series[pd.datetime(2014, 1, 2):pd.datetime(2014, 1, 4)]\nOut[8]: \n2014-01-02 09:00:00    1\n2014-01-03 09:00:00    2\nFreq: D, dtype: int32\n\nIn [9]: series['02-Jan-2014':'04-Jan-2014']\nOut[9]: \n2014-01-02 09:00:00    1\n2014-01-03 09:00:00    2\n2014-01-04 09:00:00    3   <------------------------- ???\nFreq: D, dtype: int32\n\nIn [10]: \n```\n"},{"labels":["api",null],"text":"from the ML\nhttps://groups.google.com/forum/?fromgroups#!topic/pydata/Lc10-vz4NpU\n\nnot really sure if this is a bug, more of a convention\n\n```\nIn [58]: data = \"YEAR,DOY,TEMP\\n2001,1,10.5\\n2001,2,11.8\\n2001,3,67.4\"\n\nIn [59]: def date_converter(x):                                                                                                         \n        return dt.datetime.strptime(x, '%Y %j')\n\nIn [60]: pandas.read_csv(StringIO(data), parse_dates=[[0,1]], index_col=0, sep=\",\", keep_date_col=True, date_parser=date_converter)\nOut[60]: \n            YEAR DOY  TEMP\nYEAR_DOY                  \n2001-01-01  2001   1  10.5\n2001-01-02  2001   2  11.8\n2001-01-03  2001   3  67.4\n\nIn [61]: data = \"YEAR,DOY,TEMP\\n2001.0,1.0,10.5\\n2001.0,2.0,11.8\\n2001.0,3,67.4\"\n\nIn [62]: def date_converter2(*args):                                                                                                     \n    return dt.datetime(int(float(args[0])),int(float(args[1])),1)\n   ....: \n\nIn [63]: pandas.read_csv(StringIO(data), parse_dates=[[0,1]], index_col=0, sep=\",\", keep_date_col=True, date_parser=date_converter2)\nOut[63]: \n              YEAR  DOY  TEMP\nYEAR_DOY                     \n2001-01-01  2001.0  1.0  10.5\n2001-02-01  2001.0  2.0  11.8\n2001-03-01  2001.0    3  67.4\n\n```\n"},{"labels":["api",null],"text":"It seems it returns different objects when passed array vs string:\n\n```\nIn [1]: pd.to_datetime(['2012'])[0]\nOut[1]: Timestamp('2012-01-01 00:00:00', tz=None)\n\nIn [2]: pd.to_datetime('2012')\nOut[2]: datetime.datetime(2012, 1, 1, 0, 0)\n```\n\nAlso worth mentioning (though different):\n\n```\nIn [3]: pd.Timestamp('2012')\nOut[3]: Timestamp('2012-06-13 00:00:00', tz=None)\n```\n\nSeems a little inconsistent... (cc #3859)\n"},{"labels":["api",null,null,null],"text":"would be nice as @jreback says to implement this since indices are supposed to be immutable. currently they try to hash the underlying `ndarray` which of course fails because it is mutable.\n\n[succinct reasoning behind why you need immutability for hashables](http://stackoverflow.com/a/2671398/564538)\n\n[python docs quote about using `^` (exclusive or) in the implementation](http://stackoverflow.com/a/2671402/564538)\n\nbut see [this answer](http://stackoverflow.com/questions/5386694/fast-way-to-hash-numpy-objects-for-caching) for a way to hash numpy arrays.\n"},{"labels":["api"],"text":"See this [SO answer](http://stackoverflow.com/a/15411073/1240268), they want to use memoisation.\n\nOP points out this gets different results from (presumably it does it off id)\n\n```\nhash(pd.DataFrame([1,2,3])) \n```\n\nShould they be hashable or should hash raise?  (does it defeat the point of hashing if hashing is expensive?)  cc @cpcloud \n"},{"labels":["api",null],"text":"Groupbys from TGs seem to have some useful functionality suppressed/hidden (?)\n\n```\nimport datetime as DT\n\ndf = pd.DataFrame({\n'Branch' : 'A A A A A B'.split(),\n'Buyer': 'Carl Mark Carl Joe Joe Carl'.split(),\n'Quantity': [1,3,5,8,9,3],\n'Date' : [\nDT.datetime(2013,1,1,13,0),\nDT.datetime(2013,1,1,13,5),\nDT.datetime(2013,10,1,20,0),\nDT.datetime(2013,10,2,10,0),\nDT.datetime(2013,12,2,12,0),                                      \nDT.datetime(2013,12,2,14,0),\n]})\n\ndf = df.set_index('Date', drop=False)\ng = df.groupby(pd.TimeGrouper('6M'))\n\nIn [104]: g.gr\ng.group_keys  g.grouper     g.groups\n\nIn [104]: g.groups\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-104-b207e9b2ead3> in <module>()\n----> 1 g.groups\n\n/Users/234BroadWalk/pandas/pandas/core/groupby.pyc in __getattr__(self, attr)\n    235\n    236         raise AttributeError(\"'%s' object has no attribute '%s'\" %\n--> 237                              (type(self).__name__, attr))\n    238\n    239     def __getitem__(self, key):\n\nAttributeError: 'DataFrameGroupBy' object has no attribute 'groups'\n```\n\ncc #3794\n"},{"labels":["api",null,null],"text":"```\nimport pandas as pd\n\npd.Timestamp(pd.Timestamp.min)\n```\n\nThrows: ValueError: Out of bounds nanosecond timestamp: 1-01-01 00:00:00\n\nPandas version: 0.11.1_dev\nNumpy version: 1.7.1\n\nThe the min and max methods on the Timestamp class return simple Python datetime objects, but more importantly they do not adhere to the limitations of Timestamps. They return the default datetime(1,1,1) and datetime(9999, 12, 31, 23, 59, 59, 999999) respectively - both of which are well outside of the valid Timestamp range.\n"},{"labels":["api",null],"text":"``` python\nIn [1]: import pandas as pd\n\nIn [2]: pd.__version__\nOut[2]: '0.11.1.dev-45d298d'\n\nIn [3]: pd.Series(10)\nOut[3]: 10\n```\n\nOk, when setting index explicitly\n\n```\nIn [4]: pd.Series(10, index=[0])\nOut[4]:\n0    10\ndtype: int64\n```\n"},{"labels":["api",null,null,null],"text":"http://stackoverflow.com/questions/17027470/pandas-groupby-and-multiindex\n\nnot sure how useful this is....but say we provide `dropna=True` (which is the current behavior), but if `dropna=False` then can reindex by all the combinations of each index level as a reindexer...\n"},{"labels":["api"],"text":"Should we change the name of `save` to `to_pickle` inline with all of the other ways to export things. At the moment the docs are pretty light on what it does.\n\n(I guess similarly with `load` to `from_pickle`.)\n\n_... not sure I actually have an opinion, but thought I'd throw it out there._ Thoughts?\n\n(this so question: http://stackoverflow.com/questions/16971803/serialization-of-a-pandas-dataframe)\n"},{"labels":["api",null,null],"text":"Example:\n\n```\nIn [15]: np.log(Series([1,2,3,4], dtype='object'))\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-15-25deca6462b7> in <module>()\n----> 1 np.log(Series([1,2,3,4], dtype='object'))\n\nAttributeError: log\n```\n\nThis is really a [numpy issue](http://projects.scipy.org/numpy/ticket/1013) (opened by Wes back in 2009, as Andy Hayden pointed out). But since object arrays are especially common in database output, as was the case in this [SO question](http://stackoverflow.com/questions/16968433/error-when-trying-to-apply-log-method-to-pandas-data-frame-column-in-python/1696849), there some places in pandas where we can help avoid this error.\n"},{"labels":["api",null],"text":"obvs something like `diff` is fine, but i think sum/prod and most ufuncs should raise a `TypeError` maybe try the ops on the `values` attribute and if it doesn't raise continue\n"},{"labels":["api",null],"text":""},{"labels":["api",null],"text":"fair warning in 0.11.1 that in 0.12 this is going to change\n\ntop-level imports of `ExcelFile/ExcelObject` are unchanged\nthis only affects directly importing from the parsers module\n"},{"labels":["api",null,null,null],"text":"see also #3707\n\nAdd a `fill_value` option to `resample()` so that it is possible to resample a `TimeSeries` without creating `NaN` values. If the series is an int dtype and an int is passed to `fill_value`, it should be possible to resample the series without casting the values to floats.\n\n```\n>>> dates = (datetime(2013, 1, 1), datetime(2013,1,2), datetime(2013,3,1))\n>>> s = Series([1,2,4],index=dates)\n>>> s\n2013-01-01    1\n2013-01-02    2\n2013-03-01    4\ndtype: int64\n>>> s.resample('M', how='sum')\n2013-01-31     3\n2013-02-28   NaN\n2013-03-31     4\nFreq: M, dtype: float64\n>>> s.resample('M', how='sum', fill_value=0)\n2013-01-31     3\n2013-02-28     0\n2013-03-31     4\nFreq: M, dtype: int64\n```\n"},{"labels":["api",null],"text":"It would be nice if dataframe instances implemented `__nonzero__` so you could test if a dataframe is empty or not as if it were a boolean.\n"},{"labels":["api",null],"text":"DataFrame from #3688. This might be related to that:\n\n``` python\ndf = pd.DataFrame({'bar': {0: 1, 1: 1, 2: 1}, 'foo': {0: 0, 1: 1, 2: 2}, 'foo1': {0: 1, 1: 2, 2: 3}, 'hello': {0: 'a', 1: 'a', 2: 'a'}}, columns=['bar', 'foo', 'foo', 'hello'])\nprint df.T.sum(1) == df.sum()  # fine. + is str cat\nprint df.T.mean(1).isnull().all()  # prints True\n```\n\nI think non numeric should be dropped...\n"},{"labels":["api",null],"text":"Perhaps the following [groupby aggregation](http://pandas.pydata.org/pandas-docs/dev/groupby.html#cython-optimized-aggregation-functions) should work only the numeric columns, as they would when using the dataframe:\n\n```\nIn [1]: df = pd.DataFrame({'bar': {0: 1, 1: 1, 2: 1}, 'foo': {0: 0, 1: 1, 2: 2}, 'foo1': {0: 1, 1: 2, 2: 3}, 'hello': {0: 'a', 1: 'a', 2: 'a'}}, columns=['bar', 'foo', 'foo', 'hello'])\n        df.columns = ['bar', 'foo', 'foo', 'hello']\n\nIn [2]: df\nOut[2]:\n   bar  foo  foo hello\n0    1    0    1     a\n1    1    1    2     a\n2    1    2    3     a\n\nIn [3]: df.mean()  # hello is ignored\nOut[13]:\nbar    1\nfoo    1\nfoo    2\ndtype: float64\n\nIn [4]: df.groupby(level=0, axis=1).mean()\n---------------------------------------------------------------------------\nDataError                                 Traceback (most recent call last)\n<ipython-input-4-7c2612a8fbda> in <module>()\n----> 1 df.groupby(level=0, axis=1).mean()\n\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/groupby.pyc in mean(self)\n    351         \"\"\"\n    352         try:\n--> 353             return self._cython_agg_general('mean')\n    354         except GroupByError:\n    355             raise\n\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/groupby.pyc in _cython_agg_general(self, how, numeric_only)\n   1569\n   1570     def _cython_agg_general(self, how, numeric_only=True):\n-> 1571         new_blocks = self._cython_agg_blocks(how, numeric_only=numeric_only)\n   1572         return self._wrap_agged_blocks(new_blocks)\n   1573\n\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/groupby.pyc in _cython_agg_blocks(self, how, numeric_only)\n   1616\n   1617         if len(new_blocks) == 0:\n-> 1618             raise DataError('No numeric types to aggregate')\n   1619\n   1620         return new_blocks\n\nDataError: No numeric types to aggregate\n```\n\n_From this [SO question](http://stackoverflow.com/questions/16678551/pandas-create-new-dataframe-that-averages-duplicates-from-another-dataframe), where I gave very hacky workaround._\n\ncc #3683 @jreback was this the question you were talking about? This ones related but in the sense of coming up against non unique problems... Thought I should mention it here anyway.\n"},{"labels":["api",null],"text":"This currently breaks\n\n```\nIn [1]: df = DataFrame([[1,1,1,5],[1,1,2,5],[2,1,3,5]],columns=['foo','bar','foo','hello'])\n\nIn [2]: df['string'] = 'bar'\n\nIn [3]: df\nIndexError: ('index out of bounds', u'occurred at index hello')\n```\n"},{"labels":["api",null,null,null],"text":"Not sure how to handle this yet. It looks like NaNs do not become a level. Should they? Maybe so. Also describe fails if NaNs are present.\n\n```\npandas.Categorical([np.nan, np.nan, 1, 1, 1, 2, 3, 4, 5, 5, 4, 3, 3]).describe()\n```\n"},{"labels":["api",null],"text":"see #3582 and #3675. Use `DataFrame.fillna()` and `DataFrame.replace()` instead.\n"},{"labels":["api",null],"text":"https://github.com/pydata/pandas/pull/3663\n\nmaybe allow these:\n\n`pd.set_option(pat,val,pat,val`\n"},{"labels":["api",null],"text":"see #3603 deprecated in 0.11.1\n"},{"labels":["api",null,null],"text":"This is the example in io.rst for Stata (in current master)\ncoming from PR #3270 and issue #1512\n\n```\nIn [5]: from pandas.io.stata import StataWriter\n\nIn [6]: df = DataFrame(randn(10,2),columns=list('AB'))\n\nIn [7]: writer = StataWriter('stata.dta',df)\n\nIn [8]: writer.write_file()\n```\n\nI am not sure if you have enough information saved to know that this needs a\n`df.set_index('index')`\n?\n\n```\nIn [10]: from pandas.io.stata import StataReader\n\nIn [11]: reader = StataReader('stata.dta')\n\nIn [12]: reader.data()\nOut[12]: \n   index         A         B\n0      0  0.818436 -0.616332\n1      1 -0.673509  2.209445\n2      2 -0.074915  0.444409\n3      3  0.984456 -1.397691\n4      4 -0.402488  0.884691\n5      5  0.407234  0.499808\n6      6 -0.041578  0.724288\n7      7 -0.110134  0.707406\n8      8  0.986992  1.281154\n9      9 -1.491163 -0.686034\n```\n"},{"labels":["api"],"text":"Currently masking by boolean vectors it doesn't matter which syntax you use:\n\n```\ndf[mask]\ndf.iloc[mask]\ndf.loc[mask]\n```\n\nare all equivalent. Should mask `df.iloc[mask]` mask by position? (this makes sense if mask is integer index).\n\nThis [SO question](http://stackoverflow.com/questions/16603765/what-is-the-most-idiomatic-way-to-index-an-object-with-a-boolean-array-in-pandas).\n"},{"labels":["api"],"text":"This seems a bit surprising:\n\n``` python\n>>> from string import ascii_lowercase as letters\n>>> from pandas import DataFrame\n>>> df = DataFrame(list(letters[:4]), columns=['a'])\n>>> df\n   a\n0  a\n1  b\n2  c\n3  d\n>>> df.replace({'a': 'b'})\n   a\n0  a\n1  a\n2  c\n3  d\n>>> df.replace({'a': 'c'})\n   a\n0  a\n1  b\n2  b\n3  d\n```\n\nDoes this have to do with padding?\n"},{"labels":["api",null,null],"text":"```\nPanel.fillna() \n```\n\nshould accept \n\n```\n(axis= [0, 1, 2] or ['items', 'major_axis', 'minor_axis'])\n```\n\nas an argument.\nRight now it essentially defaults to\n\n```\n(axis='major_axis')\n```\n"},{"labels":["api",null],"text":"This code used to work some time around version 0.10, but now generates a ValueError:\n\nIn [14]: y\nOut[14]: \n<class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 3 (major_axis) x 2 (minor_axis)\nItems axis: z1 to z2\nMajor_axis axis: 0 to 2\nMinor_axis axis: a to b\n\nIn [15]: y.transpose('minor', 'major', 'items')\nOut[15]: \n<class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 3 (major_axis) x 2 (minor_axis)\nItems axis: a to b\nMajor_axis axis: 0 to 2\nMinor_axis axis: z1 to z2\n## In [16]: y.transpose( items='minor', major='major', minor='items')\n\nValueError                                Traceback (most recent call last)\n<ipython-input-16-00ebdc16128e> in <module>()\n----> 1 y.transpose( items='minor', major='major', minor='items')\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.12.0.dev_be25266-py2.7-linux-x86_64.egg/pandas/core/panel.pyc in transpose(self, _args, *_kwargs)\n   1176                 except (IndexError):\n   1177                     raise ValueError(\n-> 1178                         \"not enough arguments specified to transpose!\")\n   1179 \n   1180         axes = [self._get_axis_number(kwargs[a]) for a in self._AXIS_ORDERS]\n\nValueError: not enough arguments specified to transpose!\n"},{"labels":["api",null,null,null,null,null],"text":"e.g. http://stackoverflow.com/questions/16319106/whats-the-equivalent-of-cut-qcut-for-pandas-date-fields\n"},{"labels":["api",null],"text":"This would be nice. The idea is simple: be able to modify the elements of vectorized string list or tuples, e.g., after splitting them, a la \n\n``` python\ns = Series(['a/b/c', 'd/e/f'])\nsp = s.split('/')\nsp.str[0] = 'x'\nassert np.all(sp.str[0] == 'x')\n\n# as well as \nsp.str.set(0, 'y')\nassert np.all(sp.str[0] == 'y')\n```\n"},{"labels":["api",null],"text":"concat is supposed to be called with objs = a list or dict. Not multiple objects as as separate args. RIGHT: concat([df1,df2]) WRONG: concat(df1,df2)\nThe exception thrown in the latter case is seriously cryptic.\n\n---\n\ndf1 = pd.DataFrame(randn(5,3), columns=['a','b','c'])\ndf2 = pd.DataFrame(randn(5,3), columns=['a','b','c'])\npd.concat(df1,df2)\n... File \"/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/frame.py\", line 587, in **nonzero**\n    raise ValueError(\"Cannot call bool() on DataFrame.\")\n## ValueError: Cannot call bool() on DataFrame.\n\nVersion: 0.10.1\n"},{"labels":["api",null,null],"text":"Here is a case where I where the HDFStore would complain:\n\n```\nIn [60]: store\nOut[60]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: water/expected2.h5\n/spelling            series       (shape->[2])\n\nIn [61]: store.remove('spellllling')\n```\n\nIs there a reason to keep it silent?\n"},{"labels":["api",null,null],"text":"With 0.11 out, Pandas supports more dtypes than before, which is very useful to us science folks. However, some data is intrinsically multi-dimensional, high enough dimensional so that using labels on columns is impractical (for instance, images). \n\nI understand DataFrames or Panels are usually the recommended panacea for this problem. This works if the datatype doesn't have any annotation. For instance, for each frame of a video, I have electrophysiology traces, timestamps, and environmental variables measured. \n\nI have a working solution where I explicitly separate out the non-scalar data from the scalar data. I use Pandas exclusively for the scalar data, and then a dictionary of multi-D arrays for the array data. \n\nWhat is the work and overhead involved in supporting multi-D data types? I would love to keep my entire ecosystem in Pandas, as it's much faster and richer than just NumPy data wrangling.\n\nSee below for the code that I hope is possible to run, with fixes.\n\nIf you can point me to a place in the codebase where I can tinker, that would also be much appreciated.\n\n```\nimport pandas as pd\nmydtype=np.dtype('(3,3)f4')\npd.Series(np.zeros(3,), dtype=mydtype)\n```\n\n```\nException: Data must be 1-dimensional\n```\n"},{"labels":["api",null],"text":"related to #3418\n\nshould auto generate class methods for the io\nand delay the imports until actually called\n\n(including html, excel, csv, hdf, dta)\n\ndeprecate these  methods as well:\nhttps://mail.google.com/mail/u/0/?shva=1#inbox/13ec1cd9b60c5bda\n\nat the very least should deprecate:\n\n```\nfrom_csv -> pd.read_csv\nfrom_dta -> pd.read_dta\n```\n"},{"labels":["api",null,null,null],"text":"http://stackoverflow.com/questions/16022094/using-pandas-to-read-text-file-with-leading-whitespace-gives-a-nan-column\n"},{"labels":["api"],"text":"https://github.com/pydata/pandas/pull/3314#issuecomment-16300121, `options.display.dayfirst` modifies parsing behaviour in `parse_time_string`, which controls \"smart slicing\" for timeseries.\nConsider seperating the display and parsing behaviour. the parsing\nbehaviour belongs in `options.mode`. with a warning in 0.12 and the change\nin 0.13 if we do it. \n\nopinions?\n\n```\nIn [2]: pd.describe_option(\"dayfirst\")\ndisplay.date_dayfirst: \n: boolean\n        When True, prints and parses dates with the day first, eg 20/01/2005\n```\n\nThis has ancient roots: https://github.com/pydata/pandas/commit/6876725e#diff-7c0967e4a4ffec815d7be12079eaed6aR126\n"},{"labels":["api",null],"text":"from mailing list\n\n```\nI have a sort of philosophical question about the use of\nindexes (especially MultiIndexes) versus just keeping data in\ncolumns.  When using groupby, you tend to get a lot of results\nwith MultiIndexes, and the indexes are convenient for simple\naccessing of items.  However, I've found that index objects lack\nkey features of ordinary columns.  I often find myself swapping a\nparticular dimension back and forth from index to column, either\nbecause I need one or other, or because Pandas gives me one when\nI want the other.  What I'm wondering is if I'm using pandas in a\nnonidiomatic way, or if there's some way to get around these\ndifficulties I'm having, or what.\n\nThe three main things I've noticed right now to be irritating\nabout Index objects are:\n\nA) Extracting and using the level values is awkward.  When I have\na column, I can get the values just with df.SomeCol or\ndf['SomeCol'].  For indexes, I have to do\ndf.index.get_level_values('IndexLevel'), and even then I just get\nanother Index instance, which I may have to convert to a series\nfor other things, because. . .\n\nB) Indexes do not support the convenient convenient operations on\nSeries, in particular Series.map.  This means that, although I\ncan easily do df1.ix[df.SomeCol.map(someThingElse)], I cannot do\nthis when SomeCol is an index instead of just a column in the\ndata.  I have to extract the index level values as above and then\nconvert to a series before I can map them.\n\nC) There doesn't appear to be a way to group a DataFrame by a\ncombination of columns and index levels.  groupby allows a \"by\"\nargument for columns and a \"level\" argument for index levels, but\nusing both gives an error.  Even if I could do this, it's not\nclear how I would specify the order of the grouping.\n\nThe solutions that come to mind for these problems are: A) give\nMultiIndex objects a simple means of accessing the level values\nas a Series.  Something like df.index.levels.Level or df\nindex.levels['Level'].  Basically make MultiIndexes indexable in\nsomewhat the same way that DataFrames already are. B) Give\nIndexes a map-like operator, and maybe some of the other useful\nstuff from Series.  C) Provide some way of grouping using both\ncolumns and index levels.  Maybe some sort of \"IndexGroup\" class\nthat would wrap a level name, so you could do groupby([\"Column\",\nIG(\"IndexLevel\"), \"OtherColumn\"]) to insert an index level in the\ngrouping order.\n\nPandas provides a lot of functionality for slicing and dicing the\ndata in the different ways, but I feel like sometimes I'm forced\nto slice it and dice it back and forth by converting indexes to\ncolumns and vice versa instead of being able to directly access\nwhat I want.  I'd be interested to hear how/whether other people\ndeal with these issues.  Are there ways of doing these things\nthat I'm missing?\n```\n"},{"labels":["api",null,null,null],"text":"Have you ever written code that looks like this:\r\n\r\n```\r\nif isinstance(d.index, MultiIndex):\r\n    results = []\r\n    for l in d.index.levels:\r\n       for x in baz(l):\r\n          results.append(foo)\r\nelif  isinstance(d.index, Index):\r\n    for x in d.index:\r\n       foo\r\n```\r\n\r\nI've had to special case the handling of index vs. multindex several times in the past.\r\nConceptually, I should be able to treat index as a private case of MultIndex\r\nwith nlevels =1, and supporting that in the API would make things nicer.\r\n\r\n---\r\n\r\n**Edit by @cpcloud:**\r\nTasks :\r\n### API Unification\r\n\r\nMethod unification is relatively simple:\r\n- [ ]  `Index.from_tuples` and `Index.from_arrays` are just `MultiIndex.from_tuples` and `MultiIndex.from_arrays` moved to classmethods of `Index`.\r\n- [ ] `droplevel`, ` just raises on Index (right? what would it mean?): #21115\r\n- [ ] `has_duplicates` is straightforward\r\n- [ ] `truncate` should be equivalent to slicing\r\n- [ ] `reorder_levels` raises if not level=0 or name of index\r\n- [ ] `equal_levels` - straightforward\r\n- [ ] `levshape` - (len(ind),)\r\n- [ ] `sortorder` - None\r\n- [ ] `get_loc_level` - I think meaningless with tuple, raises whatever if not 0 or index name\r\n- [ ] `is_lexsorted` - doesn't need to change\r\n- [ ] `is_lexosrted_tuple` - doesn't need to change\r\n- [ ] `is_monotonic_*`\r\n- [ ] `lexsort_depth` - doesn't need to be changed at all\r\n- [ ] `searchsorted`\r\n- [ ] `repeat`\r\n- [ ] `levels` and `labels` property for Index - question on whether it should be sorted.\r\n- [ ] change to `rename` behavior: `Index` will accept either string or single-element list; MI continues to handle only list\r\n"},{"labels":["api",null,null],"text":"``` python\nIn [10]: ix = [-352.737091, 183.575577]\n    ...: df=pd.DataFrame([0,1],index=ix)\n    ...: df.to_csv(\"/tmp/1.csv\")\n    ...: df2=pd.DataFrame.from_csv(\"/tmp/1.csv\")\n    ...: print df\n    ...: print df2\n             0\n-352.737091  0\n 183.575577  1\n                            0\n2105-11-21 22:43:41.128654  0\n1936-11-21 22:43:41.128654  1\n```\n\nand if you try:\n\n```\n\nIn [13]: df2.index.astype('f')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-13-8d8276604fe4> in <module>()\n----> 1 df2.index.astype('f')\n\n/home/user1/src/pandas/pandas/tseries/index.pyc in astype(self, dtype)\n    624             return self.asi8.copy()\n    625         else:  # pragma: no cover\n--> 626             raise ValueError('Cannot cast DatetimeIndex to dtype %s' % dtype)\n    627 \n    628     @property\n\nValueError: Cannot cast DatetimeIndex to dtype float32\n```\n\nrelated [SO](http://stackoverflow.com/questions/10591000/specifying-data-type-in-pandas-csv-reader)\n"},{"labels":["api",null,null,null],"text":"http://stackoverflow.com/questions/15462344/writing-the-result-of-multiple-calls-on-a-manipulated-dataframe\n"},{"labels":["api",null,null],"text":"``` python\nIn [17]: s=pd.date_range(\"2001-1-1\", periods=12)\nIn [20]: ts['2001']\nOut[20]: \n2001-01-01     0\n2001-01-02     1\n2001-01-03     2\n2001-01-04     3\n2001-01-05     4\n2001-01-06     5\n2001-01-07     6\n2001-01-08     7\n2001-01-09     8\n2001-01-10     9\n2001-01-11    10\n2001-01-12    11\nFreq: D, dtype: int64\n\nIn [24]: df=pd.DataFrame(range(len(s)),index=s)\nIn [25]: df['2001']\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nKeyError: u'no item named 2001'\n\nIn [27]: df.index is ts.index\nOut[27]: True\n```\n\nrelated #2091\n"},{"labels":["api",null,null,null],"text":"```\n>>> def applym(df):\n...     print df.irow(0)['a']\n...     return DataFrame({'a':[1],'b':[1]})\n\n>>> df = DataFrame({'a':[1,1,1,2,3],'b':['a','a','a','b','c']})\n\n>>> df.groupby('b').apply(applym)\n1\n1\n2\n3\n     a  b\nb        \na 0  1  1\nb 0  1  1\nc 0  1  1\n>>>\n```\n\napplym is called twice on the first group.\n"},{"labels":["api",null,null],"text":"what is the best way to safely slice using multiple index labels? (i.e. not create new NaN rows?) is the best way to do this to intersect the index with the searched values before searching?\n\n```\nimport pandas\ndf = pandas.DataFrame([[1,2], [3,4]], index=['X', 'Y'], columns=['A', 'B'])\ndf.ix[['X', 'Z'], :]\n\nOut[1]: \n    A   B\nX   1   2\nZ NaN NaN    < -- why does this not error?\n```\n"},{"labels":["api"],"text":"Closing #2330 , but leaving this here so it'll get picked up one day if needed.\nrelated #2318.\n\nhttps://groups.google.com/forum/m/#!topic/pydata/ksIPthf2NJI\n"},{"labels":["api",null,null,null],"text":"Using pandas 0.10. If we create a Dataframe with a multi-index, then delete all the rows with value X, we'd expect the index to no longer show value X. But it does.\n   Note the apparent inconsistency between \"index\" and \"index.levels\" -- one shows the values have been deleted but the other doesn't.\n\n``` python\nimport pandas\n\nx = pandas.DataFrame([['deleteMe',1, 9],['keepMe',2, 9],['keepMeToo',3, 9]], columns=['first','second', 'third'])\nx = x.set_index(['first','second'], drop=False)\n\nx = x[x['first'] != 'deleteMe'] #Chop off all the 'deleteMe' rows\n\nprint x.index #Good: Index no longer has any rows with 'deleteMe'. But....\n\nprint x.index.levels #Bad: index still shows the \"deleteMe\" values are there. But why? We deleted them.\n\nx.groupby(level='first').sum() #Bad: it's creating a dummy row for the rows we deleted!\n```\n\nWe don't want the deleted values to show up in that groupby. Can we eliminate them?\n"},{"labels":["api",null],"text":"Not quite sure if this is intentional behavior, but noticed that df.copy() accepts argument deep and series.copy() does not.  \n\n> > > from pandas import Series, DataFrame\n> > > s=Series([1,2,3])\n> > > df=DataFrame(s)\n> > > df.copy(deep=True)\n> > >       0\n> > >    0  1\n> > >    1  2\n> > >    2  3\n> > > s.copy(deep=True)\n> > >    Traceback (most recent call last):\n> > >    File \"<stdin>\", line 1, in <module>\n> > >     TypeError: copy() got an unexpected keyword argument 'deep'\n"},{"labels":["api",null,null],"text":"Summary:\n\nInput:\n\n``` python\ndf.D.ix['c1','d1']\nt1    0\nt2    0\nt3    1\nt4    1\nt5    1\nName: D\n```\n\nOperation:\n\n``` python\ngrouped = df.groupby('D')\nfor i,j in grouped:\n    print 'D:',i\n    print 'Actual index[2]:',j.index[0][2]\n    print 'First element of levels[2]:',j.index.levels[2][0]\n```\n\nOutput:\n\n``` python\nD: 0.0\nActual index[2]: t1\nFirst element of levels[2]: t1\nD: 1.0\nActual index[2]: t3\nFirst element of levels[2]: t1\n```\n\nDetails:\n\nhttp://nbviewer.ipython.org/4482106/\n"},{"labels":["api",null,null],"text":"This seems to be solvable my using df.sortlevel, but Wesley encouraged me to submit it for further investigation.\nI wrote a pretty detailed investigation here:\n\nhttp://nbviewer.ipython.org/4465051/\n\nwith the gist\n\nhttps://gist.github.com/4465051\n\nIn short words:\n2-level indexing for a 3-level multi-index fails for me, when the size of one of the first 2 levels is larger than 9 and _not_ using sortlevel.\nIn examples:\n\ndf.ix['c1','det4'] works as long as the sizes of the multi-index are up to (9,9,x) with x anything.\nBut if the levels size is (10,9,x) I am forced to use sortlevel(0) to make it work. (see notebook).\n"},{"labels":["api",null,null],"text":"Currently is \"F\"-like ordering\n"},{"labels":["api",null],"text":"I want to use the quantities package and pandas to process scientific data.  However, pandas strips the unit(s) of the data stored in numpy arrays if I create a dataframe out of them:\n\n``` python\na = np.random.rand(10)*pq.s\nb = np.random.rand(10)*pq.A\ndf = pd.DataFrame({'current':b, 't':a}, columns=['t','current'])\n```\n\n``` python\nIn [1]: df\nOut[1]: \n         t   current\n0  0.663397  0.435423\n1  0.038498  0.101763\n2  0.960983  0.091785\n3  0.262863  0.364734\n4  0.154440  0.274169\n5  0.953129  0.052678\n6  0.389961  0.272535\n7  0.961604  0.559451\n8  0.747192  0.438268\n9  0.789207  0.568685\n\n```\n\nWhat do you think,  should pandas have support for writing the unit of each column in the corresponding columnname if the column is a quantities array,  or should it be part of the quantities package.\n\n``` python\nIn [1]: df\nOut[1]: \n       t [s]  current [A]\n0  0.663397  0.435423\n1  0.038498  0.101763\n2  0.960983  0.091785\n....\n```\n"},{"labels":["api",null,null,null,null],"text":"I noticed that when using a boolean comparison operator on a column with null values, Pandas does not propogate the null values to the result. For example, if I do the following:\n\n```\ndf.foo > 0\n```\n\nthe resulting series includes False entries where there were missing values in `foo`.\n"},{"labels":["api",null,null,null],"text":"concat thows an `AssertionError` when passed a ciollection of mixed Series and Dataframes e.g.\n\n``` python\ns1 = pd.TimeSeries(np.sin(linspace(0, 2*pi, 100)), \n                   index=pd.date_range('01-Jan-2013', \nperiods=100, freq='H')) \n\ns2 = pd.TimeSeries(np.cos(linspace(0, 2*pi, 100)), \n                  index=pd.date_range('01-Jan-2013', \nperiods=100, freq='H')) \n\ndf = pd.DataFrame(np.cos(linspace(0, 2*pi, \n100)).reshape(-1,1), \n                   index=pd.date_range('01-Jan-2013', \nperiods=100, freq='H')) \n\nIn [23]: pd.concat([df,df], axis=1).shape \nOut[23]: (100, 2) \n\nIn [24]: pd.concat([s1,s2], axis=1).shape \nOut[24]: (100, 2) \n\nIn [25]: pd.concat([s1,s2,s1], axis=1).shape \nOut[25]: (100, 3) \n\nIn [26]: pd.concat([s1,df,s2], axis=1).shape \nTraceback (most recent call last):\n\n  File \"<ipython-input-9-4512588e71f2>\", line 1, in <module>\n    pd.concat([s1,df,s2], axis=1).shape\n\n  File \"c:\\dev\\code\\pandas\\pandas\\tools\\merge.py\", line 881, in concat\n    return op.get_result()\n\n  File \"c:\\dev\\code\\pandas\\pandas\\tools\\merge.py\", line 960, in get_result\n    columns=self.new_axes[1])\n\n  File \"c:\\dev\\code\\pandas\\pandas\\core\\frame.py\", line 376, in __init__\n    mgr = self._init_dict(data, index, columns, dtype=dtype)\n\n  File \"c:\\dev\\code\\pandas\\pandas\\core\\frame.py\", line 505, in _init_dict\n    dtype=dtype)\n\n  File \"c:\\dev\\code\\pandas\\pandas\\core\\frame.py\", line 5181, in _arrays_to_mgr\n    mgr = BlockManager(blocks, axes)\n\n  File \"c:\\dev\\code\\pandas\\pandas\\core\\internals.py\", line 499, in __init__\n    self._verify_integrity()\n\n  File \"c:\\dev\\code\\pandas\\pandas\\core\\internals.py\", line 584, in _verify_integrity\n    raise AssertionError('Block shape incompatible with manager')\n\nAssertionError: Block shape incompatible with manager\n\nIn [27]: pd.__version__\nOut[27]: '0.10.0.dev-fbd77d5'\n```\n"},{"labels":["api",null,null],"text":"when there are ties, whether the next rank will be the nearest integer to the current rank or tie-count ranks away.\n"},{"labels":["api",null,null,null,null],"text":"if resampled index bin edges don't correspond to the original index, snap the starting data according to the specified convention\n\nhttp://stackoverflow.com/questions/12844900/upsampling-to-weekly-data-in-pandas\n"},{"labels":["api",null,null],"text":"```\ndf3 = DataFrame({\"a\":[1,2,3,4], \"b\":[1,2,3,4]})\ndf3.ix[:,[\"b\",\"c\"]]\nOut[1]: \n   b   c\n0  1 NaN\n1  2 NaN\n2  3 NaN\n3  4 NaN\n```\n\nI would have (had...) expected that the `df3.ix[:,[\"b\",\"c\"]]` will throw an error :-(\n"},{"labels":["api",null,null],"text":"1. Returns DataFrame for SeriesGroupBy.transform\n2. Returns DataFrame with MultiIndexed columns for DataFrameGroupBy.transform\n3. `chain` parameter to control whether transforms are applied in sequence or not\n"},{"labels":["api",null,null,null],"text":"see related discussion in #2325\n\nPossible and/or desirable to add an inplace keyword to dropna? Maybe an out keyword, so you can avoid typing the assignment? [**Edit:** I guess out doesn't really make sense like it does for numpy ufuncs.] Don't really mind if there's a copy even I just get tired of always doing assignment to the same name when I'm doing pre-, post-processing, etc.\n\n```\ndataframe = dataframe.action()\n```\n"},{"labels":["api",null,null,null],"text":"a la groupby-aggregate\n"},{"labels":["api",null,null],"text":"Something seems to be wrong with s1 == s2 when s1 and s2 don't have the same index. Here is a snippet example:\n\n``` python\nimport operator\nimport pandas\ns1 = pandas.Series([1,2], ['a','b'])\ns2 = pandas.Series([2,3], ['b','c'])\ns1 == s2\ns2 == s1\n```\n\nwith the output:\n\n```\nInIn [5]: s1 == s2\nOut[5]: \na    False\nb    False\n\nIn [6]: s2 == s1\nOut[6]: \nb    False\nc    False\n```\n\nOn the other hand using combine works fine:\n\n```\nIn [7]: s1.combine(s2, operator.eq)\nOut[7]: \na    0\nb    1\nc    0\n\nIn [8]: s2.combine(s1, operator.eq)\nOut[8]: \na    0\nb    1\nc    0\n```\n\nI guess you can first align s1 and s2 and then compare them, but is there a good reason why this couldn't work out of the box?\n\nThere doesn't seem to be any tests for pandas.Series. __eq__ for two series with a different index in pandas/pandas/tests/test_series.py. I have a patch lying around to add such a test and I could commit it if that's useful.\n"},{"labels":["api",null,null],"text":""},{"labels":["api",null,null,null],"text":""},{"labels":["api",null,null,null],"text":""},{"labels":["api",null],"text":"There is a lot of semi-duplication / differences in the `Series()` and `DataFrame()` methods.\n\nMaybe it's worth taking a look through them and regularising.\n\nFwiw, I prefer  `tostring` vs. `to_string`\n\nAlso, would be nice to have `tohtml` on things, though expanding it is beyond the scope of this.\n"},{"labels":["api",null,null],"text":"was: should DataFrames have a name attribute?\n\n@y-p\n"},{"labels":["api",null,null,null],"text":"from @lodagro:\n\nBeen thinking a bit on uniformering signatures for plot, hist, boxplot and what they do/return.\nFor reference, below an overview what pandas and matplotlib.pyplot have.\n\nsome things that come to mind\n- +1 for what you did with boxplot, _by_ and _column_ are handy, this could also be used for other kind of plots.\n- [ ] bar-plot goes through plot, why not bar() itself.\n- [ ] maybe pie() to?\n- [ ] plot offers possibility to control _sharex_ and _sharey_, others don not have this control.\n- [ ] not all functions use the same subplot layout approach. DataFrame.plot() can plot all lines on a single axis or a nx1 layout. DataFrame.hist() uses nxn layout and DataFrame.boxplot is clever and can do nxm, but the user has no control. Maybe add _nrow_, _ncol_ arguments? Default to None, meaning pandas can control layout, if either one defined pandas should compute the other one. Can get tricky for plot(), need to do something with _subplots_ argument\n- [ ] usage of **_kwds_, e.g boxplot has it, does not use this. maybe add _subplot_kw_ and _figure_kw_ arguments for dispatching argmuments -- like matplotlib does.\n- [ ] Series.plot has _style_, DataFrame.plot not -- later could maybe use style/colum (like _formatters_ in to_string)?\n- [ ] rot not used everywhere\n- [ ] probably many users of pandas are familiar with matplotlib too, in general align plotting signatures and return objects with matplotlib would be a good thing to do?\n\nMaybe if i stare at it a bit longer i may have some more ideas, but this is getting long already. What do you think?\n# for reference\n## Series:\n\n```\nplot(self, label=None, kind='line', use_index=True, rot=30, ax=None, style='-',\n     grid=True, **kwds)\n\nhist(self, ax=None, grid=True, **kwds)\n```\n## DataFrame:\n\n```\nboxplot(self, column=None, by=None, ax=None, fontsize=None,\n            rot=0, grid=True, **kwds)\n\nplot(self, subplots=False, sharex=True, sharey=False, use_index=True,\n         figsize=None, grid=True, legend=True, rot=30, ax=None,\n         kind='line', **kwds)\n\ndef hist(self, grid=True, **kwds):\n```\n## matplotlib.pyplot\n\n```\nfigure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None,\n             frameon=True, FigureClass=<class 'matplotlib.figure.Figure'>,\n             **kwargs)\n\nfig, ax = subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True,\n                   subplot_kw=None, **fig_kw)\n---> always creates a new figure\n\nplot(*args, **kwargs)\n    returns list of matplotlib.lines.Line2D\n\nboxplot(x, notch=0, sym='b+', vert=1, whis=1.5, positions=None,\n            widths=None, patch_artist=False, bootstrap=None, hold=None)\n    Returns a dictionary, mapping each component of the boxplot\n    to a list of the :class:`matplotlib.lines.Line2D`\n    instances created.\n\nplt.pie(x, explode=None, labels=None, colors=None, autopct=None,\n        pctdistance=0.6, shadow=False, labeldistance=1.1, hold=None)\n   Return value:\n      If *autopct* is None, return the tuple (*patches*, *texts*):\n\n        - *patches* is a sequence of\n          :class:`matplotlib.patches.Wedge` instances\n\n        - *texts* is a list of the label\n          :class:`matplotlib.text.Text` instances.\n\n      If *autopct* is *None*, return the tuple (*patches*, *texts*)\n      If *autopct* is not *None*, return the tuple (*patches*, *texts*, *autotexts*)\n\nplt.bar(left, height, width=0.8, bottom=None, hold=None, **kwargs)\n    Return value is a list of matplotlib.patches.Rectangle instances\n```\n"}]