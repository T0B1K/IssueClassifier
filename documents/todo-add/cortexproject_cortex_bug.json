[{"labels":["bug"],"text":"I encountered an issue where the querier component would consistently return 503:s on /api/prom/api/v1/query if given \"unusual\" configuration. There are no logs emitted that indicate what is going wrong, and adding tracing there isn't much to go on either:\r\n![image](https://user-images.githubusercontent.com/214867/91046954-99600880-e619-11ea-96b9-00df6427677a.png)\r\n\r\n\r\nThe config is found here: https://gist.github.com/carlpett/544bc3f807140e51aca53aa8dd68fd5b. Notably, I had a querier block **without any content**. That is, like this:\r\n```yaml\r\nquerier:\r\nquery_range:\r\n  [...]\r\n```\r\n\r\nFor some reason, this appears to break the querier setup such that it fails every request with `tsdb.ErrNotReady` here:\r\nhttps://github.com/cortexproject/cortex/blob/2cb22f830dfd37b199978ad6191a18f444cdae61/vendor/github.com/prometheus/prometheus/web/api/v1/api.go#L263\r\n\r\nI haven't determined exactly _why_ having this config breaks, yet.\r\n\r\n(Following Slack discussions [here](https://cloud-native.slack.com/archives/CCYDASBLP/p1598006503011500))"},{"labels":["bug"],"text":"These lines: \r\nhttps://github.com/cortexproject/cortex/blob/a4aad5da5e4ea05110d7ff389e736b62ecbe6f34/pkg/querier/distributor_queryable.go#L91-L104\r\n\r\nFrom this PR:\r\nhttps://github.com/cortexproject/cortex/pull/2904\r\n\r\nare currently preventing metadata queries from correctly hitting the ingesters in chunk storage.  If a metadata query has a time range from before the ingesters it should still be passed in.  Ingesters ignore ranges for metadata queries so it's ok to pass older time ranges to them.\r\n\r\nhttps://github.com/cortexproject/cortex/blob/master/pkg/ingester/ingester.go#L833-L837\r\n\r\ncc @gouthamve \r\n\r\n"},{"labels":[null,"bug"],"text":"I use configdb to make a http request to get rule group config, when first run cortex ruler, it will store user rule group and rules to ./rules/username/filename, but when i restart cortex ruler with no any update, it will get rule group from configdb, and then mapper to ./rules/username/filename, ruler check there is no any update, just continue to boot. As a result, it ignore load any rules...\r\n\r\nIn short, \r\n1. ruler first start and get rules from configdb which make a http request\r\n2. ruler set user rules to userManagers and mapping rules to a file\r\n3. restart ruler\r\n4. ruler second get fules form configdb\r\n5. ruler compare and mapping rules to file\r\n6. no update, then ruler alse not load to userManager\r\n7. as a result, no any rules load"},{"labels":[null,null,"bug"],"text":"Currently the chunk cache will be named without a `.` character between `chunk` and the backend. For instance `chunksmemcache` is the name of the chunk cache in metrics labels if you are using memcache. This is not very clean formatting and the other caches are named with a `.` character between the cache type and backend."},{"labels":["bug"],"text":"I have tried to run Cortex with release-1.2 branch.\r\nBut I found the error when starting Cortex with configurations below.\r\n\r\n**Configs**\r\n\r\n```yaml\r\n...\r\n\"ingester\":\r\n  \"lifecycler\":\r\n    \"ring\":\r\n      \"kvstore\":\r\n        \"store\": \"multi\"\r\n        \"consul\":\r\n          \"host\": \"consul.host:8500\"\r\n        \"multi\":\r\n          \"primary\": \"consul\"\r\n          \"secondary\": \"memberlist\"\r\n...\r\n```\r\n\r\n**Error logs**\r\n\r\n```\r\npanic: a previously registered descriptor with the same fully-qualified name as Desc{fqName: \"cortex_kv_request_duration_seconds\", help: \"Time spent on kv store requests.\", constLabels: {kv_name=\"ingester-ring\",type=\"multi\"}, variableLabels: [operation status_code]} has different label names or a different help string\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/prometheus/client_golang/prometheus.(*wrappingRegisterer).MustRegister(0xc000817b30, 0xc0003fa980, 0x1, 0x1)\r\n\t/my/path/cortex/vendor/github.com/prometheus/client_golang/prometheus/wrap.go:102 +0x194\r\ngithub.com/prometheus/client_golang/prometheus/promauto.Factory.NewHistogramVec(0x5a04e80, 0xc000817b30, 0x509ffbf, 0x6, 0x0, 0x0, 0x50df2ab, 0x1b, 0x50f141d, 0x20, ...)\r\n\t/my/path/cortex/vendor/github.com/prometheus/client_golang/prometheus/promauto/auto.go:362 +0x158\r\ngithub.com/cortexproject/cortex/pkg/ring/kv.newMetricsClient(0xc000453e30, 0x5, 0x5a487c0, 0xc000b6c5c0, 0x5a04e80, 0xc000817b30, 0x0, 0x0)\r\n\t/my/path/cortex/pkg/ring/kv/metrics.go:43 +0x251\r\ngithub.com/cortexproject/cortex/pkg/ring/kv.createClient(0xc000453e30, 0x5, 0x50ab0f0, 0xb, 0xc0007f6520, 0x1a, 0x0, 0x0, 0x4a817c800, 0x0, ...)\r\n\t/my/path/cortex/pkg/ring/kv/client.go:159 +0x2c4\r\n```"},{"labels":["bug"],"text":"This panic happened while calling `/flush` endpoint on ingester in internal cluster. Ingester was running `grafana/cortex-cortex:r92-a0674c0c2` (https://github.com/grafana/cortex/commits/a0674c0c2f528f8aa5297768604e17ca7ebb047f)\r\n\r\n```\r\nlevel=info ts=2020-06-18T08:23:49.848675718Z caller=flush.go:42 msg=\"starting to flush all the chunks\"\r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x279a9b7]\r\n\r\ngoroutine 1288 [running]:\r\ngithub.com/cortexproject/cortex/pkg/ingester.(*Ingester).flushChunks(0xc0006e8800, 0x3ee0260, 0xc002814d20, 0xc000e0c6b0, 0x3, 0x87ff6a90a1aa9836, 0xc0191ae160, 0xb, 0xb, 0xc1c44a5180, ...)\r\n\t/go/src/github.com/cortexproject/cortex/pkg/ingester/flush.go:344 +0x637\r\ngithub.com/cortexproject/cortex/pkg/ingester.(*Ingester).flushUserSeries(0xc0006e8800, 0x20, 0xc000e0c6b0, 0x3, 0x87ff6a90a1aa9836, 0x1, 0x0, 0x0)\r\n\t/go/src/github.com/cortexproject/cortex/pkg/ingester/flush.go:291 +0x843\r\ngithub.com/cortexproject/cortex/pkg/ingester.(*Ingester).flushLoop(0xc0006e8800, 0x20)\r\n\t/go/src/github.com/cortexproject/cortex/pkg/ingester/flush.go:205 +0x10a\r\ncreated by github.com/cortexproject/cortex/pkg/ingester.(*Ingester).startFlushLoops\r\n\t/go/src/github.com/cortexproject/cortex/pkg/ingester/ingester.go:256 +0x68\r\n```"},{"labels":[null,"bug"],"text":"We had a malfunctioning store-gateway instance logging `write: broken pipe` each time it tried to fetch chunks from GCS. I looks like the issue has started with a broken TCP connection (which is OK) but then didn't recover closing it and opening a new one. Restarting the store-gateway fixed the issue, but we should fix the root cause.\r\n\r\nThis kind of logs were continuously repeated:\r\n```\r\nlevel=warn ts=2020-06-10T07:15:35.181518285Z caller=grpc_logging.go:55 method=/gatewaypb.StoreGateway/Series duration=11.744754ms err=\"rpc error: code = Aborted desc = fetch series for block 01E95411Z8PHCFPNMCAHD91VAQ: preload chunks: read range for 133: get range reader: failed to get object attributes: 10428/01E95411Z8PHCFPNMCAHD91VAQ/chunks/000134: Get https://storage.googleapis.com/storage/v1/b/REDACTED-BUCKET/o/10428%2F01E95411Z8PHCFPNMCAHD91VAQ%2Fchunks%2F000134?alt=json&prettyPrint=false&projection=full: write tcp REDACTED-LOCAL-IP:46668->REDACTED-GCS-IP:443: write: broken pipe\" msg=\"gRPC\\n\"\r\n```\r\n"},{"labels":[null,"bug"],"text":"https://github.com/cortexproject/cortex/blob/master/pkg/ingester/ingester.go#L833-L834\r\n\r\n> // TODO Right now we ignore start and end.\r\n\r\nThis breaks away from the Prometheus API standard for /api/v1/series (https://prometheus.io/docs/prometheus/latest/querying/api/#finding-series-by-label-matchers) which causes problems with things like Grafana generating variable values from labels (https://grafana.com/docs/grafana/latest/features/datasources/prometheus/#templating) for something not occurring in the current timeframe"},{"labels":[null,"bug"],"text":"Our staging system was having a lot of trouble at the time; an ingester restarted in the middle of chunk transfer, and the new one came up like this:\r\n\r\n```\r\nlevel=info ts=2020-04-23T08:17:50.723234677Z caller=server.go:147 http=[::]:80 grpc=[::]:9095 msg=\"server listening on addresses\"\r\nlevel=info ts=2020-04-23T08:17:50.807409996Z caller=main.go:109 msg=\"Starting Cortex\" version=\"(version=, branch=, revision=)\"\r\nlevel=info ts=2020-04-23T08:17:50.808716894Z caller=module_service.go:58 msg=initialising module=store\r\nlevel=info ts=2020-04-23T08:17:50.80885024Z caller=module_service.go:58 msg=initialising module=memberlist-kv\r\nlevel=info ts=2020-04-23T08:17:50.808923431Z caller=module_service.go:58 msg=initialising module=runtime-config\r\nlevel=info ts=2020-04-23T08:17:50.828857014Z caller=module_service.go:58 msg=initialising module=ingester\r\nlevel=info ts=2020-04-23T08:17:50.831292972Z caller=cortex.go:329 msg=\"Cortex started\"\r\nlevel=info ts=2020-04-23T08:17:50.832498033Z caller=lifecycler.go:483 msg=\"not loading tokens from file, tokens file path is empty\"\r\nlevel=info ts=2020-04-23T08:17:50.847328813Z caller=lifecycler.go:523 msg=\"existing entry found in ring\" state=JOINING tokens=0 ring=ingester\r\n```\r\n\r\nIt never came out of that state.\r\n\r\nI think it should have gone to PENDING.\r\n\r\nThis is the previous log of the same pod:\r\n\r\n```\r\nlevel=info ts=2020-04-23T08:16:13.599829183Z caller=server.go:147 http=[::]:80 grpc=[::]:9095 msg=\"server listening on addresses\"\r\nlevel=info ts=2020-04-23T08:16:13.635349396Z caller=main.go:109 msg=\"Starting Cortex\" version=\"(version=, branch=, revision=)\"\r\nlevel=info ts=2020-04-23T08:16:13.636162969Z caller=module_service.go:58 msg=initialising module=memberlist-kv\r\nlevel=info ts=2020-04-23T08:16:13.640778915Z caller=module_service.go:58 msg=initialising module=store\r\nlevel=info ts=2020-04-23T08:16:13.640839028Z caller=module_service.go:58 msg=initialising module=runtime-config\r\nlevel=info ts=2020-04-23T08:16:13.642611475Z caller=module_service.go:58 msg=initialising module=ingester\r\nlevel=info ts=2020-04-23T08:16:13.645246274Z caller=cortex.go:329 msg=\"Cortex started\"\r\nlevel=info ts=2020-04-23T08:16:13.64597218Z caller=lifecycler.go:483 msg=\"not loading tokens from file, tokens file path is empty\"\r\nlevel=info ts=2020-04-23T08:16:13.657000497Z caller=lifecycler.go:507 msg=\"instance not found in ring, adding with no tokens\" ring=ingester\r\nlevel=info ts=2020-04-23T08:16:13.915753953Z caller=lifecycler.go:687 msg=\"changing instance state from\" old_state=PENDING new_state=JOINING ring=ingester\r\nlevel=info ts=2020-04-23T08:16:13.934309827Z caller=transfer.go:62 msg=\"processing TransferChunks request\" from_ingester=ingester-77c7f78dfc-9k885\r\nlevel=error ts=2020-04-23T08:17:05.767313959Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\"\r\nlevel=error ts=2020-04-23T08:17:15.122672732Z caller=lifecycler.go:208 msg=\"error talking to the KV store\" ring=ingester err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:43.102705665Z caller=client.go:155 msg=\"error CASing\" key=collectors/ring err=\"Put http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?cas=853028: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\"\r\nlevel=error ts=2020-04-23T08:17:43.12035925Z caller=lifecycler.go:208 msg=\"error talking to the KV store\" ring=ingester err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:43.158005204Z caller=lifecycler.go:208 msg=\"error talking to the KV store\" ring=ingester err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:43.161254282Z caller=lifecycler.go:208 msg=\"error talking to the KV store\" ring=ingester err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:43.775036079Z caller=transfer.go:195 msg=\"TransferChunks failed, not in ACTIVE state.\" state=JOINING\r\nlevel=info ts=2020-04-23T08:17:44.824577939Z caller=lifecycler.go:687 msg=\"changing instance state from\" old_state=JOINING new_state=PENDING ring=ingester\r\nlevel=error ts=2020-04-23T08:17:44.824725798Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.824772356Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.824809695Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.824841311Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.824874085Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.824905197Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.824934337Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.824978719Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.825005878Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.825032307Z caller=client.go:115 msg=\"error getting key\" key=collectors/ring err=\"Get http://consul.cortex.svc.cluster.local.:8500/v1/kv/collectors/ring?stale=: context canceled\"\r\nlevel=error ts=2020-04-23T08:17:44.828248718Z caller=transfer.go:200 msg=\"error rolling back failed TransferChunks\" err=\"failed to CAS collectors/ring\"\r\n```"},{"labels":[null,"bug"],"text":"After #2437, the querier no longer passes requests from the worker to the weaveworks/common server. This means none of the instrumentation/logging middleware is used on the request.\r\n\r\nhttps://github.com/cortexproject/cortex/blob/fa0fc64235063d770d6aac223b11e016cf64fcf2/pkg/cortex/modules.go#L213\r\n\r\nThe two possible solutions I see are as follows:\r\n\r\n1. Update the weaveworks common server to allow for more control over the instrumentation middleware it configures.\r\n\r\n2. Use a separate set of metrics instrumented specifically for the queries the worker services."},{"labels":[null,"bug"],"text":"Yesterday I've noticed that, in a Cortex cluster with 3 ingesters and a replication factor of 3, during the ingesters rollout there are gaps in queried series (for the series not flushed to storage yet). My gut feeling is that this is a bug in the ring replication strategy, but further investigation needs to be done."},{"labels":["bug"],"text":"Currently the weaveworks/common server has the option for a `-server.path-prefix` flag. Setting this flag in Cortex will cause all requests routed to the prometheus API struct to fail with a 404.\r\n\r\nPrometheus registers routes with a hardcoded `/api/prom/api/v1/` prefixed route. When requests are routed from the weaveworks server to the Prometheus API `promRouter` they are expected to have the `/api/prom/api/v1` prefix. If the flag `-server.path-prefix` is set to `/example`, then traffic will only be routed to the promrouter at the `/example/api/prom/api/v1` route. Since this path is different then the hardcoded promrouter prefix, it will always return a 404."},{"labels":["bug"],"text":"While upgrading to Cortex 1.0.0 and doing a rolling update of ingesters, I've noticed only the last rolled out ingester has a zone set in the ring, while all previous one (already rolled out) haven't.\r\n\r\nThe issue is caused by the co-existence of old and new ingesters. Old ingesters don't know the zone in the ring data structure so when they do deserialise the ring, the zone is lost. This issue disappear once the last ingester rollout is completed but at this point the zone for all ingesters (except the last one) is already lost."},{"labels":["bug"],"text":"Following the deployment of #2317 into my staging environment, distributors, queriers and ruler all started erroring like this:\r\n\r\ntime=\"2020-03-29T22:43:33Z\" level=warning msg=\"POST /api/prom/push (500) 2.254634ms Response: \\\"at least 2 live replicas required, could only find 1\\\\n\\\" ws: false; Content-Encoding: snappy; Content-Length: 16461; Content-Type: application/x-protobuf; User-Agent: Prometheus/2.12.0; X-Prometheus-Remote-Write-Version: 0.1.0; \" traceID=77042d0abcee28fd\r\n\r\nIngester was on `master-0473dd88` and all ingesters are in the same AZ."},{"labels":[null,"bug"],"text":"If a transfer fails the userState struct is discarded. None of the metrics it reports are updated to reflect this. Most notably the following:\r\n\r\n```\r\ncortex_ingester_memory_series_removed_total\r\ncortex_ingester_memory_series\r\ncortex_ingester_memory_users\r\n```"},{"labels":["bug"],"text":"As of commit 59bbc46c, in ingester, querier and ruler.\r\nBeginning of log:\r\n\r\n```\r\nlevel=info ts=2020-03-19T17:56:09.556287259Z caller=server.go:147 http=[::]:80 grpc=[::]:9095 msg=\"server listening on addresses\"\r\npanic: duplicate metrics collector registration attempted\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/prometheus/client_golang/prometheus.(*Registry).MustRegister(0xc0000ce550, 0xc000838670, 0x1, 0x1)\r\n\t/go/pkg/mod/github.com/prometheus/client_golang@v1.5.0/prometheus/registry.go:400 +0xad\r\ngithub.com/thanos-io/thanos/pkg/discovery/dns.NewProvider(0x3a17800, 0xc00013b650, 0x3a70400, 0xc0000ce550, 0x3388a7a, 0x6, 0xc0008385f0)\r\n\t/go/pkg/mod/github.com/thanos-io/thanos@v0.11.0/pkg/discovery/dns/provider.go:74 +0x36a\r\ngithub.com/cortexproject/cortex/pkg/chunk/cache.NewMemcachedClient(0x7ffd811bb6a1, 0x26, 0x338f0c5, 0x9, 0x0, 0x0, 0x5f5e100, 0x40, 0xdf8475800, 0x0, ...)\r\n\t/go/src/github.com/cortexproject/cortex/pkg/chunk/cache/memcached_client.go:101 +0x19c\r\n```\r\n\r\nI guess we need integration tests that include memcached."},{"labels":["bug"],"text":"HI, I am new to Cortex and working on a POC see how it work and suitable for my infrastructure,\r\nwhile I am testing to config with Redis cache server I got the following errors when I do query.\r\n\r\n\r\n```\r\nlevel=error ts=2020-02-14T04:58:54.186907969Z caller=redis_cache.go:87 msg=\"failed to get from redis\" name=store.index-cache-read.redis err=\"ERR wrong number of arguments for 'mget' command\"\r\nlevel=error ts=2020-02-14T04:58:54.187315046Z caller=redis_cache.go:87 msg=\"failed to get from redis\" name=store.index-cache-read.redis err=\"ERR wrong number of arguments for 'mget' command\"\r\n level=error ts=2020-02-14T04:58:54.187487524Z caller=redis_cache.go:87 msg=\"failed to get from redis\" name=chunksredis err=\"ERR wrong number of arguments for 'mget' command\"\r\n level=error ts=2020-02-14T04:58:54.189418228Z caller=redis_cache.go:87 msg=\"failed to get from redis\" name=chunksredis err=\"ERR wrong number of arguments for 'mget' command\"\r\n```\r\n\r\nI am using Cortex v0.6.1 and Redis server version 5\r\n\r\nAnd this is the config file I am using. \r\n\r\n```\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: cortex\r\n  namespace: kube-tools\r\ndata:\r\n  single-process-config.yaml: |\r\n    # Disable the requirement that every request to Cortex has a\r\n    # X-Scope-OrgID header. `fake` will be substituted in instead.\r\n    auth_enabled: false\r\n    server:\r\n      http_listen_port: 9009\r\n      # Configure the server to allow messages up to 100MB.\r\n      grpc_server_max_recv_msg_size: 104857600\r\n      grpc_server_max_send_msg_size: 104857600\r\n      grpc_server_max_concurrent_streams: 1000\r\n    distributor:\r\n      shard_by_all_labels: true\r\n      pool:\r\n        health_check_ingesters: true\r\n    ingester_client:\r\n      grpc_client_config:\r\n        # Configure the client to allow messages up to 100MB.\r\n        max_recv_msg_size: 104857600\r\n        max_send_msg_size: 104857600\r\n        use_gzip_compression: true\r\n    ingester:\r\n      #chunk_idle_period: 15m\r\n      lifecycler:\r\n        # The address to advertise for this ingester.  Will be autodiscovered by\r\n        # looking up address on eth0 or en0; can be specified if this fails.\r\n        # address: 127.0.0.1\r\n        # We want to start immediately and flush on shutdown.\r\n        join_after: 0\r\n        claim_on_rollout: false\r\n        final_sleep: 0s\r\n        num_tokens: 512\r\n        # Use an in memory ring store, so we don't need to launch a Consul.\r\n        ring:\r\n          kvstore:\r\n            store: inmemory\r\n          replication_factor: 1\r\n    # Use local storage - BoltDB for the index, and the filesystem\r\n    # for the chunks.\r\n    schema:\r\n      configs:\r\n      - from: 2019-07-29\r\n        store: boltdb\r\n        object_store: filesystem\r\n        schema: v10\r\n        index:\r\n          prefix: index_\r\n          period: 168h\r\n    storage:\r\n      boltdb:\r\n        directory: /tmp/cortex/index\r\n      filesystem:\r\n        directory: /tmp/cortex/chunks\r\n      index_queries_cache_config:\r\n        redis:\r\n          endpoint: my-redis-host:6379\r\n          password: xxxxx\r\n    limits:\r\n      ingestion_rate: 100000\r\n      max_series_per_metric: 0\r\n    chunk_store:\r\n      chunk_cache_config:\r\n        redis:\r\n          endpoint: my-redis-host:6379\r\n          password: xxxxx\r\n      write_dedupe_cache_config:\r\n        redis:\r\n          endpoint: my-redis-host:6379\r\n          password: xxxxx  \r\n    query_range:\r\n      results_cache:\r\n        cache:\r\n          redis:\r\n            endpoint: my-redis-host:6379\r\n            password: xxxxx\r\n```\r\n```"},{"labels":[null,"bug"],"text":"Trying to update from `ed7c302f` to `v0.5.0-rc.0` fails as follows:\r\n\r\n```\r\nlevel=info ts=2020-01-10T10:26:00.169747333Z caller=cortex.go:236 msg=initialising module=server\r\nlevel=info ts=2020-01-10T10:26:00.170110809Z caller=server.go:117 http=[::]:80 grpc=[::]:9095 msg=\"server listening on addresses\"\r\nlevel=info ts=2020-01-10T10:26:00.170230687Z caller=cortex.go:236 msg=initialising module=configs\r\nlevel=info ts=2020-01-10T10:26:00.23272648Z caller=postgres.go:89 msg=\"running database migrations...\"\r\nlevel=error ts=2020-01-10T10:26:00.236942291Z caller=log.go:141 msg=\"error initializing cortex\" err=\"pq: column \\\"dirty\\\" does not exist in line 0: SELECT version, dirty FROM \\\"schema_migrations\\\" LIMIT 1\\ndatabase migrations failed\\ngithub.com/cortexproject/cortex/pkg/configs/db/postgres.New\\n\\t/go/src/github.com/cortexproject/cortex/pkg/configs/db/postgres/postgres.go:93\\ngithub.com/cortexproject/cortex/pkg/configs/db.New\\n\\t/go/src/github.com/cortexproject/cortex/pkg/configs/db/db.go:88\\ngithub.com/cortexproject/cortex/pkg/cortex.(*Cortex).initConfigs\\n\\t/go/src/github.com/cortexproject/cortex/pkg/cortex/modules.go:422\\ngithub.com/cortexproject/cortex/pkg/cortex.(*Cortex).initModule\\n\\t/go/src/github.com/cortexproject/cortex/pkg/cortex/cortex.go:238\\ngithub.com/cortexproject/cortex/pkg/cortex.(*Cortex).init\\n\\t/go/src/github.com/cortexproject/cortex/pkg/cortex/cortex.go:232\\ngithub.com/cortexproject/cortex/pkg/cortex.New\\n\\t/go/src/github.com/cortexproject/cortex/pkg/cortex/cortex.go:185\\nmain.main\\n\\t/go/src/github.com/cortexproject/cortex/cmd/cortex/main.go:73\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\\nruntime.goexit\\n\\t/usr/local/go/src/runtime/asm_amd64.s:1357\\nerror initialising module: configs\\ngithub.com/cortexproject/cortex/pkg/cortex.(*Cortex).initModule\\n\\t/go/src/github.com/cortexproject/cortex/pkg/cortex/cortex.go:239\\ngithub.com/cortexproject/cortex/pkg/cortex.(*Cortex).init\\n\\t/go/src/github.com/cortexproject/cortex/pkg/cortex/cortex.go:232\\ngithub.com/cortexproject/cortex/pkg/cortex.New\\n\\t/go/src/github.com/cortexproject/cortex/pkg/cortex/cortex.go:185\\nmain.main\\n\\t/go/src/github.com/cortexproject/cortex/cmd/cortex/main.go:73\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\\nruntime.goexit\\n\\t/usr/local/go/src/runtime/asm_amd64.s:1357\"\r\n```\r\n\r\n~Apparently the update via intermediate steps did work in my staging environment.~ No it didn't; I had stopped it updating due to the issue at https://github.com/cortexproject/cortex/pull/1802#issuecomment-562556715"},{"labels":[null,"bug"],"text":"In our staging env we had two config changes close together, so a new rollout started which killed this ingester five seconds after it started up:\r\n\r\n```\r\nlevel=info ts=2020-01-03T11:55:10.951682804Z caller=cortex.go:232 msg=initialising module=overrides\r\nlevel=info ts=2020-01-03T11:55:10.952038111Z caller=cortex.go:232 msg=initialising module=store\r\nlevel=info ts=2020-01-03T11:55:10.969465088Z caller=cortex.go:232 msg=initialising module=server\r\nlevel=info ts=2020-01-03T11:55:10.969820269Z caller=server.go:117 http=[::]:80 grpc=[::]:9095 msg=\"server listening on addresses\"\r\nlevel=info ts=2020-01-03T11:55:10.969947951Z caller=cortex.go:232 msg=initialising module=ingester\r\nlevel=info ts=2020-01-03T11:55:10.970193865Z caller=lifecycler.go:471 msg=\"not loading tokens from file, tokens file path is empty\"\r\nlevel=info ts=2020-01-03T11:55:10.970308989Z caller=main.go:76 msg=\"Starting Cortex\" version=\"(version=, branch=, revision=)\"\r\nlevel=info ts=2020-01-03T11:55:10.97488251Z caller=lifecycler.go:495 msg=\"instance not found in ring, adding with no tokens\" ring=ingester\r\nlevel=info ts=2020-01-03T11:55:11.021295228Z caller=lifecycler.go:668 msg=\"changing instance state from\" old_state=PENDING new_state=JOINING ring=ingester\r\nlevel=info ts=2020-01-03T11:55:11.037757645Z caller=transfer.go:87 msg=\"processing TransferChunks request\" from_ingester=ingester-864cd8dccc-49m2d\r\nlevel=info ts=2020-01-03T11:55:15.657768834Z caller=signals.go:54 msg=\"=== received SIGINT/SIGTERM ===\\n*** exiting\"\r\nlevel=info ts=2020-01-03T11:55:15.657828267Z caller=cortex.go:258 msg=stopping module=ingester\r\nlevel=info ts=2020-01-03T11:55:15.657892847Z caller=transfer.go:435 msg=\"nothing to transfer\"\r\nlevel=info ts=2020-01-03T11:55:15.657910275Z caller=transfer.go:415 msg=\"transfer successfully completed\"\r\nlevel=info ts=2020-01-03T11:55:45.672475738Z caller=lifecycler.go:449 msg=\"instance removed from the KV store\" ring=ingester\r\nlevel=info ts=2020-01-03T11:55:45.672542927Z caller=lifecycler.go:337 msg=\"member.loop() exited gracefully\" ring=ingester\r\nlevel=info ts=2020-01-03T11:55:45.672597055Z caller=cortex.go:258 msg=stopping module=store\r\nlevel=info ts=2020-01-03T11:55:45.672746709Z caller=cortex.go:258 msg=stopping module=overrides\r\nlevel=info ts=2020-01-03T11:55:45.672763053Z caller=cortex.go:258 msg=stopping module=server\r\n```\r\n\r\nHowever the process has not exited and the ingester transferring _to_ it has not noticed yet.\r\nFiled https://github.com/weaveworks/common/pull/171 because the process was not responding to SIGQUIT so I couldn't see where it was stuck."},{"labels":[null,"bug"],"text":"Yesterday we've got all distributors continuously `OOMKilled` in one of our Cortex clusters. The root cause analysis outlined this issue has been caused by a single ingester which was running on a failing Kubernetes node which was running but very slow.\r\n\r\nThis issue is due to how the quorum works. When the distributors receive a `Push()` request, the time series are sharded and then sent to 3 ingesters (we have a replication factor of `3`). The distributor's `Push()` request completes as soon as all series are pushed to at least 2 ingesters.\r\n\r\nIn the case of a very slow ingester, the distributor piles up the number of in-flight requests towards the slow ingester, while the inbound `Push()` request is completed as soon as the other ingesters successfully complete the ingestion.\r\n\r\nThis causes the memory used by the distributors to increase due to the in-flight requests towards the slow ingester.\r\n\r\nIn a high traffic Cortex cluster, distributors can hit the memory limit before the timeout of the in-flight requests towards the slow ingester is expired, causing all distributors to be `OOMKilled` (and subsequent distributors restarts will OOM again until the very slow ingester is not removed from the ring)."},{"labels":["bug"],"text":"Ingesters can make two cache calls before each chunk flush - one for index write dedupe and one for chunk write dedupe.\r\n\r\nBy default these will time out at 100ms, but if the cache is having trouble we don't even want to wait that long - the only penalty of skipping the cache lookup is extra IO, whereas slowing the flush rate to 10/sec will cause a backlog in flushing, eventually OOM of ingesters.\r\nAlso every timeout generates a log line, which across 50 goroutines is 5,000 logs/sec with default settings and with default Docker logging that also puts the system under stress.\r\n\r\nIf we drop the timeout a lot I suspect this will exacerbate the problem seen at #1821.\r\n\r\nAlso I wonder if this behaviour is related to what #1666 was trying to fix.\r\n\r\nHere's the bones of an idea: decouple the two things, so a different set of goroutines attempts to talk to memcached.  The flush code will wait at most 1ms for a response; the memcached side stops even attempting to talk if it sees issues and waits 5-10 seconds to reconnect before resuming."},{"labels":["bug"],"text":"I suspect when something goes wrong the querier stops reading responses so all the remaining ones are abandoned on an error send.\r\n\r\nProfile shows them stuck here:\r\n\r\nhttps://github.com/cortexproject/cortex/blob/6f74b9c4bcffcce6e4149a354fc4cbf2dc390a2f/pkg/querier/lazy_querier.go#L28\r\n\r\n![image](https://user-images.githubusercontent.com/8125524/68994698-e3ba8a80-083a-11ea-9c53-63dd334f871b.png)\r\n\r\n"},{"labels":[null,"bug"],"text":"In [`GetChunksForSeries`](https://github.com/cortexproject/cortex/blob/78e0607b405eba31c151d34d333de201f76a9c33/pkg/chunk/schema.go#L728-L733), we do a `RangeValueStart` on `bucket.from`. Now I thought if the `bucket.hashKey` is same, the `bucket.from` is also same, turns out that is soo not true.\r\n\r\n[`bucket.from`](https://github.com/cortexproject/cortex/blob/78e0607b405eba31c151d34d333de201f76a9c33/pkg/chunk/schema_config.go#L293-L306) is the relative milliseconds from the actual bucket start time.\r\n\r\nNow I did not realise this while building the long-term caching approach, I thought if the hashKey and RangeKey match, everything else matches: https://github.com/cortexproject/cortex/blob/78e0607b405eba31c151d34d333de201f76a9c33/pkg/chunk/schema_caching.go#L99-L100\r\n\r\nBut when we split the ranges, we split the actual from and to, to \"cacheable\" and \"active\" ranges here: https://github.com/cortexproject/cortex/blob/78e0607b405eba31c151d34d333de201f76a9c33/pkg/chunk/schema_caching.go#L66\r\n\r\nand end up picking only the \"active\" range query on merge. This means the `bucket.from` is higher than it should be and we end up filtering chunks out. This causes gaps in the queries and we're dropping entire chunks to the floor.\r\n\r\n"},{"labels":["bug"],"text":"```\r\nlevel=error ts=2019-09-16T12:39:17.30841883Z caller=api.go:1110 msg=\"error marshaling json response\" err=\"v1.response.Data: v1.queryData.Result: promql.Matrix: promql.Series.Points: []promql.Point: unsupported value: NaN\"\r\n```\r\n\r\nNot sure what causes it but myself and a user on Slack have both encountered this message."},{"labels":["bug"],"text":"my configure\r\n```\r\n        - -consul.hostname=consul:8500\r\n        - -distributor.replication-factor=3\r\n        - -distributor.ha-tracker.enable=true\r\n        - -distributor.ha-tracker.enable-for-all-users=true\r\n        - -distributor.ha-tracker.consul.hostname=consul:8500\r\n        - -distributor.ha-tracker.prefix=ha/\r\n        - -distributor.ha-tracker.store=consul\r\n        - -distributor.ha-tracker.cluster=\"prometheus\"\r\n        - -distributor.ha-tracker.replica=\"prometheus_replica\"\r\n```\r\ndistributor log err:\r\ncaller=client.go:226 msg=\"error getting path\" prefix=ha/ err=null\r\n\r\nuse cortex:v0.2.0-rc.0\r\n\r\n"},{"labels":["bug"],"text":"`ring.DoBatch()` will exit if the context is cancelled, but the goroutines it starts don't take any notice.\r\n\r\nIn normal distributor operation the calls to ingesters will themselves time out, but the code then gets blocked on `tracker.done` which nobody is reading from.\r\n\r\nBeginning of goroutine dump:\r\n```\r\ngoroutine profile: total 1169\r\n528 @ 0x42f50f 0x43fac9 0x43fa9f 0x43f749 0x474875 0x1008a6b 0x45cd31\r\n#\t0x43f748\tsync.runtime_Semacquire+0x38\t\t\t\t\t/usr/local/go/src/runtime/sema.go:56\r\n#\t0x474874\tsync.(*WaitGroup).Wait+0x64\t\t\t\t\t/usr/local/go/src/sync/waitgroup.go:130\r\n#\t0x1008a6a\tgithub.com/cortexproject/cortex/pkg/ring.DoBatch.func2+0x2a\t/go/src/github.com/cortexproject/cortex/pkg/ring/batch.go:80\r\n\r\n305 @ 0x42f50f 0x40502b 0x405001 0x404de5 0xff5bb1 0x1008a0e 0x45cd31\r\n#\t0xff5bb0\tgithub.com/cortexproject/cortex/pkg/ring.(*batchTracker).record+0x110\t/go/src/github.com/cortexproject/cortex/pkg/ring/batch.go:118\r\n#\t0x1008a0d\tgithub.com/cortexproject/cortex/pkg/ring.DoBatch.func1+0xcd\t\t/go/src/github.com/cortexproject/cortex/pkg/ring/batch.go:73\r\n```\r\n"},{"labels":[null,"bug"],"text":"We have faced an outage on our Cortex cluster, because of spikes on ingestion rate and we had HPA(based on mem) on ingestor which scaled the ingesters from 5 to 9.  Due to this scaling and above logic, the code started returning the Error `available ingesters are less than required`. \r\n\r\nAs per our investigation, the code in `ring.go` will find the first n active ingesters from the list of token it gets from the ring and passes it to [replication_stategy.go](https://github.com/cortexproject/cortex/blob/master/pkg/ring/replication_strategy.go#L40) based on the key, which returns above error. So, if the list doesn’t contain quorum number of active ingesters it will throw an error. In our case, we had 4 ingesters in pending state after the HPA event. As per our understanding, due to the above situation, we were getting the required number of ingesters not available.\r\n\r\n\r\nIn ring code, here [https://github.com/cortexproject/cortex/blob/master/pkg/ring/ring.go#L232](https://github.com/cortexproject/cortex/blob/master/pkg/ring/ring.go#L232)\r\n\r\nWhat does it mean by `write the extra replica somewhere`?"},{"labels":["bug"],"text":"After merging #929 I have observed multiple notifications for the same alert, and also zero notifications.\r\n\r\nSorry not much idea what is wrong right now but wanted to get the word out.\r\n"},{"labels":[null,"bug"],"text":"If the number of series being considered is a multiple of the batch size (currently 128) and the query has a filter, and the last series does not meet the filter conditions, then the last batch will not be sent.\r\n\r\nIn the same area, the batching check is on the number of items _checked_ not on items that pass through the filter, so batch sizes can be both smaller and larger than 128."},{"labels":[null,"bug"],"text":"Logs from an ingester at address `10.244.251.100`:\r\n\r\n```\r\nJune 6th 2019, 08:28:33.577    level=info ts=2019-06-06T07:28:33.577526677Z caller=cortex.go:239 msg=stopping module=ingester\r\nJune 6th 2019, 08:28:37.858    level=info ts=2019-06-06T07:28:37.858651585Z caller=transfer.go:218 msg=\"sending chunks\" to_ingester=10.244.227.137:9095\r\nJune 6th 2019, 08:32:35.502    level=info ts=2019-06-06T07:32:35.502129301Z caller=transfer.go:272 msg=\"successfully sent chunks\" to_ingester=10.244.227.137:9095\r\nJune 6th 2019, 08:33:05.661    level=info ts=2019-06-06T07:33:05.661630134Z caller=lifecycler.go:291 msg=\"member.loop() exited gracefully\"\r\nJune 6th 2019, 08:33:05.661    level=info ts=2019-06-06T07:33:05.661879021Z caller=cortex.go:239 msg=stopping module=server\r\nJune 6th 2019, 08:33:05.661    level=info ts=2019-06-06T07:33:05.661586078Z caller=lifecycler.go:367 msg=\"ingester removed from consul\"\r\n```\r\n\r\nLogs from a querier trying to talk to it:\r\n```\r\nJune 6th 2019, 08:28:34.164 level=warn ts=2019-06-06T07:28:34.163957075Z caller=logging.go:49 traceID=15482fd69ecc5603 msg=\"GET /api/prom/user_stats (500) 158.178µs Response: \\\"rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \\\\\\\"transport: Error while dialing dial tcp 10.244.251.100:9095: connect: connection refused\\\\\\\"\\\\n\\\" ws: false; Accept-Encoding: gzip; Uber-Trace-Id: 15482fd69ecc5603:80a1b145fb19898:25aeaac212bdf087:0; User-Agent: Go-http-client/1.1; X-Scope-Orgid: 13085; \"\r\nJune 6th 2019, 08:28:34.244 level=warn ts=2019-06-06T07:28:34.24475902Z caller=logging.go:49 traceID=355b3f418d9be500 msg=\"GET /api/prom/user_stats (500) 133.39µs Response: \\\"rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \\\\\\\"transport: Error while dialing dial tcp 10.244.251.100:9095: connect: connection refused\\\\\\\"\\\\n\\\" ws: false; Accept-Encoding: gzip; Uber-Trace-Id: 355b3f418d9be500:e27e2986d2d8e49:663334830d61cd92:0; User-Agent: Go-http-client/1.1; X-Scope-Orgid: 14537; \"\r\n...\r\nJune 6th 2019, 08:33:04.412 level=warn ts=2019-06-06T07:33:04.412779057Z caller=logging.go:49 traceID=479e862751234885 msg=\"GET /api/prom/user_stats (500) 251.2µs Response: \\\"rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \\\\\\\"transport: Error while dialing dial tcp 10.244.251.100:9095: connect: connection refused\\\\\\\"\\\\n\\\" ws: false; Accept-Encoding: gzip; Uber-Trace-Id: 479e862751234885:19881cc5841beafa:3a5ac60bd24b386d:0; User-Agent: Go-http-client/1.1; X-Scope-Orgid: 15178; \"\r\nJune 6th 2019, 08:33:05.281 level=warn ts=2019-06-06T07:33:05.281099669Z caller=logging.go:49 traceID=710dc5ccf13e9ac9 msg=\"GET /api/prom/user_stats (500) 187.317µs Response: \\\"rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \\\\\\\"transport: Error while dialing dial tcp 10.244.251.100:9095: connect: connection refused\\\\\\\"\\\\n\\\" ws: false; Accept-Encoding: gzip; Uber-Trace-Id: 710dc5ccf13e9ac9:1eda228a3d8ab2d7:1a7b3f1b5019d2dd:0; User-Agent: Go-http-client/1.1; X-Scope-Orgid: 2; \"\r\n```\r\n\r\nThis didn't used to happen. My hypothesis is that this changed in the single-binary refactor.\r\n"},{"labels":[null,"bug"],"text":"As of now, Table Manager creates Table Client only for newest config as seen here:\r\nhttps://github.com/cortexproject/cortex/blob/master/cmd/table-manager/main.go#L47-L62\r\nRetention code calls deleteTable only on this table client for newest config.\r\nIf someone would use different config for different time span then Table Manager would not delete expired index tables from previous configs.\r\nShould this be fixed or its never going to be a case where people would have multiple configs?"},{"labels":["bug"],"text":"We don't pin it in the build container, so as far as I can tell we are using whichever one was in the Docker build cache for whoever built the build container.\r\n\r\nThis is a bit scary for debugging / reproducability."},{"labels":[null,"bug"],"text":"I had Table Manager Autoscaling working, but I've noticed that it stopped working recently and gives the following error every time it polls:\r\n\r\n```\r\nlevel=debug ts=2019-05-01T06:16:09.258445284Z caller=schema_config.go:378 msg=\"Expected Table\" tableName=cortex_weekly_2569 provisionedRead=300 provisionedWrite=1 useOnDemandMode=false\r\nlevel=debug ts=2019-05-01T06:16:09.25850215Z caller=schema_config.go:378 msg=\"Expected Table\" tableName=cortex_weekly_2570 provisionedRead=300 provisionedWrite=1 useOnDemandMode=false\r\nlevel=debug ts=2019-05-01T06:16:09.258513214Z caller=schema_config.go:378 msg=\"Expected Table\" tableName=cortex_weekly_2571 provisionedRead=300 provisionedWrite=1 useOnDemandMode=false\r\nlevel=debug ts=2019-05-01T06:16:09.258524722Z caller=schema_config.go:378 msg=\"Expected Table\" tableName=cortex_weekly_2572 provisionedRead=300 provisionedWrite=1 useOnDemandMode=false\r\nlevel=debug ts=2019-05-01T06:16:09.258534871Z caller=schema_config.go:378 msg=\"Expected Table\" tableName=cortex_weekly_2573 provisionedRead=300 provisionedWrite=1 useOnDemandMode=false\r\nlevel=debug ts=2019-05-01T06:16:09.258545773Z caller=schema_config.go:399 msg=\"Table is Active\" tableName=cortex_weekly_2573 provisionedRead=300 provisionedWrite=100 useOnDemandMode=false useWriteAutoScale=true useReadAutoScale=false\r\nlevel=info ts=2019-05-01T06:16:09.258561427Z caller=table_manager.go:218 msg=\"synching tables\" num_expected_tables=6 expected_tables=6\r\nlevel=debug ts=2019-05-01T06:16:09.296262473Z caller=table_manager.go:388 msg=\"checking provisioned throughput on table\" table=cortex_weekly_2571\r\nlevel=error ts=2019-05-01T06:16:10.279615926Z caller=table_manager.go:206 msg=\"error syncing tables\" err=\"unknown value type \\\"\\\"\"\r\n```\r\n\r\nI also sometimes see this error:\r\n```\r\nlevel=error ts=2019-05-01T06:27:03.859303114Z caller=table_manager.go:197 msg=\"error syncing tables\" err=\"expected three values: 4\"\r\n```\r\n\r\n\r\n\r\nThis happens with `table-manager:master-79bba2f9` _(Today)_ and `table-manager:master-f18cc59b` _(2nd March 2019 which was an arbitrary tag older than some recent changes to table_manager.go)_, with the following arguments:\r\n```\r\n-log.level=debug\r\n-server.http-listen-port=80\r\n-dynamodb.url=dynamodb://***************:*********************************@ap-southeast-2\r\n-dynamodb.v9-schema-from=2019-04-01\r\n-dynamodb.use-periodic-tables\r\n-dynamodb.periodic-table.period=168h\r\n-dynamodb.periodic-table.prefix=cortex_weekly_\r\n-dynamodb.periodic-table.from=2019-04-01\r\n-dynamodb.periodic-table.write-throughput=100\r\n-dynamodb.periodic-table.write-throughput.scale.enabled=true\r\n-dynamodb.periodic-table.write-throughput.scale.target-value=80\r\n-dynamodb.periodic-table.write-throughput.scale.min-capacity=10\r\n-dynamodb.periodic-table.write-throughput.scale.max-capacity=1000\r\n-dynamodb.periodic-table.write-throughput.scale.in-cooldown=3600\r\n-dynamodb.periodic-table.write-throughput.scale.out-cooldown=3600\r\n-dynamodb.periodic-table.write-throughput.scale.role-arn=\"arn:aws:iam::****************:role/aws-service-role/dynamodb.application-autoscaling.amazonaws.com/AWSServiceRoleForApplicationAutoScaling_DynamoDBTable\"\r\n-metrics.url=http://cortex-nginx/api/prom/\r\n-metrics.queue-length-query=sum(avg_over_time(cortex_ingester_flush_queue_length{job=\"cortex/cortex-ingester\"}[2m]))\r\n```"},{"labels":[null,"bug"],"text":"One of the rollouts failed because of this: https://github.com/cortexproject/cortex/issues/1349\r\n\r\nBut the thing was, the series that were already transferred weren't being flushed, because we never actually \"store\" the data:\r\nhttps://github.com/cortexproject/cortex/blob/590e72c616def79c35c9add08ee3e9d28e1349f0/pkg/ingester/transfer.go#L128\r\n\r\nWe should make sure we flush half-transferred data too.\r\n"},{"labels":["bug",null],"text":"We found one of the rollouts failed with:\r\n```\r\nlevel=info ts=2019-04-25T15:42:47.885425968Z caller=lifecycler.go:456 msg=\"changing ingester state from\" old_state=PENDING new_state=JOINING\r\nlevel=info ts=2019-04-25T15:42:47.918991563Z caller=transfer.go:83 msg=\"processing TransferChunks request\" from_ingester=ingester-6bcf4b69cf-wz6sp\r\n\r\nlevel=error ts=2019-04-25T15:43:55.494427327Z caller=transfer.go:54 msg=\"TranferChunks failed, not in ACTIVE state.\" state=JOINING\r\nlevel=info ts=2019-04-25T15:43:55.494516868Z caller=lifecycler.go:456 msg=\"changing ingester state from\" old_state=JOINING new_state=PENDING\r\nlevel=warn ts=2019-04-25T15:45:23.717665467Z caller=grpc_logging.go:47 method=/cortex.Ingester/TransferChunks duration=55.413853328s err=\"rpc error: code = Code(429) desc = per-user series limit (500000) exceeded\" msg=\"gRPC\\n\"\r\n```\r\n\r\nWe should not apply limits during handover. \r\n\r\nFurther, the sending ingester should not have more series than the limits. We maybe hitting this \"harmless\" race here: https://github.com/cortexproject/cortex/blob/590e72c616def79c35c9add08ee3e9d28e1349f0/pkg/ingester/user_state.go#L190-L194"},{"labels":[null,"bug"],"text":"We had an ingester running on a node which spontaneously restarted.\r\nKubernetes started the ingester process, but it refused to go Ready.\r\nWe have excellent debugging for this condition now:\r\n\r\n```\r\n$ curl http://10.244.202.176/ready\r\nNot ready: this ingester owns no tokens\r\n```\r\n\r\nAt the top of the ingester logs on restart was:\r\n\r\n```\r\nlevel=info ts=2019-04-01T10:26:31.676674615Z caller=lifecycler.go:383 msg=\"existing entry found in ring\" state=ACTIVE tokens=0\r\n```\r\n\r\nI believe this is because that code doesn't handle the normalise-tokens case."},{"labels":[null,"bug"],"text":"We used to tell ours to wait for two minutes, but that was torn up by #1185 \r\n\r\nToday I see this:\r\n```\r\nlevel=info ts=2019-03-28T16:41:45.673567131Z caller=lifecycler.go:456 msg=\"changing ingester state from\" old_state=ACTIVE new_state=LEAVING\r\nlevel=error ts=2019-03-28T16:41:45.688616369Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:45.695897001Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:45.746735562Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:45.807703134Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:46.334636223Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:46.46819466Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:47.212946695Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:47.370324186Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:48.672456187Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:51.35543268Z caller=transfer.go:199 msg=\"transfer failed\" err=\"cannot find ingester to transfer chunks to: no pending ingesters\"\r\nlevel=error ts=2019-03-28T16:41:51.355469649Z caller=lifecycler.go:466 msg=\"Failed to transfer chunks to another ingester\" err=\"terminated after 10 retries\"\r\n```\r\n\r\nso it isn't even waiting the 100ms minimum I expect from cursory examination of the code.\r\n\r\nPossibly related: #792 "},{"labels":[null,"bug"],"text":"If you have 3 ingesters, and replication factor 3, then a rolling update will give this error:\r\n\r\n    at least 3 live ingesters required, could only find 2\r\n\r\nThis is because the distributor increases the size of the quorum required when it finds ingesters joining and leaving.\r\n\r\nStrikes me there is something wrong here."},{"labels":[null,"bug"],"text":"Grew to 10GB over the last day:\r\n\r\n![image](https://user-images.githubusercontent.com/8125524/54491795-1b8ebe00-48b9-11e9-9e9b-041fdb1eee58.png)\r\n\r\nI can't figure out how the leak comes about, but httpgrpc is implicated by the profiler and switching from httpgrpc to http makes it go away (that's the drop at the end of the chart).\r\n\r\nThis is the stack from the profiler:\r\n\r\n```\r\nroot\r\nruntime.goexit\r\ngrpc.(*Server).serveStreams.func1.1\r\ngrpc.(*Server).handleStream\r\ngrpc.(*Server).processUnaryRPC\r\ngrpc.(*Server).sendResponse\r\ngrpc.encode\r\nproto.(*codec).Marshal\r\nproto.codec.Marshal\r\nhttpgrpc.(*HTTPResponse).Marshal\r\n```\r\n\r\nand source code of the last frame:\r\n```\r\n/go/src/github.com/cortexproject/cortex/vendor/github.com/weaveworks/common/httpgrpc/httpgrpc.pb.go\r\n\r\n  Total:     10.90GB    10.90GB (flat, cum) 99.66%\r\n    411            .          .           \treturn i, nil \r\n    412            .          .           } \r\n    413            .          .            \r\n    414            .          .           func (m *HTTPResponse) Marshal() (dAtA []byte, err error) { \r\n    415            .          .           \tsize := m.Size() \r\n    416      10.90GB    10.90GB           \tdAtA = make([]byte, size) \r\n    417            .          .           \tn, err := m.MarshalTo(dAtA) \r\n    418            .          .           \tif err != nil { \r\n    419            .          .           \t\treturn nil, err \r\n    420            .          .           \t} \r\n    421            .          .           \treturn dAtA[:n], nil \r\n```"},{"labels":[null,"bug"],"text":"Example - JSON from Grafana Query Inspector is below.\r\n\r\n![image](https://user-images.githubusercontent.com/8125524/54491470-bd60db80-48b6-11e9-9f9f-131f597546f2.png)\r\n\r\nRecreating this may be sensitive to the min step - to demonstrate I put `50m` in Grafana, and `60m` doesn't show a gap. Maybe it depends on the relationship between start time and step.\r\n\r\nTimestamp 1552777800 is Sat Mar 16 23:10:00 UTC 2019\r\nThe following point, 1552783800, is Sun Mar 17 00:50:00 UTC 2019 so midnight has been skipped.\r\n\r\n```\r\n{\r\n  \"xhrStatus\": \"complete\",\r\n  \"request\": {\r\n    \"method\": \"GET\",\r\n    \"url\": \"https://frontend.dev.weave.works/api/prom/api/v1/query_range?query=go_goroutines%7Binstance%3D%22query-frontend-6f47f48ffd-lj4pf%22%7D&start=1552752000&end=1552818000&step=3000\",\r\n    \"withCredentials\": true\r\n  },\r\n  \"response\": {\r\n    \"status\": \"success\",\r\n    \"data\": {\r\n      \"resultType\": \"matrix\",\r\n      \"result\": [\r\n        {\r\n          \"metric\": {\r\n            \"__name__\": \"go_goroutines\",\r\n            \"_weave_service\": \"query-frontend\",\r\n            \"instance\": \"query-frontend-6f47f48ffd-lj4pf\",\r\n            \"job\": \"cortex/query-frontend\",\r\n            \"kubernetes_namespace\": \"cortex\",\r\n            \"kubernetes_pod_name\": \"query-frontend-6f47f48ffd-lj4pf\",\r\n            \"node\": \"ip-172-20-1-59.ec2.internal\"\r\n          },\r\n          \"values\": [\r\n            [\r\n              1552754400,\r\n              \"162\"\r\n            ],\r\n            [\r\n              1552757400,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552760400,\r\n              \"160\"\r\n            ],\r\n            [\r\n              1552763400,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552766400,\r\n              \"160\"\r\n            ],\r\n            [\r\n              1552769400,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552772400,\r\n              \"160\"\r\n            ],\r\n            [\r\n              1552775400,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552777800,\r\n              \"162\"\r\n            ],\r\n            [\r\n              1552783800,\r\n              \"160\"\r\n            ],\r\n            [\r\n              1552786800,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552789800,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552792800,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552795800,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552798800,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552801800,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552804800,\r\n              \"160\"\r\n            ],\r\n            [\r\n              1552807800,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552810800,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552813800,\r\n              \"161\"\r\n            ],\r\n            [\r\n              1552816800,\r\n              \"161\"\r\n            ]\r\n          ]\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```"},{"labels":[null,"bug"],"text":"We noticed one case when using the query-frontend that duplicate datapoints are returned. This is only for latest datapoints and it goes away when the cache is cleared.\r\n\r\nThe following is the result returned:\r\n![cortex-dupes](https://user-images.githubusercontent.com/7354143/54120428-aab15700-441d-11e9-8c52-a07a33c5bb78.png)\r\n\r\n\r\nFurther, deleting the caches is only a temporary fix and the duplicates return after a couple of queries. And the more queries you do, the more duplicates there are. This looks like a cache alignment issue tbh."},{"labels":[null,"bug"],"text":"```\r\nquantile_over_time(0.99, sum(rate(cortex_gw_request_duration_seconds_count{job=~\"(.+)/cortex-gw\", route=\"cortex-read\"}[5m]))[24h:])\r\n```\r\n\r\nNB this doesn't happen with prometheus 2.8rc0.  Either we need to update vendor or we're missing some config."},{"labels":[null,null,"bug"],"text":"I setup cortex with cassandra chunk storage.\r\nNot only deployment successful and PromQL working well(http://nginx:30080/api/prom/api/v1/query?query=up) but also cortex ring (Distributor/ring) show all active status.\r\n\r\nBut when flushing triggered, ingester pod raised error and terminated with metric initialized.\r\nI think it is supposed to relate `-ingester.max-chunk-idle` duration.\r\nI try changing `-ingester.max-chunk-idle` parameter, I can see terminated interval changed\r\n\r\n```\r\nNAME                              READY     STATUS    RESTARTS   AGE\r\nalertmanager-5d4df4bb5c-bdd5j     1/1       Running   0          4h\r\ncassandra-0                       1/1       Running   0          4h\r\ncassandra-1                       1/1       Running   0          4h\r\ncassandra-2                       1/1       Running   0          4h\r\nconfigs-db-75c6b4df96-hbqjs       1/1       Running   0          4h\r\nconfigs-dcd654584-n56tz           1/1       Running   0          4h\r\nconsul-76669d6c85-t4lxj           1/1       Running   0          4h\r\ndistributor-54868b7499-kk6n9      1/1       Running   0          4h\r\ningester-856747d6b9-6qfc4         1/1       Running   22         2h\r\ningester-856747d6b9-fzghz         1/1       Running   22         2h\r\ningester-856747d6b9-pwdk2         1/1       Running   22         2h\r\nmemcached-cf8545f6d-kd7l2         1/1       Running   0          4h\r\nnginx-55b9f744d5-p47xm            1/1       Running   0          4h\r\nquerier-bd7b99df7-nrjc5           1/1       Running   0          4h\r\nquery-frontend-6b8ffd69d5-v2mtw   1/1       Running   0          4h\r\nruler-7c9bfcd58-sf5qx             1/1       Running   0          4h\r\ntable-manager-6886cddcdd-b6gw7    1/1       Running   0          4h\r\n```\r\n\r\n```\r\n- name: ingester\r\n        image: ingester:20190304\r\n        imagePullPolicy: IfNotPresent\r\n        args:\r\n        - -log.level=debug\r\n        - -ingester.join-after=30s\r\n        - -ingester.claim-on-rollout=false\r\n        - -ingester.num-tokens=512\r\n        - -ingester.chunk-encoding=3\r\n        - -consul.hostname=consul.monitoring.svc.cluster.local:8500\r\n        - -chunk.storage-client=cassandra\r\n        - -cassandra.addresses=cassandra.monitoring.svc.cluster.local:9042\r\n        - -cassandra.keyspace=cortex\r\n        - -dynamodb.periodic-table.prefix=cortex_weekly_\r\n        - -dynamodb.periodic-table.from=2019-01-01\r\n        - -dynamodb.original-table-name=cortex\r\n        - -memcached.hostname=memcached.monitoring.svc.cluster.local\r\n        - -memcached.service=memcached\r\n        - -store.index-cache-write.memcached.hostname=memcached.monitoring.svc.cluster.local.\r\n        - -store.index-cache-write.memcached.expiration=24h\r\n```\r\n\r\n```\r\nlevel=debug ts=2019-03-04T07:48:11.633987338Z caller=flush.go:210 msg=\"Ingester.flushLoop() exited\"\r\npanic: runtime error: integer divide by zero\r\ngoroutine 637 [running]:\r\ngithub.com/cortexproject/cortex/pkg/chunk.(*PeriodicTableConfig).TableFor(0xc4201ca450, 0x16947a7ab6a, 0x10285e0, 0xc42b1d7140)\r\n    /go/src/github.com/cortexproject/cortex/pkg/chunk/schema_config.go:438 +0xe2\r\ngithub.com/cortexproject/cortex/pkg/chunk.SchemaConfig.ChunkTableFor(0xc4201ca360, 0x2, 0x2, 0x0, 0x0, 0x7ffdec0462b4, 0x9, 0x0, 0x0, 0x0, ...)\r\n    /go/src/github.com/cortexproject/cortex/pkg/chunk/schema_config.go:428 +0x83\r\ngithub.com/cortexproject/cortex/pkg/chunk/cassandra.(*StorageClient).PutChunks(0xc4201588c0, 0x13953e0, 0xc4302d5da0, 0xc42e1f5570, 0x1, 0x1, 0x0, 0x0)\r\n    /go/src/github.com/cortexproject/cortex/pkg/chunk/cassandra/storage_client.go:273 +0x155\r\ngithub.com/cortexproject/cortex/pkg/chunk.(*store).PutOne(0xc4201661e0, 0x13953e0, 0xc4302d5da0, 0x16947a7ab6a, 0x16947a7ab6a, 0x5b3b66a5db2d3e08, 0xc4203e33cd, 0x1, 0x16947a7ab6a, 0x16947a7ab6a, ...)\r\n    /go/src/github.com/cortexproject/cortex/pkg/chunk/chunk_store.go:133 +0x119\r\ngithub.com/cortexproject/cortex/pkg/chunk.compositeStore.Put.func1(0x16947a7ab6a, 0x16947a7ab6a, 0x1395620, 0xc4201661e0, 0x0, 0x0)\r\n    /go/src/github.com/cortexproject/cortex/pkg/chunk/composite_store.go:63 +0x91\r\ngithub.com/cortexproject/cortex/pkg/chunk.compositeStore.forStores(0xc4201996e0, 0x2, 0x2, 0x16947a7ab6a, 0x16947a7ab6a, 0xc43038ba10, 0x15f0820a, 0xc42a71e478)\r\n    /go/src/github.com/cortexproject/cortex/pkg/chunk/composite_store.go:140 +0x1de\r\ngithub.com/cortexproject/cortex/pkg/chunk.compositeStore.Put(0xc4201996e0, 0x2, 0x2, 0x13953e0, 0xc4302d5da0, 0xc42e1f5420, 0x1, 0x1, 0xc42e1f5500, 0xc43026af00)\r\n    /go/src/github.com/cortexproject/cortex/pkg/chunk/composite_store.go:62 +0x131\r\ngithub.com/cortexproject/cortex/pkg/ingester.(*Ingester).flushChunks(0xc42018ed80, 0x13953e0, 0xc4302d5da0, 0x5b3b66a5db2d3e08, 0xc4302d5dd0, 0xc422f5f428, 0x1, 0x1, 0xc4300e8e10, 0x13953e0)\r\n    /go/src/github.com/cortexproject/cortex/pkg/ingester/flush.go:332 +0x3d2\r\ngithub.com/cortexproject/cortex/pkg/ingester.(*Ingester).flushUserSeries(0xc42018ed80, 0x1a, 0xc4203e33cd, 0x1, 0x5b3b66a5db2d3e08, 0x0, 0x0, 0x0)\r\n    /go/src/github.com/cortexproject/cortex/pkg/ingester/flush.go:280 +0x855\r\ngithub.com/cortexproject/cortex/pkg/ingester.(*Ingester).flushLoop(0xc42018ed80, 0x1a)\r\n    /go/src/github.com/cortexproject/cortex/pkg/ingester/flush.go:221 +0xec\r\ncreated by github.com/cortexproject/cortex/pkg/ingester.New\r\n    /go/src/github.com/cortexproject/cortex/pkg/ingester/ingester.go:181 +0x372\r\n```"},{"labels":["bug"],"text":"As mentioned at #1203 I have seen ingesters reporting their tokens were half-owned by the new ingester and half by the leaving one, at the end of handover.  Since the leaving ingester continues to run for 30 seconds after hand-over, incoming samples continue to spill over to other ingesters.\r\n\r\nI believe this didn't happen before I turned on `-ingester.normalise-tokens`"},{"labels":["bug",null],"text":""},{"labels":["bug"],"text":"```\r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x149d3de]\r\n\r\ngoroutine 770 [running]:\r\ngithub.com/cortexproject/cortex/pkg/chunk/aws.dynamoDBRequestAdapter.Retryable(0xc42265a000, 0x1aa9ee2)\r\n\t/go/src/github.com/cortexproject/cortex/pkg/chunk/aws/dynamodb_storage_client.go:457 +0xe\r\ngithub.com/cortexproject/cortex/pkg/chunk/aws.dynamoDBStorageClient.queryPage(0xc420478380, 0x4000000000000000, 0x0, 0x0, 0x0, 0x186a0, 0x3ff4cccccccccccd, 0x1b05684, 0x51, 0x1b0fe58, ...)\r\n\t/go/src/github.com/cortexproject/cortex/pkg/chunk/aws/dynamodb_storage_client.go:380 +0x38d\r\ngithub.com/cortexproject/cortex/pkg/chunk/aws.dynamoDBStorageClient.query(0xc420478380, 0x4000000000000000, 0x0, 0x0, 0x0, 0x186a0, 0x3ff4cccccccccccd, 0x1b05684, 0x51, 0x1b0fe58, ...)\r\n\t/go/src/github.com/cortexproject/cortex/pkg/chunk/aws/dynamodb_storage_client.go:337 +0x472\r\ngithub.com/cortexproject/cortex/pkg/chunk/aws.(dynamoDBStorageClient).(github.com/cortexproject/cortex/pkg/chunk/aws.query)-fm(0x1c655e0, 0xc4225eb050, 0xc420896300, 0x16, 0xc422532100, 0x3b, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cortexproject/cortex/pkg/chunk/aws/dynamodb_storage_client.go:285 +0xb4\r\ngithub.com/cortexproject/cortex/pkg/chunk/util.DoParallelQueries.func1(0x1c655e0, 0xc4224e5890, 0xc4224a36e0, 0xc4224a3740, 0xc422656200, 0xc422648860)\r\n\t/go/src/github.com/cortexproject/cortex/pkg/chunk/util/util.go:42 +0x121\r\ncreated by github.com/cortexproject/cortex/pkg/chunk/util.DoParallelQueries\r\n\t/go/src/github.com/cortexproject/cortex/pkg/chunk/util/util.go:34 +0xf9\r\n```"},{"labels":[null,"bug"],"text":"The feature implemented in #1044 results in handed-over chunks lacking the \"footer\" to add more values.\r\n\r\nA simple way to work round the problem would be to reset the global flag before beginning a hand-over.\r\n\r\nMaybe cleaner to add a \"closed\" flag to each chunk so it knows whether further writing is expected?"},{"labels":[null,"bug"],"text":"Running version `master-c824ad2f`\r\nat the end of the log:\r\n\r\n```\r\nlevel=info ts=2018-10-18T15:24:52.854666978Z caller=transfer.go:247 msg=\"successfully sent chunks\" to_ingester=10.244.253.152:9095\r\nlevel=info ts=2018-10-18T15:24:52.87543753Z caller=lifecycler.go:343 msg=\"ingester removed from consul\"\r\nlevel=info ts=2018-10-18T15:24:52.875490858Z caller=lifecycler.go:267 msg=\"Ingester.loop() exited gracefully\"\r\npanic: close of closed channel\r\ngoroutine 1 [running]:\r\ngithub.com/cortexproject/cortex/pkg/chunk/cache.(*backgroundCache).Stop(0xc4201f6420, 0x0, 0x0)\r\n/go/src/github.com/cortexproject/cortex/pkg/chunk/cache/background.go:77 +0x2f\r\ngithub.com/cortexproject/cortex/pkg/chunk.(*Fetcher).Stop(0xc42035e8c0)\r\n/go/src/github.com/cortexproject/cortex/pkg/chunk/chunk_store_utils.go:92 +0x58\r\ngithub.com/cortexproject/cortex/pkg/chunk.(*store).Stop(0xc4202de2a0)\r\n/go/src/github.com/cortexproject/cortex/pkg/chunk/chunk_store.go:99 +0x32\r\ngithub.com/cortexproject/cortex/pkg/chunk.compositeStore.Stop(0xc42036e0c0, 0x6, 0x8)\r\n/go/src/github.com/cortexproject/cortex/pkg/chunk/composite_store.go:218 +0x42\r\nmain.main()\r\n```"},{"labels":["bug"],"text":"Suppose it is configured to cache for 15 minutes. Any time it runs a DB query, it caches what it found in the database for the next 15 minutes.\r\nIf chunks are flushed from an ingester during those 15 minutes, the index cache will not know about them, so they will be missing from the returned result.\r\n\r\nProposed solution is to add a parameter to the ingester to retain chunks for N minutes after flushing.  This would also fix #801.\r\n"},{"labels":[null,null,"bug"],"text":"Relevant log line:\r\n```\r\nlevel=error ts=2018-10-04T09:20:28.637542898Z caller=frontend.go:203 msg=\"error processing request\" try=4 err=null resp=\"&HTTPResponse{Code:500,Headers:[&Header{Key:Access-Control-Allow-Methods,Values:[GET, OPTIONS],} &Header{Key:Access-Control-Allow-Origin,Values:[*],} &Header{Key:Access-Control-Expose-Headers,Values:[Date],} &Header{Key:Content-Type,Values:[application/json],} &Header{Key:Content-Encoding,Values:[gzip],} &Header{Key:Access-Control-Allow-Headers,Values:[Accept, Authorization, Content-Type, Origin],}],Body:[31 139 8 0 0 0 0 0 0 255 68 200 65 10 2 49 12 5 208 171 132 191 118 46 208 115 120 129 140 243 209 66 155 104 147 34 131 120 119 17 145 89 61 120 47 68 106 206 64 1 199 240 129 211 207 243 126 39 10 170 37 135 105 251 55 10 186 218 190 164 47 95 165 107 94 110 213 174 98 158 162 173 249 147 91 57 182 233 202 22 210 103 164 172 148 105 245 49 41 110 226 70 137 186 17 239 15 0 0 0 255 255 1 0 0 255 255 49 130 136 63 129 0 0 0],}\"\r\n```\r\n\r\nMe debugging it: https://play.golang.com/p/iqo4VwjVWrg \r\n\r\nTurns out the error is:\r\n```\r\n{\"status\":\"error\",\"errorType\":\"internal\",\"error\":\"many-to-many matching not allowed: matching labels must be unique on one side\"}\r\n```\r\n\r\nWe should also print the body properly and not make people jump through the hoops I did to figure it out."},{"labels":[null,null,"bug"],"text":"The AWS SDK considers such errors \"retryable\", which might be OK if we were getting hold of new credentials, but since we don't have any code to do that we should just bail out.\r\n"},{"labels":["bug"],"text":"example:\r\n\r\n```\r\n\r\nlevel=info ts=2018-09-05T04:40:49.78712824Z caller=dynamodb_table_client.go:252 msg=\"updating provisioned throughput on table\" table=prod_chunk_data_weekly_2478 old_read=300 old_write=4000 new_read=300 new_write=0\r\nlevel=info ts=2018-09-05T04:40:49.787108305Z caller=metrics_autoscaling.go:167 msg=\"metrics scale-down\" table=prod_chunk_data_weekly_2478 write=0\r\nlevel=info ts=2018-09-05T04:40:49.787051811Z caller=metrics_autoscaling.go:96 msg=\"checking metrics\" table=prod_chunk_data_weekly_2478 queueLengths=\"[5810.625 1916.75 1705]\" errorRate=0 usageRate=0\r\nlevel=error ts=2018-09-05T04:40:49.958690094Z caller=table_manager.go:134 msg=\"error syncing tables\" err=\"InvalidParameter: 1 validation error(s) found.\\n- minimum field value of 1, UpdateTableInput.ProvisionedThroughput.WriteCapacityUnits.\\n\"\r\n\r\n```\r\n\r\nIt shouldn't be going through metrics_autoscaling at all; this is a table from months ago."},{"labels":[null,"bug"],"text":"This is the entire logfile of one ingester in our dev cluster, deployed from commit `f9de1cab4a13aeb71c7740c9c3fbffde3f8d14a4`:\r\n\r\n```\r\nlevel=info ts=2018-08-30T16:59:43.652949863Z caller=override.go:33 msg=\"per-tenant overides disabled\"\r\nlevel=info ts=2018-08-30T16:59:43.660970342Z caller=lifecycler.go:362 msg=\"entry not found in ring, adding with no tokens\"\r\nlevel=info ts=2018-08-30T16:59:50.943534793Z caller=gokit.go:36 msg=\"=== received SIGINT/SIGTERM ===\\n*** exiting\"\r\nlevel=info ts=2018-08-30T16:59:50.946449637Z caller=transfer.go:194 msg=\"sending chunks\" to_ingester=10.244.229.56:9095\r\n```\r\n\r\nIt shouldn't attempt to transfer chunks when it never actually got started."},{"labels":[null,"bug"],"text":"We got into a state where we had multiple ingesters with 0 tokens; we don't quite know how.  Worse, its \"viral\" - when deleted those ingesters and new ones started, they were also given 0 tokens.\r\n\r\nRelated to #663 I think."},{"labels":[null,"bug"],"text":"_From @gouthamve on August 22, 2018 14:10_\n\nhttps://github.com/grafana/cortex/blob/master/cmd/querier/main.go#L92\n\n_Copied from original issue: grafana/cortex#13_"},{"labels":[null,"bug"],"text":"Believe this was caused by #898 \r\n\r\nhttps://github.com/weaveworks/cortex/commit/5c03d254dd4bae6aeebee481ae635d9d7026d8cf#diff-d7122b828e5806f5da647427488f01f1R362\r\nplus this: https://github.com/weaveworks/cortex/commit/075c22fd401664cd79f86c512ec49e0dc504e044#diff-e39b4d67019ef4238a6408e2aabe4725R248"},{"labels":[null,"bug"],"text":"If I run a certain query in graph mode, I am told: `Error: rpc error: code = Code(413) desc = exceeded maximum number of samples in a query (100000)`\r\nIf I run the same query in table mode, instead I am told: ` Server error (rpc error: code = Code(500) desc = \u001f�\b�������4�M �0\u0014\u0006���V-���*ЕW�\u00021��B��{ ��w\u0017\u0015g3�<Śo�� U�����^)N�ܨٯ�-N�\u0006|�!�H̸���<�FDZ�\fn���H~[RO�=ݨ(w�Ou�a��xt�a:~\u0018��\u0006����\u0001�����+W����� ) `\r\n\r\n"},{"labels":[null,"bug"],"text":"#907 broke it here: https://github.com/weaveworks/cortex/commit/3006e3901d607f25fa2e213ae05d00380e80a8bc#diff-180f56d9aaf5a9aa079e6fc9cfcf1bc8R245\r\n\r\n```\r\n        for _, s := range ts.Samples {\r\n            if err := d.cfg.validationConfig.ValidateSample(userID, metricName, s); err != nil {\r\n\t\tlastPartialErr = err\r\n                continue\r\n            }\r\n        }\r\n\r\n\tkeys = append(keys, key)\r\n```\r\n\r\nthe `continue` on error now has no effect."},{"labels":["bug"],"text":"Seems that one or two of our alertmanagers will bounce a few times on startup, then eventually get going.\r\n\r\nNote the `mesh` code will be removed if we address #793 \r\n\r\nSymptom:\r\n```\r\nlevel=info ts=2018-08-06T10:07:04.474111574Z caller=multitenant.go:434 msg=\"blank Alertmanager configuration; using fallback\" user_id=9230\r\npanic: [gossip] duplicate channel nflog:9230\r\n\r\ngoroutine 29 [running]:\r\ngithub.com/weaveworks/cortex/vendor/github.com/weaveworks/mesh.(*Router).NewGossip(0xc42028c0e0, 0xc44f87be64, 0xa, 0xc9d200, 0xc4559b9700, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/mesh/router.go:136 +0x399\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*gossipFactory).newGossip(0xc4201d8c80, 0xc44f87be64, 0xa, 0xc9d200, 0xc4559b9700, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/mesh.go:182 +0xf6\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.New.func1(0xc9d200, 0xc4559b9700, 0xbd8ac0, 0xc4527f1c20)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/alertmanager.go:72 +0x59\r\ngithub.com/weaveworks/cortex/vendor/github.com/prometheus/alertmanager/nflog.WithMesh.func1(0xc4559b9700, 0xc4559b9700, 0x410928)\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/alertmanager/nflog/nflog.go:173 +0x3b\r\ngithub.com/weaveworks/cortex/vendor/github.com/prometheus/alertmanager/nflog.New(0xc4201c19c0, 0x6, 0x6, 0x0, 0x0, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/alertmanager/nflog/nflog.go:252 +0xdb\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.New(0xc44f016be0, 0xc44f016be0, 0x4, 0xc101a2)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/alertmanager.go:70 +0x5c4\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*MultitenantAlertmanager).newAlertmanager(0xc420286780, 0xc421a4f000, 0x4, 0xc455ddefc0, 0xc455ddefc0, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/multitenant.go:476 +0x113\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*MultitenantAlertmanager).setConfig(0xc420286780, 0xc421a4f000, 0x4, 0x0, 0xc421a545a0, 0x0, 0x0, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/multitenant.go:457 +0x63b\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*MultitenantAlertmanager).addNewConfigs(0xc420286780, 0xc4200c2690)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/multitenant.go:386 +0x280\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*MultitenantAlertmanager).Run(0xc420286780)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/multitenant.go:298 +0x85\r\ncreated by main.main\r\n    /go/src/github.com/weaveworks/cortex/cmd/alertmanager/main.go:49 +0x212\r\n```"},{"labels":[null,"bug"],"text":"The chunk merge iterator currently assumes that it'll never be asked for samples beyond 24hrs of the most recent seek.  This works nicely for queries like `rate(foo[1m])` and the sequence of calls is `Seek`, `Next`, `Next`, `Next`, `Seek`, `Next`, `Next`... and so on.\r\n\r\nHowever the query engine wraps the iterators in a buffer, which removes seek calls that only move the iterator forward - replacing them with repeated calls to next.  So if the steps of the query are longer than the vector ranges (ie a `rate[1m]` over a long period with a step > 60s) or the query is very simple (ie just `foo`), `Seek` is only called once on the `ChunkMergeIterator`, and after 24hrs it stops returning samples."},{"labels":[null,"bug"],"text":"```\r\npanic: [gossip] duplicate channel nflog:10130\r\n\r\ngoroutine 42 [running]:\r\ngithub.com/weaveworks/cortex/vendor/github.com/weaveworks/mesh.(*Router).NewGossip(0xc42028c000, 0xc445340c93, 0xb, 0xc35460, 0xc443b2f480, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/mesh/router.go:136 +0x399\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*gossipFactory).newGossip(0xc4202609e0, 0xc445340c93, 0xb, 0xc35460, 0xc443b2f480, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/mesh.go:182 +0xf6\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.New.func1(0xc35460, 0xc443b2f480, 0xb78600, 0xc445624150)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/alertmanager.go:72 +0x59\r\ngithub.com/weaveworks/cortex/vendor/github.com/prometheus/alertmanager/nflog.WithMesh.func1(0xc443b2f480, 0xc443b2f480, 0x410928)\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/alertmanager/nflog/nflog.go:173 +0x3b\r\ngithub.com/weaveworks/cortex/vendor/github.com/prometheus/alertmanager/nflog.New(0xc42044b9c0, 0x6, 0x6, 0x0, 0x0, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/alertmanager/nflog/nflog.go:252 +0xdb\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.New(0xc4455e6b40, 0xc4455e6b40, 0x5, 0xbac2bd)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/alertmanager.go:70 +0x5c4\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*MultitenantAlertmanager).newAlertmanager(0xc420268680, 0xc42046a850, 0x5, 0xc44539cf50, 0xc44539cf50, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/multitenant.go:476 +0x113\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*MultitenantAlertmanager).setConfig(0xc420268680, 0xc42046a850, 0x5, 0x0, 0xc4201bf380, 0x0, 0x0, 0x0, 0x0)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/multitenant.go:457 +0x63b\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*MultitenantAlertmanager).addNewConfigs(0xc420268680, 0xc4200b4690)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/multitenant.go:386 +0x280\r\ngithub.com/weaveworks/cortex/pkg/alertmanager.(*MultitenantAlertmanager).Run(0xc420268680)\r\n    /go/src/github.com/weaveworks/cortex/pkg/alertmanager/multitenant.go:298 +0x85\r\ncreated by main.main\r\n    /go/src/github.com/weaveworks/cortex/cmd/alertmanager/main.go:50 +0x250\r\n```\r\nIt happed before (July 12 for example):\r\n![screen shot 2018-07-24 at 10 21 09 am](https://user-images.githubusercontent.com/4003503/43155120-5d8597e0-8f2b-11e8-9a98-977cf37c86b0.png)\r\n"},{"labels":["bug"],"text":"https://github.com/weaveworks/cortex/blob/c1193e4ae27553608dc13f398607f72b21c39a05/build-image/Dockerfile#L19\r\nThis means that it always install a version from master which is different from the version of `github.com/gogo/protobuf` library in vendor.\r\nThis might lead to uncompilable code, for example like #892 "},{"labels":["bug"],"text":"If I understand correctly, the decision about whether to capture a trace should be made by one service, as close to the request source as possible.\r\nThen that decision should be passed along with the request and honored by each service involved in the request processing.\r\n\r\nCurrently, if a service is configured to send samples to jaeger, it also defaults to a sampling rate of 10 per second. If both the auth layer, distributor and ingester were configured thusly, you would see 10-20 traces per second from the distributor, and 20-30 traces per second from the ingester. Additionally you'd see some traces which start at the distributor or the ingester.\r\n\r\nFor cortex I think whether to sample should be decided by the (exersise-for-the-reader) auth proxy, and the ruler & ingester - since the ruler & ingester originate work based on timers.\r\n"},{"labels":[null,"bug"],"text":"Had one ingester OOM a few times in succession today; the thing that stands out is the number of goroutines:\r\n\r\n![image](https://user-images.githubusercontent.com/8125524/42032192-ad794356-7ad0-11e8-9160-5f1ca001e8af.png)\r\n\r\nNo obvious cause; in the logs I can see someone was playing silly with long label values, but not at high rates, and if that was the problem I would expect it to hit 3 ingesters rather than 1."},{"labels":[null,"bug"],"text":"I set a 3-day silence on Friday; it evaporated somewhere through the weekend.\r\n\r\nI see \" Persist silences\" was noted as a TODO on #50 \r\n"},{"labels":[null,"bug"],"text":"because of [this line](https://github.com/weaveworks/cortex/blob/master/pkg/ingester/ingester_flush.go#L228):\r\n\r\n```\r\n\t\ti.chunkAge.Observe(model.Now().Sub(chunkDesc.FirstTime).Seconds())\r\n```\r\n\r\nIt's always wrong to compare \"our\" idea of `Now` with \"theirs\".\r\n\r\nNot that I have a better idea right now."},{"labels":[null,"bug"],"text":"The ruler generates the `GeneratorURL` from some static strings, which do not include the (internal) instance ID, and do not have a means of including the the user-facing instance ID.\r\n\r\nThe base URL defaults to `\"\"` , which gives us values like `\"generatorURL\": \"/graph?...\"`.\r\nIt is plumbed through here:\r\n- https://github.com/weaveworks/cortex/blob/773fcf1aeff0f99a5b51cf3148034b9e227435c2/pkg/ruler/ruler.go#L202\r\n- https://github.com/weaveworks/cortex/blob/773fcf1aeff0f99a5b51cf3148034b9e227435c2/pkg/ruler/ruler.go#L283\r\n- https://github.com/weaveworks/cortex/blob/773fcf1aeff0f99a5b51cf3148034b9e227435c2/pkg/ruler/ruler.go#L309\r\n"},{"labels":["bug"],"text":"It used to be just master.  I don't think we need all the branches.\r\n"},{"labels":[null,null,"bug"],"text":"The `/ring` page (maybe others too) does a redirect which will break if you put it behind a reverse proxy that changes the path.\r\n\r\nPrometheus has a `--web.external-url` option to address this."},{"labels":["bug"],"text":"Note: This was only found when using the Dynamo storage layer. We haven't tested this against the other backends.\r\n\r\nDuring some of our testing as we add more different data sources to our cortex installations, we found that it was possible to send in time series that had incredibly large names. The resulting timeseries and its chunks could not be flushed to the storage layer due to ValidationException errors thrown from dynamo due to the hash being larger than 2KB. \r\n\r\nThe primary issue is that this data is retried indefinitely in the flushLoop function of the ingester if the immediate field of the operation is set to true. The flushing of the ingester is a bit naive here and that should be ok if the ingester (or distributor) validated that the incoming sample timeseries met the constraints for storage. \r\n\r\nI'm having a look at doing a fix and will start a WIP PR once I have something more than a few paragraphs of text. But a few statements I'm thinking about regarding behaviour here:\r\n\r\n- Validation and similar errors from a storage layer should never be retried. It seems futile to do anything other than drop the data at that point. Logging an event about the data & user might be useful for tracking or reporting\r\n- ~~Data that doesn't meet the constraints of the storage layer should be rejected ideally by the Ingester or Distributor, so that the sending party can be given an error. This way we are transparent in the action of rejecting/dropping the data, vs doing it behind the scenes in the background.~~"},{"labels":["bug"],"text":"If a client is sending samples stamped older than `now - idle-time`, they will get flushed immediately.\r\n\r\n#576 allows a limit to be set, outside of which Cortex will reject the upload, but really we should record `now` at the time the sample was received so \"idle\" can be computed properly.\r\n\r\nSimilarly `MaxChunkAge` is compared against `now`, but it should be `last-first`.\r\n\r\nCode is in `Ingester.shouldFlushChunk()`.\r\n\r\n"},{"labels":["bug"],"text":"https://github.com/weaveworks/cortex/pull/712 caused ingester flags to be registered twice in the `lite` binary, which instantiates both an ingester and a distributor:\r\n\r\n```\r\n./cmd/lite/lite \r\n./cmd/lite/lite flag redefined: ingester.client.max-recv-message-size\r\npanic: ./cmd/lite/lite flag redefined: ingester.client.max-recv-message-size\r\n```"},{"labels":[null,"bug"],"text":"```\r\n./cmd/ruler/ruler \r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x40 pc=0xc7fcc5]\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/weaveworks/cortex/pkg/chunk/storage.NewStorageClient(0x20ffb62, 0x3, 0x0, 0x4000000000000000, 0x0, 0xa, 0x20, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/weaveworks/cortex/pkg/chunk/storage/factory.go:39 +0x75\r\nmain.main()\r\n\t/go/src/github.com/weaveworks/cortex/cmd/ruler/main.go:51 +0x48c\r\n```\r\n\r\nIt should of course give a helpful error message about something missing instead."},{"labels":[null,"bug"],"text":"If auto-scaling is off for the active table and on for inactive tables, then the inactive setting is applied during the 'grace' period when the next weekly table is created.\r\n\r\n😿 "},{"labels":[null,"bug"],"text":"We replicate to three ingesters but after losing two of them in an Unfortunate Incident, queries would sometimes contain results from just the two restarted instances which didn't have recent history.\r\n\r\nIt rights itself once those chunks age out of the remaining ingester into the store, but not great in the meantime.\r\n"},{"labels":["bug"],"text":"When a rules update arrives, `scheduler.addNewConfigs()` calls `addWorkItem(now)`, and the ruler scheduler de-dupes items added to the queue on the basis of their userD.\r\n\r\nBut, the item is not in the queue while it is being executed.  Thus we can get something like this:\r\n\r\n![overlapping ruler group execution](https://user-images.githubusercontent.com/8125524/36681918-cdce606a-1b11-11e8-95c6-8967b8c8969b.png)\r\n\r\n"},{"labels":[null,null,"bug"],"text":"The chunk store fetch code assumes that fingerprints uniquely identify a timeseries; this is fairly likely when they are looking at a single metric, but we still could get clashes.\r\n\r\nPerhaps use the sha256 from the index?"},{"labels":[null,"bug"],"text":"At least with the DynamoDB back, end, where we write to multiple different tables, the code makes no attempt to keep writing chunks to some tables while others are queued up.  With different provisioning rates for different tables this can result in a blow-up in chunk queues.\r\n\r\n"},{"labels":[null,"bug"],"text":"After running an evaluation, it [reschedules](https://github.com/weaveworks/cortex/blob/master/pkg/ruler/scheduler.go#L73) that item to 15 seconds after it was supposed to run last time. If we are running a bit slow, and it's already more 15 seconds after it was supposed to run last time, it will go into the queue at an overdue time.\r\n\r\nWhen we receive a rules update, we [add it in](https://github.com/weaveworks/cortex/blob/master/pkg/ruler/scheduler.go#L200) to run 'now'. If the ruler has been overloaded for a while, 'now' will be a long time after the head of the queue, perhaps hours.\r\n\r\nThe simple mitigation is to add enough workers so it isn't overloaded.  But I think it could floor the due time of everything in the queue to be 'now' at worst, then we wouldn't get gaps in execution."},{"labels":["bug"],"text":"Symptom: we occasionally get massive numbers of goroutines in the ruler like this:\r\n\r\n```\r\ngoroutine profile: total 133338\r\n#   0x809b8a    github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport.(*recvBufferReader).read+0x28a                 /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport/transport.go:132\r\n#   0x809846    github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport.(*recvBufferReader).Read+0x66                  /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport/transport.go:121\r\n#   0x80abc4    github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport.(*transportReader).Read+0x54                   /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport/transport.go:395\r\n#   0x475445    io.ReadAtLeast+0x85                                                         /usr/local/go/src/io/io.go:309\r\n#   0x4755b7    io.ReadFull+0x57                                                            /usr/local/go/src/io/io.go:327\r\n#   0x80ab0e    github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport.(*Stream).Read+0xbe                        /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport/transport.go:379\r\n#   0x831ec4    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.(*parser).recvMsg+0x64                           /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/rpc_util.go:285\r\n#   0x832f0c    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.recv+0x4c                                    /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/rpc_util.go:394\r\n#   0x820965    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.recvResponse+0x275                               /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/call.go:75\r\n#   0x822014    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.invoke+0x9c4                                 /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/call.go:302\r\n#   0x85d419    github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/middleware.ClientUserHeaderInterceptor+0x109               /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/middleware/grpc_auth.go:17\r\n#   0x877ee3    github.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware.ChainUnaryClient.func1.1.1+0xd3               /go/src/github.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware/chain.go:61\r\n#   0x875df3    github.com/weaveworks/cortex/vendor/github.com/grpc-ecosystem/grpc-opentracing/go/otgrpc.OpenTracingClientInterceptor.func1+0x5e3   /go/src/github.com/weaveworks/cortex/vendor/github.com/grpc-ecosystem/grpc-opentracing/go/otgrpc/client.go:69\r\n#   0x877ee3    github.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware.ChainUnaryClient.func1.1.1+0xd3               /go/src/github.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware/chain.go:61\r\n#   0x878112    github.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware.ChainUnaryClient.func1+0x132                  /go/src/github.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware/chain.go:68\r\n#   0x82146c    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.(*ClientConn).Invoke+0xdc                            /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/call.go:149\r\n#   0x821620    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.Invoke+0xc0                                  /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/call.go:159\r\n#   0xa8d321    github.com/weaveworks/cortex/pkg/ingester/client.(*ingesterClient).Push+0xd1                                /go/src/github.com/weaveworks/cortex/pkg/ingester/client/cortex.pb.go:1587\r\n#   0xaa3948    github.com/weaveworks/cortex/pkg/ingester/client.(*closableIngesterClient).Push+0x88                            <autogenerated>:1\r\n#   0xbe82ef    github.com/weaveworks/cortex/pkg/distributor.(*Distributor).sendSamplesErr.func1+0xaf                           /go/src/github.com/weaveworks/cortex/pkg/distributor/distributor.go:424\r\n#   0x8dd15c    github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.CollectedRequest+0x1dc                  /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:143\r\n#   0x8dd505    github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.TimeRequestHistogram+0xd5                   /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:169\r\n#   0xbe47df    github.com/weaveworks/cortex/pkg/distributor.(*Distributor).sendSamplesErr+0x39f                            /go/src/github.com/weaveworks/cortex/pkg/distributor/distributor.go:423\r\n#   0xbe42cf    github.com/weaveworks/cortex/pkg/distributor.(*Distributor).sendSamples+0x7f                                /go/src/github.com/weaveworks/cortex/pkg/distributor/distributor.go:377\r\n#   0xbe8226    github.com/weaveworks/cortex/pkg/distributor.(*Distributor).Push.func2+0x76                             /go/src/github.com/weaveworks/cortex/pkg/distributor/distributor.go:352\r\n```\r\n\r\nThere is no timeout on the context used to call `Push()` in the ruler.\r\n\r\nLooks like [the idea](https://github.com/weaveworks/cortex/blob/aca2c72/pkg/ingester/client/client.go#L22) was to supply `grpc.WithTimeout()`, but that's a timeout on the _dial_, not on each individual call. Also it is documented to do nothing unless you also supply `WithBlock`, and we don't."},{"labels":[null,null,"bug"],"text":"For some instances we see \"sample timestamp out of order for series\" in our logs with a gap between previous and new timestamp of 15 or 30 seconds.\r\nIf this was going wrong in the sending Prometheus we would see the same error on all ingester replicas.  We do not: they are reported sporadically on one ingester at a time.  From this I deduce the out-of-order is happening inside Cortex.\r\n\r\nHere is my best theory: suppose some client Prometheus has hundreds of samples queued up for remote write, then the following can happen:\r\n- Prometheus sends 100 samples to distributor.\r\n- Distributor replicates the data three times and fires up three goroutines to deliver the data.\r\n- Once two of the calls have returned from ingesters, distributor returns success to prometheus.\r\n- Third call continues, on its goroutine.\r\n- Prometheus sends the next 100 samples; distributor (likely on another node) fires up another 3 goroutines.\r\n- One of those goroutines can overtake the third one from the previous call.\r\n\r\n"},{"labels":[null,"bug",null],"text":"There is supposed to be a metric exposed called `log_messages` so you can alert on too many errors, warnings, etc.  But it doesn't work.  [UPDATE: I think right now it is giving all zeros]\r\n\r\nIf there is [another library](https://github.com/weaveworks/promrus/blob/0599d76/promrus.go#L20) in your program trying to use a counter named `log_messages`, it's unpredictable which one wins.\r\n\r\nMaybe some kind of namespacing in the name should be added, but from the Prometheus docs it seems that the `AlreadyRegisteredError` is designed for the case above.\r\n"},{"labels":["bug"],"text":"The machine from which the transfer came rebooted at around 10:47:10:\r\n\r\n```\r\nts=2018-01-18T10:46:40.783849266Z caller=log.go:108 level=info msg=\"entry not found in ring, adding with no tokens\"\r\nts=2018-01-18T10:46:45.230917369Z caller=log.go:108 level=info msg=\"changing ingester state from\" old_state=PENDING new_state=JOINING\r\nts=2018-01-18T10:46:45.279055116Z caller=log.go:108 level=info msg=\"processing TransferChunks request from ingester\" ingester=ingester-6b5559dc4f-52mf4\r\ntime=\"2018-01-18T10:47:03Z\" level=warning msg=\"GET /ready (500) 128.044µs\"\r\ntime=\"2018-01-18T10:47:03Z\" level=warning msg=\"Is websocket request: false\\nGET /ready HTTP/1.1\\r\\nHost: 10.244.229.61:80\\r\\nConnection: close\\r\\nAccept-Encoding: gzip\\r\\nConnection: close\\r\\nUser-Agent: kube-probe/1.8\\r\\n\\r\\n\"\r\ntime=\"2018-01-18T10:47:03Z\" level=warning msg=\"Response: \"\r\ntime=\"2018-01-18T10:47:13Z\" level=warning msg=\"GET /ready (500) 111.238µs\"\r\ntime=\"2018-01-18T10:47:13Z\" level=warning msg=\"Is websocket request: false\\nGET /ready HTTP/1.1\\r\\nHost: 10.244.229.61:80\\r\\nConnection: close\\r\\nAccept-Encoding: gzip\\r\\nConnection: close\\r\\nUser-Agent: kube-probe/1.8\\r\\n\\r\\n\"\r\ntime=\"2018-01-18T10:47:13Z\" level=warning msg=\"Response: \"\r\ntime=\"2018-01-18T10:47:23Z\" level=warning msg=\"GET /ready (500) 100.248µs\"\r\ntime=\"2018-01-18T10:47:23Z\" level=warning msg=\"Is websocket request: false\\nGET /ready HTTP/1.1\\r\\nHost: 10.244.229.61:80\\r\\nConnection: close\\r\\nAccept-Encoding: gzip\\r\\nConnection: close\\r\\nUser-Agent: kube-probe/1.8\\r\\n\\r\\n\"\r\ntime=\"2018-01-18T10:47:23Z\" level=warning msg=\"Response: \"\r\ntime=\"2018-01-18T10:47:33Z\" level=warning msg=\"GET /ready (500) 105.513µs\"\r\ntime=\"2018-01-18T10:47:33Z\" level=warning msg=\"Is websocket request: false\\nGET /ready HTTP/1.1\\r\\nHost: 10.244.229.61:80\\r\\nConnection: close\\r\\nAccept-Encoding: gzip\\r\\nConnection: close\\r\\nUser-Agent: kube-probe/1.8\\r\\n\\r\\n\"\r\n```\r\n\r\nThis pattern continues it never goes ready.\r\n\r\nThere appears to be no logging at either end if the transfer fails.\r\n"},{"labels":["bug"],"text":"Exhibit 1:\r\n```\r\nfunc (i *Ingester) query(ctx context.Context, from, through model.Time, matchers []*labels.Matcher) (model.Matrix, error) {\r\n\ti.queries.Inc()\r\n\r\n\ti.userStatesMtx.RLock()\r\n\tdefer i.userStatesMtx.RUnlock()\r\n\tstate, err := i.userStates.getOrCreate(ctx)\r\n```\r\n\r\nIn what sense is this a _Read_ lock if we can _Create_ a user state inside it?\r\nAnd why do we hold this lock (over all user states) for the duration of the query over one?\r\nThere are five other `RLock` calls which are very similar.\r\n\r\nExhibit 2:\r\n```\r\n// TransferChunks receives all the chunks from another ingester.\r\nfunc (i *Ingester) TransferChunks(stream client.Ingester_TransferChunksServer) error {\r\n[...]\r\n\ti.userStatesMtx.Lock()\r\n\tdefer i.userStatesMtx.Unlock()\r\n\r\n\tif err := i.ChangeState(ring.ACTIVE); err != nil {\r\n\t\treturn err\r\n\t}\r\n\ti.userStates = userStates\r\n\r\n\treturn nil\r\n```\r\n\r\nThis is the only place a write-lock is taken on `userStatesMtx`, and only for as long as it takes to CAS the state change.\r\n\r\nSince there is a lock `mtx` inside `userStates` protecting its data, I suspect that `userStatesMtx` is left-over from some earlier implementation that needed it."},{"labels":["bug"],"text":"This is because we wrap the Prometheus logger with [one that maintains counters of log messages](https://github.com/weaveworks/cortex/blob/master/pkg/util/log.go#L108).\r\n\r\nThis was introduced in #610.\r\n"},{"labels":["bug"],"text":"Here is a list of gaps between operations, observed on a real Cortex system:\r\n\r\n\r\n0.05 | 0.11 | 0.10 | 0.11 | 0.18 | 0.50 | 0.21 | 0.29 | 0.60 | 0.25 | 0.15 | 0.41 | 1.13 | 1.02 | 0.64 | 0.46 | 0.49 | 1.28 | 0.97\r\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\r\n\r\n(times in seconds)\r\nWe give up after 20 iterations, total time 9.1 seconds. \r\n\r\nThe algorithm is \"Decorrelated Jitter\" from https://www.awsarchitectureblog.com/2015/03/backoff.html, and as far as I can see we implement it correctly, it's just a bad algorithm."},{"labels":["bug"],"text":"Messages to Slack and email are coming through without any text:\r\n<img width=\"680\" alt=\"screen shot 2017-11-17 at 10 09 53 am\" src=\"https://user-images.githubusercontent.com/2802257/32963572-4563415e-cb85-11e7-9d3b-591e26d6ab5a.png\">\r\n\r\nLast known good message (note the timestamp):\r\n<img width=\"466\" alt=\"screen shot 2017-11-17 at 10 16 42 am\" src=\"https://user-images.githubusercontent.com/2802257/32963581-4e14cfd4-cb85-11e7-875a-d673a0f0abc1.png\">\r\n\r\nFrom the events database:\r\n```\r\n{\r\n  \"browser\": {\r\n    \"type\": \"monitor\",\r\n    \"text\": \"\",\r\n    \"attachments\": [\r\n      {\r\n        \"fallback\": \"[FIRING:41] ScrapeFailed (warning) | /api/prom/alertman\\nager/#/alerts?receiver=fallback\",\r\n        \"text\": \"\\n\",\r\n        \"color\": \"danger\",\r\n        \"mrkdwn_in\": [\r\n          \"fallback\",\r\n          \"pretext\",\r\n          \"text\"\r\n        ]\r\n      }\r\n    ],\r\n    \"timestamp\": \"2017-11-03T20:52:\\n52.290691285Z\"\r\n  },\r\n  \"email\": {\r\n    \"subject\": \"monitor\",\r\n    \"body\": \"\"\r\n  },\r\n  \"slack\": {\r\n    \"username\": \"AlertManager\",\r\n    \"attachments\": [\r\n      {\r\n        \"title\": \"[FIRING:41] ScrapeFa\\niled (warning)\",\r\n        \"title_link\": \"/api/prom/alertmanager/#/alerts?receiver=fallback\",\r\n        \"text\": \"\\n\",\r\n        \"fallback\": \"[FIRING:41] ScrapeFailed (warni\\nng) | /api/prom/alertmanager/#/alerts?receiver=fallback\",\r\n        \"color\": \"danger\",\r\n        \"mrkdwn_in\": [\r\n          \"fallback\",\r\n          \"pretext\",\r\n          \"text\"\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n^ The slack.attachments[0].text field should have some text in it. Same with email.body.\r\n\r\nPossibly related: \r\nhttps://github.com/weaveworks/notification/pull/117\r\n\r\nhttps://github.com/weaveworks/cortex/commit/d714b622296ee32a85648480b59f8cc54793c263#diff-687f7844641224cb5825b565a83d58b5\r\n\r\n"},{"labels":["bug"],"text":"Identified by debugging a customer issue. The issue is somewhere around https://github.com/weaveworks/cortex/blob/master/pkg/alertmanager/multitenant.go#L440"},{"labels":["bug"],"text":"When Ctrl-C copying the YAML config from https://frontend.dev.weave.works/org/proud-wind-05/alerting/receivers and pasting it into an editor, all leading whitespace is gone.\r\n\r\nIt should be possible to copy&paste this without breaking the content."},{"labels":["bug"],"text":"When executing `{job=\"fluxy/prose\",__name__!=\"\"}` across a range, we see a `422 Unprocessable Entity` response.\r\n\r\n`{\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"unexpected error\"}`\r\n\r\nWe also see the querier panic.\r\n```\r\nlevel=error msg=\"parser panic: runtime error: invalid memory address or nil pointer dereference\r\npanic(0x18ceb00, 0x281c420)\r\n/usr/local/go/src/runtime/panic.go:489 +0x2cf\r\n```\r\n\r\nThis does not occur with an instant query."},{"labels":["bug"],"text":"We receive errors when using a table managed store and we receive samples which have timestamps after the current table."},{"labels":[null,"bug"],"text":"We had a `ruler` process hang; `Push()` is waiting in two goroutines for something to come back to it; however there are no stack traces showing any of the `sendSamples` routines it is waiting on.  Many other things in the environment were hosed: Consul had no leader, etc.\r\n\r\nI can't see by eyeballing `sendSamples` how it can go wrong, however the stack traces say it did.\r\n\r\n```\r\ngoroutine 26113781 [select, 364 minutes]:\r\ngithub.com/weaveworks/cortex/pkg/distributor.(*Distributor).Push(0xc42059e500, 0x7f517c5f39c0, 0xc42f6c34a0, 0xc43bf56720, 0x3083918, 0x0, 0x0)\r\n\t/go/src/github.com/weaveworks/cortex/pkg/distributor/distributor.go:364 +0xec8\r\ngithub.com/weaveworks/cortex/pkg/ruler.appenderAdapter.Append(0x2ff7060, 0xc42059e500, 0x7f517c5f39c0, 0xc42f6c34a0, 0xc4325366e0, 0x0, 0x0)\r\n\t/go/src/github.com/weaveworks/cortex/pkg/ruler/compat.go:28 +0xb2\r\ngithub.com/weaveworks/cortex/pkg/ruler.(*appenderAdapter).Append(0xc433a475e0, 0xc4325366e0, 0x0, 0x0)\r\n\t<autogenerated>:2 +0x72\r\ngithub.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules.(*Group).Eval.func1(0xc42a4ffe10, 0x1fd2b69, 0x9, 0xc42d8eb8b0, 0x15e2d655b20, 0x3012b60, 0xc420550990)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules/manager.go:296 +0x247\r\ncreated by github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules.(*Group).Eval\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules/manager.go:315 +0x138\r\n\r\ngoroutine 26113740 [select, 364 minutes]:\r\ngithub.com/weaveworks/cortex/pkg/distributor.(*Distributor).Push(0xc42059e500, 0x7f517c5f39c0, 0xc42f6c34a0, 0xc43bf56700, 0x3083918, 0x0, 0x0)\r\n\t/go/src/github.com/weaveworks/cortex/pkg/distributor/distributor.go:364 +0xec8\r\ngithub.com/weaveworks/cortex/pkg/ruler.appenderAdapter.Append(0x2ff7060, 0xc42059e500, 0x7f517c5f39c0, 0xc42f6c34a0, 0xc43ff07580, 0x0, 0x0)\r\n\t/go/src/github.com/weaveworks/cortex/pkg/ruler/compat.go:28 +0xb2\r\ngithub.com/weaveworks/cortex/pkg/ruler.(*appenderAdapter).Append(0xc433a475e0, 0xc43ff07580, 0x0, 0x0)\r\n\t<autogenerated>:2 +0x72\r\ngithub.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules.(*Group).Eval.func1(0xc42a4ffe10, 0x1fd2b69, 0x9, 0xc42d8eb8b0, 0x15e2d655b20, 0x3012b60, 0xc420b933b0)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules/manager.go:296 +0x247\r\ncreated by github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules.(*Group).Eval\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules/manager.go:315 +0x138\r\n```\r\n"},{"labels":["bug"],"text":"Running cortex lite locally. I haven't confirmed if this is happening with other configurations.\r\n\r\nAfter setting up the alert manager config and adding 8 rules, I see:\r\n`time=\"2017-08-09T13:02:26Z\" level=debug msg=\"Evaluating 8 rules...\" orgID=1 source=\"ruler.go:244\"` and the rules are evaluated.\r\n\r\nAfter adding a new rule (copied and pasted from an existing rule and changing the name), I see:\r\n`time=\"2017-08-09T13:02:27Z\" level=debug msg=\"Adding 1 configurations\" source=\"scheduler.go:179\"`\r\n\r\nHowever the logs still show only 8 rules are evaluated. Querying the new rule also returns no data. Only after restarting the ruler, the new rules are evaluated.\r\n"},{"labels":["bug"],"text":"https://circleci.com/gh/weaveworks/cortex/1277\r\n\r\n```\r\n\r\n--- FAIL: TestChunksToIterators (0.00s)\r\n\r\n\tassertions.go:241: \r\n                        \r\n\tError Trace:\tchunk_test.go:188\r\n\r\n\t\t\r\n\tError:\t\tNot equal: []local.SeriesIterator{util.SampleStreamIterator{ss:(*model.SampleStream)(0xc420591d00)}, util.SampleStreamIterator{ss:(*model.SampleStream)(0xc420591e00)}} (expected)\r\n\r\n\t\t\t        != []local.SeriesIterator{util.SampleStreamIterator{ss:(*model.SampleStream)(0xc420591f20)}, util.SampleStreamIterator{ss:(*model.SampleStream)(0xc420591ea0)}} (actual)\r\n\r\n\t\t\t\r\n\r\n\t\t\tDiff:\r\n\r\n\t\t\t--- Expected\r\n\r\n\t\t\t+++ Actual\r\n\r\n\t\t\t@@ -2,3 +2,3 @@\r\n\r\n\t\t\t  (util.SampleStreamIterator) {\r\n\r\n\t\t\t-  ss: (*model.SampleStream)(0xc420591d00)(foo{bar=\"baz\", toms=\"code\"} =>\r\n\r\n\t\t\t+  ss: (*model.SampleStream)(0xc420591f20)(foo2{bar=\"baz\", toms=\"code\"} =>\r\n\r\n\t\t\t 0 @[1502205814.871])\r\n\r\n\t\t\t@@ -6,3 +6,3 @@\r\n\r\n\t\t\t  (util.SampleStreamIterator) {\r\n\r\n\t\t\t-  ss: (*model.SampleStream)(0xc420591e00)(foo2{bar=\"baz\", toms=\"code\"} =>\r\n\r\n\t\t\t+  ss: (*model.SampleStream)(0xc420591ea0)(foo{bar=\"baz\", toms=\"code\"} =>\r\n\r\n\t\t\t 0 @[1502205814.871])\r\n\r\n\t\t```"},{"labels":["bug"],"text":"We are seeing 404 errors for some of the requests the ruler is making to the alertmanager.\r\n\r\nruler logs: \r\n`\r\ntime=\"2017-08-03T13:17:38Z\" level=error msg=\"Error sending alerts: bad response status 404 Not Found\" alertmanager=\"http://3138343130383837.alertmanager.cortex.svc.cluster.local:80/api/prom/alertmanager/api/v1/alerts\" count=48 source=\"notifier.go:370\r\n`\r\n\r\nalertmanager logs: \r\n`\r\ntime=\"2017-08-03T13:17:08Z\" level=warning msg=\"POST /api/prom/alertmanager/api/v1/alerts (404) 169.984µs\"\r\n`"},{"labels":["bug",null],"text":"https://circleci.com/gh/weaveworks/cortex/1260?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\r\n\r\n```--- FAIL: TestChunksToIterators (0.00s)\r\n\r\n\tError Trace:\tchunk_test.go:188\r\n\r\n\tError:\t\tNot equal: []local.SeriesIterator{util.SampleStreamIterator{ss:(*model.SampleStream)(0xc421923340)}, util.SampleStreamIterator{ss:(*model.SampleStream)(0xc421923440)}} (expected)\r\n\r\n\t\t\t        != []local.SeriesIterator{util.SampleStreamIterator{ss:(*model.SampleStream)(0xc421923560)}, util.SampleStreamIterator{ss:(*model.SampleStream)(0xc4219234e0)}} (actual)\r\n\r\n\t\t\t\r\n\r\n\t\t\tDiff:\r\n\r\n\t\t\t--- Expected\r\n\r\n\t\t\t+++ Actual\r\n\r\n\t\t\t@@ -2,3 +2,3 @@\r\n\r\n\t\t\t  (util.SampleStreamIterator) {\r\n\r\n\t\t\t-  ss: (*model.SampleStream)(0xc421923340)(foo{bar=\"baz\", toms=\"code\"} =>\r\n\r\n\t\t\t+  ss: (*model.SampleStream)(0xc421923560)(foo2{bar=\"baz\", toms=\"code\"} =>\r\n\r\n\t\t\t 0 @[1501673219.651])\r\n\r\n\t\t\t@@ -6,3 +6,3 @@\r\n\r\n\t\t\t  (util.SampleStreamIterator) {\r\n\r\n\t\t\t-  ss: (*model.SampleStream)(0xc421923440)(foo2{bar=\"baz\", toms=\"code\"} =>\r\n\r\n\t\t\t+  ss: (*model.SampleStream)(0xc4219234e0)(foo{bar=\"baz\", toms=\"code\"} =>\r\n\r\n\t\t\t 0 @[1501673219.651])```"},{"labels":[null,"bug"],"text":"If any significant number of samples are pushed to cortex which map to an 'inactive' dynamo table (ie. not the current week), they can only be written at a global throughput limit of 1 chunk/second. In the meantime, all ingesters are trying and failing to write these bad chunks, resulting in a large queue and very little useful work being performed.\r\n\r\nTo fix this, we need to do one of the following:\r\n* Reject samples that are too old\r\n* Increase limits dynamically on older tables (an extension of the work in https://github.com/weaveworks/cortex/pull/507)"},{"labels":["bug"],"text":"When looking at [some dashboards](https://frontend.dev.weave.works/admin/prod-grafana/dashboard/file/services.json?refresh=10s&orgId=1&from=now-7d%2Fd&to=now-7d%2Fd) at \"This day last week\" (\"now-7d\"), sometimes partial data or \"No data points\" is displayed. Here's an example:\r\n![screenshot from 2017-07-13 10-29-07](https://user-images.githubusercontent.com/109109/28159910-770befd6-67b6-11e7-8f17-feaeabc0db22.png)\r\nand here is the same range from a the standalone prometheus\r\n![screenshot from 2017-07-13 10-34-17](https://user-images.githubusercontent.com/109109/28160051-edb6103a-67b6-11e7-900e-da918de683df.png)\r\n\r\nFurthermore, the display is unstable, fluctuating between showing some (different) data, no data, or all data."},{"labels":[null,"bug"],"text":"Symptom: after updating alerting rules in the config server, `ruler` was still running the previous rules for several minutes.\r\n\r\nThere is a race between the ticker triggering a config update and the scheduler re-scheduling rules it has just evaluated.  I enhanced the logging to make this clearer:\r\n\r\n```\r\ntime=\"2017-07-05T17:29:42Z\" level=debug msg=\"Scheduler: work item added: {orgID=1 scheduled=2017-07-05 17:29:42 [\"ALERT AAA\" \"ALERT BBB\"]}\" source=\"scheduler.go:204\" \r\ntime=\"2017-07-05T17:29:42Z\" level=debug msg=\"Evaluated rule \"ALERT AAA\"\" source=\"manager.go:287\" \r\ntime=\"2017-07-05T17:29:42Z\" level=debug msg=\"Scheduler: work item {orgID=1 scheduled=2017-07-05 17:29:42 [\"ALERT AAA\"]} rescheduled for 2017-07-05 17:29:57\" source=\"scheduler.go:228\" \r\ntime=\"2017-07-05T17:29:42Z\" level=debug msg=\"Scheduler: work item added: {orgID=1 scheduled=2017-07-05 17:29:57 [\"ALERT AAA\"]}\" source=\"scheduler.go:204\" \r\n```\r\n\r\nThe first line comes from a config update with two rules, but it is immediately overwritten by the re-scheduling of the previous rule set which had only one rule.\r\n\r\nIn practice this race happens very often because the config polling loop is using the same duration as the rule re-evaluation, so a trivial improvement would be to make those durations relatively prime.\r\n\r\nPerhaps a better fix would be to look up the current rules when re-scheduling rather than using the set we have at hand."},{"labels":["bug"],"text":"When using Grafana backed by Cortex, choosing the 'This Month' time range results in all graphs failing to render with the following error:\r\n\r\n![screenshot from 2017-06-28 15-31-16](https://user-images.githubusercontent.com/1504438/27642554-1032d860-5c17-11e7-802c-eb696bbcf02b.png)\r\n\r\nPresumably because the periodic tables for future weeks have not yet been created in dynamoDB.\r\n\r\nQueries that range into the future should return no data rather than this error.\r\n"},{"labels":["bug"],"text":"Again whilst testing the reboot daemon the following situation arose: a cortex ingester `ingester-3378111688-nyll4` with address `10.244.228.234` was drained by the reboot daemon, and a new ingester `ingester-3378111688-d5lpl` with address `10.244.202.96` scheduled to take its place:\r\n\r\nFrom draining ingester:\r\n```\r\ntime=\"2017-06-14T08:59:37Z\" level=info msg=\"=== received SIGINT/SIGTERM ===\r\n*** exiting\" \r\n2017/06/14 08:59:37 transport: http2Server.HandleStreams failed to read frame: read tcp 10.244.228.234:9095->10.244.253.70:55116: use of closed network connection\r\ntime=\"2017-06-14T08:59:37Z\" level=info msg=\"Changing ingester state from ACTIVE -> LEAVING\" source=\"ingester_lifecycle.go:314\" \r\ntime=\"2017-06-14T08:59:37Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:415\" \r\ntime=\"2017-06-14T08:59:39Z\" level=error msg=\"Failed to flush user 2: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T08:59:42Z\" level=error msg=\"Failed to flush user 2123: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T08:59:47Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T08:59:49Z\" level=info msg=\"Sending chunks to 10.244.202.96:9095\" source=\"ingester_lifecycle.go:349\" \r\ntime=\"2017-06-14T08:59:53Z\" level=error msg=\"Failed to flush user 910: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T08:59:59Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:01Z\" level=error msg=\"Failed to flush user 910: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:05Z\" level=error msg=\"Failed to flush user 2: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:10Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:10Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:12Z\" level=error msg=\"Failed to flush user 2: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:17Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:18Z\" level=error msg=\"Failed to flush user 3166: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:25Z\" level=error msg=\"Failed to flush user 2: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:31Z\" level=error msg=\"Failed to flush user 910: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:45Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:47Z\" level=error msg=\"Failed to flush user 3711: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:51Z\" level=error msg=\"Failed to flush user 3708: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:55Z\" level=error msg=\"Failed to flush user 910: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:57Z\" level=error msg=\"Failed to flush user 2: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:00:58Z\" level=error msg=\"Failed to flush user 3166: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:01:06Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:01:14Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:01:14Z\" level=error msg=\"Failed to flush user 2: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:01:36Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:01:36Z\" level=error msg=\"Failed to flush user 3165: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:01:39Z\" level=error msg=\"Failed to flush user 3938: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:01:54Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:01:59Z\" level=error msg=\"Failed to flush user 3165: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:01Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:04Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:06Z\" level=error msg=\"Failed to flush user 3681: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:07Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:11Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:14Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:23Z\" level=error msg=\"Failed to flush user 3966: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:28Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:37Z\" level=error msg=\"Failed to flush user 2830: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:51Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:02:59Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:03:06Z\" level=error msg=\"Failed to flush user 3630: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:03:19Z\" level=error msg=\"Failed to flush user 3970: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:03:22Z\" level=error msg=\"Failed to flush user 3713: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:03:23Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:03:34Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:03:35Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:03:52Z\" level=error msg=\"Failed to flush user 3531: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:15Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:17Z\" level=error msg=\"Failed to flush user 2: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:18Z\" level=error msg=\"Failed to flush user 1282: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:20Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:36Z\" level=error msg=\"Failed to flush user 3966: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:42Z\" level=error msg=\"Failed to flush user 2848: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:44Z\" level=error msg=\"Failed to flush user 3166: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:46Z\" level=error msg=\"Failed to flush user 3531: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:49Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:04:53Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:05:19Z\" level=error msg=\"Failed to flush user 2867: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:05:25Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:05:29Z\" level=error msg=\"Failed to flush user 3960: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:05:36Z\" level=error msg=\"Failed to flush user 910: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:05:51Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:05:52Z\" level=error msg=\"Failed to flush user 910: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:05:58Z\" level=error msg=\"Failed to flush user 3706: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:06:22Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:06:36Z\" level=error msg=\"Failed to flush user 3531: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:06:56Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:07:11Z\" level=error msg=\"Failed to flush user 1902: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:07:16Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:07:29Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:07:56Z\" level=error msg=\"Failed to flush user 3391: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:07:57Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:08:55Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:08:59Z\" level=error msg=\"Failed to flush user 2123: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:09:02Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:09:16Z\" level=error msg=\"Failed to flush user 3869: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:10:18Z\" level=error msg=\"Failed to flush user 3531: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:10:41Z\" level=error msg=\"Failed to flush user 3965: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:11:00Z\" level=error msg=\"Failed to flush user 2: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:11:02Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:11:14Z\" level=error msg=\"Failed to flush user 3708: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:11:21Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:11:22Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:11:33Z\" level=error msg=\"Failed to flush user 3706: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:11:34Z\" level=error msg=\"Failed to flush user 1282: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:12:05Z\" level=error msg=\"Failed to flush user 2650: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:12:13Z\" level=error msg=\"Failed to flush user 2650: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:12:25Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:12:25Z\" level=error msg=\"Failed to flush user 3968: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:12:36Z\" level=error msg=\"Failed to flush user 3857: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:13:37Z\" level=error msg=\"Failed to flush user 3708: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:13:45Z\" level=error msg=\"Failed to flush user 3913: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:14:13Z\" level=error msg=\"Failed to flush user 3869: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:14:13Z\" level=error msg=\"Failed to flush user 910: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:14:26Z\" level=error msg=\"Failed to flush user 910: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:14:57Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:15:14Z\" level=error msg=\"Failed to flush user 3889: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:15:40Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:16:31Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:17:00Z\" level=error msg=\"Failed to flush user 3871: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:17:14Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:17:16Z\" level=error msg=\"Failed to flush user 3708: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:17:35Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:17:53Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:18:48Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:19:13Z\" level=error msg=\"Failed to flush user 3946: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:19:15Z\" level=error msg=\"Failed to flush user 3630: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:19:36Z\" level=error msg=\"Failed to flush user 2950: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:21:31Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:22:34Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:22:39Z\" level=error msg=\"Failed to flush user 3342: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:23:44Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:24:48Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:24:55Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:25:24Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:25:56Z\" level=error msg=\"Error CASing collectors/ring, trying again 4952650\" source=\"consul_client.go:187\" \r\ntime=\"2017-06-14T09:26:40Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:28:03Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:28:31Z\" level=error msg=\"Error CASing collectors/ring, trying again 4952859\" source=\"consul_client.go:187\" \r\ntime=\"2017-06-14T09:29:52Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:31:54Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:32:10Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:33:25Z\" level=error msg=\"Failed to flush user 3745: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\" \r\ntime=\"2017-06-14T09:34:44Z\" level=error msg=\"Failed to flush user 2602: failed to write chunk after 20 retries, 1 values remaining\" source=\"ingester_flush.go:121\"\r\n```\r\n\r\nFrom new ingester:\r\n```\r\ntime=\"2017-06-14T08:59:40Z\" level=info msg=\"Entry not found in ring, adding with no tokens.\" source=\"ingester_lifecycle.go:234\" \r\ntime=\"2017-06-14T08:59:49Z\" level=info msg=\"Changing ingester state from PENDING -> JOINING\" source=\"ingester_lifecycle.go:314\" \r\ntime=\"2017-06-14T08:59:57Z\" level=warning msg=\"GET /ready (500) 115.914µs\" \r\ntime=\"2017-06-14T09:00:07Z\" level=warning msg=\"GET /ready (500) 87.622µs\" \r\ntime=\"2017-06-14T09:00:17Z\" level=warning msg=\"GET /ready (500) 107.212µs\" \r\ntime=\"2017-06-14T09:00:27Z\" level=warning msg=\"GET /ready (500) 95.875µs\" \r\ntime=\"2017-06-14T09:00:33Z\" level=info msg=\"Changing ingester state from JOINING -> ACTIVE\" source=\"ingester_lifecycle.go:314\"\r\n```\r\n\r\nWe can see the the terminating ingester does attempt to send chunks to the new one, but does not manage to remove itself from the ring and exit cleanly before the 40m shutdown limit is reached and it is killed by k8s, leaving the ring in an unhealthy state."},{"labels":[null,null,"bug"],"text":"DynamoDB complains if it is passed an empty array of tags\r\nIf we update the configuration to not tag table, the code to tag the table will not currently run."},{"labels":["bug"],"text":"On dev, with the new DynamoDB chunk storage."},{"labels":["bug"],"text":"Noticed whilst testing reboot daemon. Graceful shutdown starts at `2017-05-18T12:27:29Z`, ends with `panic` ~20 minutes later:\r\n\r\n```\r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 2/e6628b47e01047c9:15c1b0d1667:15c1b824666:3e81ebad in chunk cache: read tcp 10.244.204.190:40610->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 1/216f68277729fe76:15c1a948226:15c1b833b6e:83e67bed in chunk cache: read tcp 10.244.204.190:40526->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 2/f69d36a7e2a64f36:15c1ad62bb1:15c1b82f9ed:aaa76645 in chunk cache: read tcp 10.244.204.190:40126->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 2/14e097acd4ec1a8d:15c1a9bb03c:15c1b83c712:4bae19f8 in chunk cache: read tcp 10.244.204.190:40544->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 1/f840156d3c43ef48:15c1b37c5d7:15c1b834dc6:8a3bd39d in chunk cache: read tcp 10.244.204.190:40598->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 2/a4e585b33e2ab972:15c1ae54316:15c1b824666:b82aab85 in chunk cache: read tcp 10.244.204.190:40596->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 2/3b3433829b20cd50:15c1ad03cc5:15c1b8373d0:3dfc3fc8 in chunk cache: read tcp 10.244.204.190:40592->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 2/2c7ab806ca2ad657:15c1b2f6ad5:15c1b836a7e:8a839e03 in chunk cache: read tcp 10.244.204.190:40754->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 1/6f2cdc54745dd4d:15c1ab514ce:15c1b834dc6:11c11646 in chunk cache: read tcp 10.244.204.190:40608->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 1/d7cebfe59fb11019:15c1aba5877:15c1b834dc6:3a0a79a1 in chunk cache: read tcp 10.244.204.190:40614->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 2/a08e846aed434b97:15c1b028f7e:15c1b8246d1:63d8f8d4 in chunk cache: read tcp 10.244.204.190:40590->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 1/8961e8e6284849e6:15c1b0e09d4:15c1b83af05:4106da30 in chunk cache: read tcp 10.244.204.190:40618->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:22Z\" level=warning msg=\"Could not store 1/b4142c10022a5e4a:15c1b5a03ad:15c1b82fbf5:820e8b62 in chunk cache: read tcp 10.244.204.190:40756->10.244.204.170:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:24:30Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample timestamp out of order for series ALERTS{alertname=\"PodRestartingTooMuch\", alertstate=\"pending\", containerStatuses=\"0\", instance=\"kube-api-exporter-3558593383-8hbp1\", job=\"monitoring/kube-api-exporter\", monitor=\"dev\", name=\"reboot-required-27vxu\", namespace=\"monitoring\", node=\"ip-172-20-1-135.ec2.internal\", severity=\"warning\"}; last timestamp: 1495110269.011, incoming timestamp: 1495110264.456) 438.338µs\" \r\ntime=\"2017-05-18T12:24:31Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample timestamp out of order for series ALERTS{alertname=\"PodRestartingTooMuch\", alertstate=\"pending\", containerStatuses=\"0\", instance=\"kube-api-exporter-3558593383-8hbp1\", job=\"monitoring/kube-api-exporter\", monitor=\"dev\", name=\"reboot-required-27vxu\", namespace=\"monitoring\", node=\"ip-172-20-1-135.ec2.internal\", severity=\"warning\"}; last timestamp: 1495110269.011, incoming timestamp: 1495110264.456) 458.886µs\" \r\ntime=\"2017-05-18T12:24:31Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample timestamp out of order for series ALERTS{alertname=\"PodRestartingTooMuch\", alertstate=\"pending\", containerStatuses=\"0\", instance=\"kube-api-exporter-3558593383-8hbp1\", job=\"monitoring/kube-api-exporter\", monitor=\"dev\", name=\"reboot-required-27vxu\", namespace=\"monitoring\", node=\"ip-172-20-1-135.ec2.internal\", severity=\"warning\"}; last timestamp: 1495110269.011, incoming timestamp: 1495110264.456) 326.013µs\" \r\ntime=\"2017-05-18T12:24:31Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample timestamp out of order for series ALERTS{alertname=\"PodRestartingTooMuch\", alertstate=\"pending\", containerStatuses=\"0\", instance=\"kube-api-exporter-3558593383-8hbp1\", job=\"monitoring/kube-api-exporter\", monitor=\"dev\", name=\"reboot-required-27vxu\", namespace=\"monitoring\", node=\"ip-172-20-1-135.ec2.internal\", severity=\"warning\"}; last timestamp: 1495110269.011, incoming timestamp: 1495110264.456) 5.469532ms\" \r\ntime=\"2017-05-18T12:24:31Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample timestamp out of order for series ALERTS{alertname=\"PodRestartingTooMuch\", alertstate=\"pending\", containerStatuses=\"0\", instance=\"kube-api-exporter-3558593383-8hbp1\", job=\"monitoring/kube-api-exporter\", monitor=\"dev\", name=\"reboot-required-27vxu\", namespace=\"monitoring\", node=\"ip-172-20-1-135.ec2.internal\", severity=\"warning\"}; last timestamp: 1495110269.011, incoming timestamp: 1495110264.456) 302.727µs\" \r\ntime=\"2017-05-18T12:24:31Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample timestamp out of order for series ALERTS{alertname=\"PodRestartingTooMuch\", alertstate=\"pending\", containerStatuses=\"0\", instance=\"kube-api-exporter-3558593383-8hbp1\", job=\"monitoring/kube-api-exporter\", monitor=\"dev\", name=\"reboot-required-27vxu\", namespace=\"monitoring\", node=\"ip-172-20-1-135.ec2.internal\", severity=\"warning\"}; last timestamp: 1495110269.011, incoming timestamp: 1495110264.456) 315.314µs\" \r\ntime=\"2017-05-18T12:24:31Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample timestamp out of order for series ALERTS{alertname=\"PodRestartingTooMuch\", alertstate=\"pending\", containerStatuses=\"0\", instance=\"kube-api-exporter-3558593383-8hbp1\", job=\"monitoring/kube-api-exporter\", monitor=\"dev\", name=\"reboot-required-27vxu\", namespace=\"monitoring\", node=\"ip-172-20-1-135.ec2.internal\", severity=\"warning\"}; last timestamp: 1495110269.011, incoming timestamp: 1495110264.456) 404.846µs\" \r\ntime=\"2017-05-18T12:24:32Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample timestamp out of order for series ALERTS{alertname=\"PodRestartingTooMuch\", alertstate=\"pending\", containerStatuses=\"0\", instance=\"kube-api-exporter-3558593383-8hbp1\", job=\"monitoring/kube-api-exporter\", monitor=\"dev\", name=\"reboot-required-27vxu\", namespace=\"monitoring\", node=\"ip-172-20-1-135.ec2.internal\", severity=\"warning\"}; last timestamp: 1495110269.011, incoming timestamp: 1495110264.456) 280.243µs\" \r\ntime=\"2017-05-18T12:24:32Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample timestamp out of order for series ALERTS{alertname=\"PodRestartingTooMuch\", alertstate=\"pending\", containerStatuses=\"0\", instance=\"kube-api-exporter-3558593383-8hbp1\", job=\"monitoring/kube-api-exporter\", monitor=\"dev\", name=\"reboot-required-27vxu\", namespace=\"monitoring\", node=\"ip-172-20-1-135.ec2.internal\", severity=\"warning\"}; last timestamp: 1495110269.011, incoming timestamp: 1495110264.456) 407.2µs\" \r\n2017/05/18 12:25:13 grpc: Server.processUnaryRPC failed to write status stream error: code = Canceled desc = \"context canceled\"\r\ntime=\"2017-05-18T12:25:32Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 1.617774ms\" \r\ntime=\"2017-05-18T12:25:32Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 2.00823ms\" \r\ntime=\"2017-05-18T12:25:32Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_free{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 259135717376, incoming value: 259135725568) 1.926956ms\" \r\ntime=\"2017-05-18T12:25:32Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 3.108352ms\" \r\ntime=\"2017-05-18T12:25:32Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_free{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 259135717376, incoming value: 259135725568) 1.335111ms\" \r\ntime=\"2017-05-18T12:25:32Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 1.304315ms\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_free{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 259135717376, incoming value: 259135725568) 1.339464ms\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 1.109753ms\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_free{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 259135717376, incoming value: 259135725568) 1.227185ms\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 912.841µs\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_free{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 259135717376, incoming value: 259135725568) 836.232µs\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 983.051µs\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_free{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 259135717376, incoming value: 259135725568) 879.234µs\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 1.535631ms\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_free{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 259135717376, incoming value: 259135725568) 846.107µs\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 1.013157ms\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_free{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 259135717376, incoming value: 259135725568) 867.041µs\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_avail{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 242737668096, incoming value: 242737676288) 1.257355ms\" \r\ntime=\"2017-05-18T12:25:33Z\" level=warning msg=\"gRPC /cortex.Ingester/Push (sample with repeated timestamp but different value for series node_filesystem_free{device=\"overlay\", fstype=\"overlay\", instance=\"prom-node-exporter-28rta\", job=\"monitoring/prom-node-exporter\", monitor=\"prod\", mountpoint=\"/mnt/root/mnt/containers/docker/overlay2/4f9edeb63ae1d677bbf075a167eaea3fa4e7785c1a1d7a33f323e3c5d207bc65/merged\", node=\"ip-172-20-3-251.ec2.internal\"}; last value: 259135717376, incoming value: 259135725568) 1.084959ms\" \r\n2017/05/18 12:27:00 grpc: Server.processUnaryRPC failed to write status stream error: code = Canceled desc = \"context canceled\"\r\ntime=\"2017-05-18T12:27:29Z\" level=info msg=\"=== received SIGINT/SIGTERM ===\r\n*** exiting\" \r\n2017/05/18 12:27:31 transport: http2Server.HandleStreams failed to read frame: read tcp 10.244.204.190:9095->10.244.229.90:33490: use of closed network connection\r\ntime=\"2017-05-18T12:28:03Z\" level=error msg=\"Error getting collectors/ring: Unexpected response code: 500\" source=\"consul_client.go:144\" \r\ntime=\"2017-05-18T12:28:04Z\" level=error msg=\"Error CASing collectors/ring, trying again 548284\" source=\"consul_client.go:187\" \r\ntime=\"2017-05-18T12:28:04Z\" level=error msg=\"Error CASing collectors/ring, trying again 548290\" source=\"consul_client.go:187\" \r\ntime=\"2017-05-18T12:28:04Z\" level=error msg=\"Error CASing collectors/ring, trying again 548291\" source=\"consul_client.go:187\" \r\ntime=\"2017-05-18T12:28:04Z\" level=error msg=\"Error CASing collectors/ring, trying again 548294\" source=\"consul_client.go:187\" \r\ntime=\"2017-05-18T12:28:04Z\" level=error msg=\"Error CASing collectors/ring, trying again 548296\" source=\"consul_client.go:187\" \r\ntime=\"2017-05-18T12:28:27Z\" level=warning msg=\"Could not store 2/15c907c7a388ac63:15c1b267eb6:15c1b8246d1:e166c71b in chunk cache: read tcp 10.244.204.190:46940->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:32:40Z\" level=warning msg=\"Could not store 1/7a6f3c75bdcf416d:15c1a1eafa5:15c1b86b79d:bd139bc2 in chunk cache: read tcp 10.244.204.190:55262->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:34:33Z\" level=warning msg=\"Could not store 2/1b113cfa20b7834f:15c1a95d28c:15c1b86d5cc:1231fbc3 in chunk cache: read tcp 10.244.204.190:54180->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:34:33Z\" level=warning msg=\"Could not store 2/67396f00f6a79e14:15c1a84a804:15c1b86d5cc:f6dc824c in chunk cache: memcache: connect timeout to 10.244.254.185:11211\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:34:38Z\" level=warning msg=\"Could not store 2/a24c25e6b87355fa:15c1a9647be:15c1b86d5cc:76268d39 in chunk cache: read tcp 10.244.204.190:52300->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:34:53Z\" level=warning msg=\"Could not store 1/145875a526ff0b48:15c1a90416a:15c1b86c2dc:f3605324 in chunk cache: read tcp 10.244.204.190:57156->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:35:16Z\" level=warning msg=\"Could not store 1/c1175fc9bd178f7:15c1aa0e2d6:15c1b86e98f:d8c74ca7 in chunk cache: read tcp 10.244.204.190:41758->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:35:45Z\" level=warning msg=\"Could not store 1/7c484f0f2641218c:15c1a96ee75:15c1b86cc9c:19ae0280 in chunk cache: read tcp 10.244.204.190:43482->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:37:27Z\" level=warning msg=\"Could not store 1/1fba125b0951e12c:15c1ad30b85:15c1b86b79d:50ef347c in chunk cache: read tcp 10.244.204.190:48948->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:37:27Z\" level=warning msg=\"Could not store 1/742fbed4b0532a38:15c1ac9391f:15c1b86bcae:b063e916 in chunk cache: read tcp 10.244.204.190:60618->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:37:49Z\" level=warning msg=\"Could not store 1/cf1b579bf701a5ee:15c1ad195dd:15c1b86dc1e:6afcad3 in chunk cache: read tcp 10.244.204.190:42570->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:38:32Z\" level=warning msg=\"Could not store 1/366106a67d58ea8d:15c1acc8c13:15c1b866637:1fec73ed in chunk cache: read tcp 10.244.204.190:38964->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:39:24Z\" level=warning msg=\"Could not store 1/5601d58ce868f289:15c1aaf9748:15c1b86bd68:70994224 in chunk cache: read tcp 10.244.204.190:39302->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:39:40Z\" level=warning msg=\"Could not store 1/d605a9d81c412dc9:15c1ad41e55:15c1b86e00e:34c612c8 in chunk cache: read tcp 10.244.204.190:52310->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:39:42Z\" level=warning msg=\"Could not store 1/e01c5f28498d5140:15c1ad5019c:15c1b86d8f0:138217fa in chunk cache: read tcp 10.244.204.190:45130->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:40:15Z\" level=warning msg=\"Could not store 1/2d5df6657f667429:15c1addae70:15c1b86d338:da60e578 in chunk cache: read tcp 10.244.204.190:55484->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:40:22Z\" level=warning msg=\"Could not store 2/5f5872168dadd5fe:15c1af5c0be:15c1b86dd2e:3912988e in chunk cache: read tcp 10.244.204.190:34474->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:40:27Z\" level=warning msg=\"Could not store 1/6a555181d2eee6a5:15c1ae2fa51:15c1b86a0cb:7fd3a5aa in chunk cache: read tcp 10.244.204.190:42060->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:40:29Z\" level=warning msg=\"Could not store 1/f398a3fd6072f476:15c1af18db5:15c1b86c8d2:eef33eba in chunk cache: read tcp 10.244.204.190:41430->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:40:30Z\" level=warning msg=\"Could not store 1/12d8d4e3a3c96d7:15c1add6431:15c1b86c391:e5360772 in chunk cache: memcache: connect timeout to 10.244.229.10:11211\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:40:37Z\" level=warning msg=\"Could not store 2/140f81dec43ba440:15c1ae7d2e4:15c1b86e58d:a5811278 in chunk cache: read tcp 10.244.204.190:46054->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:41:06Z\" level=warning msg=\"Could not store 1/40697f15c45e1f2a:15c1af5a7bd:15c1b86c42d:3a83efae in chunk cache: read tcp 10.244.204.190:35262->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:41:24Z\" level=warning msg=\"Could not store 1/5113e93f9b37e4a3:15c1afbe82e:15c1b86d696:448a88aa in chunk cache: read tcp 10.244.204.190:48272->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:41:34Z\" level=warning msg=\"Could not store 1/65d3f7b224b6aa2e:15c1afa338c:15c1b86bc1c:5421ede8 in chunk cache: read tcp 10.244.204.190:47190->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:41:34Z\" level=warning msg=\"Could not store 1/bc32c66283fb0ca6:15c1aef4ef3:15c1b86d402:81c2ce85 in chunk cache: read tcp 10.244.204.190:33484->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:41:37Z\" level=warning msg=\"Could not store 1/a4987730f05c9f3c:15c1af248aa:15c1b86d402:4b5f24dc in chunk cache: read tcp 10.244.204.190:53180->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:41:44Z\" level=warning msg=\"Could not store 2/25b359f11bc85b69:15c1af39434:15c1b86bffe:789ad00c in chunk cache: read tcp 10.244.204.190:59832->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:00Z\" level=warning msg=\"Could not store 2/a1a8ffe9318e936d:15c1afa4d89:15c1b86d618:d0508c00 in chunk cache: memcache: connect timeout to 10.244.254.185:11211\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:05Z\" level=warning msg=\"Could not store 2/ce93d8297235e3dc:15c1af54478:15c1b86d618:1bab7176 in chunk cache: read tcp 10.244.204.190:44316->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:05Z\" level=warning msg=\"Could not store 2/6083f55bc5ac6cba:15c1afd9a7f:15c1b86bcdb:b9e97439 in chunk cache: read tcp 10.244.204.190:49584->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:06Z\" level=warning msg=\"Could not store 1/cc65109678855cc6:15c1b0e79fb:15c1b86de4b:651e5d17 in chunk cache: read tcp 10.244.204.190:50186->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:08Z\" level=warning msg=\"Could not store 2/6c440493266aee2e:15c1b047862:15c1b86eec5:e0b5760a in chunk cache: read tcp 10.244.204.190:56550->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:14Z\" level=warning msg=\"Could not store 1/2c73166b5aea3ad3:15c1af5d722:15c1b86b8fa:f0753d6c in chunk cache: read tcp 10.244.204.190:58856->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:23Z\" level=warning msg=\"Could not store 2/19c7f448ffccdc46:15c1afd9a7f:15c1b86bcdb:4a1b1d9d in chunk cache: read tcp 10.244.204.190:33248->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:24Z\" level=warning msg=\"Could not store 1/5c8e82efff2cbccf:15c1b008195:15c1b86dc1e:bb425894 in chunk cache: read tcp 10.244.204.190:38364->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:24Z\" level=warning msg=\"Could not store 1/2fcb96640a0e35f0:15c1b11a64e:15c1b86d64e:dd9305ee in chunk cache: read tcp 10.244.204.190:40554->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:24Z\" level=warning msg=\"Could not store 1/7762213d0af3968:15c1af5fe35:15c1b86e00e:c10bde33 in chunk cache: read tcp 10.244.204.190:34824->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:24Z\" level=warning msg=\"Could not store 1/1f2f25e11167c396:15c1b09bf6d:15c1b86b79d:27255185 in chunk cache: memcache: connect timeout to 10.244.253.66:11211\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:34Z\" level=warning msg=\"Could not store 1/dda1e9dc9791c329:15c1b14a9c5:15c1b86e00e:e38be6b1 in chunk cache: read tcp 10.244.204.190:59392->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:34Z\" level=warning msg=\"Could not store 1/d70ee73473eb49a2:15c1b0eb269:15c1b86dc1e:52387d73 in chunk cache: read tcp 10.244.204.190:49656->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:44Z\" level=warning msg=\"Could not store 1/834c6276c28679fe:15c1b0eecfd:15c1b86dc1e:af8607a0 in chunk cache: read tcp 10.244.204.190:41932->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:45Z\" level=warning msg=\"Could not store 2/db0350dbf0affb4d:15c1b1ab7c4:15c1b86c004:26c2c04b in chunk cache: read tcp 10.244.204.190:59072->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:48Z\" level=warning msg=\"Could not store 1/c66bea3bd97dea27:15c1b0f3065:15c1b86e4ee:de7e84a5 in chunk cache: read tcp 10.244.204.190:46750->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:42:57Z\" level=warning msg=\"Could not store 1/fddec50d6ed263b2:15c1b0c7197:15c1b86e53f:d6469b35 in chunk cache: memcache: connect timeout to 10.244.229.10:11211\" source=\"chunk_store.go:147\" \r\n2017/05/18 12:42:58 transport: http2Server.HandleStreams failed to read frame: read tcp 10.244.204.190:9095->10.244.253.117:50182: read: no route to host\r\ntime=\"2017-05-18T12:42:58Z\" level=info msg=\"Changing ingester state from ACTIVE -> LEAVING\" source=\"ingester_lifecycle.go:315\" \r\ntime=\"2017-05-18T12:42:58Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:43:00Z\" level=warning msg=\"Could not store 1/7b5aa135f8af2480:15c1b164c75:15c1b86e895:40e305c in chunk cache: memcache: connect timeout to 10.244.254.185:11211\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:03Z\" level=warning msg=\"Could not store 1/93178261d17df0fd:15c1b2176a5:15c1b86a115:4225aa88 in chunk cache: read tcp 10.244.204.190:44186->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:03Z\" level=warning msg=\"Could not store 1/6253477f47fc9dd1:15c1b096d62:15c1b86dac2:e887b542 in chunk cache: read tcp 10.244.204.190:59288->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:03Z\" level=warning msg=\"Could not store 1/2732d80a7ac4451e:15c1b1f11a3:15c1b86c09b:bab61964 in chunk cache: memcache: connect timeout to 10.244.229.10:11211\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:07Z\" level=warning msg=\"Could not store 1/78fb0410e56416b1:15c1b0f7845:15c1b86b79d:b17fe4c1 in chunk cache: read tcp 10.244.204.190:48618->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:10Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:43:14Z\" level=warning msg=\"Could not store 1/853725eb9c740168:15c1b1e082f:15c1b86a186:1315d846 in chunk cache: read tcp 10.244.204.190:51660->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:22Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:43:24Z\" level=warning msg=\"Could not store 1/93971140aab5b444:15c1b20992e:15c1b86e895:3048c8b6 in chunk cache: read tcp 10.244.204.190:42214->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:24Z\" level=warning msg=\"Could not store 1/f22b11b90ac14219:15c1b129a6d:15c1b86e00e:24820f9e in chunk cache: read tcp 10.244.204.190:41820->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:24Z\" level=warning msg=\"Could not store 1/a7656b510329e470:15c1b1d4523:15c1b86c8d2:814a2f0d in chunk cache: read tcp 10.244.204.190:34014->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:28Z\" level=warning msg=\"Could not store 1/aa11c301941f6107:15c1b1561fe:15c1b86e87e:9c662ac in chunk cache: read tcp 10.244.204.190:39546->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:30Z\" level=warning msg=\"Could not store 2/c1b56c005b5345c2:15c1b197ed9:15c1b86e6ab:6924285 in chunk cache: memcache: connect timeout to 10.244.254.185:11211\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:32Z\" level=warning msg=\"Could not store 1/c3542cb19b068d0b:15c1b154d82:15c1b86d402:e7d814cc in chunk cache: read tcp 10.244.204.190:58664->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:34Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:43:35Z\" level=warning msg=\"Could not store 1/5e1ec92b89e0835f:15c1b20560d:15c1b86e00e:db31c77e in chunk cache: read tcp 10.244.204.190:44158->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:36Z\" level=warning msg=\"Could not store 2/6ba9fb68792d22b0:15c1b2383b6:15c1b86d96b:7c12ddb6 in chunk cache: read tcp 10.244.204.190:33902->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:41Z\" level=warning msg=\"Could not store 2/254b54efd858020a:15c1b25047c:15c1b86c004:280cbe84 in chunk cache: read tcp 10.244.204.190:58096->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:46Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:43:53Z\" level=warning msg=\"Could not store 1/592bc7cd4ac47912:15c1b2aef85:15c1b86b79d:c2abacf1 in chunk cache: read tcp 10.244.204.190:35756->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:55Z\" level=warning msg=\"Could not store 2/ac6ee890996b374b:15c1b1e10f9:15c1b86e4e9:38de6cec in chunk cache: read tcp 10.244.204.190:37622->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:43:58Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:44:03Z\" level=warning msg=\"Could not store 1/81a7824d937244fd:15c1b2c4575:15c1b86e895:8e923d9a in chunk cache: read tcp 10.244.204.190:44416->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:44:03Z\" level=warning msg=\"Could not store 2/d2507de5bbbe9284:15c1b249e19:15c1b86ced1:9cea092 in chunk cache: read tcp 10.244.204.190:44240->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:44:10Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:44:13Z\" level=warning msg=\"Could not store 2/3bc995ed5566e95e:15c1b2b52d4:15c1b86e055:920b4fdb in chunk cache: read tcp 10.244.204.190:37148->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:44:22Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:44:25Z\" level=warning msg=\"Could not store 1/bcf3ba9043963eee:15c1b316959:15c1b86c8d2:9f2ba9c9 in chunk cache: read tcp 10.244.204.190:37328->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:44:34Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:44:41Z\" level=warning msg=\"Could not store 1/8a02ddd81efc4897:15c1b38a16e:15c1b86e87e:586732d8 in chunk cache: read tcp 10.244.204.190:47854->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:44:45Z\" level=warning msg=\"Could not store 2/5c867925235745da:15c1b31409e:15c1b86daaf:b789d63c in chunk cache: read tcp 10.244.204.190:48726->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:44:46Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:44:58Z\" level=error msg=\"Error looking for pending ingester: no pending ingesters\" source=\"ingester_lifecycle.go:413\" \r\ntime=\"2017-05-18T12:44:58Z\" level=error msg=\"Failed to transfer chunks to another ingester: cannot find ingester to transfer chunks to: no pending ingesters\" source=\"ingester_lifecycle.go:324\" \r\ntime=\"2017-05-18T12:44:59Z\" level=info msg=\"Ingester removed from consul\" source=\"ingester_lifecycle.go:216\" \r\ntime=\"2017-05-18T12:44:59Z\" level=info msg=\"Ingester.loop() exited gracefully\" source=\"ingester_lifecycle.go:132\" \r\ntime=\"2017-05-18T12:45:03Z\" level=warning msg=\"Could not store 2/c9e2c56a7c5712a3:15c1b3b133c:15c1b86d5cc:14e29b49 in chunk cache: read tcp 10.244.204.190:44188->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:07Z\" level=warning msg=\"Could not store 1/c9eb4065c3c4d769:15c1b3c6d51:15c1b8695b1:a4f439c3 in chunk cache: read tcp 10.244.204.190:49144->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:11Z\" level=warning msg=\"Could not store 2/f77fead0d2bcbd15:15c1b364866:15c1b86d96b:33b44aaa in chunk cache: read tcp 10.244.204.190:48626->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:11Z\" level=warning msg=\"Could not store 2/afb5c6509908afed:15c1b34aa9c:15c1b86d5cc:189a98af in chunk cache: read tcp 10.244.204.190:49774->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:19Z\" level=warning msg=\"Could not store 2/1e70fa21a2ed675:15c1b3614bd:15c1b86a5bc:d0422c15 in chunk cache: read tcp 10.244.204.190:50338->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:20Z\" level=warning msg=\"Could not store 1/95a5e258f96f93c0:15c1b4002a4:15c1b86bc1c:4b893390 in chunk cache: read tcp 10.244.204.190:50580->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:20Z\" level=warning msg=\"Could not store 1/7f9d8e65a7cb735f:15c1b3daa7e:15c1b86e87e:952d09 in chunk cache: read tcp 10.244.204.190:35732->10.244.254.185:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:39Z\" level=warning msg=\"Could not store 2/5b4d53a65fe17b20:15c1b43c620:15c1b86d618:dbba63e1 in chunk cache: read tcp 10.244.204.190:36578->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:39Z\" level=warning msg=\"Could not store 2/9627a091e07d4c40:15c1b43b016:15c1b86c004:12ba5d71 in chunk cache: read tcp 10.244.204.190:47232->10.244.253.66:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:39Z\" level=warning msg=\"Could not store 2/fdaa8741e20a17a4:15c1b42db6c:15c1b86d5cc:c9904d0b in chunk cache: read tcp 10.244.204.190:39744->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:41Z\" level=warning msg=\"Could not store 2/51638836d7e9fbc0:15c1b437a99:15c1b86c529:b2be59d8 in chunk cache: read tcp 10.244.204.190:45160->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:45:47Z\" level=warning msg=\"Could not store 2/b75a72ee2a46dc02:15c1b4394e6:15c1b86df76:e4fbe3bf in chunk cache: read tcp 10.244.204.190:46500->10.244.229.10:11211: i/o timeout\" source=\"chunk_store.go:147\" \r\ntime=\"2017-05-18T12:46:07Z\" level=error msg=\"Failed to flush user 1: RequestError: send request failed\r\ncaused by: Put https://weaveworks-dev-chunks.s3.amazonaws.com/1/f0df458293ade0e7%3A15c1b42ff34%3A15c1b86847c%3Aa9ed8dcc: EOF\" source=\"ingester_flush.go:113\" \r\npanic: enqueue on closed queue\r\n\r\ngoroutine 99 [running]:\r\ngithub.com/weaveworks/cortex/pkg/util.(*PriorityQueue).Enqueue(0xc4202ad840, 0x13f4800, 0xc42b2ebf80)\r\n\t/go/src/github.com/weaveworks/cortex/pkg/util/priority_queue.go:76 +0x1cc\r\ngithub.com/weaveworks/cortex/pkg/ingester.(*Ingester).flushLoop(0xc4202a2a00, 0x7)\r\n\t/go/src/github.com/weaveworks/cortex/pkg/ingester/ingester_flush.go:120 +0x184\r\ncreated by github.com/weaveworks/cortex/pkg/ingester.New\r\n\t/go/src/github.com/weaveworks/cortex/pkg/ingester/ingester.go:258 +0xc91\r\n```"},{"labels":["bug"],"text":"Querying for `job:scope_request_duration_seconds:99quantile{job=\"scope/collection\"} * 1e3` for dev shows incorrect results in cortex dev, i.e. https://frontend.dev.weave.works/prom/proud-wind-05:\r\n![screenshot from 2017-04-12 02-04-38](https://cloud.githubusercontent.com/assets/109109/24936842/a013e71a-1f24-11e7-8045-501b15189321.png)\r\n\r\nThe correct result is shown when expanding the recording rule\r\n![screenshot from 2017-04-12 02-04-53](https://cloud.githubusercontent.com/assets/109109/24936854/ae984a88-1f24-11e7-8ade-2111f3cd02de.png)\r\nas confirmed by querying the recording rule in the prod cortex looking at dev\r\n![screenshot from 2017-04-12 02-05-18](https://cloud.githubusercontent.com/assets/109109/24936874/cd5e1b6e-1f24-11e7-9883-873d5ddf6a9f.png)\r\nand directly in the dev prometheus\r\n![screenshot from 2017-04-12 02-08-20](https://cloud.githubusercontent.com/assets/109109/24936927/f2711708-1f24-11e7-8846-8ad20548e09b.png)\r\n"},{"labels":["bug"],"text":"Dev cortex, looking at prod:\r\n![screenshot_2017-03-19_08-32-14](https://cloud.githubusercontent.com/assets/109109/24079396/ee660c58-0c7e-11e7-9f40-b4f7b8d5771c.png)\r\nPrometheus in prod:\r\n![screenshot_2017-03-19_08-33-21](https://cloud.githubusercontent.com/assets/109109/24079398/f78d9058-0c7e-11e7-8d2b-c008698f2220.png)\r\nLooks a bit better in prod cortex, but still not right:\r\n![screenshot_2017-03-19_08-37-50](https://cloud.githubusercontent.com/assets/109109/24079418/696482ea-0c7f-11e7-8b67-469a9712eb5f.png)\r\n"},{"labels":["bug"],"text":"For example I have query:\r\n```\r\nquery_range?query=sum(rate(grpc_server_handled_total%7Bjob%3D~%22XXX%22%7D%5B5m%5D))%20by%20(job)&start=1489670714&end=1489681514&step=30\r\n```\r\nSo just query_range for this:\r\n```\r\nsum(rate(grpc_server_handled_total{job=~\"XXX\"}[5m])) by (job)\r\n```\r\nFor time range exactly 3h wide.\r\n\r\nCall takes long time (~40s) and after that time I am getting 502 from nginx with log line:\r\n```\r\n2017/03/16 16:25:53 [error] 10#10: *4173078 upstream prematurely closed connection while reading response header from upstream, client: <IP>, server: , request: \"GET /api/prom/api/v1/query_range?query=sum(rate(grpc_server_handled_total%7Bjob%3D~%22XXX%22%7D%5B5m%5D))%20by%20(job)&start=1489670714&end=1489681514&step=30 HTTP/1.1\", upstream: \"http://<IP>:80/api/prom/api/v1/query_range?query=sum(rate(grpc_server_handled_total%7Bjob%3D~%22XXX%22%7D%5B5m%5D))%20by%20(job)&start=1489670714&end=1489681514&step=30\", host: \"nginx.cortex.svc.eu1-testing.k8s.improbable.io\", referrer: \"http://localhost:3000/dashboard/db/cortex-validation?from=now-3h&to=now&var-job=XXX&editorTab=Metrics&panelId=4&fullscreen&edit\"\r\n\r\n```\r\nThe same result I can find for plain HTTP request. (above log line is a call from Grafana)\r\n\r\nNothing really from querier log.\r\n\r\nIf I just remove the `~` from query - I have **proper** results after `2683ms`"},{"labels":[null,"bug"],"text":"Follow up of: https://github.com/weaveworks/cortex/issues/327\r\n\r\nMy QoD is still valid. I dived into the issue and it seems that the confusion here is because Prometheus wrongly parses my mistaken `start=1489025420000` timestamp. From query perspective it is not really start > time problem, but actually underlying int64 overflow I fixed here:  https://github.com/prometheus/prometheus/pull/2501 caused start to be huge negative value.\r\n\r\nThis means that QoD is still possible and is caused not by `start > end`, but actually huge range (e.g `start=MinInt64&end=MaxInt64`)\r\n\r\n@tomwilkie do you have any idea how to handle this properly? IMO Some limiter or further validation is needed. "},{"labels":["bug"],"text":"Causes query service to DoS dynamodb"},{"labels":["bug"],"text":"Between Monday and Tuesday I accident revoked access to S3, so Cortex on my cluster could not make any request to S3. (Dynamo was fine)\r\n\r\nApart of the facts that:\r\n* there was not any failure metric for s3 request (will do that in next PR)\r\n* actual metrics for `chunk age` , `chunkLength` and `chunkUtilization` were reported before actual request, so the reported data was not true:\r\n```\r\nfunc (i *Ingester) flushChunks(ctx context.Context, fp model.Fingerprint, metric model.Metric, chunkDescs []*desc) error {\r\n\twireChunks := make([]cortex_chunk.Chunk, 0, len(chunkDescs))\r\n\tfor _, chunkDesc := range chunkDescs {\r\n\t\ti.chunkUtilization.Observe(chunkDesc.C.Utilization())\r\n\t\ti.chunkLength.Observe(float64(chunkDesc.C.Len()))\r\n\t\ti.chunkAge.Observe(model.Now().Sub(chunkDesc.FirstTime).Seconds())\r\n\t\twireChunks = append(wireChunks, cortex_chunk.NewChunk(fp, metric, chunkDesc.C, chunkDesc.FirstTime, chunkDesc.LastTime))\r\n\t}\r\n\treturn i.chunkStore.Put(ctx, wireChunks)\r\n}\r\n``` \r\n(Will fill separate issue for that)\r\n\r\nI found that there might be some open file descriptors leak (probable because of not closed connections) because of that:\r\n![image](https://cloud.githubusercontent.com/assets/6950331/23714769/53d508ec-0422-11e7-8985-2c2083db91dd.png)\r\n\r\nWill investigate that more soon, but if someone has free cycles.. feel free to fix that. (:"},{"labels":["bug"],"text":"When shutting down an ingester for routine maintenance, I noticed an increase in chunk store flush failures due to \"ValidationException\":\r\n\r\n<img width=\"1039\" alt=\"screen shot 2017-03-07 at 12 10 55\" src=\"https://cloud.githubusercontent.com/assets/125674/23655900/2beb0186-032f-11e7-85ce-53a6158992ea.png\">\r\n\r\nAlso, the series queue flush length stalled, rather than steadily decreased\r\n\r\n<img width=\"1044\" alt=\"screen shot 2017-03-07 at 12 11 36\" src=\"https://cloud.githubusercontent.com/assets/125674/23655920/49952f90-032f-11e7-8821-a5fd51b0f962.png\">\r\n\r\nChecking the logs, I saw lots of errors like these:\r\n\r\n```\r\nrequest id: DSCAO85A35JL2HLMUCPRPOMTSJVV4KQNSO5AEMVJF66Q9ASUAAJG\" source=\"ingester.go:516\"\r\ntime=\"2017-03-07T12:06:28Z\" level=error msg=\"Failed to flush user: ValidationException: One or more parameter values were invalid: Aggregated size of all range keys has exceeded the size limit of 1024 bytes\\n\\tstatus code: 400, request id: O3EBMNFFUCSU252KDI1TKHTE9NVV4KQNSO5AEMVJF66Q9ASUAAJG\"\r\n```\r\n\r\nThis represents a serious problem, as we cannot shut down these ingesters cleanly."},{"labels":["bug"],"text":"<img width=\"1266\" alt=\"screen shot 2017-03-06 at 11 19 39\" src=\"https://cloud.githubusercontent.com/assets/444037/23608307/af60c2be-0260-11e7-8e60-50f6131c18c1.png\">\r\n"},{"labels":["bug"],"text":"The following graphs are all for the query\r\n```\r\nsum(rate(container_cpu_usage_seconds_total{image!=\"\"}[5m])) by (pod_name, namespace)\r\n```\r\nand were executed at the same time on cortex-dev, prom-dev, cortex-prod, and prom-prod:\r\n![screenshot_2017-02-27_18-45-46](https://cloud.githubusercontent.com/assets/109109/23374634/fc333e1c-fd1c-11e6-93d2-7861c2a5267c.png)\r\n![screenshot_2017-02-27_18-38-30](https://cloud.githubusercontent.com/assets/109109/23374433/30d2ca26-fd1c-11e6-9daf-7aaa7b3c6d16.png)\r\n![screenshot_2017-02-27_18-38-57](https://cloud.githubusercontent.com/assets/109109/23374435/32cfd558-fd1c-11e6-8173-8cefc19e98c0.png)\r\n![screenshot_2017-02-27_18-39-53](https://cloud.githubusercontent.com/assets/109109/23374436/34a31cb4-fd1c-11e6-8bc5-c3dd2ff436e3.png)\r\n\r\nThe cortex graphs have large troughs of 0-value data, which are not present in the prom graphs.\r\n\r\nMissing data? query timeouts? bug in query engine?"},{"labels":["bug"],"text":"In https://frontend.dev.weave.works/prom/proud-wind-05/wrapper I entered\r\n```\r\nirate(node_cpu{job=\"monitoring/prom-node-exporter\",mode=\"idle\"}[1m])\r\n```\r\nin our expression browser, and some of the returned data is for modes other than \"idle\", e.g. \"user\" and \"system\".\r\n![screenshot_2017-02-26_14-29-16](https://cloud.githubusercontent.com/assets/109109/23340566/708bcf9c-fc30-11e6-883f-64dcfcd08167.png)\r\n\r\nEntering the same query into the dev prometheus, only produces data for \"idle\", as expected."},{"labels":["bug"],"text":"I'm seeing multiple data races in the ruler:\r\n\r\n```\r\n==================\r\nWARNING: DATA RACE\r\nRead at 0x0000038b4880 by goroutine 85:\r\n  github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go.StartSpanFromContext()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go/gocontext.go:44 +0x4f\r\n  github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.TimeRequestHistogramStatus()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:41 +0xab\r\n  github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.TimeRequestHistogram()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:27 +0x83\r\n  github.com/weaveworks/cortex/ruler.(*scheduler).poll()\r\n      /go/src/github.com/weaveworks/cortex/ruler/scheduler.go:157 +0x1c8\r\n  github.com/weaveworks/cortex/ruler.(*scheduler).updateConfigs()\r\n      /go/src/github.com/weaveworks/cortex/ruler/scheduler.go:141 +0x3c\r\n  github.com/weaveworks/cortex/ruler.(*scheduler).Run()\r\n      /go/src/github.com/weaveworks/cortex/ruler/scheduler.go:104 +0x26d\r\n\r\nPrevious write at 0x0000038b4880 by main goroutine:\r\n  github.com/weaveworks/cortex/server.New()\r\n      /go/src/github.com/weaveworks/cortex/server/server.go:75 +0x655\r\n  main.main()\r\n      /go/src/github.com/weaveworks/cortex/cmd/ruler/main.go:52 +0x73a\r\n\r\nGoroutine 85 (running) created at:\r\n  github.com/weaveworks/cortex/ruler.(*Server).run()\r\n      /go/src/github.com/weaveworks/cortex/ruler/ruler.go:141 +0x7b\r\n==================\r\n==================\r\nWARNING: DATA RACE\r\nRead at 0x00c4203fe7aa by goroutine 85:\r\n  github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing.(*tracerImpl).getSpan()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing/tracer.go:265 +0x40\r\n  github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing.(*tracerImpl).startSpanWithOptions()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing/tracer.go:288 +0xa1\r\n  github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing.(*tracerImpl).StartSpan()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing/tracer.go:261 +0x185\r\n  github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go.startSpanFromContextWithTracer()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go/gocontext.go:54 +0x31a\r\n  github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go.StartSpanFromContext()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go/gocontext.go:44 +0xc6\r\n  github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.TimeRequestHistogramStatus()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:41 +0xab\r\n  github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.TimeRequestHistogram()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:27 +0x83\r\n  github.com/weaveworks/cortex/ruler.(*scheduler).poll()\r\n      /go/src/github.com/weaveworks/cortex/ruler/scheduler.go:157 +0x1c8\r\n  github.com/weaveworks/cortex/ruler.(*scheduler).updateConfigs()\r\n      /go/src/github.com/weaveworks/cortex/ruler/scheduler.go:141 +0x3c\r\n  github.com/weaveworks/cortex/ruler.(*scheduler).Run()\r\n      /go/src/github.com/weaveworks/cortex/ruler/scheduler.go:104 +0x26d\r\n\r\nPrevious write at 0x00c4203fe7a8 by main goroutine:\r\n  github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing.NewTracer()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing/tracer.go:238 +0x2e3\r\n  github.com/weaveworks/cortex/vendor/github.com/weaveworks-experiments/loki/pkg/client.NewTracer()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks-experiments/loki/pkg/client/tracer.go:20 +0x173\r\n  github.com/weaveworks/cortex/server.New()\r\n      /go/src/github.com/weaveworks/cortex/server/server.go:71 +0x1e5\r\n  main.main()\r\n      /go/src/github.com/weaveworks/cortex/cmd/ruler/main.go:52 +0x73a\r\n\r\nGoroutine 85 (running) created at:\r\n  github.com/weaveworks/cortex/ruler.(*Server).run()\r\n      /go/src/github.com/weaveworks/cortex/ruler/ruler.go:141 +0x7b\r\n==================\r\n==================\r\nWARNING: DATA RACE\r\nRead at 0x00c4203fe7c2 by goroutine 85:\r\n  github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing.(*tracerImpl).startSpanWithOptions()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing/tracer.go:346 +0x305\r\n  github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing.(*tracerImpl).StartSpan()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing/tracer.go:261 +0x185\r\n  github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go.startSpanFromContextWithTracer()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go/gocontext.go:54 +0x31a\r\n  github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go.StartSpanFromContext()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/opentracing/opentracing-go/gocontext.go:44 +0xc6\r\n  github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.TimeRequestHistogramStatus()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:41 +0xab\r\n  github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.TimeRequestHistogram()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:27 +0x83\r\n  github.com/weaveworks/cortex/ruler.(*scheduler).poll()\r\n      /go/src/github.com/weaveworks/cortex/ruler/scheduler.go:157 +0x1c8\r\n  github.com/weaveworks/cortex/ruler.(*scheduler).updateConfigs()\r\n      /go/src/github.com/weaveworks/cortex/ruler/scheduler.go:141 +0x3c\r\n  github.com/weaveworks/cortex/ruler.(*scheduler).Run()\r\n      /go/src/github.com/weaveworks/cortex/ruler/scheduler.go:104 +0x26d\r\n\r\nPrevious write at 0x00c4203fe7c0 by main goroutine:\r\n  github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing.NewTracer()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/openzipkin/zipkin-go-opentracing/tracer.go:238 +0x2e3\r\n  github.com/weaveworks/cortex/vendor/github.com/weaveworks-experiments/loki/pkg/client.NewTracer()\r\n      /go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks-experiments/loki/pkg/client/tracer.go:20 +0x173\r\n  github.com/weaveworks/cortex/server.New()\r\n      /go/src/github.com/weaveworks/cortex/server/server.go:71 +0x1e5\r\n  main.main()\r\n      /go/src/github.com/weaveworks/cortex/cmd/ruler/main.go:52 +0x73a\r\n\r\nGoroutine 85 (running) created at:\r\n  github.com/weaveworks/cortex/ruler.(*Server).run()\r\n      /go/src/github.com/weaveworks/cortex/ruler/ruler.go:141 +0x7b\r\n==================\r\n```"},{"labels":["bug"],"text":"See on dev, ingesters not flushing."},{"labels":["bug"],"text":"Noticed they were restarting more than expected:\r\n\r\n```\r\n$ kubectl get pod --namespace=cortex \r\nNAME                            READY     STATUS    RESTARTS   AGE\r\n...\r\nruler-3796937216-etvy1          1/1       Running   47         4d\r\n...\r\n```\r\n\r\n```\r\n$ kubectl logs --namespace=cortex ruler-3796937216-etvy1 -p\r\ntime=\"2017-01-31T11:00:54Z\" level=info msg=\"Ruler up and running\" source=\"ruler.go:141\" \r\ntime=\"2017-01-31T11:01:55Z\" level=warning msg=\"Error fetching from cache: read tcp 10.244.203.20:42064->10.244.204.99:11211: i/o timeout\" source=\"chunk_store.go:455\" \r\ntime=\"2017-01-31T11:04:40Z\" level=warning msg=\"Error fetching from cache: read tcp 10.244.203.20:33288->10.244.254.87:11211: i/o timeout\" source=\"chunk_store.go:455\" \r\ntime=\"2017-01-31T11:05:11Z\" level=warning msg=\"Error fetching from cache: read tcp 10.244.203.20:37170->10.244.253.92:11211: i/o timeout\" source=\"chunk_store.go:455\" \r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x89870d]\r\n\r\ngoroutine 36436 [running]:\r\npanic(0xab2fe0, 0xc420016050)\r\n    /usr/local/go/src/runtime/panic.go:500 +0x1a1\r\ngithub.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/notifier.(*Notifier).Send(0x0, 0xc422332d50, 0x1, 0x1)\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/notifier/notifier.go:211 +0x4d\r\ngithub.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules.(*Group).sendAlerts(0xc429cfe550, 0xc420432af0, 0x159f4340f7d, 0x159f4340f7d, 0xc42012f380)\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules/manager.go:330 +0x5ae\r\ngithub.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules.(*Group).Eval.func1(0xc4384e8740, 0xb79a11, 0x8, 0xc429cfe550, 0x159f4340f7d, 0x10f4e80, 0xc420432af0)\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules/manager.go:275 +0xb52\r\ncreated by github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules.(*Group).Eval\r\n    /go/src/github.com/weaveworks/cortex/vendor/github.com/prometheus/prometheus/rules/manager.go:301 +0x147\r\n```"},{"labels":["bug"],"text":"See https://gist.githubusercontent.com/juliusv/4566ce03ad1af97b7ab42ae971267bf4/raw/11cf46ca555d39c3fe1254c5030066d8c6c0173d/distributor_crash.txt\r\n\r\nThe crash happens in this line: https://github.com/weaveworks/cortex/blob/master/vendor/github.com/grpc-ecosystem/grpc-opentracing/go/otgrpc/shared.go#L29"},{"labels":["bug"],"text":"<img width=\"912\" alt=\"screen shot 2017-01-26 at 21 32 07\" src=\"https://cloud.githubusercontent.com/assets/444037/22365965/3015de38-e472-11e6-8c5d-1ae712be6996.png\">\r\n"},{"labels":["bug"],"text":"Getting this from the query service whilst shutting down an ingester.  I suspect #237."},{"labels":["bug"],"text":"Not many details right now; rolled e515598 to dev, and everything locked up.  Rolled back distributor and querier, still locked up.  Rolled back ingester, and eventually everything worked.\r\n\r\nThis is what was rolled out:\r\n```\r\n7846dd7 HTTP over gRPC: encode HTTP requests as gRPC requests (#235)\r\nf01a7a5 Merge pull request #239 from weaveworks/238-ruler-goroutines\r\nd965706 Fix empty-label query semantics (#228)\r\n9fb3d9d Remove UI code. (#230)\r\n365faad Try and flush failed chunks repeatedly when we're shutting down. (#234)\r\nc47661d Merge pull request #237 from weaveworks/grpc-cleanup\r\n9764257 Merge pull request #236 from weaveworks/fix-log\r\n```"},{"labels":["bug"],"text":"```\r\nfatal error: concurrent map writes\r\n\r\ngoroutine 585 [running]:\r\nruntime.throw(0x9c6e03, 0x15)\r\n\t/usr/local/go/src/runtime/panic.go:566 +0x95 fp=0xc4200d6648 sp=0xc4200d6628\r\nruntime.mapassign1(0x94b9e0, 0xc4216f9da0, 0xc4200d67e8, 0xc4200d67f8)\r\n\t/usr/local/go/src/runtime/hashmap.go:458 +0x8ef fp=0xc4200d6730 sp=0xc4200d6648\r\ngithub.com/weaveworks/cortex/vendor/github.com/weaveworks/common/middleware.ClientUserHeaderInterceptor(0x7f8798387350, 0xc4212b10b0, 0x9c6ad0, 0x15, 0x9573c0, 0xc4216be520, 0x948a60, 0xe89ec8, 0xc420092000, 0xa01608, ...)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/middleware/grpc_auth.go:24 +0x15a fp=0xc4200d6820 sp=0xc4200d6730\r\ngithub.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware.ChainUnaryClient.func1.1.1(0x7f8798387350, 0xc4212b10b0, 0x9c6ad0, 0x15, 0x9573c0, 0xc4216be520, 0x948a60, 0xe89ec8, 0xc420092000, 0x0, ...)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware/chain.go:58 +0xd4 fp=0xc4200d68a8 sp=0xc4200d6820\r\ngithub.com/weaveworks/cortex/vendor/github.com/grpc-ecosystem/grpc-opentracing/go/otgrpc.OpenTracingClientInterceptor.func1(0x7f8798387350, 0xc4212b10b0, 0x9c6ad0, 0x15, 0x9573c0, 0xc4216be520, 0x948a60, 0xe89ec8, 0xc420092000, 0xc421401e20, ...)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/grpc-ecosystem/grpc-opentracing/go/otgrpc/client.go:67 +0x45b fp=0xc4200d69f0 sp=0xc4200d68a8\r\ngithub.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware.ChainUnaryClient.func1.1.1(0x7f8798387350, 0xc42171e180, 0x9c6ad0, 0x15, 0x9573c0, 0xc4216be520, 0x948a60, 0xe89ec8, 0xc420092000, 0x0, ...)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware/chain.go:58 +0xd4 fp=0xc4200d6a78 sp=0xc4200d69f0\r\ngithub.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware.ChainUnaryClient.func1(0x7f8798387350, 0xc42171e180, 0x9c6ad0, 0x15, 0x9573c0, 0xc4216be520, 0x948a60, 0xe89ec8, 0xc420092000, 0xa01608, ...)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/mwitkow/go-grpc-middleware/chain.go:65 +0x141 fp=0xc4200d6b08 sp=0xc4200d6a78\r\ngithub.com/weaveworks/cortex/vendor/google.golang.org/grpc.Invoke(0x7f8798387350, 0xc42171e180, 0x9c6ad0, 0x15, 0x9573c0, 0xc4216be520, 0x948a60, 0xe89ec8, 0xc420092000, 0x0, ...)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/call.go:141 +0xdd fp=0xc4200d6b90 sp=0xc4200d6b08\r\ngithub.com/weaveworks/cortex.(*ingesterClient).Push(0xc42013e190, 0x7f8798387350, 0xc42171e180, 0xc4216be520, 0x0, 0x0, 0x0, 0x8010102, 0x1, 0xffffffffffffffff)\r\n\t/go/src/github.com/weaveworks/cortex/cortex.pb.go:351 +0xd2 fp=0xc4200d6c18 sp=0xc4200d6b90\r\ngithub.com/weaveworks/cortex/distributor.(*ingesterClient).Push(0xc4216be4e0, 0x7f8798387350, 0xc42171e180, 0xc4216be520, 0x0, 0x0, 0x0, 0x64276b1, 0xc4200d6cd8, 0x537092)\r\n\t<autogenerated>:8 +0x92 fp=0xc4200d6c78 sp=0xc4200d6c18\r\ngithub.com/weaveworks/cortex/distributor.(*Distributor).sendSamples.func1(0x7f8798387350, 0xc42171e180, 0xe665e0, 0xc4200170b0)\r\n\t/go/src/github.com/weaveworks/cortex/distributor/distributor.go:330 +0xa5 fp=0xc4200d6ce8 sp=0xc4200d6c78\r\ngithub.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.TimeRequestHistogramStatus(0x7f8798387350, 0xc420e96db0, 0x9c7d34, 0x17, 0xc42016a150, 0xa013d8, 0xc4200d6ec8, 0xc42013e190, 0xc420092000)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:45 +0x14d fp=0xc4200d6dc8 sp=0xc4200d6ce8\r\ngithub.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument.TimeRequestHistogram(0x7f8798387350, 0xc420e96db0, 0x9c7d34, 0x17, 0xc42016a150, 0xc4200d6ec8, 0x5, 0xc420daa6b8)\r\n\t/go/src/github.com/weaveworks/cortex/vendor/github.com/weaveworks/common/instrument/instrument.go:27 +0x70 fp=0xc4200d6e20 sp=0xc4200d6dc8\r\ngithub.com/weaveworks/cortex/distributor.(*Distributor).sendSamples(0xc4201f4420, 0x7f8798387350, 0xc420e96db0, 0xc421363420, 0xc421085c00, 0x64, 0x80, 0x57c1e4, 0xc4213e94f0)\r\n\t/go/src/github.com/weaveworks/cortex/distributor/distributor.go:332 +0x1d5 fp=0xc4200d6f08 sp=0xc4200d6e20\r\ngithub.com/weaveworks/cortex/distributor.(*Distributor).Push.func1(0xc420b946c0, 0xc4201f4420, 0x7f8798387350, 0xc420e96db0, 0xc421363420, 0xc421085c00, 0x64, 0x80)\r\n\t/go/src/github.com/weaveworks/cortex/distributor/distributor.go:301 +0x7a fp=0xc4200d6f70 sp=0xc4200d6f08\r\nruntime.goexit()\r\n\t/usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc4200d6f78 sp=0xc4200d6f70\r\ncreated by github.com/weaveworks/cortex/distributor.(*Distributor).Push\r\n\t/go/src/github.com/weaveworks/cortex/distributor/distributor.go:302 +0x7e7\r\n```"},{"labels":["bug"],"text":"In Prometheus, a non-existent label `foo` is equivalent to an empty label `foo=\"\"` (in fact, labels with empty values are removed before a series is stored). That is in Prometheus, `{foo=\"\"}` returns all series with no foo label, as does `{foo=~\"\"}`. Cortex currently returns no series for these, because it actually looks for a `foo` label to be present.\r\n\r\nI stumbled over this when looking at https://github.com/weaveworks/cortex/pull/191."},{"labels":["bug"],"text":"When we rollout new ingesters, the distributor never stop the clients for the old ones, spamming the logs with:\r\n\r\n```\r\n2017/01/18 15:22:21 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 10.244.254.81:9095: getsockopt: no route to host\"; Reconnecting to {10.244.254.81:9095 <nil>}\r\n2017/01/18 15:22:43 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 10.244.253.93:9095: getsockopt: no route to host\"; Reconnecting to {10.244.253.93:9095 <nil>}\r\n2017/01/18 15:23:37 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 10.244.204.160:9095: getsockopt: no route to host\"; Reconnecting to {10.244.204.160:9095 <nil>}\r\n```"},{"labels":["bug"],"text":"They are a valid utf-8 character.\r\n\r\nWe should validate data based on: https://prometheus.io/docs/concepts/data_model/"},{"labels":["bug"],"text":"See https://github.com/weaveworks/cortex/pull/182#discussion_r94783241"},{"labels":["bug"],"text":"They should have 128.\r\n\r\n![screen shot 2016-11-26 at 20 29 20](https://cloud.githubusercontent.com/assets/444037/20643090/13096160-b417-11e6-9a20-34c702c339b7.png)\r\n"},{"labels":["bug"],"text":"The ring has got too big.  "},{"labels":["bug"],"text":"Currently, when identifying ingesters to write data to, we include those that are in the process of shutting down. These ingesters (correctly) refuse to accept new writes, as they are busy writing everything they have to dynamo. This causes writes to fail, which causes a loss of user data. \r\n\r\nInstead, we should:\r\n- mark \"leaving\" ingesters as such\r\n- skip over them for writing, not counting them to the replica total\r\n- include them querying, counting them to the replica total\r\n\r\n"},{"labels":["bug"],"text":"When we attempt to write to dynamo, we will sometimes get errors. Currently, we abort on error.\r\n\r\nThis is almost never the right behaviour, because failing to write to dynamo (esp. during shut down) means loss of user data. Instead, we should retry.\r\n\r\nNote that one major source of errors is being throttled by Dynamo. In this case, we should also retry, but we should be especially careful that we back off, lest we cause a cascading failure. "},{"labels":["bug"],"text":"As part of understanding & fixing #61 and making sure it doesn't happen again, we want to gather data on:\r\n\r\n- [ ] how big the backlog of flushes for chunks\r\n- [x] (maybe) how many chunks there are in the system\r\n- [ ] how many chunks we're dropping\r\n\r\n"},{"labels":["bug"],"text":"```\ntime=\"2016-10-10T13:17:29Z\" level=error msg=\"Failed to flush chunks for series: RequestError: send request failed\\ncaused by: Put https://weaveworks-prod-chunks.s3.amazonaws.com/2/17142991289174007630%3A1476104540889%3A1476105230889: dial tcp: lookup weaveworks-prod-chunks.s3.amazonaws.com on 10.0.0.10:53: dial udp 10.0.0.10:53: connect: network is unreachable\" source=\"cortex.go:440\"\n```\n\nThis might be a cause of data loss.\n"},{"labels":["bug"],"text":"```\ntime=\"2016-09-22T09:41:54Z\" level=error msg=\"Error CASing collectors/ring: Unexpected response code: 413 (Value exceeds 524288 byte limit)\" source=\"consul_client.go:146\"\ntime=\"2016-09-22T09:41:54Z\" level=fatal msg=\"Failed to pick tokens in consul: failed to CAS collectors/ring\" source=\"ingester_lifecycle.go:113\"\n```\n"}]