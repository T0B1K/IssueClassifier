[{"labels":[null,"enhancement"],"text":"Currently, mypy fails if the environment like optuna/optuna:py3.7-dev has some integration dependencies installed as follows (though the command itself was ran the other day):\r\n\r\nIf this problem is solved, we can easily run mypy checks locally.\r\n\r\n```shelll\r\n~/.ghq/github.com/optuna/optuna yet-another-dumb-branch*\r\n‚ùØ docker run -it -v $(pwd):/workspaces optuna/optuna:py3.7-dev /bin/bash -c \"black --check .; flake8 .; mypy .\"\r\n\r\n...\r\n\r\noptuna/dashboard.py:235: error: Argument 1 to \"coroutine\" has incompatible type \"Callable[[_DashboardApp], None]\"; expected \"Callable[..., Generator[Any, Any, <nothing>]]\"\r\noptuna/integration/chainer.py:13: error: Cannot assign to a type\r\noptuna/integration/chainer.py:13: error: Incompatible types in assignment (expression has type \"Type[object]\", variable has type \"Type[Extension]\")\r\noptuna/integration/chainer.py:74: error: Incompatible types in assignment (expression has type \"Optional[Any]\", variable has type \"Union[float, Variable]\")\r\noptuna/integration/chainer.py:77: error: Argument 1 to \"float\" has incompatible type \"Union[float, Variable]\"; expected \"Union[SupportsFloat, _SupportsIndex, str, bytes, bytearray]\"\r\ntests/test_logging.py:82: error: \"CaptureFixture\" has no attribute \"at_level\"\r\ntests/test_logging.py:84: error: \"CaptureFixture\" has no attribute \"text\"\r\ntests/test_logging.py:88: error: \"CaptureFixture\" has no attribute \"at_level\"\r\ntests/test_logging.py:90: error: \"CaptureFixture\" has no attribute \"text\"\r\ntests/test_logging.py:94: error: \"CaptureFixture\" has no attribute \"at_level\"\r\ntests/test_logging.py:96: error: \"CaptureFixture\" has no attribute \"text\"\r\noptuna/structs.py:78: error: Function is missing a type annotation for one or more arguments\r\ntests/integration_tests/test_fastai.py:82: error: Cannot assign to a method\r\ntests/integration_tests/test_chainer.py:47: error: Argument 3 to \"ChainerPruningExtension\" has incompatible type \"TimeTrigger\"; expected \"Union[Tuple[int, str], IntervalTrigger, ManualScheduleTrigger]\"\r\ntests/integration_tests/test_chainer.py:89: error: Argument 1 to \"__call__\" of \"ChainerPruningExtension\" has incompatible type \"_MockTrainer@83\"; expected \"Trainer\"\r\ntests/integration_tests/test_chainer.py:103: error: Argument 1 to \"_observation_exists\" of \"ChainerPruningExtension\" has incompatible type \"_MockTrainer@97\"; expected \"Trainer\"\r\ntests/integration_tests/test_chainer.py:105: error: Argument 1 to \"_observation_exists\" of \"ChainerPruningExtension\" has incompatible type \"_MockTrainer@97\"; expected \"Trainer\"\r\ntests/integration_tests/test_chainer.py:111: error: Argument 1 to \"_observation_exists\" of \"ChainerPruningExtension\" has incompatible type \"_MockTrainer@97\"; expected \"Trainer\"\r\ntests/integration_tests/test_chainer.py:113: error: Argument 1 to \"_observation_exists\" of \"ChainerPruningExtension\" has incompatible type \"_MockTrainer@97\"; expected \"Trainer\"\r\ntests/integration_tests/test_chainer.py:123: error: Argument 1 to \"_get_float_value\" of \"ChainerPruningExtension\" has incompatible type \"List[<nothing>]\"; expected \"Union[float, Variable]\"\r\nFound 20 errors in 6 files (checked 190 source files)\r\n```"},{"labels":[null,"enhancement",null,null],"text":"We've supported fastai with `FastAIPruningCallback` though it has targeted fastai v1.\r\nRecently, FastAI V2 is released; thus we'd love to support it.\r\nBeing that said, early stopping callback of v2 which will be the base class of `FastAIPruningCallback` differs from that of v1 in e.g., the `__init__`'s arguments.\r\n\r\n- v2: https://github.com/fastai/fastai/blob/72590db2e66af6dd0eaa8c8874a80f11a4b8cbc2/fastai/callback/tracker.py#L47-L62\r\n- v1: https://github.com/fastai/fastai1/blob/ff9d8edaca74114ce3045ea00183fa5cfa897904/fastai/callbacks/tracker.py#L53-L75"},{"labels":[null,"enhancement",null,null],"text":"Currently, if CmaEsSampler is used with categorical variables, it throws this error\r\n```\r\nThe parameter 'x' in trial#1 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\r\n```\r\nAs somebody who isn't familiar with the details of CmaEsSampler, I was not aware that it didn't work with categorical variables, and thought it was an issue with how I was setting the parameter ranges\r\n\r\nIf the CmaEsSampler is called with categorical variables, it should probably throw an error/warning upfront explaining the issue."},{"labels":[null,"enhancement",null,null],"text":"## Motivation\r\n\r\nDifferent concrete storages should behave similar. There is currently a logging behavior inconsistency when a study is being created and it should be fixed. More concretely [`RDBStorage` logs the event](https://github.com/optuna/optuna/blob/master/optuna/storages/_rdb/storage.py#L168), while `InMemoryStorage` and `RedisStorage` don't.\r\n\r\nA PR author might also want to have a look at how the study creation method of the storage is called from `create_study` [here](https://github.com/optuna/optuna/blob/master/optuna/study.py#L787).\r\n\r\n## Description\r\n\r\nSee above."},{"labels":["enhancement",null],"text":"## Motivation\r\n\r\nOptuna uses several modules to check library versions such as `packaging ` and `distutils`.  We'd like to rewrite them using `packaging` as discussed in #1560.\r\n\r\n## Description\r\n\r\nPlease update the version checking in the following files:\r\n\r\nThe following files use `distutils`:\r\n- [x] optuna/dashboard.py\r\n- [x] optuna/visualization/_plotly_imports.py\r\n\r\nThe following files use `pkg_resources`:\r\n- [x] examples/allennlp/allennlp_jsonnet.py\r\n- [x] examples/allennlp/allennlp_simple.py\r\n- [x] examples/chainer_simple.py\r\n- [x] examples/pruning/chainer_integration.py\r\n- [x] examples/pytorch_lightning_simple.py\r\n- [x] examples/tensorflow_eager_simple.py\r\n- [x] tests/integration_tests/test_tfkeras.py\r\n\r\nYou can submit a PR for each of the files."},{"labels":[null,"enhancement",null,null],"text":"## TL;DR\r\nThis PR requests to deprecate the return value of [`BaseStorage.set_trial_intermediate_value`](https://github.com/optuna/optuna/blob/24300dc57661bff7961499a2d52534fcd41ad258/optuna/storages/base.py#L144) method.\r\n\r\n## General background\r\n[`BaseStorage.set_trial_intermediate_value`](https://github.com/optuna/optuna/blob/24300dc57661bff7961499a2d52534fcd41ad258/optuna/storages/base.py#L144) method returns False when the specified step already has a value. Its motivation is identical with #1308. The method is solely used in [`trial.report`](https://github.com/optuna/optuna/blob/v1.4.0/optuna/trial.py#L807), but the return value is ignored.\r\n\r\n## Proposal\r\nRemove the return value and change the method's behavior to \"always overwrite existing values\". See #1308 for similar discussion.\r\n\r\n## Note\r\nThis is out of the scope of this issue, but #852 is a related issue and we should add appropriate handling in [`trial.report`](https://github.com/optuna/optuna/blob/v1.4.0/optuna/trial.py#L807)."},{"labels":[null,"enhancement",null,null],"text":"## TL;DR\r\nThis issue requests to deprecate the return value of `BaseStorage.set_trial_param` and change the behavior to \"always overwrite existing values\".\r\n\r\n## General background\r\n[`BaseStorage.set_trial_param`](https://github.com/optuna/optuna/blob/24300dc57661bff7961499a2d52534fcd41ad258/optuna/storages/base.py#L120) method returns True and set a new parameter when the specified parameter-name does not already have a value. Otherwise, it returns False and does not overwrite the parameter with the new value.\r\nThis behavior was necessary to prevent race-conditioning. However, the current optuna doesn't allow users to directly manipulate the storage and optuna internals do not modify parameters from multiple workers/threads, and thus, the functionality is no more required.\r\n\r\n## Proposal and motivation\r\nThis issue requests to deprecate the return value of `BaseStorage.set_trial_param` and change its behavior to \"always overwrite existing values\".\r\nInstead of using the return value, we'd better check whether the parameter already exists first by ``param_name in storage.get_trial_params(trial_id)`` and then perform sampling operation in [this method in trial](https://github.com/optuna/optuna/blob/24300dc57661bff7961499a2d52534fcd41ad258/optuna/trial.py#L981)\r\nThis change has two benefits.\r\n1. We can remove unnecessary computation for re-sampling when the parameter value already exists.\r\n2. The storage method becomes more consistent with other methods such as `set_trial_user_attrs`, which always overwrite existing values. It also simplifies some storage implemetations."},{"labels":["enhancement",null,null,null],"text":"## Description\r\n`Study.optimize` calls `gc.collect()` after every objective  evaluation. This change was added in #377. In #377 we added the gc invocation because ChainerMN example loads the full MNIST dataset in every objective call and it wasted too much memory. I understand that the gc invocation is useful for some users. However, the gc call can be a bottleneck in the whole `optuna` optimization loop.\r\nI'd like to request for comments about changing the default behavior of `Study.optimize` to *don't* invoke gc.\r\n### Pros\r\n- We need the gc in only some special environments such as CircleCI (otherwise, python automatically invoke gc when necessary).\r\n- Removing the gc invocation improves optimization speed.\r\n### Cons\r\n- Some user programs might suffer from OOM in certain environments.\r\n- The current optuna optimization loop is much faster compared to `v1.3` and further speed up might be beneficial for only a small fraction of users (Please refer to the benchmarks in the next section). It might be sufficient to add a performance tips section to the official doc.\r\n\r\n## Microbenchmark\r\nThe followings are some microbenchmark results. (I used a similar setup with #1135.)\r\n### Without GC (master)\r\n```\r\noptimization with 2000 trials / 30 params: 259.5s\r\noptimization with 2000 trials /  2 params: 16.2s\r\noptimization with 1000 trials / 30 params: 72.6s\r\noptimization with 1000 trials /  2 params: 4.9s\r\n```\r\n(Remark: Performance depends on which samplers to use.)\r\n### With GC (master)\r\n```\r\noptimization with 2000 trials / 30 params: 363.7s\r\noptimization with 2000 trials /  2 params: 105.6s\r\noptimization with 1000 trials / 30 params: 122.8s\r\noptimization with 1000 trials /  2 params: 48.5s\r\n```\r\n(Remark: GC performance also depends on the user program.)\r\n### Without GC (v1.3)\r\nAs for reference, I add a benchmark result on `v1.3`.\r\n```\r\noptimization with 1000 trials / 30 params: 325.7s\r\noptimization with 1000 trials /  2 params: 83.6s\r\n```\r\n(Remark: Compared to `v1.3`, the current master has additional functionality in `InMemoryStorage` (#1228)  and performs appropriate CoW handling (#1139), which incur overheads to master.)\r\n### With GC (v1.3)\r\n```\r\noptimization with 1000 trials /  2 params: 235.6s\r\n```"},{"labels":[null,"enhancement",null,null],"text":"## Motivation\r\nThe current optuna supports multi-thread execution. However, it does not result in any speed gain in general use cases because of python GIL. Rather, the execution time can become longer when we specify n_jobs > 1.\r\nThe multi-thread support just complicates storage codes and causes hard-to-catch bugs. For example, I suspect the thread-safety relates to #820\r\nAdditionally, it forces storage classes to copy data here and there, which increases overhead even at single-thread execution.\r\n\r\n## Proposal\r\n~~I'd like to propose removing the whole multi-thread support and leave the n_job argument for future multi-process support.~~\r\nI'd like to propose removing the whole multi-thread support and instead replace it with multi-process execution. It requires multi-process support by `InMemoryStorage` and some changes in `Study` logics e.g. `study.stop`"},{"labels":[null,null,null,"enhancement",null,null,null,null],"text":"## Motivation\r\n\r\nCurrent storage class implementations have several issues listed in the following.\r\n- Inconsistent error handlings between backends.\r\n- Significant bottlenecks in the storage class on training with / querying to RDB.\r\n- O(trial^2) running time even with random-sampler and CMA-ES.\r\n- Undefined consistency models for multi-worker optimizations.\r\n\r\nSome of these limitations come from the optuna's origin, that is, optuna was initially\r\ndeveloped for deep-learning applications with ~1000 trials.\r\nGiven increasing applications optuna, it will be a good time to reconsider the design\r\nand implementations of storage classes.\r\n\r\nThis issue is meant to be used for open discussions and PR tracking.\r\n\r\nRelated PR: #1155 \r\n\r\n## Roadmaps\r\n\r\nSome of the roadmaps come from committers' discussions, but they are open for discussions. Comments are appreciated.\r\n\r\n\r\n### Documentation and implementation consistency\r\n\r\n- [x] Document consistency models for storage classes.\r\n    - #1174 \r\n- [x] Document each method, especially concerning error-handling.\r\n    - #1175 \r\n- [x] Add tests and fix implementation on error-handling.\r\n    - #1191 \r\n- [x] Support `load` method in storage implementations.\r\n    - #1263\r\n    - The method name [needs discussion](https://github.com/optuna/optuna/pull/1174#discussion_r422022176).\r\n- [ ] (üéâ contribution-welcomeüéâ) Log when new studies are created in all backends.\r\n- [ ] (üéâ contribution-welcomeüéâ) Define the default study name.\r\n\r\n### Performance improvements\r\n\r\n- [x] Add cache for unfinished trials.\r\n    - #1140\r\n- [x] Support bulk updates of trial attributes.\r\n    - #1140 \r\n- [x] Make storages sync only once per trial.\r\n- [ ] Support incremental load of DB contents.\r\n- [ ] Support indexing of finished trials.\r\n- [x] Remove unnecessary deep-copies.\r\n    - https://github.com/optuna/optuna/pull/1228#issuecomment-627147492\r\n- Miscs\r\n    - #1264 \r\n    - #1274\r\n\r\n### Refactoring\r\n\r\n- [x] Remove unnecessary return values.\r\n    - #1337 \r\n    - #1327 \r\n- [ ] (üéâ contribution-welcomeüéâ) Add more default implementations in `BaseStorage` class.\r\n- [ ] (üéâ contribution-welcomeüéâ) Removed unnecessary backend-specific method implementations.\r\n\r\n### Miscellaneous issues\r\n\r\n- [ ] (üéâ contribution-welcomeüéâ) Support heart-beat of trials.\r\n- [x] Support multi-studies in in-memory storage.\r\n    - #1191\r\n    - #1228\r\n- [ ] (üéâ contribution-welcomeüéâ) Fix race-condition in waiting queue.\r\n- [ ] (üéâ contribution-welcomeüéâ) Raise errors when a single trial is modified from multiple workers at the same time.\r\n- [ ] Remove multi-threading support and add multi-processing support.\r\n    - #1232"},{"labels":[null,"enhancement",null],"text":"Currently, the abstract methods of the `BaseTrial` class aren't decorated by `@abc.abstractmethod`, but I think that it is not intentional. We should apply the decorator to the methods since it is a good Python convention to follow."},{"labels":["enhancement"],"text":"In LightGBM, `ndcg` and `map` have many aliases. As for `ndcg`, aliases are `lambdarank`, `rank_xendcg`, `xendcg`, `xe_ndcg`, `xe_ndcg_mart`, and `xendcg_mart`. They all work on the original LightGBM. However, LightGBM Tuner in Optuna doesn't support them.\r\n\r\nIn addition, the metric `accuracy` coded in LightGBM Tuner doesn't exist in the original LightGBM. This should be removed.\r\n\r\n## Expected behavior\r\n\r\nBased on @toshihikoyanase's code ( https://colab.research.google.com/drive/1NH3CtNuo7hsZWi4u7E1YOPHKWyP-7x0o#scrollTo=IfrDKfJFRfHv), the following code should work:\r\n\r\n```python\r\n# import lightgbm as lgb\r\nimport optuna.integration.lightgbm as lgb\r\n\r\ndtrain = lgb.Dataset('rank.train')\r\ndval = lgb.Dataset('rank.test')\r\n\r\nparams = {\r\n    'objective': 'lambdarank',\r\n    'metric': 'lambdarank',  # alias to ndcg.\r\n    'eval_at': 3,\r\n    'boosting_type': 'gbdt',\r\n}\r\n\r\nmodel = lgb.train(params,\r\n                  dtrain,\r\n                  valid_sets=[dtrain, dval],\r\n                  verbose_eval=True,\r\n                  early_stopping_rounds=2,\r\n                )\r\n```\r\n\r\n## Environment\r\n\r\n- Optuna version: v1.1.0\r\n- Python version: 3.7.6\r\n- OS: Linux 4.4.0-87-generic\r\n- LightGBM version: 2.3.1\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n/home/marzio/gitws/optuna/optuna/_experimental.py:87: ExperimentalWarning: train is experimental (supported from v0.18.0). The interface can change in the future\r\n.\r\n  ExperimentalWarning\r\ntune_feature_fraction, val_score: inf:   0%|                                                                                               | 0/7 [00:00<?, ?it/s]\r\n[1]     training's ndcg@10: 0.812671    training's ndcg@20: 0.865182    valid_1's ndcg@10: 0.653369     valid_1's ndcg@20: 0.738632\r\nTraining until validation scores don't improve for 2 rounds\r\n[2]     training's ndcg@10: 0.856561    training's ndcg@20: 0.907013    valid_1's ndcg@10: 0.683942     valid_1's ndcg@20: 0.774721\r\n[3]     training's ndcg@10: 0.876966    training's ndcg@20: 0.925433    valid_1's ndcg@10: 0.700149     valid_1's ndcg@20: 0.782201\r\n[4]     training's ndcg@10: 0.89233     training's ndcg@20: 0.93465     valid_1's ndcg@10: 0.70814      valid_1's ndcg@20: 0.785663\r\n[5]     training's ndcg@10: 0.900897    training's ndcg@20: 0.940685    valid_1's ndcg@10: 0.715149     valid_1's ndcg@20: 0.793221\r\n[6]     training's ndcg@10: 0.910312    training's ndcg@20: 0.946929    valid_1's ndcg@10: 0.723119     valid_1's ndcg@20: 0.799063\r\n[7]     training's ndcg@10: 0.917917    training's ndcg@20: 0.953784    valid_1's ndcg@10: 0.719499     valid_1's ndcg@20: 0.799715\r\n[8]     training's ndcg@10: 0.921102    training's ndcg@20: 0.955849    valid_1's ndcg@10: 0.717558     valid_1's ndcg@20: 0.794649\r\nEarly stopping, best iteration is:\r\n[6]     training's ndcg@10: 0.910312    training's ndcg@20: 0.946929    valid_1's ndcg@10: 0.723119     valid_1's ndcg@20: 0.799063\r\n[W 2020-02-26 14:17:40,334] Setting status of trial#0 as TrialState.FAIL because of the following error: KeyError('lambdarank')\r\nTraceback (most recent call last):\r\n  File \"/home/marzio/gitws/optuna/optuna/study.py\", line 648, in _run_trial\r\n    result = func(trial)\r\n  File \"/home/marzio/gitws/optuna/optuna/integration/lightgbm_tuner/optimize.py\", line 269, in __call__\r\n    val_score = self._get_booster_best_score(booster)\r\n  File \"/home/marzio/gitws/optuna/optuna/integration/lightgbm_tuner/optimize.py\", line 146, in _get_booster_best_score\r\n    val_score = booster.best_score[valid_name][metric]\r\nKeyError: 'lambdarank'\r\nTraceback (most recent call last):\r\n  File \"example_lambdarank.py\", line 22, in <module>\r\n    early_stopping_rounds=2,\r\n  File \"/home/marzio/gitws/optuna/optuna/_experimental.py\", line 90, in new_func\r\n    return func(*args, **kwargs)  # type: ignore\r\n  File \"/home/marzio/gitws/optuna/optuna/integration/lightgbm_tuner/__init__.py\", line 31, in train\r\n    booster = auto_booster.run()\r\n  File \"/home/marzio/gitws/optuna/optuna/integration/lightgbm_tuner/optimize.py\", line 408, in run\r\n    self.tune_feature_fraction()\r\n  File \"/home/marzio/gitws/optuna/optuna/integration/lightgbm_tuner/optimize.py\", line 454, in tune_feature_fraction\r\n    self.tune_params([param_name], len(param_values), sampler)\r\n  File \"/home/marzio/gitws/optuna/optuna/integration/lightgbm_tuner/optimize.py\", line 516, in tune_params\r\n    study.optimize(objective, n_trials=n_trials, catch=())\r\n  File \"/home/marzio/gitws/optuna/optuna/study.py\", line 319, in optimize\r\n    gc_after_trial, None)\r\n  File \"/home/marzio/gitws/optuna/optuna/study.py\", line 597, in _optimize_sequential\r\n    self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)\r\n  File \"/home/marzio/gitws/optuna/optuna/study.py\", line 627, in _run_trial_and_callbacks\r\n    trial = self._run_trial(func, catch, gc_after_trial)\r\n  File \"/home/marzio/gitws/optuna/optuna/study.py\", line 648, in _run_trial\r\n    result = func(trial)\r\n  File \"/home/marzio/gitws/optuna/optuna/integration/lightgbm_tuner/optimize.py\", line 269, in __call__\r\n    val_score = self._get_booster_best_score(booster)\r\n  File \"/home/marzio/gitws/optuna/optuna/integration/lightgbm_tuner/optimize.py\", line 146, in _get_booster_best_score\r\n    val_score = booster.best_score[valid_name][metric]\r\nKeyError: 'lambdarank'\r\ntune_feature_fraction, val_score: inf:   0%|                                                                                               | 0/7 [00:03<?, ?it/s]\r\n```\r\n"},{"labels":["enhancement"],"text":"<!-- Please write a clear and concise description of the feature proposal. -->\r\n\r\n## Motivation\r\n\r\n<!-- Please write the motivation for the proposal.\r\n\r\nIf your feature request is related to a problem, please describe a clear and concise description of what the problem is. -->\r\nIf we introduce a formatter to Optuna and just use it, we do not need to think about formatting.\r\n\r\n## Description\r\n\r\n<!-- Please write a detailed description of the new feature. -->\r\nThere are famous formatters in python:\r\n\r\n1. [autopep8](https://github.com/hhatto/autopep8) (current)\r\n2. [Black](https://github.com/psf/black)\r\n    - example of applying black #957 \r\n3. [yapf](https://github.com/google/yapf)\r\n\r\nc.f. https://blog.frank-mich.com/python-code-formatters-comparison-black-autopep8-and-yapf/\r\n\r\nWe need to discuss what formatter is best.\r\n\r\n### Differences of above formatters\r\n#### black\r\n- Use `\"` instead of `'`.\r\n- Add trailing commas to expressions that are split by comma where each element is on its own line. \r\n- Break a line before a binary operator when splitting a block of code over multiple lines.\r\n\r\n#### black vs yapf\r\nc.f. https://news.ycombinator.com/item?id=17155048\r\n\r\n> - YAPF would at times not produce deterministic formatting (formatting the same file the second time with no changes in between would create a different formatting); Black treats this as a bug;\r\n> - YAPF would not format all files that use the latest Python 3.6 features (we have a lot of f-strings, there's cases of async generators, complex unpacking in collections and function calls, and so on); Black solves that;\r\n> - YAPF is based on a sophisticated algorithm that unwinds the line and applies \"penalty points\" for things that the user configured they don't like to see. With a bit of dynamic programming magic it arrives at a formatting with the minimal penalty value. This works fine most of the time. When it doesn't, and surprised people ask you to explain, you don't really know why. You might be able to suggest changing the penalty point value of a particular decision from, say, 47 to 48. It might help with this particular situation... but break five others in different places of the codebase.\r\n"},{"labels":[null,"enhancement",null,null,null],"text":"The motivation is similar to #732. When the number of trials is too many, outputting some warning would be more user friendly."},{"labels":[null,"enhancement",null,null],"text":"As mentioned in https://github.com/pfnet/optuna/pull/666#issuecomment-550229925, the points in the contour plot are sometimes too big for the figures.\r\n![current-contour-plot](https://user-images.githubusercontent.com/3255979/68284423-d8d85b00-00c0-11ea-8904-900d9bf65ed5.png)\r\n\r\nThe appropriate size of points seems to depend on the size of the plot area because the point size looks good if the same plot is rendered in the large plot area as follows:\r\n![image (3)](https://user-images.githubusercontent.com/3255979/68289570-b6970b00-00c9-11ea-90d3-679914817597.png)\r\n\r\nIn addition to the point size, I think we can improve the visibility of contour plots by changing the point color. For instance, we can add borders to points like slice plots (see https://github.com/pfnet/optuna/pull/661)."},{"labels":[null,"enhancement",null,null],"text":"The current parallel coordinate plot does not support log scale axes, and it is hard to analyze the results. For instance, in the following figure, `x` and `y` are sampled from `UniformDistribution` and `LogUniformDistribution`, respectively. Both parameters are randomly sampled, but the values of `y` gather around `0` while those of `x` are uniformly distributed.\r\n\r\n![newplot (38)](https://user-images.githubusercontent.com/3255979/68287656-6cf8f100-00c6-11ea-9367-b71770d3c2b6.png)\r\n\r\nAccording to this issue (https://github.com/plotly/plotly.js/issues/2292), plotly does not support log scale axes in parallel coordinate plots for now. If plotly supports it, please implement log scale axes in parallel coordinate plots."},{"labels":[null,"enhancement",null],"text":"As you can see in the following image, axis labels overlap when parameter names are too long. I think we can solve this issue if we limit the maximum length of the labels and/or tilt the labels.\r\n\r\n![newplot (39)](https://user-images.githubusercontent.com/3255979/66977250-19efd780-f0e0-11e9-87ba-a6616083da31.png)\r\n\r\n**Conditions**\r\n- Optuna version: 0.17.1\r\n- Python version: 3.7.2\r\n- OS: macOS 10.13\r\n\r\n**Code to reproduce**\r\n\r\n```python\r\nimport optuna\r\n\r\ndef objective(trial):\r\n    x = trial.suggest_uniform('x' * 100, -10, 10)\r\n    y = trial.suggest_uniform('y' * 100, -10, 10)\r\n    return x - y\r\n\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=20)\r\n\r\noptuna.visualization.plot_parallel_coordinate(study)\r\n```"},{"labels":["enhancement"],"text":"The following image is an example of slice plots. The points highlighted by the red circles have similar colors to the background, and it is difficult for users to find such points. I think we can improve the readability of slice plots by changing the color scale and/or background color.\r\n\r\n![slice-plot-low-contrast](https://user-images.githubusercontent.com/3255979/66909515-da75ac80-f047-11e9-94ca-3958ddf81c39.png)\r\n\r\n## Code to reproduce\r\n\r\n```python\r\nimport optuna\r\n\r\ndef objective(trial):\r\n    x = trial.suggest_uniform('x', -10, 10)\r\n    y = trial.suggest_uniform('y', -10, 10)\r\n    return x - y\r\n\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=20)\r\n\r\noptuna.visualization.plot_slice(study)\r\n```\r\n\r\n## Library version\r\n\r\noptuna: 0.17.1\r\nplotly: 4.1.1"},{"labels":["enhancement"],"text":"Hi there,\r\n\r\nToday, I have released OptGBM.\r\nhttps://github.com/Y-oHr-N/OptGBM\r\n\r\nThis package provides a scikit-learn compatible estimator that tunes the hyperparameters of LightGBM with Optuna.\r\n\r\nIf you like this package, please consider merging into Optuna. I hope this package will contribute to the development of Optuna.\r\n\r\nFinally, #507 implements similar features, so I might be able to help you.\r\n\r\nThanks.\r\n"},{"labels":["enhancement"],"text":"In current implementation, RDBStorage is incompatible with multi-thread mode. This is because db object cannot be share among multi-threads, in some RDB backends. E.g., sqlite3 raises the following error when a session is used in different threads:\r\n\r\n```\r\n(sqlite3.ProgrammingError) SQLite objects created in a thread can only be used in that same thread.The object was created in thread id 140736163832640 and this is thread id 123145433608192 \r\n``` \r\n\r\nThough we've decided not to support this combination for now, we could revisit this issue after wrapping up multi-node/multi-process mode."}]