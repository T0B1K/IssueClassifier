[{"text": "The documentation is becoming too undecipherable for any practical applications using Tensorflow.\r\n\r\nLook at this one, https://www.tensorflow.org/guide/data\r\n\r\nHow would anyone learn anything for a streaming data from Disk ? Not many how-tos for data that don't fit into memory. And that page.. just like a man-page for API. We end up wasting many man hours without good documentation."},
{"text": "The `tf-nightly-2.0-preview` package does not exist anymore. Please upgrade all notebooks to use `%tensorflow_version 2.x` like:\r\n\r\n```\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n```\r\n\r\nAffects:\r\n\r\n* [/site/ko/tutorials/keras/text_classification.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/text_classification.ipynb)\r\n* [/site/ko/tutorials/keras/overfit_and_underfit.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/overfit_and_underfit.ipynb)\r\n* [/site/ko/tutorials/structured_data/feature_columns.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/structured_data/feature_columns.ipynb)\r\n"},
{"text": "The `tf-nightly-2.0-preview` package does not exist anymore. Please upgrade all notebooks to use `%tensorflow_version 2.x` like:\r\n\r\n```\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n```\r\n\r\nAffects:\r\n\r\n* [/site/zh-cn/lite/convert/python_api.md](https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/lite/convert/python_api.md)\r\n* [/site/zh-cn/tutorials/estimator/boosted_trees.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/estimator/boosted_trees.ipynb)\r\n"},
{"text": "The `tf-nightly-2.0-preview` package does not exist anymore. Please upgrade all notebooks to use `%tensorflow_version 2.x` like:\r\n\r\n```\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n```\r\n\r\nAffects:\r\n\r\n* [/site/ja/lite/convert/python_api.md](https://github.com/tensorflow/docs-l10n/blob/master/site/ja/lite/convert/python_api.md)\r\n* [/site/ja/guide/function.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ja/guide/function.ipynb)\r\n* [/site/ja/tutorials/load_data/csv.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/load_data/csv.ipynb)\r\n"},
{"text": "This package does not exist anymore. Please upgrade all notebooks to use `%tensorflow_version 2x`.\r\n\r\nhttps://github.com/tensorflow/docs-l10n/search?q=tf-nightly-2.0-preview&unscoped_q=tf-nightly-2.0-preview"},
{"text": "I found issues about after convert to docs that generated extra notebook cell.\r\n\r\n- https://www.tensorflow.org/tutorials/keras/classification in Japanesse version\r\n\r\n![image](https://user-images.githubusercontent.com/2786333/77142134-896c5780-6ac2-11ea-94f8-8e97e35c7521.png)\r\n\r\n- Original: notebook \r\n- https://github.com/masa-ita/tf-docs/blob/b8cd622e042ce322345d5fa30858087ee1abcab1/site/ja/tutorials/keras/basic_classification.ipynb\r\n\r\n"},
{"text": "Is it possible to correct some Portuguese notebooks, as they have some typos? I'm a Portuguese native speaker."},
{"text": "The default Matplotlib setup in Colab doesn't include Chinese or Korean fonts, so these characters don't render.\r\n\r\nI believe this is one of the reasons we have not been translating figure text.\r\n\r\nI can get the browser to render this text by outputting svg-text:\r\n\r\n```\r\nfrom IPython.display import set_matplotlib_formats\r\nset_matplotlib_formats('svg')\r\nmatplotlib.rcParams['svg.fonttype'] = 'none'\r\n```\r\n\r\nBut that messes up a lot of the formatting.\r\n\r\nSome searching shows that it might just be a matter of installing the right fonts and adding them to the `matplotlib.rc` configuration, but I haven't found a end-to-end setup that works yet. \r\n\r\nDoes anyone have experience setting this up?\r\n\r\n"},
{"text": "https://www.tensorflow.org/install/lang_java\r\n\r\n\r\nThe JNI download link on the Java page is still points to 1.14.0, it should be updated to 2.3.0.\r\n\r\npage source:\r\n```\r\n<td>Linux CPU only</td>\r\n\u00a0 | <td class=\"devsite-click-to-copy\"><a href=\"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-linux-x86_64-1.14.0.tar.gz\">https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-linux-x86_64-2.3.0.tar.gz</a></td>\r\n\u00a0 | </tr>\r\n\u00a0 | <tr>\r\n\u00a0 | <td>Linux GPU support</td>\r\n\u00a0 | <td class=\"devsite-click-to-copy\"><a href=\"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-gpu-linux-x86_64-1.14.0.tar.gz\">https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-gpu-linux-x86_64-2.3.0.tar.gz</a></td>\r\n\u00a0 | </tr>\r\n\u00a0```"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/xla/operation_semantics#conv_convolution\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/python/xla.py#L239:L269\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\n [tf2xla.python.xla.conv](https://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/python/xla.py#L239:L269) points to the operation semantics for `ConvWithGeneralPadding` but actually wraps the more general `ConvGeneralDilated`. It would make sense to actually have documentation about the operation semantics of this more general operation.\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/graph_optimization\r\n\r\n## Description of issue (what needs changing):\r\nIt should be cleared which optimizers Grappler applies by default. \r\nFor now it's not clear if i should turn on a lot of features by myself\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? - No.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/device\r\n\r\n## Description of issue (what needs changing):\r\n\r\nAt least on 2.3.0, it seems to me that \r\n```\r\nimport numpy.random as npr\r\nimport tensorflow as tf\r\nwith tf.device(\"GPU\"):\r\n  A=tf.convert_to_tensor(npr.randn(500))\r\n```\r\nwill create an eager tensor `A` on the CPU device (it will not allocate ram on the gpu).  This is counter-intuitive to someone who has only read the doc as it is written.  My understanding is that this happens because tf.convert_to_tensor isn't an op, and tf.device only deals with ops.  \r\n\r\n### Clear description\r\n\r\nThe doc is pretty short now, and I don't think it would hurt to add a little remark, something like this:\r\n\r\n*Note* -- `tf.convert_to_tensor` does not create an op.  As such, it ignores the contexts created by tf.device.  To ensure a given tensor is assigned memory on a particular device, one can wrap convert_to_tensor inside a `tf.identity` op.\r\n\r\nThoughts?"},
{"text": "### Description:\r\n\r\nHey @lamberta @MarkDaoust @yashk2810, \r\n\r\nI've put together a few small commits to update the TensorFlow docs for more inclusive language. It's to do with ~~\"Native\"~~ -> \"Built-in\". (Source: an a11y [presentation](https://docs.google.com/presentation/d/1UVHzuMo5Ef1zUCZ3qdFwDQh-aVHpkYnY73TrJ2yHt3E/edit#slide=id.g6fe49527a0_0_334) by @heyawhite\u2014a tech writer at Google). \r\n\r\n[Link to diffs](https://github.com/tensorflow/docs/compare/master...8bitmp3:master).\r\n\r\nIf you're OK with these changes, I can submit a PR.\r\n\r\n### Submit a pull request?\r\n\r\nYes, can do\r\n\r\n### Affected docs:\r\n\r\n- TF testing best practices guide\r\n- TensorFlow (R1) C++ API guide\r\n- TF 1.x Eager mode notebook\r\n- TensorFlow Customization basics notebook\r\n- Build TensorFlow on Windows guide\r\n- TensorFlow 2 migration notebook\r\n- tf.function notebook\r\n- TF create an op in C++ guide\r\n- TF (R1) Adding a new op in C++ guide"},
{"text": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nCurrently https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss and https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder has different default blank index.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss sets default blank index to be 0.\r\n\r\nWhreas https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder doesn't have an API for setting blank index, and it assumes to be `num_category - 1` (see https://github.com/tensorflow/tensorflow/blob/cd7da16dd6c17df428dc9ec105c0c8f11e5fd4f5/tensorflow/core/kernels/ctc_decoder_ops.cc#L331)\r\n\r\n**Describe the expected behavior**\r\n\r\nThis is very unexpected - I would assume they have the same default value since they both work with CTC. Or at least both should provide API to change the blank index. \r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/install/source\r\n\r\n## Description of issue (what needs changing):\r\nAproximately the RAM that used when compiling. IMO, \"Building TensorFlow from source can use **a lot of** RAM\", is not clear how \"a lot of\" is how many GB. I have VM with 6vCPU 10GB RAM and I frequently run out of memory. This run out of memory is not only slowing down the compiling, but also make system freeze/unresponsive, Every this happens, I need to increase the RAM by 500MB to unlock the system.\r\n\r\n### Clear description\r\nWhy we need this? Because we don't want to sleep with the machine compiling overnight. And when we wakes up, it turns out the compiling failed, or the machine frozen, and it's really waste of time. This \"aproximately RAM usage\" should help anticipate this though.\r\n\r\n### How about `--local_ram_resources`?\r\nWell, I already tried with this flag `--local_ram_resources=HOST_RAM*.2` as described from [here](https://docs.bazel.build/versions/master/user-manual.html#flag--local_{ram,cpu}_resources). Is this mean the building process will only take 20% RAM? Is this global or for every thread? Is this flag supported on the `v2.3.0` tag on this repo? I watched `htop` and seen many tasks at once using more than 20%. I run out of memory again even with this, this is ridiculous. \ud83d\ude2d \r\n\r\n### Submit a pull request?\r\nTo this [repo](https://github.com/tensorflow/docs)? Yes if I can get how aproximately the RAM usage... I think it's max around 3GB per CPU core? Correct me if I'm wrong.\r\n\r\n### Other information\r\nWell it seems I got a lot of RAM, but I running out of memory. The VM is live CD (not installed), and also the swap.... yeah it's zram (5GB) instead of swapfile. Because... uh..., I don't want to kill the SSD :(\r\n\r\n### Related issue\r\n#30047"},
{"text": "https://www.tensorflow.org/tutorials/structured_data/time_series#part_2_forecast_a_multivariate_time_series\r\n\r\nI do not understand how this can be time series ? The data is \"equidistant\" with each other. And the time itself is not considered in predicting the values, but rather just as a series. When you include TIME AS A VECTOR, i would accept that it is a Timeseries.\r\n\r\nThe title is MISLEADING and also we need an example for vectorizing Time... for an actual Timeseries."},
{"text": "Here is the guide to Tensorflow Lite Support Library:\r\nhttps://www.tensorflow.org/lite/guide/lite_support\r\n\r\nAn exert:\r\n\r\n> The [TensorFlow Lite Android Support Library](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/support/java) is designed to help process the input and output of TensorFlow Lite models, and make the TensorFlow Lite interpreter easier to use.\r\n\r\nWhich links to:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/support/java\r\n\r\nwhich is 404 Page not found.\r\n "},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\r\n\r\n## Description of issue (what needs changing):\r\n\r\n1. `mode`: \"{auto, min, max}\" should be `{auto, min, max}`, I guess (minor)\r\n2.  `save_best_only`: I believe the description is incomplete. With `save_best_only=True`, not only will \"the latest best model [...] not be overwritten\": but also the current model is not written at all, even if it has another filename than the \"latest best model\". This is kind of implied by the name of the parameter, but the description should include that, too."},
{"text": "The documentation for tf.image.rgb_to_yuv says \"Outputs a tensor of the same shape as the images tensor, containing the YUV value of the pixels. The output is only well defined if the value in images are in [0,1].\" Does that mean the RGB values should be [0,1]?\r\n\r\nIf so, the usage example added confusion:\r\n```\r\nx = [[[1.0, 2.0, 3.0],\r\n      [4.0, 5.0, 6.0]],\r\n    [[7.0, 8.0, 9.0],\r\n      [10.0, 11.0, 12.0]]]\r\ntf.image.rgb_to_yuv(x)\r\n```\r\n\r\nClearly, x does not lie in [0,1]\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n\r\nhttps://www.tensorflow.org/lite/performance/gpu_advanced\r\n\r\n\r\n## Description of issue (what needs changing):\r\n\r\n    bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate                  # for static library\r\n    bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so  # for dynamic library\r\n\r\nshould be changed to,\r\n\r\n    bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate                  # for static library\r\n    bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so  # for dynamic library\r\n\r\nbecause gl_delegate is not GPU delegate runtime library, it is for OpenGL delegate, right?\r\n\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "TensorFlow documentation issue. \r\n\r\n## URL(s) with the issue:\r\nThe link/page not found(404); \"images -> object detection API\" under https://www.tensorflow.org/tutorials/\r\nnamely: https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n\r\n## Description of issue (what needs changing):\r\nMaterial was moved to https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/fft\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/fft\r\n\r\n## Description of issue (what needs changing):\r\nThe description is simply \"Fast Fourier transform.\" which isn't fully-specified. What is the exact function computed? Is there a normalization term of 1/sqrt(N) or 1/N? Or is the normalization constant entirely in the inverse FFT (which has equally underspecified documentation)?\r\n\r\n### Clear description\r\nA mathematical formula that specifies what `tf.signal.fft` implements would be nice. Likewise with the other FFT methods, the inverse FFT methods, and STFT methods."},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose)\r\n\r\n## Description of issue (what needs changing):\r\nThe documentation claims that padding options {\"valid\" and \"same\"} are supported, but when following the code path to deconv_output_length at line 140 [here](https://github.com/tensorflow/tensorflow/blob/0c227aed65e62f741a88c9915923d262710fc8c9/tensorflow/python/keras/utils/conv_utils.py#L140) there is the option for {\"full\"} as well.\r\n\r\nAdditionally, the equation provided for calculating output shape merely says \"padding\" for a variable which is represented as a string in the API. This makes for a guessing game of how to achieve the desired output shape.\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L16](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L16l)\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/losses/sigmoid_cross_entropy\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nUnclear rank dependency of input `weights`. According to the document, `weights` could have the same rank as `labels`, and must be broadcastable to `labels`, but it is unclear what `labels` is. \r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nYes\r\n\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2"},
{"text": "# Documentation for state in AbstractRNNCell could be more clear.\r\n\r\nIn the documentation for the `AbstractRNNCell` it does not make it clear that the state is a tuple. https://www.tensorflow.org/api_docs/python/tf/keras/layers/AbstractRNNCell\r\n\r\nThis was a gotcha for me when I defined a custom RNN cell that had a single state. It kept adding an axis to that state whenever I performed an operation on it.\r\n\r\nFor example, the code within the call method the class implementing `AbstractRNNCell`\r\n```logging.info(f'states: {states}')\r\nlogging.info(f'state_update: {state_update}')\r\nnew_states = tf.math.add(states, state_update)\r\nlogging.info(f'new_states: {new_states}')\r\n```\r\n\r\nleads to the confusing output\r\n\r\n```06-12 12:28 root         INFO     states: (<tf.Tensor 'Placeholder_3:0' shape=(32, 4) dtype=float32>,)\r\n06-12 12:28 root         INFO     state_update: Tensor(\"add_1:0\", shape=(32, 4), dtype=float32)\r\n06-12 12:28 root         INFO     new_states: Tensor(\"Add_2:0\", shape=(1, 32, 4), dtype=float32)\r\n```\r\n\r\nUpon implementing the state as a tuple of length one, the issue was solved. I think this could be made more clear in the documentation.\r\n\r\nMany thanks."},
{"text": "I'm trying to implement the code in this notebook https://github.com/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb\r\n\r\nThe lines for updating the training loss and accuracy are incorrect:\r\n```python\r\ntraining_loss.update_state(loss * strategy.num_replicas_in_sync)\r\ntraining_accuracy.update_state(labels, logits)\r\n```\r\nI don't understand the intent behind updating the loss with the product of the number of replicas and the batch loss but it gives the wrong result. Changing the line to\r\n```python\r\ntraining_loss.update_state(labels, logits)\r\n```\r\nappears to solve the bug.\r\n\r\nI also changed the definition of `training_loss` from a `metrics.Mean` to a `metrics.SparseCategoricalCrossentropy`. "},
{"text": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the time-series forecasting tutorial, the normalization is done prior to obtaining time-series windows. \r\nConsider this:\r\n`uni_data = (uni_data-uni_train_mean)/uni_train_std`\r\nThis is done before:\r\n```\r\nx_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\r\n                                           univariate_past_history,\r\n                                           univariate_future_target)\r\n```\r\nThis is causing the past_history samples using values of future targets as well during the normalization. This is a bias. In reality, we cannot use future values to normalize current values.\r\nThis, I think, is a bias and a bug.\r\n\r\n### Correct links\r\n\r\n\r\n\r\n### Parameters defined\r\n\r\n\r\n### Returns defined\r\n\r\n\r\n### Raises listed and defined\r\n\r\n\r\n### Usage example\r\n\r\nNormalization should be done after extraction of sequences and only using the LHS of the sequence. I still dont know if normalizing the RHS of the sequence is desired. but does not hurt as long as we denormalize\r\n\r\n### Request visuals, if applicable\r\n\r\n\r\n### Submit a pull request?\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv1d_transpose\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nUnclear type and dimension dependency of input `filters`. According to the document, `filters` should have the same type as `value` and the `in_channel` dimension must match that of `value`, but it is unclear what `value` is.\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv1d_transpose: Yes\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose:  No, the \"Raises\" list is not provided or defined\r\n\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2\r\n  "},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/ragged/constant\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFormat issue. In the \"Args\" section, format of description of `ragged_rank` is problematic. The default value should be `max(0, K-1-len(inner_shape))`\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nYes\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: https://www.tensorflow.org/lite/convert/python_api\r\n\r\n## Description of issue (what needs changing):\r\nUnder 'Converting a Keras model' it has the code `tf.gfile.GFile` and that code has moved to `tf.io.gfile.GFile`\r\n\r\n### Clear description\r\nThis change should be made so that the code runs.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? Yes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? No errors\r\n\r\n### Usage example\r\n\r\nIs there a usage example? No\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content? N/A\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/flowers_tf_lite.ipynb\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/lite/codelabs/flower_classification/android/finish/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L292\r\n\r\n## Description of issue (what needs changing):\r\nIn the above codelabs tutorial, we see image has been rescaled to [0-1] by dividing by 255. Since pre-trained weights (imagenet) are trained by feed [-1 1] normalized image. Ideally tutorial should add that step to correctly leverage transfer learning. \r\n\r\n### Clear description\r\nSo what is happening is we create a tflite model trained with [0-1] based preprocessing. On android client we are doing [-1 1] based preprocessing before feeding to tflite model.\r\nCan someone please clarify?\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "Hi all,\r\nthe 'auto' mode in ReduceLROnPlateau  and ModelCheckpoint  are looking for specific string 'acc' in the name of the metrics to be monitor. this actually leads to unlickly scenarious of not working properly even while using metrics that are defined in tfa and hoping tf will be aware of the direction . this can be added in the doc to make the developers understand how to name their metrics or to set min max mode on their own. \r\nthanks\r\n\r\nhttps://github.com/tensorflow/addons/issues/1865\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThat documentation uses deprecated code like 'tf.Session()'\r\n\r\n### Submit a pull request?\r\n\r\nNo, because I don't really know how it should be used now.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe docs is missing `get_weights`, `set_weights` method and `metrics` property.\r\n\r\n`get_weights` method and `metrics` property are defined in the src. But not in the generated docs.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/training.py#L190-L197\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/training.py#L361-L410\r\n\r\n`set_weights` method is mentioned in [keras docs](https://keras.io/api/models/model_saving_apis/#setweights-method).\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3DTranspose\r\n\r\n## Description of issue (what needs changing):\r\n\r\nOutput shape computation is shown as below on above documentation\r\n```\r\nnew_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\r\noutput_padding[0])\r\nnew_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +\r\noutput_padding[1])\r\nnew_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] +\r\noutput_padding[2])\r\n```\r\nbut padding is either 'valid' or 'same'\r\nso, is padding computed based on traditional convolution computation (ref: https://www.tensorflow.org/api_docs/python/tf/nn/convolution) and then used here?\r\nThis is unclear from current documentation how `same`/`valid` mode is being used.\r\n\r\n\r\n### Clear description\r\n\r\nClarification about how these modes are reflected in computing actual padding and then used in specified formula.\r\n\r\n### Computation of output shape\r\n`deconv_output_length` from keras/utils/conv_utils.py is used for computing the output shape considering output_padding which should be reflected into documentation concisely\r\nref: https://github.com/tensorflow/tensorflow/blob/dd2ea875d92eeb83e81b1cb92e29e61d488e98b2/tensorflow/python/keras/utils/conv_utils.py#L168\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/split\r\n\r\n## Description of issue (what needs changing):\r\n\r\n> If `num_or_size_splits` is an integer, then `value` is split along the dimension axis into `num_split` smaller tensors. This requires that `value.shape[axis]` is divisible by `num_split`.\r\n\r\nWhat is `num_split` here? I think this should be\r\n\r\n> If `num_or_size_splits` is an integer, then **we call it `num_split` and** `value` is split along the dimension axis into `num_split` smaller tensors. This requires that `value.shape[axis]` is divisible by `num_split`.\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/control_dependencies\r\n\r\n## Description of issue (what needs changing):\r\nDo we need to mention the debug use case in https://www.tensorflow.org/api_docs/python/tf/debugging/assert_equal#returns ?\r\n### Clear description\r\nWe declare in the note\r\n> Note: In TensorFlow 2 with eager and/or Autograph, you should not require this method, as code executes in the expected order. Only use tf.control_dependencies when working with v1-style code or in a graph context such as inside Dataset.map.\r\nBut there is any direct reference to the `assert_equal` use case\r\n\r\nFor example, why should someone use this method? How is it useful?\r\nTake a look at the issue [here](https://github.com/tensorflow/addons/issues/1794)"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/quickstart/beginner\r\n\r\n## Description of issue (what needs changing):\r\n\r\nTutorial makes use of model.evaluate(), and the documentation says that this is usually done on a \"Validation Set\". Everything else I read (glossary, docs for model.fit()... including validation set parameters) points to this relating to a \"Test Set\" since it occurs after the training phase, and the parameters passed are \"x_test\" and \"y_test\". The confusion is unhelpful to beginners. Change from \"Validation Set\" to \"Test Set\"?\r\n\r\n### Correct links\r\n\r\nn/a\r\n\r\n### Parameters defined\r\n\r\nn/a\r\n\r\n### Returns defined\r\n\r\nn/a\r\n\r\n### Raises listed and defined\r\n\r\nn/a\r\n\r\n### Usage example\r\n\r\nn/a\r\n\r\n\r\n### Request visuals, if applicable\r\nn/a\r\n\r\n### Submit a pull request?\r\n\r\nNo. I'm a beginner, so I don't want to do anything, lest I create more confusion.\r\n"},
{"text": "This is about [this tutorial](https://www.tensorflow.org/guide/data) on input pipelines, and in particular the following note under \"Reading input data\" > \"Consuming NumPy arrays\":\r\n\r\n> Note: The above code snippet will embed the features and labels arrays in your TensorFlow graph as tf.constant() operations. This works well for a small dataset, but wastes memory---because the contents of the array will be copied multiple times---and can run into the 2GB limit for the tf.GraphDef protocol buffer.\r\n\r\nCould we have a slightly more detailed justification for this note, namely as to why `tf.data.Dataset.from_tensor_slices()` is suboptimal in this case. In particular:\r\n\r\n1. In which way are the contents of the array copied multiple times?\r\n2. What is the alternative to that code if we don't want to run into the 2GB limit? What is the best practice in general?"},
{"text": "## Description of issue (what needs changing):\r\nTensorflow gives many errors, and most of them aren't very helpful.  Something like \"module tensorflow has no attribute reset_graph.\"  Can we change the error messages so they are more constructive?  In this situation, the issue was partially solved by downgrading to tensorflow 1.12.  It would be helpful if instead of the \"reset_graph\" error message, we could get a message more like: \"this version of tensorflow is incompatible with the current project.  Please downgrade to tensorflow 1.12 using: pip install tensorflow==1.12\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\nTo keep from tearing their own hair out.\r\n\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/dtypes/DType\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe dtypes, such as, `tf.qint8`, `tf.quint8`, `tf.qint16`, `tf.quint16`, `tf.qint32`, are a bit unclear. What is `Quantized` suppose to mean? Where should a reader go to learn more about it? Clicking on the hyperlink of any of the dtypes of the above leads to [tf](https://www.tensorflow.org/api_docs/python/tf) which is just text. It does not give info about variable itself ( what is `quantization`? How and why is it an `int` ? )\r\n\r\n### Clear description\r\n\r\nNo clear description about what `quantization` really means. \r\nGoogling for `quantized tensorflow` leads us to,\r\n1): [Post training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)\r\n2): [TensorFlow Lite 8-bit quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec)\r\n3): [Converting Quantized Models](https://www.tensorflow.org/lite/convert/quantization)\r\n4): [tf.quantization.quantize](https://www.tensorflow.org/api_docs/python/tf/quantization/quantize)\r\n5): [Post-training dynamic range quantization](https://www.tensorflow.org/lite/performance/post_training_quant)\r\n\r\nNone of which give a quick definition into what `quantization` is and what it is in Tensorflow.\r\n\r\n### Correct links\r\n\r\nThe link is correct, it is this https://www.tensorflow.org/api_docs/python/tf/dtypes/DType#tf.qint32 .\r\n\r\n### Parameters defined\r\n\r\nNot related to code.\r\n\r\n### Returns defined\r\n\r\nNot based on code.\r\n\r\n### Raises listed and defined\r\n\r\nNot related to code.\r\n\r\n### Usage example\r\n\r\nNot related to code.\r\n\r\n### Request visuals, if applicable\r\n\r\nNot really.\r\n\r\n### Submit a pull request?\r\n\r\nI do not plan to. Don't really have the time. \r\n\r\nThis is similar to issue #15 , closed, at [here](https://github.com/tensorflow/tensorflow/issues/15) and #494, [here](https://github.com/tensorflow/tensorflow/issues/494).\r\n\r\nThank you! And have a nice day. \r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/data\r\n\r\n## Description of issue (what needs changing):\r\nHow to store tf.dataset object to file?\r\nFor instance,\r\n```\r\ndataset1 = tf.data.Dataset.from_tensor_slices(\r\n    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))\r\ndataset1\r\n```\r\nHow to store the dataset1 to file?\r\n\r\n### Clear description\r\n\r\nFor me, a saved copy of tokenized dataset saves lot of training time.\r\n```python\r\nfrom transformers import AlbertTokenizer\r\nimport tensorflow as tf\r\nimport DataReader\r\nimport Tokenizer\r\n\r\n\r\ndef encode(type, dataPath='./qgdata/nq-train-sample.json'):\r\n    entries = DataReader.read(dataPath)\r\n    encoding = []\r\n    for entry in entries:\r\n        if type == 'context':\r\n            context = Tokenizer.encode(\r\n                entry['passage'], entry['answer'], entry['question'], True)\r\n            encoding.append(context)\r\n        else:\r\n            question = Tokenizer.encode(\r\n                entry['passage'], entry['answer'], entry['question'], False)\r\n            encoding.append(question)\r\n    data = tf.data.Dataset.from_generator(\r\n        lambda: encoding, tf.int64, output_shapes=512)\r\n    return data\r\n\r\n\r\ndef make_dataset(dataPath='./qgdata/nq-train-sample.json', batch_size=1):\r\n    contextData = encode('context', dataPath)\r\n    questionData = encode('question', dataPath)\r\n    dataset = tf.data.Dataset.zip((contextData, questionData))\r\n    return dataset.batch(batch_size)\r\n```\r\nInstead of running this batching script before each training, it would be very efficient to store the tokenzied dataset object to file and avoid retokenizing.\r\n\r\n### Usage example\r\nMaybe like:\r\n```python\r\ndataset1 = tf.data.Dataset.from_tensor_slices(\r\n    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))\r\ndataset1.save_dataset(path_to_store)\r\n```\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/performance/gpu_advanced\r\n\r\n## Description of issue (what needs changing):\r\n\r\nAfter a couple of days digging through documentation and source code, I'm still very confused about the current state of GPU support in tensorflow/lite.\r\n\r\n1. https://www.tensorflow.org/lite/performance/gpu_advanced#android_cc : talks about C/C++, which gives the illusion that one might use the lite/c API. But as far as I can see, the `ModifyGraphWithDelegate` function is not present in lite/c (why? It would be very helpful), even though it has the concept of delegates;\r\n2. https://www.tensorflow.org/lite/performance/gpu_advanced#android_cc : suggests a build command that generates a 60MB shared library... I don't see any benefit in giving such suggestion, since other commands listed in other pages will generate properly optimized binaries;\r\n3. https://www.tensorflow.org/lite/performance/gpu_advanced#android_cc : building on (2.), I'm also under the impression that building the delegate as a separate shared lib would not be the best option for minimizing the overall size - in this case, a target for building the delegate + libtensorflowlite together would be highly appreciated, at least as a documentation snippet (not to mention prebuilt binaries, which are referred by the team as \"coming soon\" in several not-so-recent issue comments);\r\n3. https://www.tensorflow.org/lite/performance/gpu_advanced#inputoutput_buffers : suggests the use of `GpuDelegate` which, as far as I understand comes from `lite/delegates/gpu/gl_delegate.h` and as such is deprecated. A big notice in the source code warns to migrate to the new implementation before the end of 2019, so it probably shouldn't be in documentation;\r\n4. https://www.tensorflow.org/lite/performance/gpu_advanced#inputoutput_buffers : \r\nWhile a replacement exists (`lite/delegates/gpu/delegate.h`), it does not have any `bindGlBufferToTensor()` function, and it is not clear how to achieve the same thing with the new delegate. There are several unanswered questions on SO about this;\r\n5. https://www.tensorflow.org/lite/performance/gpu_advanced#inputoutput_buffers : the example uses a SSBO, however the delegate seems to support [textures as well?](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/gl_delegate.h#L53-L57) If this can be a different way to send initial input, it would be nice to have it documented;\r\n\r\nIt is hard for us to plan the adoption of TFLite without a clear view over what you have, or at least where you're heading. For example, I'm especially interested in using GL buffers as input (sounds like a game changer), but I have no clue about what's the state of this in TFLite. Same with using delegates in lite/c, the abstraction is there but `ModifyGraphWithDelegate` is not. So doc fixes apart, could we have a very brief description of where TFLite + GPU/delegates is headed and what's coming in the next couple of months, so that people can decide if it meets their needs and plan accordingly? \r\n\r\nI understand that some of these APIs are marked as experimental and I really appreciate your work. Thanks!\r\n"},
{"text": "Apparently it is possible to render edges between data points in the embeddings projector but I cannot find the documentation for this anywhere. It is also not clear what other things might be possible which are not obvious or documented.\r\nA column called `__next__` may have special meaning, judging from one of the examples, but it is not clear what exactly it can be used for, what the requirements on the input are or what other special meaning column names there may exist.\r\nOr am I missing some obvious documentation here?"},
{"text": "## URL(s) with the issue:\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/ios\r\n## Description of issue (what needs changing):\r\n\r\nThe README must be provided with an explanation of how to change the settings of the project. At the moment I have problems adjusting the development team (\"**Signing**\").\r\nError message: No profile for team 'Rob De Putter (Personal Team)' matching 'Wildcard' found: Xcode couldn't find any provisioning profiles matching 'GPC87JXMXD/Wildcard'. Install the profile (by dragging and dropping it onto Xcode's dock item) or select a different one in the Signing & Capabilities tab of the target editor.\r\n\r\n<img width=\"1421\" alt=\"Schermafbeelding 2020-04-05 om 14 54 09\" src=\"https://user-images.githubusercontent.com/36565271/78500273-4f9b8200-774d-11ea-8b6a-4f214fa2fe81.png\">\r\n<img width=\"1154\" alt=\"Schermafbeelding 2020-04-05 om 14 54 37\" src=\"https://user-images.githubusercontent.com/36565271/78500283-6641d900-774d-11ea-9bca-6aec48736816.png\">\r\n<img width=\"1152\" alt=\"Schermafbeelding 2020-04-05 om 14 54 30\" src=\"https://user-images.githubusercontent.com/36565271/78500289-6cd05080-774d-11ea-984b-d6f3710d8f08.png\">\r\n\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\nNo examples of generating Arduino IDE specific files from source (.cc) files\r\n\r\n### Clear description\r\nThere is documentation and code (generate_microlite_projects() function,transform_arduino_source.py, etc.) suggesting that the Make build system allows for easily creating files and a directory from source files to be used in the Arduino IDE but there are no Arduino examples that show how this is or should be done. \r\n\r\nIt would be useful to show how the hello_world project example was built for the Arduino IDE from the repo's source using Make.\r\n\r\n### Usage example\r\nIs there a usage example?\r\nNot for Arduino."},
{"text": "Hi!\r\n\r\nPlease see:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_hinge  \r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/losses.py#L866-L882\r\n\r\nBoth mention:\r\n> y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided they will be converted to -1 or 1.\r\n\r\nWhile the code is --as expected-- a transcription of keras' one:\r\n```\r\n# ...\r\ny_pred = ops.convert_to_tensor(y_pred)\r\ny_true = math_ops.cast(y_true, y_pred.dtype)\r\npos = math_ops.reduce_sum(y_true * y_pred, axis=-1)\r\nneg = math_ops.reduce_max((1. - y_true) * y_pred, axis=-1)\r\nreturn math_ops.maximum(0., neg - pos + 1.)\r\n```\r\n\r\nAnd this code is meant to work with one-hot-encoded tensors. See the original discussion here: https://github.com/keras-team/keras/issues/2830"},
{"text": "## URL(s) with the issue: \r\nhttps://youtu.be/aNrqaOAt5P4?list=PLQY2H8rRoyvzuJw20FG82Lgm2SZjTdIXU&t=660\r\n\r\n## Description of issue (what needs changing): \r\nIn the [TF Dev Summit 2020](https://youtu.be/aNrqaOAt5P4?list=PLQY2H8rRoyvzuJw20FG82Lgm2SZjTdIXU&t=660\r\n), Paige Bailey has talked about **Keras Tuner** and has shown its implementation. I liked the functionality but I couldn't information/documentation about it in [tensorflow.org site](https://www.tensorflow.org/).\r\n\r\n### Clear description: \r\nThis being a New Functionality, the documentation about that functionality in the Website would help the Community.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? : N/A\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?: N/A\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : N/A\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? : N/A\r\n\r\n### Usage example\r\n\r\nIs there a usage example? N/A"},
{"text": "It would be useful to show the lifecycle of an API in the docs. That is, show when say, `tf.data.Dataset.take` was added or when a certain is removed / renamed. It could be extended to arguments of each API as well. Or to put it in one line,\r\n``` \r\nExpose version control information of APIs directly on tf docs\r\n```\r\nIt would really help developers keep up with rapid development of TF even better."},
{"text": "## URL(s) with the issue: The information related to Arguments corresponding to the Pre-Trained Models defined under  https://www.tensorflow.org/api_docs/python/tf/keras/applications is missing.\r\n\r\nSome examples links are shown below:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNet\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/InceptionV3\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16\r\n\r\n## Description of issue (what needs changing): The information corresponding to Arguments should be specified like that it is Specified in [Keras Website](https://keras.io/applications/#vgg16).\r\n\r\nFor example, why should someone use this method? How is it useful? : If someone want to know what Arguments should be passed while trying to use these Pre-Trained Models, information is lacking in TF.Org site and the Developers should go to Keras Website. The information is not available in the Source Code corresponding to those TF Pre-Trained Models.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? : Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : Yes\r\n\r\n### Usage example\r\n\r\nIs there a usage example? : NO\r\n\r\n### Submit a pull request? : No\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "I see the core here https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime\r\n\r\nBut there is no instruction about how to run TF with distributed system (C++ language). \r\nPlease share with us some documents about distributed TF c++.\r\n\r\nThanks,"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: https://www.tensorflow.org/api_docs/python/tf/Variable\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing): \r\n\r\n### Clear description\r\n\r\nTF 1.13.1\r\nI can't find anything detail about differences between VarHandlOp and VariableV2. \r\nAs far as I know it seems that VarHandleOp implement in Keras and VariableV2 in TF 1.X. \r\nHow can I convert VarHandleOp to VariableV2 in tf.keras?\r\nI'm using some model processing tool that can only run under VariableV2 ops.\r\n\r\n### Correct links\r\n\r\nhttps://git.codingcafe.org/Mirrors/tensorflow/tensorflow/commit/e4a5c5356063d7f7b324a5771fe296bb199b532c\r\nSomelink above is all I could find.\r\n\r\n### Parameters defined\r\n\r\n---\r\n\r\n### Returns defined\r\n\r\n---\r\n\r\n### Raises listed and defined\r\n\r\n---\r\n\r\n### Usage example\r\n\r\nIs this a usage example?\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/VariableV2?hl=pl\r\nIs that a kind of example?\r\n\r\n### Request visuals, if applicable\r\n\r\n---\r\n\r\n### Submit a pull request?\r\n---\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50V2\r\n\r\n## Description of issue (what needs changing):\r\n\r\nTF provides two type of ResNet models. The first is ResNet50 which is implemented by the following code:\r\n\r\nhttps://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py#L423-L441\r\n\r\nwhere, `stack1` is **basic** version of residual function:\r\nhttps://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py#L64-L127\r\n\r\nAnd the second is ResNet50V2 which is implemented by the following code:\r\n\r\nhttps://github.com/keras-team/keras-applications/blob/b34c10628a0ab436542e9160f98de72b49084bbe/keras_applications/resnet_common.py#L483-L501\r\n\r\nwhere, `stack2` is **bottleneck** version of residual function:\r\nhttps://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py#L175-192\r\n\r\nThe original [paper](https://arxiv.org/pdf/1512.03385.pdf) lists different type of ResNet in Table 1.\r\n\r\nBy the original  definition, the `ResNet50` should be 34-layer ResNet in the Table 1.\r\n\r\nFrom the implementation by `pytorch`: \r\n\r\nhttps://github.com/pytorch/vision/blob/cc43e0a98368055d7a661651a2b9dbf28a19e533/torchvision/models/resnet.py#L244-L266\r\n\r\nThey claim the first one is ResNet34.\r\n\r\nSo I suggest that the `ResNet50` should change its name.\r\n\r\n\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\n* https://www.tensorflow.org/guide/migrate#1_replace_v1sessionrun_calls\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThere is no information provided to the end-user on how to convert simple `Session.run` calls into `tf.function` calls for TensorFlow 2. For people who are only interested in running saved models and not building their own architectures, the lack of information makes it difficult to fully migrate away from TensorFlow 1.x.\r\n\r\n### Clear description\r\n\r\nIf I am doing a SavedModel-based system with TensorFlow 1.x (I was provided the model, I did not make the model), there should be a direct explanation of how to convert `Session.run` calls into more modern `tf.function` calls. Here is an example of the code I'm trying to convert to TensorFlow 2, but I can't complete the conversion because of a lack of documentation for this use case:\r\n\r\n```python\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n    tf.saved_model.loader.load(sess, [\"serve\"], path_to_model)\r\n    cap = cv2.VideoCapture(camera_id)\r\n    ret, frame = cap.read()\r\n    ret, encoded = cv2.imencode(\".jpg\", frame)\r\n    inferred = sess.run([\"detection_scores:0\", \"detection_boxes:0\"], feed_dict={\r\n        \"encoded_image_string_tensor:0\": [encoded.tobytes(),]\r\n    })\r\n```\r\n\r\nEssentially, I'm looking for a piece of documentation with code equivalency for these sort of examples.\r\n\r\n### Correct links\r\n\r\n***Not applicable***\r\n\r\n### Parameters defined\r\n\r\n***Not applicable***\r\n\r\n### Returns defined\r\n\r\n***Not applicable***\r\n\r\n### Raises listed and defined\r\n\r\n***Not applicable***\r\n\r\n### Usage example\r\n\r\n***Not applicable***\r\n\r\n### Request visuals, if applicable\r\n\r\n***Not applicable***\r\n\r\n### Submit a pull request?\r\n\r\nI can't submit a pull request because of the lack of documentation on the issue at hand. I could, however, submit a pull request to resolve the problem once I know how to resolve the problem.\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/rgb_to_yuv?version=nightly\r\n\r\n## Description of issue (what needs changing):\r\n(1) The example image `x` has pixel values which are not in the range of [0,1]. So, it can't be fed to `rgb_to_yuv` directly without scaling it.\r\n(2) Users of the API need example which is close to practical scenario. In this case, nobody wants to see the values changed by the function but they want correct implementation and pre-processing example.\r\n\r\n### Submit a pull request? \r\nYes\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://github.com/tensorflow/models/tree/master/official/vision/detection\r\n\r\n## Description of issue (what needs changing):\r\n\r\nHi,\r\nIn the \"Train a vanilla ResNet-50 based RetinaNet.\" it is said to use the \"path to the pre-trained Resnet-50 checkpoint\". There is no link to any pre-trained model. \r\n![image](https://user-images.githubusercontent.com/54512903/76514447-2f331d00-6458-11ea-8d63-dd474964f22c.png)\r\n\r\nI tried to use the resnet-50 which is in https://github.com/tensorflow/models/tree/master/official/vision/image_classification because it was likely to be a correct implementation as an official one. \r\n![image](https://user-images.githubusercontent.com/54512903/76515131-5b02d280-6459-11ea-973f-aabff6078c2d.png)\r\nUnfortunately, when I use :\r\npython main.py --strategy_type=one_device --num_gpus=1 --model_dir=\"my_models\" --mode=train --config_file=\"my_retinanet.yaml\"\r\n\r\nWith this yaml : \r\ntype: 'retinanet'\r\ntrain: \r\n  checkpoint:\r\n    path: pretrained_model\\home\\hongkuny\\hongkuny_keras_resnet50_gpu_8_fp32_eager_graph_cfit\\checkpoints\r\n    prefix: resnet50\\\r\n  train_file_pattern: tfrecords\\train.record\r\neval:\r\n  eval_file_pattern: tfrecords\\test.record\r\n\r\nI got nothing load because the weight seems to be wrongly named in this file, so it is not matching.\r\n\r\n![image](https://user-images.githubusercontent.com/54512903/76515306-a4532200-6459-11ea-90d7-b144b6498151.png)\r\n \r\nI also tried some other models from the zoo assuming we might load weights from resnet based objects detection models but I got the same probleme. \r\n\r\nI think we might add a link to a correct checkpoint of a compatible pre-trained model in order to avoid roaming around incompatible models. \r\n\r\nregards, \r\n\r\nSwann"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe module documentation is very terse and only states that it contains \"preprocessing utils\". It does not state the specific purpose.\r\n\r\nIt would be helpful if it defined the intended use of the `tf.keras.preprocessing` module (_e.g._ to clean up or transform tf.data.Datasets before they are fed to the model).\r\n\r\nAlso, since the `tf.feature_column` module has similar functionality, it would be nice to describe when to use one or the other, or how they are intended to be used together."},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\r\n\r\n## Description of issue (what needs changing):\r\nThis method does not compute the precision *on average* when top_k is set. This could lead to bad evaluations, especially when the sample_weight is set and used as counts.\r\n\r\n### Clear description\r\n\r\nTo see the issue, it's enough to run \r\n```\r\nm = tf.keras.metrics.Precision(top_k=2)\r\nm.update_state([0, 0, 1, 1], [1, 1, 1., 1.])\r\n\r\nprint('Final result: ', m.result().numpy()) # Returns 0 but should return 0.5\r\n```\r\nIt always computes the precision according to the given order and returns 0. However it should return 0.5 if it's on average.\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/datasets/overview\r\n\r\n## Description of issue (what needs changing):\r\n\r\n \ud83d\ude00I completed step 1 and  went to \"https://www.tensorflow.org/datasets/overview\" to get started with TFDS. I launched the code lab to continue with the overview. The code lab is a great option to easily run python and tensorflow! \r\n\r\n\ud83d\ude11- I completed the first command to install tensorflow and tensorflow-datasets\r\n![image](https://user-images.githubusercontent.com/6283150/75946296-9433b500-5e51-11ea-921e-f44a6fc1573e.png)\r\n\r\nThe download ran but it was not clear which version of tensorflow was downloaded. The reason I was confused and wanted to know which version was installed is the disclaimer above states version >=1.15 is required.\r\n\ud83d\ude11- In the second command, I received an error message after running the python script.\r\n![image](https://user-images.githubusercontent.com/6283150/75946322-a44b9480-5e51-11ea-8874-51a4863093ae.png)\r\n\r\nI was not clear if this was just a warning message, or an error due to my current tensorflow version. \r\n\r\n\ud83d\ude00Step 2 was delightful!\r\n![image](https://user-images.githubusercontent.com/6283150/75946336-aca3cf80-5e51-11ea-949a-0ca03348d768.png)\r\n\r\nAdding in a disclaimer to include citations is great! \ud83d\ude11However, why is it after the download step? This seems out of place and disrupts the developer workflow. \r\n\r\nNext was step 3 to initiate eager execution.\r\n\ud83d\ude11Without a baseline on what EE is, I felt required to read the eager execution page before I could move forward. It\u2019s frustrating when a developer guide links out to other documentation, or I feel compelled to read the other pages, because it causes disruption in grasping one concept at a time. This frustration can be a \"drop off\" point for developers trying to onboard Tensorflow.\r\n\r\n![image](https://user-images.githubusercontent.com/6283150/75946528-35227000-5e52-11ea-9312-09405b8e3043.png)\r\n\r\nEnable_V2_Behavior is the command run after asking the user to enable eager execution. Why is that? (After reading the eager execution documentation this was clear, but it took time to dig for this info).\r\n\r\n\ud83d\ude21Step 5 understanding what the tf \"load\" function does is frustrating.\r\n![image](https://user-images.githubusercontent.com/6283150/75946560-44092280-5e52-11ea-85be-ded98f6365c3.png)\r\n\r\n\ud83d\ude21I\u2019m strongly encouraged to read the official TensorFlow guide which is over 30 pages of material. I am 5 steps down this getting started guide, and then sent to another page that will reasonably take 4+ focused hours to additionally complete. This is very frustrating when I am trying to just get an overview of tensorflow datasets.\r\n\r\n\ud83d\ude00Step 5 does a great job here showing an example directly in relation to the above paragraph on versioning! I\u2019m delighted and can move on without needing to read the hyperlink. \r\n![image](https://user-images.githubusercontent.com/6283150/75946588-54b99880-5e52-11ea-8e80-6ec1f06b684e.png)\r\n\r\n\ud83d\ude11Step 7 is confusing since it states we can achieve the same output using the DatasetBuilder, but when you run the test it only outputs the ds_train variable, as opposed to building the graph. \r\n![image](https://user-images.githubusercontent.com/6283150/75946599-5be0a680-5e52-11ea-9cfd-37520abadae3.png)\r\n\r\n### What should happen?\r\n\r\nI have organized answers to the above friction points in the following groupings:\r\n\r\n**Tensorflow Installation**\r\nTo identify which version of Tensorflow I installed I ran a grep command in the code lab to output the following:\r\n\r\n![image](https://user-images.githubusercontent.com/6283150/75946716-bd087a00-5e52-11ea-95d3-c13f68c33df9.png)\r\n\r\nHaving something like this ^ output during installation will help users know what is downloaded and executed in the install command. \r\n\r\n**Eager Execution**\r\n\r\nA simple way to clarify what eager execution is to write a one sentence definition in the guide. For example:\r\n\r\n\"TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later.\"\r\n\r\nThis way I have a quick understanding and don\u2019t feel compelled to read the linked page which is a very long document! :)\r\nI liken this to applications having a tooltip in consumer facing applications. Adding in quick non intrusive explanations to Respect the User keep your users engaged and on the same page. \r\n\r\nAdditionally, adding in the following message to define the command, \"enable_v2_behavior\", would help clarify that Eager Execution is enabled by default tensorflow 2. \r\n\r\n![image](https://user-images.githubusercontent.com/6283150/75946725-c560b500-5e52-11ea-9229-96f8b71dc895.png)\r\n\r\n\r\n**Linking to the official guide for Tensorflow Datasets**\r\n![image](https://user-images.githubusercontent.com/6283150/75946747-d3163a80-5e52-11ea-820b-a43d13bbbe76.png)\r\n\r\nWe need to Respect the User, and provide simplicity when on boarding someone new to TFDS. They have invested time to make it down to the 5th step. If it is imperative the user get a baseline understanding of the Tensorflow API first, then we should put the disclaimer at the top of the overview to go read the guide first before continuing. \r\n\r\nIf it is not necessary, then we should summarize the API guide into 3-5 concise pillars of information that is required for the user to understand the rest of the overview. When the user completes the overview, we can encourage them to go deeper and read the rest of the guide. Similarly, an analogy is when loading a website you respect the user by building a light-weight modern site. Performant sites lazy load in images when they are needed to improve performance and minimize how much data your user needs to download, we should apply the same principles to information.  \r\n\r\n**DatasetBuilder**\r\nWhen introducing in the DatasetBuilder we should place this information right after Step 5 (calling .load), to show the two ways to load in datasets side by side. This way the user does not need to scroll back up the documentation and read before Step 6 (plotting the dataset). \r\n\r\n\r\n\r\n"},
{"text": "First of all, you have a great website to understand the Tensorflow library. I am a newbie. And want to understand the point is: I go through the examples. Except for basic image classification, there is no example : how to feed my own data wich has no label. And I want to get predictions of my own data. example :   https://stackoverflow.com/questions/60389558/how-to-feed-my-own-data-and-evaluate-at-tf-text-classification "},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/get\r\n\r\n## Description of issue (what needs changing):\r\nCurrently there is no documentation at all.\r\nIts fairly straightforward to use by inputting a string denoting default class name:\r\nex:\r\n```\r\nidentifier = \"categorical_crossentropy\"\r\ntf.keras.losses.get(identifier)\r\n```\r\n\r\nHowever, I am having issues with dictionary objects:\r\nex:\r\n```\r\nidentifier = {\"class_name\":\"categorical_crossentropy\",\"config\":{\"from_logits\":True}}\r\ntf.keras.losses.get(identifier)\r\n```\r\nReturns:\r\n```\r\nTraceback (most recent call last):\r\n  File \".\\main.py\", line 85, in <module>\r\n    loss = tf.keras.losses.get(jsn)\r\n  File \"C:\\Users\\jopatterson\\Documents\\autoprime-ml\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\", line 1186, in get\r\n    return deserialize(identifier)\r\n  File \"C:\\Users\\jopatterson\\Documents\\autoprime-ml\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\", line 1175, in deserialize\r\n    printable_module_name='loss function')\r\n  File \"C:\\Users\\jopatterson\\Documents\\autoprime-ml\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\", line 315, in deserialize_keras_object\r\n    return cls(**cls_config)\r\nTypeError: categorical_crossentropy() missing 2 required positional arguments: 'y_true' and 'y_pred'\r\n```\r\nI believe it is failing because cls is the already initialized loss function, and it is passing cls_config as its input, rather than using them as parameters during initialization.\r\n\r\n### Clear description\r\n\r\nThis is a very useful method for abstract implementations of loss objects.\r\n\r\n### Correct links\r\n\r\nThis is where the issue is occuring, within the `deserialize_keras_object` function:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils.py#L382\r\n\r\n### Parameters defined\r\n\r\nThere currently is no documentation for this, as `identifier` can be a string, dictionary or callable.\r\n\r\n### Returns defined\r\n\r\nReturns are not defined, but its fairly obvious it returns a loss function.\r\n\r\n### Raises listed and defined\r\n\r\nNo.\r\n\r\n### Usage example\r\n\r\nNo.\r\n\r\n### Request visuals, if applicable\r\n\r\nNo.\r\n\r\n### Submit a pull request?\r\n\r\nI would do this if I had enough knowledge to do so. Unfortunately I only know how it works with `identifier` as a String."},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\r\nhttps://www.tensorflow.org/guide/keras/custom_callback\r\n\r\n## Description of issue (what needs changing):\r\nThe documentation for the keras `Callback` base class contains the following generic statement about the `logs` parameter passed to its methods:\r\n```\r\nThe logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch.\r\n```\r\nand\r\n```\r\nThe logs dict contains the loss value, and all the metrics at the end of a batch or epoch. Example includes the loss and mean absolute error.\r\n```\r\non the Keras custom callbacks page.\r\n\r\nSince python passes objects by reference, the question becomes whether write-access to this logs parameter is allowed and supported. An example use case would be to provide a custom callback that populates the `logs` dictionary with some additional information that than would automatically be displayed in the progress bar and tensorboard, and recorded in history and CSV callbacks. \r\n\r\nTherefore, I think the documentation should clearly state whether \r\n1) Write-access to the `logs` dict is forbidden (in which case it might be worthwhile to pass a non-writeable dict-like type)\r\n2) Write-access to `logs` is allowed, and will not have any side-effects on any other Callback (i.e. each Callback gets an independent copy)\r\n3) The `logs` dict is writable, and changes to it are visible to any further Callback. This would also require to specify in which order the callbacks are processed.\r\n "},
{"text": "In tflite post integer quantization, `converter.representative_dataset` is necessary.\r\n\r\nHowever, the documentation never specifies the order of the fed inputs. Are they ordered by lexicographic order of the names, size of shapes or even random order? When multiple inputs are present, it is totally a guessing game.\r\n\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/README.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\nSInce , PSF has officially stopped it's support for Python2, the documentation needs to be upgraded to Python3.\r\npip2 -> pip3 \r\n### Clear description\r\n\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\nYes, I'll\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/yuv_to_rgb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe tf.image.yuv_to_rgb method specifies a YUV input of shape (H,W,3) and an RGB output of the same shape. It also notes:\r\n\r\n> The output is only well defined if the Y value in images are in [0,1], U and V value are in [-0.5,0.5].\r\n\r\nHowever YUV is natively encoded in HxWx1.5 Bytes, with values ranging from 0-255. Considering that multiple YUV-RGB conversion standards exist, it is unclear what pre-processing steps need to be done by a user who wants to pass YUV inputs to his network (https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion).\r\n\r\n### Usage example\r\n\r\nMore documentation on the proper usage of this method would be highly helpful. Specifically:\r\n\r\n- An example showing how to pre-process a raw YUV image of size HxWx1.5B to the expected shape of (H,W,3) with normalized values Y: [0,1], UV: [-0.5,0.5].\r\n- An example of how one might append this method to an RGB-trained model to enable it to accept YUV inputs during inference. A likely scenario might be exporting a frozen model to an Android device that natively captures in YUV."},
{"text": "## URL(s) with the issue:\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/applications?version=nightly](https://www.tensorflow.org/api_docs/python/tf/keras/applications?version=nightly)\r\n\r\n\r\n## Description of issue (what needs changing):\r\nThe doc corresponding to these two functions must be added in each application model doc.\r\n\r\nAre you planning to also submit a pull request to fix the issue? \r\nYes, Will mention these issue soon in those PRs.\r\n"},
{"text": "**Describe the current behavior**\r\n\r\nI haven't be able to find documentation on if SSL is used during distributed training with `tf.distribute.Strategy` with gRPC.\r\n\r\n**Describe the expected behavior**\r\n\r\nI should be able to easily find this information in the documentation, and if it's supported, then I should easily be able to turn SSL on/off in distributed training for when I prefer security vs performance."},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#define_the_loss_and_gradient_function\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Sequential\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model\r\n## Description of issue (what needs changing):\r\nIt is unclear whether the Sequential class makes use a 'training' flag fed into it during training/inference, as the tutorial above implies. \r\n\r\n### Clear description\r\nWhen building a custom model subclassing from tf.keras.Model, the standard signature for writing the `call` is as follows: `def call(self, inputs, training=None, mask=None):`\r\n\r\nIf my class includes submodels of the form Sequential, I am able to pass this flag forward but I'm unaware whether it's doing anything, as documentation from the class doesn't mention this flag.\r\nLooking at the customization tutorial above, however, the flag is passed into a Sequential model that does not include layers whose behavior change during training/inference. So I don't know if that flag is doing anything.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "For example, in SparseCategoricalAccuracy(), the words in this API does not help to give a clear picture to understand what it is doing. Why not give a formula. A formula, associated with an example, is clear enough for this API."},
{"text": "This is sort of a follow up to #33756. The TF docs have undergone huge improvements over the last couple months. However, one thing I really like about the PyTorch docs which is still (mostly) missing in the TF docs are \"**See also**\"  references.\r\n\r\nA lot of functions have similar or related functionality. Examples include:\r\n\r\n1. `tf.split`, `tf.unstack`\r\n2. `tf.size`, `tf.shape`\r\n3. `tf.repeat`, `tf.concat`, `tf.tile`, `tf.stack`\r\n4. `tf.exp`, `tf.math.log`\r\n5. `tf.keras.layers.MaxPool2D`, `tf.nn.max_pool2d `\r\n6. `tf.ones`, `tf.ones_like` and `tf.zeros`, `tf.zeros_like`\r\n\r\njust to name a few.\r\n\r\nOften people happen to find one and start using it regularly but remain unaware of the others for quite a while. Even if I do know about all of them, I often find myself wanting to compare the signature of similar functions to find the one most suitable to the current use case. In those cases, it takes way too many clicks to get from one to the other(s).\r\n\r\nIn short, would be great if the docs referenced related content. That should help guide people to use the best tool for the job right from the start."},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/Tensor\r\n\r\nThe page has the breadcrumbs `TensorFlow > API > TensorFlow Core v2.1.0 > Python`.\r\n\r\n## Description of issue (what needs changing):\r\n\r\nRunning the first example on the [Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) page results in an error.\r\n\r\n### Clear description\r\n\r\nHere is the outcome of running the first example.\r\n``` py\r\n>>> # Build a dataflow graph.\r\n... c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\r\n>>> d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\r\n>>> e = tf.matmul(c, d)\r\n>>> # Construct a `Session` to execute the graph.\r\n... sess = tf.compat.v1.Session()\r\n>>> # Execute the graph and store the value that `e` represents in `result`.\r\n... result = sess.run(e)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 960, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1108, in _run\r\n    raise RuntimeError('The Session graph is empty.  Add operations to the '\r\nRuntimeError: The Session graph is empty.  Add operations to the graph before calling run().\r\n```\r\n\r\nI understand from [here](https://kodlogs.com/34085/runtimeerror-the-session-graph-is-empty-add-operations-to-the-graph-before-calling-run) that a session is no longer required in tf v2.\r\n\r\nBut the Tensor documentation starts off with multiple session references which appears to now be obsolete or not required.\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThis is a key function that user will implement with they custom layer. Currently it is poorly documented, especially w.r.t the execution context of eager/graph. In TF 2.0, we are advocating eager execution by default. However, the call() body in keras is executed with graph context by default unless configured otherwise. It will raise error if user try to add print/debug related to items into the call body, eg print(eager_tensor.numpy()), etc.\r\n\r\nSome related question raised in  https://github.com/tensorflow/tensorflow/issues/27519.\r\n\r\n### Correct links\r\n\r\nYes\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nYes\r\n\r\n### Usage example\r\n\r\nNo\r\n\r\n### Request visuals, if applicable\r\n\r\nNo\r\n\r\n### Submit a pull request?\r\nNo\r\n"},
{"text": "when I used the tensorflow-gpu=2.1.0 to run in keras-python3.6 env, it said:\r\n\r\nUsing TensorFlow backend.\r\n2020-02-13 21:47:27.592762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\nTraceback (most recent call last):\r\n  File \"i:/Git_Lip/XWLip_lite/DC_kares_LipReading_P70_R18/code/network.py\", line 574, in <module>\r\n    models=Lip_net(**params)\r\n  File \"i:/Git_Lip/XWLip_lite/DC_kares_LipReading_P70_R18/code/network.py\", line 367, in Lip_net\r\n    input_data = Input(name='the_input', shape=(24,112,112,3), dtype='float32')\r\n  File \"D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\keras\\engine\\input_layer.py\", line 178, in Input\r\n    input_tensor=tensor)\r\n  File \"D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\keras\\engine\\input_layer.py\", line 87, in __init__\r\n    name=self.name)\r\n  File \"D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 517, in placeholder\r\n    x = tf.placeholder(dtype, shape=shape, name=name)\r\nAttributeError: module 'tensorflow' has no attribute 'placeholder'\r\n\r\nhow can I  sovle the problem?Thanks for replying."},
{"text": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/migrate#saved_models_compatibility\r\n\r\n## Description of issue (what needs changing):\r\nThe sentence below is unclear in what it means. I think there might be an extra word.\r\n\r\n\"TensorFlow 2.0 saved_models even load work in TensorFlow 1.x if all the ops are supported.\"\r\n\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n\r\n\r\n\r\n## Description of issue (what needs changing):\r\nWe need a new documentation to interact with astronomy.\r\n### Clear description\r\n\r\nLike we think astronomical animation is good for research. We need a new documentaion how to interact with Phoebe or with other by Tensorflow .\r\n\r\n\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/function?version=stable \r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\nOriginal:\r\n> It also restricts the dhape and datatype of Tensors that can be used:\r\n\r\nSo, I think it should be updated as \r\n\r\nIt also restricts the **shape** and datatype of Tensors that can be used:\r\n\r\n### Clear description\r\n\r\nFind a typo.\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c05_exercise_rock_paper_scissors.ipynb\r\n---------------------------------------------------------------\r\n![Screenshot from 2020-01-16 18-37-30](https://user-images.githubusercontent.com/29497701/72527695-5d004900-388f-11ea-84f8-57ed0c12c915.png)\r\n----------------------------------------------------------------\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI think this exercise doesn't make use of cats_vs_dogs dataset, right??\r\n\r\n### Clear description\r\n\r\nIn place of `cats_vs_dogs` dataset, `rock_paper_scissors` dataset should be mentioned.\r\n\r\n### Submit a pull request?\r\n\r\nYes, shortly"},
{"text": "## URL(s) with the issue: \r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/diag_part?version=nightly\r\n\r\n## Description of issue (what needs changing):\r\nThe `diag_part` documentation contains old examples of non-existing APIs, it uses `tf.matrix_diag_part` in the examples which does not exist.\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/datasets/api_docs/python/tfds/load\r\n\r\n## Description of issue (what needs changing):\r\nAdd a warning that `tfds.load()` can not be used for the users own Datasets, i.e. that he creates himself. To a new user trying to to load a Dataset from a set of files it is not obvious that this method is only for pre-made, immutable Datasets.\r\n\r\nAlthough it does say\r\n> Loads the named dataset into a tf.data.Dataset.\r\n\r\ni initially interpreted it such that my own Dataset can be assigned a name. \r\n\r\nI was looking for a way to split a Dataset into train and validation subsets and stumbled upon this documentation. I was redirected from https://www.tensorflow.org/datasets/splits which comes up as one of the most prominent search results when searching for \"tensorflow Dataset splits\" .\r\n\r\n## Result\r\nA user who visits https://www.tensorflow.org/datasets/api_docs/python/tfds/load will not spend 1 h of trying to understand all the documentation but will immediately realize that this is only for immutable pre-made Datasets."},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/d631c0545dac90c6390da76ed8df7c4f6a2a25bc/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c01_common_patterns.ipynb#L307\r\n\r\n`def white_noise(time, noise_level=1, seed=None):`\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI think, we should add explanation of `seed` parameter here since it's quite an important one.\r\n\r\n### Clear description\r\n\r\nSome explanation about how `seed` affects generation of random numbers every time along with links for reference can be added.\r\n\r\n### Submit a pull request?\r\n\r\nYes"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/gradients?version=stable\r\n\r\n## Description of issue (what needs changing):\r\nIt's unclear how many list items are returned from `tf.gradients`.\r\n\r\nThe second paragraph states that \"It returns a list of Tensor of length `len(xs)` where each tensor is the `sum(dy/dx)` for y in `ys`.\" The \"Returns\" section says, \"A list of `sum(dy/dx)` for each x in `xs`.\"\r\n\r\nSo... which one is it? `sum(dy/dx)` for x in `xs` or `sum(dy/dx)` for y in `ys`? Besides the inconsistency, the summation notation in this documentation is ambiguous. When it says \"`sum(dy/dx)` for x in `xs`\" does that mean `dy/dx` is summed over the `ys` axis and there is one element produced for each `xs` or the other way around?\r\n\r\nA clarifying example would help and a statement along the lines of \"returns a list of <whatever> with as many elements as `xs`\" (or `ys` -- I don't know).\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable\r\n\r\n## Description of issue (what needs changing):\r\nIn the `validation_data` part of `Model.fit()`, the third alternative reads\r\n> dataset For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` must be provided.\r\n\r\nI feel a link break should be inserted after \"dataset\"."},
{"text": "## Description of issue (what needs changing):\r\n\r\nDo we still need the `steps_per_epochs` parameter while fitting the model to training set?\r\nIn the tensorflow tutorial(which is very similar to the MNIST tutorial of Intro to Deep Learning course ), there is no such parameter...\r\n\r\n## URL(s) with the issue:\r\n\r\nUdacity Course Notebook : https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l03c01_classifying_images_of_clothing.ipynb#scrollTo=S5Uhzt6vVIB2\r\n\r\n-----------------------------------------------------------------\r\n![Screenshot from 2020-01-01 23-27-10](https://user-images.githubusercontent.com/29497701/71644446-78dfe880-2cee-11ea-9da7-5c033d8a0592.png)\r\n-----------------------------------------------------------------\r\n\r\nTensorflow Tutorial : \r\nhttps://www.tensorflow.org/tutorials/keras/classification/\r\n\r\n-----------------------------------------------------------------\r\n![Screenshot from 2020-01-01 23-26-50](https://user-images.githubusercontent.com/29497701/71644452-93b25d00-2cee-11ea-97af-70d0ece074d9.png)\r\n-----------------------------------------------------------------\r\n\r\n### Parameters defined\r\n\r\n`steps_per_epoch` parameter in `model.fit` should be removed??\r\n\r\n### Submit a pull request?\r\n\r\nI'll submit a PR right away if this issue is relevant..."},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/generative/dcgan\r\n\r\n## Description of issue (what needs changing):\r\n\r\nhttps://colab.research.google.com/gist/MokkeMeguru/614e16d83d16f1eb70b5f3b73c7d070b/batchnormalization_debug.ipynb\r\n\r\nIn you tutorial, BatchNormalization will be Actnormalization in Glow(https://arxiv.org/abs/1807.03039)\r\n\r\n### Clear description\r\n\r\nWe need Correct BathNormalization\r\n\r\n### Usage example\r\n\r\nWe should input the shape when   BatchNormalization is initialized"},
{"text": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/applications\r\n\r\n## Description of issue (what needs changing):\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: GTX 1080Ti / 11175MiB\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nHi authors and developers,\r\n\r\nI noticed that tensorflow doesn't provide a clear document explain how to use pre-trained model.\r\n\r\nSo, I wrote a benchmark which showed the accuracy of pre-trained model with applying imageNet' validation set.\r\n\r\nThe following is the result:\r\n\r\n```\r\n[Testing][pixel vales are from (0,255)][model:ResNet50] - loss: 2.711 - accuracy: 0.457\r\n[Testing][pixel vales are from (0,255)][model:DenseNet121] - loss: 39.000 - accuracy: 0.006\r\n[Testing][pixel vales are from (0,255)][model:MobileNetV2] - loss: 9.979 - accuracy: 0.003\r\n\r\n[Testing][pixel vales are from (0,1)][model:ResNet50] - loss: 8.535 - accuracy: 0.001\r\n[Testing][pixel vales are from (0,1)][model:DenseNet121] - loss: 1.895 - acc: 0.599\r\n[Testing][pixel vales are from (0,1)][model:MobileNetV2] - loss: 2.283 - accuracy: 0.523\r\n\r\n[Testing][pixel vales are normalized from (-1,1)][model:ResNet50] - loss: 8.313 - acc: 0.001\r\n[Testing][pixel vales are normalized from (-1,1)][model:DenseNet121] - loss: 1.896 - acc: 0.599\r\n[Testing][pixel vales are normalized from (-1,1)][model:MobileNetV2] - loss: 2.287 - acc: 0.524\r\n\r\n```\r\n\r\nFirst, we can see the accuracy is not comparable with the original result(Top-1 accuracy is 70% up).\r\n\r\nI thought that this issue is I'm not sure which crop and pad method is applied in the original result.\r\n\r\nTherefore, I defined a custom function `CenterCrop` to fit the model's input size.\r\n\r\nBut, we can skip this issues there.\r\n\r\nWhat I want to mention is normalization issue.\r\n\r\nIf I don't apply any normalization(run_aug=1 in code), pixel's values are defined in **(0, 255)**.\r\n\r\nAll models' accuracy are near 0.001, except for resNet50 which achieves a meaningful accuracy.\r\n\r\nIf I do normalization(run_aug=2 in code), pixel's values are defined in **(0, 1)**.\r\n\r\nThis time, DenseNet121 and MobileNetV2 have a meaningful accuracy.\r\n\r\nIf I do standard normalization(run_aug=3 in code), pixel's values are defined in **(-1, 1)**.\r\n\r\nThe results are similar to previous case. But I'm sure why those two cases have same accuracy.\r\n\r\nThose behavior let me confused.\r\n\r\nBefore applying pre-trained model, I have to which normalization method should be applied.\r\n\r\nAfter reading the source code, I found that those applications are import from `keras_application` in `tensorflow`.\r\n\r\n[keras-applications](https://github.com/keras-team/keras-applications)\r\n\r\n[weight download](https://github.com/fchollet/deep-learning-models)\r\n\r\n---\r\n\r\nI didn't test other models, such as `ResNet50V2`, `InceptionV3` and `Xception` because their input size are `299` instead of `244` and this is a time consuming task.\r\n\r\nHowever, anyone can modify the test case and do the benchmark.\r\n\r\n---\r\n\r\nBecause of licence issue for ImageNet, I can't provide imagenet in public.\r\n\r\nBut the following is the minimal test case:\r\n\r\n```python\r\n# pip install tensorflow-gpu==1.14.0\r\n# pip pandas\r\n#%%\r\nimport time\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\nfrom glob import glob\r\n\r\n#%%\r\n# input image dimensions\r\nimg_h = 224\r\nimg_w = 224\r\nchannels = 3\r\n\r\n# information for dataset\r\ndataset_path = \"dataset-imagenet/\"\r\nnum_classes = 1000\r\nnum_testing = 50000\r\n\r\n#%%\r\nclass DataGenerator:\r\n\r\n    def __init__(self, dataframe, batch_size, run_aug = True):\r\n\r\n        self.total_len  = len(dataframe.index)\r\n        self.batch_size = batch_size\r\n        self.run_aug = run_aug\r\n        self.dataframe  = dataframe\r\n        self.on_epoch_end()\r\n\r\n    def __build_pipeline(self, file_path, labelY):\r\n\r\n        # mapping function in tf\r\n        def preprocess_fn(file_path, labelY):\r\n\r\n            def fn_x(img_array):\r\n\r\n                img_array = img_array.numpy()\r\n\r\n                if self.run_aug == 1:\r\n                    # image's range is [0,255]\r\n                    image = img_array\r\n\r\n                if self.run_aug >= 2:\r\n                    # image's range is [0,1]\r\n                    image = img_array / 255.0\r\n\r\n                if self.run_aug == 3:\r\n                    # std normalization\r\n                    image[0,:,:] -= 0.485\r\n                    image[1,:,:] -= 0.456\r\n                    image[2,:,:] -= 0.406\r\n                    image[0,:,:] /= 0.229\r\n                    image[1,:,:] /= 0.224\r\n                    image[2,:,:] /= 0.225\r\n\r\n                return image\r\n\r\n            def fn_y(label):\r\n                return tf.keras.utils.to_categorical(label , num_classes)\r\n\r\n            # read image from files\r\n            image = tf.io.read_file(file_path)\r\n            image = tf.image.decode_image(image, channels=channels)\r\n            aug_size = 256\r\n            imageX = tf.compat.v1.image.resize_image_with_pad(image, aug_size, aug_size)\r\n            imageX = tf.image.resize_with_crop_or_pad(image, img_h, img_w)\r\n\r\n            # do normalizarion\r\n            [imageX] = tf.py_function(fn_x, [imageX], [tf.float32])\r\n            imageX.set_shape([img_h, img_w, channels])\r\n            imageX = tf.image.random_flip_left_right(imageX)\r\n\r\n            [labelY] = tf.py_function(fn_y, [labelY], [tf.float32])\r\n            labelY.set_shape([num_classes])\r\n\r\n            return imageX, labelY\r\n\r\n        dataset = tf.data.Dataset.from_tensor_slices( (file_path, labelY) )\r\n        dataset = dataset.shuffle(batch_size * 8)\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        dataset = dataset.batch(self.batch_size)\r\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n        self.dataset   = dataset\r\n\r\n    def  __len__(self):\r\n\r\n        return self.total_len // self.batch_size\r\n\r\n    def on_epoch_end(self):\r\n\r\n        cleanX = np.array(self.dataframe[\"File\"])\r\n        totalY = np.array(self.dataframe[\"One-hot\"])\r\n\r\n        # run permutation\r\n        rand_idx = np.random.permutation(self.total_len)\r\n        cleanX = cleanX[rand_idx]\r\n        totalY = totalY[rand_idx]\r\n\r\n        self.__build_pipeline(cleanX, totalY)\r\n\r\n#%%\r\ndef build_clf(model_name):\r\n\r\n    if model_name == \"ResNet50\":\r\n        clf_model = tf.keras.applications.ResNet50(include_top=True, pooling='max', weights='imagenet')\r\n\r\n    if model_name == \"DenseNet121\":\r\n        clf_model = tf.keras.applications.DenseNet121(include_top=True, pooling='max', weights='imagenet')\r\n\r\n    if model_name == \"MobileNetV2\":\r\n        clf_model = tf.keras.applications.MobileNetV2(include_top=True, pooling='max', weights='imagenet')\r\n\r\n    if model_name == \"InceptionV3\":\r\n        clf_model = tf.keras.applications.InceptionV3(include_top=True, weights='imagenet')\r\n\r\n\r\n    clf_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\n    return clf_model\r\n\r\n#%%\r\ndef list_testing_data(classes, file_path, onehot_map):\r\n\r\n    try:\r\n        testing_data = pd.read_pickle('imagenet_test_list.pkl')\r\n        print('[Successful] Testing_data loaded from pickle ...')\r\n    except:\r\n        testing_image_info = []\r\n        for iter_class in classes:\r\n            files = glob(os.path.join(file_path, iter_class, '*.JPEG'))\r\n            for iter_img in files:\r\n                data_info = [iter_img, iter_class]\r\n                testing_image_info.append(data_info)\r\n\r\n        testing_data = pd.DataFrame(testing_image_info, columns=['File', 'Class'])\r\n        testing_data[\"One-hot\"] = testing_data[\"Class\"].replace(onehot_map, inplace=False)\r\n\r\n        testing_data.to_pickle('imagenet_test_list.pkl')\r\n\r\n    assert(testing_data.shape[0] == num_testing, \"[Fatal] Mismatched total length of testing data\")\r\n    return testing_data\r\n\r\n#%%\r\nif __name__ == '__main__':\r\n\r\n    # set GPU\r\n    import os\r\n    if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    # Hyperparameters\r\n    batch_size = 100\r\n    epochs = 5\r\n\r\n    # load one-hot labels\r\n    file_path = dataset_path + 'val'\r\n    classes = os.listdir(file_path)\r\n    list_class = sorted( list( set(classes) ) )\r\n    onehot_map = dict( zip( list_class, list(range(0, num_classes)) ))\r\n\r\n    # load list of validation data, those data should be considered as testing data\r\n    testing_data = list_testing_data(classes, file_path, onehot_map)\r\n\r\n    # build data generator\r\n    gen_type1 = DataGenerator(testing_data, batch_size, run_aug=1)\r\n    gen_type2 = DataGenerator(testing_data, batch_size, run_aug=2)\r\n    gen_type3 = DataGenerator(testing_data, batch_size, run_aug=3)\r\n    gen_list = [gen_type1, gen_type2, gen_type3]\r\n\r\n    # build model\r\n    model_list = [\"ResNet50\", \"DenseNet121\", \"MobileNetV2\"]\r\n    \r\n    # print result for type1\r\n    test_gen = gen_type1\r\n    for model_name in model_list:\r\n        model = build_clf(model_name)\r\n        meta_string = '[Testing][pixel vales are from (0,255)][model:{:s}] '.format(model_name)\r\n        prefix_string = ''\r\n        output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())\r\n        for ii in range( len( model.metrics_names) ):\r\n            meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])\r\n\r\n        print(meta_string)\r\n\r\n    # print result for type2\r\n    test_gen = gen_type2\r\n    for model_name in model_list:\r\n        model = build_clf(model_name)\r\n        meta_string = '[Testing][pixel vales are from (0,1)][model:{:s}] '.format(model_name)\r\n        prefix_string = ''\r\n        output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())\r\n        for ii in range( len( model.metrics_names) ):\r\n            meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])\r\n\r\n        print(meta_string)\r\n\r\n    # print result for type3\r\n    test_gen = gen_type3\r\n    for model_name in model_list:\r\n        model = build_clf(model_name)\r\n        meta_string = '[Testing][pixel vales are normalized from (-1,1)][model:{:s}] '.format(model_name)\r\n        prefix_string = ''\r\n        output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())\r\n        for ii in range( len( model.metrics_names) ):\r\n            meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])\r\n\r\n        print(meta_string)\r\n```\r\n\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction?version=stable\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI intend to build up a custom loss function as follows:\r\n\r\n\r\n`\tfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\timport functools\r\n\r\n\timport numpy as np\r\n\timport tensorflow as tf\r\n\r\n\r\n\tclass GeneralDiceLoss(tf.keras.losses.Loss):\r\n\t\tdef __init__(self, reduction=tf.keras.losses.Reduction.AUTO, name='GeneralDiceLoss'):\r\n\t\t\tsuper().__init__(reduction=reduction, name=name)\r\n\t\t\tself.epsilon = 1e-16 \r\n\t\t\r\n\t\t\r\n\t\tdef get_config(self):\r\n\t\t\tconfig = super(GeneralDiceLoss, self).get_config()\r\n\t\t\treturn config\r\n\t\t\r\n\t\tdef call(self, yPred, yTrue):\r\n\t\t\t#yTrue =tf.dtypes.cast(yTrue, dtype=yPred.dtype)\r\n\t\t\t# Dot product yPred and yTrue and sum them up for each datum and class\r\n\t\t\tcrossProd=tf.multiply(yPred, yTrue)\r\n\t\t\tcrossProdSum=tf.math.reduce_sum(crossProd, axis=np.arange(2, yTrue.ndim))\r\n\t\t\t# Calculate weight for each datum and class \r\n\t\t\tweight = tf.math.reduce_sum(yTrue, axis=np.arange(2, yTrue.ndim))\r\n\t\t\tweight = tf.math.divide(1, tf.math.square(weight)+self.epsilon)\r\n\t\t\t# Weighted sum over classes\r\n\t\t\tnumerator = 2*tf.math.reduce_sum(tf.multiply(crossProdSum, weight), axis=1)\r\n\t\t\t# Saquared summation \r\n\t\t\tyySum = tf.math.reduce_sum(tf.math.square(yPred) + tf.math.square(yTrue), axis=np.arange(2, yTrue.ndim))\r\n\t\t\t# Weighted sum over classes\r\n\t\t\tdenominator = tf.math.reduce_sum(tf.multiply(weight, yySum), axis=1)\r\n\t\t\tloss = 1 - tf.math.divide(numerator, denominator+self.epsilon)\r\n\t\t\t#loss = tf.math.reduce_mean(1 - tf.math.divide(numerator, denominator+self.epsilon))\r\n\t\t\t\r\n\t\t\treturn loss\r\n`\r\n\r\nThen I create variables to have it test\r\n`\r\n\r\n\tGeneralDiceLoss()\r\n\tyPred = tf.random.uniform(shape=(16, 3, 4, 4, 4))\r\n\tyTrue = tf.round(tf.random.uniform(shape=(16, 3, 4, 4, 4)))\r\n\r\n\tloss=GeneralDiceLoss(yPred, yTrue)\r\n`\r\nBut I got an error\r\n`\r\n\r\n\t  File \"...\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\r\n\t\treturn ops.EagerTensor(value, ctx.device_name, dtype)\r\n\r\n\tTypeError: Cannot convert 'auto' to EagerTensor of dtype float\r\n`\r\n\r\nIn the doc above, \r\n1) there is NO clear indication or warning about conversion issue, not to mention there is NO dtype conversion in my code at all. \r\n2) there is NO clear example indicating which option, AUTO or SUM_OVER_BATCH_SIZE, should be adopted in one's minbatch size is greater than 1. In my case, assume my batch is 16 as exhibted in yPred and yTrue above, shall I use\r\n\r\n`\r\n\t\t\tloss = 1 - tf.math.divide(numerator, denominator+self.epsilon)\r\n`\r\nor \r\n`\r\n\t\t\tloss = tf.math.reduce_mean(1 - tf.math.divide(numerator, denominator+self.epsilon))\r\n`\r\nAnd for which option?\r\n\r\nBuilding up a custom layer/loss function is already a tough task for many practitioners, so could the doc provide more detailed explanations and examples so as to make users' life a little bit easier? Many thanks."},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/keras/classification/\r\n\r\n## Description of issue (what needs changing):\r\n\r\nUnder \"Train the model\" in \"Build the model\", the accuracy of the model on training data after 10 epochs is 0.91(91%) while it is mentioned as 0.88(88%).\r\n\r\n### Clear description\r\n\r\nSince, it is already mentioned in the tutorials that the model overfits the training data, thus the accuracy on training data should be more than that on testing data(88.3%).\r\n### Submit a pull request?\r\n\r\nIf this issue is alright, I'll be glad the submit a PR right away...\r\nThanks for the help!\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/audio\r\n\r\n## Description of issue (what needs changing):\r\nCurrently, there are no usage examples for tf.audio APIs , which makes it difficult for new users to implement the same.\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n**Audio is an area not really explored in machine learning to extent image and text has. While TensorFlow does provide a good amount of documentation for the general Args and Returns of the various functions under tf.audio, since most new users will have very little experience with audio as compared to tf.image**\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n**Yes**\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n**Yes**\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n**Yes**\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n**No**\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n**No**\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n**Formatted code blocks are present, which are satisfactory.**\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n**Yes, I think I can provide a detailed usage example.**\r\n  "},
{"text": "We've had feedback from multiple developers that it's hard to figure out how to calculate the right  int8 values for quantized inputs, and understand what int8 values mean as outputs.\r\n\r\nFor example, when feeding an image to uint8 quantized inputs, the values can be left as in their source 0 to 255 range. For int8 inputs, the developer will typically need to subtract 128 from each value, but this knowledge (and how the offset value is calculated) is not documented. In the same way, users will need to map the -128 to 127 output values to the actual real number range of their outputs, but this process is unclear.\r\n\r\nTagging the @tensorflow/micro team."},
{"text": "## URL with the issue: https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss\r\n\r\n## Description of issue:\r\nThere's no example provided for using this loss and I cannot make it work.\r\nFollowing the parameters definitions I created this toy example in tf2.0.0:\r\n\r\n```\r\nimport functools\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import (Input, Conv2D, Lambda)\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n# INPUTS\r\ninputs = Input(shape=[128, 64, 1], batch_size=32)    # [frames, num_labels, channels]\r\nlabels = Input(shape=[128], batch_size=32,  dtype=tf.int32)\r\nlabel_length = tf.constant(np.ones((32)), dtype=tf.int32)\r\nlogit_length = tf.constant(np.ones((32)),  dtype=tf.int32)\r\n# MODEL\r\nx = Conv2D(1, kernel_size=(5, 5),  padding='same')(inputs)\r\nlogits = Lambda(lambda z: tf.squeeze(z, [-1]))(x)\r\nmodel = Model(inputs, logits)\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=tf.nn.ctc_loss(\r\n    labels=labels, logits=logits, label_length=label_length,\r\n    logit_length=logit_length, logits_time_major=False,\r\n    blank_index=-1\r\n))\r\n```\r\n\r\nwhich rises: \r\n\r\n```\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    469                 dtype=dtype if dtype else None,\r\n    470                 preferred_dtype=default_dtype,\r\n--> 471                 as_ref=input_arg.is_ref)\r\n    472             if input_arg.number_attr and len(\r\n    473                 set(v.dtype.base_dtype for v in values)) > 1:\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1363             as_ref=as_ref,\r\n   1364             preferred_dtype=preferred_dtype,\r\n-> 1365             ctx=ctx))\r\n   1366   return ret\r\n   1367\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\r\n   1262     graph = get_default_graph()\r\n   1263     if not graph.building_function:\r\n-> 1264       raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\n   1265                          \"building a function.\")\r\n   1266     return graph.capture(value, name=name)\r\n\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n```\r\n\r\nThen, I tried to use it as a handle:\r\n\r\n```\r\nctc_loss = functools.partial(\r\n    tf.nn.ctc_loss,\r\n    labels,         # labels\r\n    logits,         # logits\r\n    label_length,   # label_length\r\n    logit_length,   # logit_length\r\n    False,          # logits_time_major\r\n    -1,             # blank_index\r\n)\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=ctc_loss)\r\n```\r\n\r\nwhich rises:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-147-2018e8450f34> in <module>\r\n----> 1 model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss)\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    371\r\n    372       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 373       self._compile_weights_loss_and_weighted_metrics()\r\n    374\r\n    375       # Functions for train, test and predict will\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1651       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1652       #                   layer losses.\r\n-> 1653       self.total_loss = self._prepare_total_loss(masks)\r\n   1654\r\n   1655   def _prepare_skip_target_masks(self):\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)\r\n   1732             # differentiate between use case where a custom optimizer\r\n   1733             # expects a vector loss value vs unreduced per-sample loss value.\r\n-> 1734             output_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\r\n   1735             loss_reduction = losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\r\n   1736\r\n\r\nTypeError: ctc_loss_v2() got an unexpected keyword argument 'sample_weight'\r\n```\r\n\r\nThen, I tried to embed it:\r\n\r\n```\r\ndef my_ctc_loss(\r\n    labels, logits, label_length, logit_length, logits_time_major,\r\n    blank_index, sample_weight\r\n):\r\n    return tf.nn.ctc_loss(\r\n        labels=labels, logits=logits, label_length=label_length,\r\n        logit_length=logit_length, logits_time_major=logits_time_major,\r\n        blank_index=blank_index\r\n    )\r\n\r\n\r\nctc_loss_emb = functools.partial(\r\n    my_ctc_loss,\r\n    labels,         # labels\r\n    logits,         # logits\r\n    label_length,   # label_length\r\n    logit_length,   # logit_length\r\n    False,          # logits_time_major\r\n    -1,             # blank_index\r\n    None,           # sample_weight\r\n)\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)\r\n```\r\nwhich rises: \r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-150-f20d10a91540> in <module>\r\n      9     None,           # sample_weight\r\n     10 )\r\n---> 11 model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    371\r\n    372       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 373       self._compile_weights_loss_and_weighted_metrics()\r\n    374\r\n    375       # Functions for train, test and predict will\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1651       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1652       #                   layer losses.\r\n-> 1653       self.total_loss = self._prepare_total_loss(masks)\r\n   1654\r\n   1655   def _prepare_skip_target_masks(self):\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)\r\n   1732             # differentiate between use case where a custom optimizer\r\n   1733             # expects a vector loss value vs unreduced per-sample loss value.\r\n-> 1734             output_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\r\n   1735             loss_reduction = losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\r\n   1736\r\n\r\nTypeError: my_ctc_loss() got multiple values for argument 'sample_weight'\r\n```\r\n\r\n### Usage example: Not provided\r\n\r\nSince it seems that ctc_loss has to be used differently from other losses, it will helpful to have an example that shows how to use it.\r\n\r\n### Raises listed and defined: Not defined\r\n\r\n\r\nThanks!"},
{"text": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nDoc Link:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler\r\nCode Link:\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/callbacks.py#L1311-L1358\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Parameters\r\n\r\nThe next API for `scheduler` parameter of `LearningRateScheduler` takes in 2 parameters, `epoch` and `lr` (learning rate) instead of just `epoch`, this is evident in the `on_epoch_begin` of the `LearningRateScheduler` method.\r\n\r\nThe documentation for this method is still outdated, the docs and the example code still shows the `scheduler` function takes in only `epoch` instead of both `epoch` and `lr`. I think the doc should be updated to reflect the new API.\r\n\r\nProposed change to the doc:\r\n\r\n1) update the description of `scheduler`: \r\n\r\n`\r\nschedule: a function that takes an epoch index as input (integer, indexed from 0) and current learning rate and returns a new learning rate as output (float).\r\n`\r\n(copied from the doc from keras.io)\r\n\r\n2) update the example usage to include a `scheduler` that utilize the current learning rate as well.\r\n\r\nI hope this is helpful! Happy to contribute if needed.\r\n\r\n### Submit a pull request? Yes, if this should be updated.\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "Hi, my name is Rachin Kalakheti and i am a participant of Google Code-in 2019. I felt overwhelmed to know Tensorflow is also one of the organization for this year. So, there was a task to create a notebook tutorial on Data Augmentation using tf.image. I see that currently there is no tutorial regarding the same topic. So, I would like to contribute to the community by adding my tutorial to the  Tensorflow repo. Therefore I am seeking guidance as to discuss this further.\r\nLink to my notebook tutorial: https://colab.research.google.com/drive/1skGIQhwifJY6HWO6ZnbFe4un-VuJ3VW5\r\n\r\nThank you!"},
{"text": "Porting the original website from bootstrap3  to bootstrap4\r\n\r\nSection to change:\r\n- [ ] [_alumni.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_alumni.html)\r\n- [ ] [_events-participate.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_events-participate.html)\r\n- [ ] [_events.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_events.html)\r\n- [ ] [_home.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_home.html)\r\n- [ ] [_intro.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_intro.html)\r\n- [ ] [_open-source.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_open-source.html)\r\n- [ ] [_team.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_team.html)\r\n- [ ] [_webinars.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_webinars.html)\r\n\r\n\r\n"},
{"text": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution Ubuntu Linux 18.04 x64\r\n\r\n- TensorFlow installed from (source or binary): Installed from Anaconda\r\n\r\n- TensorFlow version (use command below):\r\n`python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nunknown 2.0.0\r\nI am using TF 2.0.0.\r\n- Python version: python 3.7.4\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: Cuda release 10.1, V10.1.168; cudnn 7.6.0\r\n- GPU model and memory: Nvidia GTX 1080 11GB.\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nThis is both a code issue and a documentation problem--but mostly a code problem. \r\nI was looking at the tutorial(https://www.tensorflow.org/tutorials/structured_data/feature_columns#numeric_columns) and saw that the tutorial itself is generating warnings. So that suggests some problems in the code as well as the tutorial. \r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect the tutorial to generate no warnings--and hence demonstrate proper code functionality. As it is, it is not clear whether the warnings are generated from a bug in the code, or from spurious warnings, etc. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\nprint(len(train), 'train examples')\r\nprint(len(val), 'validation examples')\r\nprint(len(test), 'test examples')\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nbatch_size = 5 # A small batch sized is used for demonstration purposes\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\n# We will use this batch to demonstrate several types of feature columns\r\nexample_batch = next(iter(train_ds))[0]\r\n\r\n# A utility method to create a feature column\r\n# and to transform a batch of data\r\ndef demo(feature_column):\r\n  feature_layer = layers.DenseFeatures(feature_column)\r\n  print(feature_layer(example_batch).numpy())\r\n\r\nage = feature_column.numeric_column(\"age\")\r\ndemo(age) # <-- SHOULD TRIGGER OR DISPLAY THE WARNING\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nNo other materials provided."},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: N/A\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/install/source\r\n\r\n## Description of issue (what needs changing): Please add Tested build configurations for Tensorflow 1.15 and Tensorflow 1.15-gpu\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\nI cannot find the test build configurations for Tensorflow 1.15. It is useful for someone who is trying to build Tensorflow 1.15. \r\n\r\n\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells\r\n\r\n## Description of issue (what needs changing):\r\n\r\nDocumentation example does not actually use `StackedRNNCells`. There is no example for the class being documented. Ideally there would be both an example of the class and an example showing how the same behaviour would be implemented without the class.\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/community/contribute/docs \r\n\r\n## Description of issue (what needs changing):\r\nDocumentation under \"Interactive notebooks\" needs to be modified to accommodate issues created after direct editing of Jupyter Notebook in Colab.\r\n  \r\n\r\n### Clear description. \r\n\r\nDirect editing (on colab or using VSCode) Jupyter Notebook and committing as mentioned [here](https://www.tensorflow.org/community/contribute/docs) adds additional unintended changes like prettifying and escapes unicode symbols. \r\n\r\nFor example, see here: https://github.com/tensorflow/docs/pull/1238 \r\n\r\nRelated code commit : https://github.com/copperwiring/docs/commit/98f35604617d1ecc93b3dc75ac6ec4ab108536eb \r\n\r\n### Correct links\r\n\r\nYes\r\n\r\n### Parameters defined\r\n\r\nN/A\r\n\r\n### Returns defined\r\n\r\nN/A\r\n\r\n### Raises listed and defined\r\n\r\nNone.\r\n\r\n### Usage example\r\n\r\nIt is useful and needed for any PR request for Jupyter Notebook\r\n\r\n### Request visuals, if applicable\r\n\r\nN/A\r\n\r\n### Submit a pull request?\r\n\r\nI am planning to submit a PR to improve the documentation.\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: \r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tutorials/keras/classification/ \r\n\r\n## Description of issue (what needs changing):\r\nOne of the first step-by-step tutorial which explains a neural network is on Basic Classification [here](https://www.tensorflow.org/tutorials/keras/classification/). However, it can benefit from additional explanations on few terms like overfitting, optimizer etc.  \r\n\r\n### Clear description\r\nWe add one line description for *overfitting*  to help user get a first hand idea of what *overfitting* does. Later, we add an extra line with a link to TensorFlow definition of overfit where user can find more information. The suggested changes  are expected to make it easier for users to get an intuitive understanding of the term.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? Yes\r\n\r\n### Submit a pull request? \r\n\r\nYes.\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "Description of Tensorflow in README.md can be more descriptive."},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nProblem 1:\r\nCould you provides the math model of the rnn?\r\n\r\nProblem 2:   this is not return information about  the method of \r\nget_initial_state(inputs=None, batch_size=None, dtype=None).\r\nand \r\nhow to define own get_initial_state ?\r\nwhat is the connect between   state_size  and get_initial_state\r\nProblem 3, \r\ncould  you write more clear about  arguments  of the methods:  call, build, get_initial_state,\r\nsuch as the   dimension of them.  each dimension is what .\r\n\r\nProblem 4:\r\nis the  cell  defined layer by user ?\r\n\r\nthanks! \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n \r\n\r\n"},
{"text": "I have just rolled through the Getting Started section in tandem with the Substrate intro tutorial. Awesome work on the platform and the docs BTW, it was really fun to get straight in and have a jam.\r\n\r\nSome constrictive feedback:\r\nAs a newcomer to blockchain tech in general, I stumbled on the Transactions / Keyring section. I would suggest putting an even simpler high level primer at the start of the Keyring section to ensure the basic concept of a decentralised identity/wallet is clear. Also a bit more explanation about the demo accounts that are preloaded in the substrate demo node and that they're URI based would be helpful for the uninitiated amongst us.\r\n\r\nAgain, thanks for all the awesome work on this!"},
{"text": "It would be really nice to have a search bar in the documentation! I always struggle to navigate to the right subpage.\r\n\r\nJaco mentioned on Riot that there is a search plugin available for vue-press. Sadly, I have no experience with vue.js, so I can't really create a PR myself."},
{"text": "I think need some example to show submit Extrinsics, such as stake, unstake, nominate, and Voting."},
{"text": "After Events are exposed through the API and docs are generated, add section list with anchors and order the sections and methods alphabetically as done in PR #291 and PR #302 for RPC, Extrinsics, and Storage\r\n\r\n"},
{"text": "In Polkadot-JS API Docs https://github.com/polkadot-js/api/docs, what is the current procedure that is used (both in 'development' and in 'production') for generating the Markdown (.md) and HTML (.html) docs for our Gitbook at https://polkadot.js.org/api/?\r\n\r\nNote that I reviewed comments between @jacogr and @amaurymartiny in https://github.com/polkadot-js/api/pull/159, and created this PR https://github.com/polkadot-js/api/pull/167 to address Issue #154, although I'm not sure if what I've done is correct. So I've come up with the following questions:\r\n\r\n1) How are we generating the HTML .html files in docs/ directory?\r\n\t* If I delete a HTML doc file (i.e. docs/api-provider/classes/httpindex.httpprovider.html) and then run `yarn; yarn run build:htmldoc;`, the Bash Terminal logs says `Documentation generated at /polkadot-js/api/docs/html`, but the file httpindex.httpprovider.html does not exist (isn't regenerated).\r\n\r\n2) How are we generating the Markdown .md files in docs/ directory?\r\n\t* Were the Markdown .md files that are currently there generated originally generated by running `yarn clean && typedoc --theme markdown --out docs/html` (i.e. using the `markdown` instead of the `default` theme)\r\n\t\t* Note: When I previously ran `yarn; yarn run build:htmldoc;`, the Bash Terminal logs also said `To generate markdown please set option --theme markdown`.\r\n\t* Are we currently just \"manually\" modifying the Markdown .md files (instead of generating them with a script)?\r\n\r\n\t* How are we updating the Markdown .md files in docs/ directory automatically after modifying the Typedoc comments in the code comments? (i.e.\r\n\t\t```\r\n\t\t/**\r\n\t\t * @example\r\n\t\t * ```javascript\r\n\t\t...\r\n\t\t```\r\n\r\n3) Should we by using the `gitbook` command at all? https://toolchain.gitbook.com/\r\n\r\n4) Should https://polkadot.js.org/ have a {.api} link to the Github repo and a link to the docs at https://polkadot.js.org/api/?"},
{"text": "The goal is to:\r\n- Revert this change https://github.com/polkadot-js/api/pull/145/commits/16237c44cdf9828dc63fd651609f5d4910b65056\r\n- Get the CI happy again (meaning writing the missing doc)"},
{"text": "... and make sure we have proper docs here\r\n\r\n~~https://github.com/polkadot-js/dev/issues/132~~"},
{"text": "Currently these are not included in the generated documentation, but should be.\r\n\r\n(Related to https://github.com/polkadot-js/api/issues/589 where we add some actual sample of use)"},
{"text": "What is the expected output of passing a dictionary to `Series.transform`? For example:\r\n\r\n    s = pd.Series([1, 2, 3])\r\n    result1 = s.transform({'a': lambda x: x + 1})\r\n    result2 = s.transform({'a': lambda x: x + 1, 'b': lambda x: x + 2})\r\n\r\nThe docs say that `dict of axis labels -> functions` is acceptable, but I can't find any example in the docs where the output is described/shown. Under the hood, `Series.transform` is just calling `Series.aggregate` which produces the following outputs for `result1` and `result2`.\r\n\r\n````\r\n# result1\r\na  0    2\r\n   1    3\r\n   2    4\r\ndtype: int64\r\n\r\n# result2\r\na  0    2\r\n   1    3\r\n   2    4\r\nb  0    3\r\n   1    4\r\n   2    5\r\ndtype: int64\r\n````\r\n\r\n`result1` is deemed acceptable (the length of the result equals the length of the input) and is returned, but `result2` raises; it is not a transformation.\r\n\r\nI am wondering if a better return would be a DataFrame where the keys are the column names ('a' and 'b' in this example)."},
{"text": "- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.\r\n\r\n- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).\r\n\r\n---\r\n\r\n#### Question about pandas\r\n\r\nAlthough extremely common in the industry, \"dummy\" has some unfortunate history. One current use is for substitutes - mannequins, stand-ins, etc. This use grew from its original definition, \"mute person\". Mute people are not substitutes or stand-ins and I would prefer Pandas to not contribute to this view. There are other words, like \"indicator\", for statistics.\r\n\r\n[Pandas currently uses \"getdummies\" as a function name, with the documentation referencing \"indicator\" as a synonym.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.getdummies.html)\r\n\r\nCitations:\r\n\r\n1. https://www.etymonline.com/word/dummy\r\n2. https://www.etymonline.com/word/dumb\r\n3. https://www.etymonline.com/word/indicator"},
{"text": "#### Location of the documentation\r\n\r\nhttps://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.tomarkdown.html\r\n\r\n\r\n#### Documentation problem\r\n\r\nThe document of `pandas.DataFrame.tomarkdown` is unfriendly because many options are shown as `**kwargs`.\r\nI think these arguments should be documented explicitly like a `tocsv`.\r\n\r\nhttps://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.tocsv.html\r\n\r\n\r\n#### Suggested fix for documentation\r\n\r\nAll arguments supported by `tomarkdown` should be shown at document.\r\nIf don't mind, I would like to tackle this improvement."},
{"text": "Not sure if it was added intentionally, but it's possible to call numpy with the `np` attribute of the pandas module:\r\n```python\r\nimport pandas\r\nx = pandas.np.array([1, 2, 3])\r\n```\r\nWhile this is not documented, I've seen couple of places suggesting this as a \"trick\" to avoid importing numpy directly.\r\n\r\nI personally find this hacky, and I think should be removed. "},
{"text": "This is inspired by the discussion in https://github.com/scikit-learn/enhancementproposals/pull/25.\r\n\r\nNumPy defines an ``array`` protocol that allows developers to implement classes that can be converted to an array by calling ``np.asarray()`` . That makes it easy to have a common interface between libraries and it's heavily used by pandas and sklearn.\r\n\r\nIt would be great to have a similar protocol for converting something to a pandas ``DataFrame``. The goal would be to allow users to pass other data structures to libraries that expect a dataframe, say seaborn, as long as the data structures allow conversion to ``pd.DataFrame``.\r\n\r\nA workaround is for the developer of the new datastructure to provide an ``.asframe`` method, but that creates friction and requires the users to know what data type a particular library or function expects. If instead the developer of the datastructure can declare that conversion to a dataframe is possible, the library author (say seaborn) can request conversion to dataframe in a unified manner.\r\n\r\nThe implementation of this is probably pretty simple as it requires \"only\" a special case in ``pd.DataFrame.init``. The main work is probably in adding it to developer documentation and publicizing it correctly.\r\n\r\ncc @jorisvandenbossche "},
{"text": "I see lots of code that goes like this\r\n\r\n```\r\nIn [17]: df = pd.DataFrame({'A': [1, 2, 3]})                                                                                                                                        \r\n\r\nIn [18]: df                                                                                                                                                                         \r\nOut[18]: \r\n   A\r\n0  1\r\n1  2\r\n2  3\r\n\r\nIn [19]: df.sortvalues('A', ascending=False)                                                                                                                                       \r\nOut[19]: \r\n   A\r\n2  3\r\n1  2\r\n0  1\r\n\r\nIn [20]: df.sortvalues('A', ascending=False).resetindex(drop=True)                                                                                                                \r\nOut[20]: \r\n   A\r\n0  3\r\n1  2\r\n2  1\r\n```\r\n\r\nmight be nice from an API / consistency perspective to add a ``ignoreindex=False|True`` keyword to ``.sortvalues()``, and ``.dropduplicates()`` that does the reset in-line; this would give consistency similar to a ``pd.concat([......], ignoreindex=True)`` operation which users are very familiar\r\n"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nseries = pd.Series(range(10))\r\nshouldbenone = series.rename('newname', inplace=True)\r\nprint(shouldbenone)\r\n```\r\n#### Problem description\r\n\r\nWhen a method is called with the `inplace` argument, the method should:\r\n- transform the series inplace\r\n- **return `None`**\r\n\r\nIn this case, the method does:\r\n- transform the series inplace\r\n- **returns the series transformed (not a copy)**\r\n\r\n#### Expected Output\r\n`None`\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.showversions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.4.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder        : little\r\nLCALL           : None\r\nLANG             : None\r\nLOCALE           : None.None\r\n\r\npandas           : 0.25.1\r\nnumpy            : 1.16.5\r\npytz             : 2019.3\r\ndateutil         : 2.8.0\r\npip              : 19.2.3\r\nsetuptools       : 41.4.0\r\nCython           : 0.29.13\r\npytest           : 5.2.1\r\nhypothesis       : None\r\nsphinx           : 2.2.0\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 1.2.1\r\nlxml.etree       : 4.4.1\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.3\r\nIPython          : 7.8.0\r\npandasdatareader: None\r\nbs4              : 4.8.0\r\nbottleneck       : 1.2.1\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.4.1\r\nmatplotlib       : 3.1.1\r\nnumexpr          : 2.7.0\r\nodfpy            : None\r\nopenpyxl         : 3.0.0\r\npandasgbq       : None\r\npyarrow          : None\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.3.1\r\nsqlalchemy       : 1.3.9\r\ntables           : 3.5.2\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : 1.2.1\r\n\r\n</details>\r\n\r\n"},
{"text": "we should use keyword only arguments for some of our functions that have large numbers of kwargs to make it harder to make mistakes in the calling conventions, a prime example is [readcsv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.readcsv.html?highlight=readcsv#pandas.readcsv].\r\n\r\nlikely we want a signature\r\n\r\n```def readcsv(self, filepathorbuffer, *, .......```\r\n\r\nIOW *all* args, except for the first should be kwargs.\r\n\r\n We could further modify a fair number of functions, so will treat this as a tracking issue."},
{"text": "We defined validatefillvalue for DTA/TDA/PA and I'm thinking it may be worth requiring more generally and using on `ExtensionBlock`.  In particular, `ExtensionBlock.canholdelement` ATM unconditionally returns `True`.  canholdelement isn't well-documented, but my intuition as to what it should mean more or less matches what validatefillvalue means."},
{"text": "Hello,\r\n\r\nWe have a reference API based on [redoc](https://github.com/Redocly/redoc).\r\nhttps://airflow.readthedocs.io/en/latest/stable-rest-api/redoc.html\r\nIt has possibility to add code samples of using the API in different languages thanks to the x-codesamples extension. This will facilitate the use of this API and will also provide ready documentation for these clients. Less duplicate documentation.\r\n![image](https://github.com/Redocly/redoc/raw/master/docs/images/code-samples-demo.gif)\r\n\r\nWe can think about generating code samples automatically using ready-made tools (If possible)\r\nhttps://github.com/ErikWittern/openapi-snippet\r\nhttps://github.com/richardkabiling/openapi-snippet-cli/blob/master/src/index.ts\r\nhttps://github.com/cdwv/oas3-api-snippet-enricher\r\n\r\nBest regards,\r\nKamil Bregu\u0142a\r\n"},
{"text": "**Description**\r\n\r\nWe should prepare documentation for the REST API:\r\n\r\n- [Guide about API authentication (including custom)](https://github.com/apache/airflow/issues/8123)\r\n- [Guide about authorization and permission](https://github.com/apache/airflow/issues/8122)\r\n- [Migration guide from the experimental API to the REST API](https://github.com/apache/airflow/issues/8121)\r\n- [Guide \"How to use REST API\"](https://github.com/apache/airflow/issues/8120)\r\n- [REST API Reference](https://github.com/apache/airflow/issues/8119)\r\n\r\nMore information about the REST API is available:\r\n[AIP-32 - Airflow REST API - High-level info](https://github.com/apache/airflow/issues/8107)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A\r\n"},
{"text": "**Description**\r\n\r\nWe should write a guide that describes how to add new ones or use the current authentication method.\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe should prepare a guide that will describe how to manage permissions for the API. If there is a guide for Web UI, we can extend it. If it does not exist, then we must write from scratch.\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe should prepare a guide that will facilitate the migration from the experimental API to the API for Airflow 2.0.\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need documentation to be available in a user-friendly form.  The YAML file does not belong to this format. We can use swagger UI or something similar\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nUsers expect the specification to be in an accessible form.\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "This is an element of the broader `subject` -> `topic` migration (see #1192) that should be straightforward to change, because I believe the mobile apps don't access `subjectlinks` yet, so there's no compatibility work required.  (What the data is used for in the webapp is the little in-topic-field links we show when there is a link or linkifier matching the topic line of the message).\r\n\r\n@gnprice to confirm I'm reading the mobile codebase correctly that it's indeed not accessed.\r\n\r\nNoticed in #13587; tagging as a priority since this sort of API migration gets more complex when delayed.  We should be sure to look again at updating the docs as discussed in #13587 once this is complete."},
{"text": "This is really important because we've had a series of incidents of the `curl` examples in our docs being wrong.\r\n\r\nHere's a proposal for how to do this without losing any functionality (e.g. the ability to hand-decide which values to use in the examples):\r\n\r\n* We plan to use the `example` values we already have in the OpenAPI parameter data structures for this.  If we needed to, we could add support for reading a new `curlexample` value, adjacent to `example`, to the OpenAPI data in `zulip.yaml`, that has the example value to be used in the curl example for that parameter; but I don't think that should be required; it should be fine to have the `curl` example use the same example strings we use in the code.  \r\n* We add a function constructcurlexample(authemail, authapikey), with code this:\r\n```\r\ncurlfirstline = [\"curl\"] + curlmethodarguments(httpmethod)\r\nif authenticationrequired:\r\n    curlauthline = [\"-u\", \"%s:%s\" % (authemail, authapikey)]\r\nfor param, examplevalue in openapiexampleparams:\r\n    curlarguments += formatcurlargument(paramname, examplevalue)\r\nreturn curlfirstline, curlauthline, curlarguments\r\n```\r\n* We add another function, e.g. `rendercurlexample(endpoint)` that calls `constructcurlexample(\"BOTEMAILADDRESS\", ...)` and renders the nicely line-wrapped curl examples into the markdown file.\r\n* And we extend `tools/test-api` to have a loop that actually calls the curl example from the `constructcurlexample` (using `subprocess`) against the test server and checks it gets a 200 response.\r\n* We then convert the endpoints to use this new `rendercurlexample` system; we'll probably want to do this as one commit per endpoint because we may be fixing things in those individual endpoints, and the endpoints should be independent of each other (so it'll be easy to merge the easy ones fast and then revisit the rest).\r\n"},
{"text": "Something that's incredibly valuable in maintaining API documentation is making sure that we keep it up to date as the API changes.  Our API documentation isn't complete yet (see https://github.com/zulip/zulip/issues/10044), but in 1de4b94fbe04e57c5959001ec8e881f36718cb15 I added an automated test that does a comparison between our API documentation and the actual arguments declared in our view functions.  There's a bunch of TODOs remaining to make this system really nice and have it add more coverage of possible bugs (I've marked the ones with an explicit TODO comment in the test, below):\r\n\r\n* [x] Not all arguments are intended to be documented (e.g. there's backwards compatibility, etc.).  We should add an option to `REQ` with a name like `intentionallyundocumented=True` that causes it to not add the argument to the `argumentsmap` data structure. (and thus not to considered for whether we've documented all arguments that should be documented in our test).  This should let us remove a bunch of things from `BUGGYDOCUMENTATIONENDPOINTS` that aren't actually buggy, they just have arguments we don't want to document (e.g. because they're old legacy things).\r\n* [ ] Remove other things from `BUGGYDOCUMENTATIONENDPOINTS` that are there just because we added an argument and didn't update the documentation when we did so.  E.g. https://github.com/zulip/zulip/issues/11136.\r\n* [x] TODO: Figure out a way to handle this matching between the OpenAPI format for parameters and the `urls.py` format: `/messages/{messageid} <-> r'^messages/(?P<messageid>[0-9]+)$'`.  We might want to handle this by having a loop over all OpenAPI endpoints in addition to our existing loop over all `urls.py` endpoints, because doing so would allow us to also verify that every endpoint in the OpenAPI codebase is being successfully matched to the corresponding actual code.\r\n* [x] TODO: Add support for using this syntax supported by `restdispatch`: `{'POST': ('zerver.views.report.reporterror', {'intentionallyundocumented'})}),` to mark in the `urls.py` which endpoints we expect to never have public API documentation (e.g. because they're used only internally by the webapp codebase).\r\n* [x] TODO: This is lower priority than the main issue, but it'd be really cool to cross-validate the types declared in OpenAPI against the types declared in `REQ` (via the `validator` parameter).  We may not be able to do this always, but 95% of the time it's `checkint` or `checkstr` or `checkdict`, and we can certainly map those to the expected type.\r\n\r\nOur \"writing views\" developer tutorial is good reading prior to working on this, as are https://zulip.readthedocs.io/en/latest/documentation/api.html and https://zulip.readthedocs.io/en/latest/documentation/openapi.html."},
{"text": "This issue is appropriate for your only if you have experience researching Zulip code and writing basic docs. \r\n\r\nWe have docs for updating message flags:\r\n\r\nhttps://chat.zulip.org/api/update-message-flags\r\n\r\nThis lists all the flags that a user should update, but it doesn't explain what the flags mean.  Some flags are sort of self-explanatory to folks who use Zulip a lot--starred, read, mentioned, etc.--while others are a bit more obscure--summarizeinstream, forceexpand, etc.\r\n\r\nThe doc piece here would be pretty simple--just a table of flag and description.  Descriptions can be as short as \"true if user has starred message in an app.\"\r\n\r\nThe tricky part here is that for the more obscure flags, you'll want to research them to find out their precise meaning and what we expect API users to do with them."},
{"text": "Zulip has a pretty well-designed API, with a lot of good properties, but it's documentation is incomplete. \r\n The overall goal is to fully document the Zulip API on https://zulipchat.com/api, so that it's easy both for Zulip app developers (mobile, terminal, etc.) and people building custom integrations to understand how to use Zulip's API.  We have documented a few dozen endpoints, but there are many missing.  https://zulip.readthedocs.io/en/latest/documentation/api.html is our documentation on how that documentation works (thought it is somewhat out of date, in that we are migrating to using the OpenAPI format file `zerver/openapi/zulip.yaml` for some of the things like argument definitions that used to have a separate `fixtures` file).  Our general philosophy is to build a system that can be extensively unit-tested, so that we can ensure the documentation stays up-to-date as we expand and adjust the Zulip API .  Some good tasks in this area include:\r\n* [ ] Document a few endpoints.  Read 69da22d998c6f5f56931c8449f441b3fc114d59f as a recently added endpoint to refer to.  I've noted a few that seem like good starter items here.\r\n   * [ ] Document messages/${messageId}/reactions, the endpoint for adding emoji reactions\r\n   * [x] Work through the \"user groups\" endpoints mentioned below\r\n* [ ] Contribute to https://github.com/zulip/zulip/issues/12521, which has a bunch of coding tasks\r\n* [ ] Once one has done a few, update `docs/documentation/api.md` to correct the parts that are out of date.  (This is https://github.com/zulip/zulip/issues/12571)\r\n\r\nI'm very happy to provide help and support to folks working on this area.\r\n\r\n------------------------------------------------\r\n\r\nA good priority for doing this is to make sure we document all the endpoints that we need to use in the mobile (and terminal) apps, just because that's API documentation that will be readily and frequently consumed.  \r\n\r\nHere's the current list of endpoints the mobile apps interact with that don't have API docs, organized somewhat by priority.\r\n\r\nBelow are things that are not documented, and should be:\r\n* [ ] /messages/${messageId}/reactions\r\n* [x] /typing\r\n* [ ] /users \r\n\r\nAccessing uploaded files:\r\n* [ ] useruploads\r\n\r\nUser settings for notifications:\r\n* [x] settings/notifications (https://github.com/zulip/zulip/pull/10342)\r\n\r\nPresence:\r\n* [x] users/me/presence (core presence endpoint)\r\n\r\nStarring and unread counts endpoints:\r\n* [x] markallasread\r\n* [x] markstreamasread\r\n* [x] marktopicasread\r\n* [x] messages/flags\r\n\r\n------------------------------\r\n\r\nI believe the app doesn't support editing these organization settings.  Further, it should be just getting these data from `/register` with the right `eventtypes` value, so it's probably a mobile bug if we're accessing these:\r\n* [x] realm/emoji\r\n* [x] realm/filters\r\n(Unless I'm wrong and the app supports managing user groups somewhere secret?)\r\n* [x] usergroups/create\r\n* [x] usergroups/${id}\r\n* [ ] usergroups/${id}/members\r\n* [x] users/me/usergroups\r\n\r\nSame story with these stream settings/deactivation (though I could imagine changing these):\r\n* [x] streams/${id} -- does the app actually support administering streams this way? \r\nUser stream settings (color, notification settings, muted topics; does the app support administering these?): \r\n* [ ] users/me/subscriptions/mutedtopics\r\n* [x] users/me/subscriptions/properties\r\n\r\nAnd these user-level settings:\r\n* [ ] users/me/alertwords\r\n\r\n\r\n-------------------------------------------\r\n\r\nThis endpoint is mostly just for mobile/desktop to get organization icon, but has some use beyond mobile/desktop:\r\n* [x] serversettings\r\n\r\nThese are used only by mobile, so potentially lower priority for main API docs:\r\n* [x] devfetchapikey\r\n* [ ] devlistusers\r\n* [ ] fetchapikey\r\n* [ ] users/me/androidgcmregid\r\n* [ ] users/me/apnsdevicetoken\r\n\r\nAlready documented, I think (unless it's a different method):\r\n* [x] events\r\n* [x] register (Ignoring documentation of the full data format, which is its own issue)\r\n* [x] messages\r\n* [x] messages/${id}\r\n* [x] messages/${messageId}\r\n* [x] users/me/${streamId}/topics\r\n* [x] users/me/subscriptions\r\n* [x] streams\r\n\r\n--------------------------\r\n\r\n"},
{"text": "<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\r\n\r\n### Preconditions\r\n<!--- Provide a more detailed information of environment you use -->\r\n<!--- Magento version, tag, HEAD, etc., PHP & MySQL version, etc.. -->\r\n1. Install Magento 2.1.3 with sample data.\r\n2. Set up Rest integration.\r\n\r\n### Steps to reproduce\r\n<!--- Provide a set of unambiguous steps to reproduce this bug include code, if relevant  -->\r\n1. Try to delete an attribute option, using a REST request to delete /V1/products/attributes/{attributeCode}/options/{optionId} using method DELETE. An optionid is asked when, according to documentation of the attribute options, attribute options uses the following schema:\r\n```\r\n[\r\n  {\r\n    \"label\": \"string\",\r\n    \"value\": \"string\",\r\n    \"sortorder\": 0,\r\n    \"isdefault\": true,\r\n    \"storelabels\": [\r\n      {\r\n        \"storeid\": 0,\r\n        \"label\": \"string\"\r\n      }]\r\n  }\r\n]\r\n```\r\nand has no optionid as field.\r\n\r\n### Expected result\r\n<!--- Tell us what should happen -->\r\n1. Attribute options should contain an optionid to be identified.\r\n\r\n### Actual result\r\n<!--- Tell us what happens instead -->\r\n1. Attribute options are identified using the value field as an id. The value can't be set via creation because it acts just as an id.\r\n\r\n### Notes\r\nI understand this is no big of a deal, but it did make me lose time when I tried to modify and delete attribute options programmatically and so I feel like I should write this. It should be explained in the documentation that the option id is the value and that the value is not a real value but rather, just an automatic id to identify options. I think that Value is an ambiguous name for an automatic identifier.\r\n"},
{"text": "##### ISSUE TYPE\r\n<!--- Pick one below and delete the rest: -->\r\n - Documentation\r\n\r\n##### COMPONENT NAME\r\n<!-- Pick the area of AWX for this issue, you can have multiple, delete the rest: -->\r\n - UI\r\n - Installer\r\n\r\n##### SUMMARY\r\n<!-- Briefly describe the problem. -->\r\nMy mercurial repository requires a SSL certificate to authenticate. How can I configure it in AWX ?\r\n\r\n##### ENVIRONMENT\r\n<!--\r\n* AWX version: devel\r\n* AWX install method: docker on linux\r\n* Ansible version:  2.4.0\r\n* Operating System: Linux\r\n* Web Browser: Chrome\r\n-->\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea - Documentation\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nPiggybacking on https://github.com/ansible/awx/issues/166, the idea is that things created by a user in a team are created in a 'team' context by default and accessible by that team.\r\n\r\nExample:\r\n- Credentials created by the user are automatically assigned to the team (not the user)\r\n- JTs created by the user are automatically permissioned to the team\r\n- Schedules created by the user are editiable and manageable by the team\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nI think to do this in the most user-obvious and \"makes sense\" way, we'd need to logically map \"the team that gave the user permission to do the thing\" -> \"the team that is assigned to it.\"\r\n\r\nFor example, a user may be in 3 teams A, B, and C, but if team B is the one who can access the job template, team B is the one that can modify the schedules they create for it.\r\n"},
{"text": "Currently the API documentation is only available as a set of webpages that can be [here](https://www.monicahq.com/api). A user who would like to build a client of the API needs to browse 27 different pages and manually collect the information needed to build the client. Publishing Swagger document makes it easier for developers to integrate Monica into their apps and the other way around. It will also make it easy to generate the API client for the mobile app once it's decided to be rebuilt.\r\n\r\nP.S.: I tried a bit with [DarkaOnLine/L5-Swagger](https://github.com/DarkaOnLine/L5-Swagger) to see if I can do it for the project, but it doesn't seem to be a one-step thing. It's giving me errors. I'm not a PHP dev, so most of the errors are Greek to me and could take me longer to figure out."},
{"text": "**Is your feature request related to a problem? Please describe.**\r\n\r\nit is really hard to get behind the api. when coding in a type-save language it is hard to convert a received json into a usable object.\r\n\r\nfor example: https://app.quicktype.io?share=kwYLvUf0kZlJklI9hfmQ\r\n\r\nit gives me only half the truth of what is optional/nullable\r\n\r\n**Describe the solution you'd like**\r\nIt would be great to have the json schemas available from the api documentation\r\n"},
{"text": "##### ISSUE TYPE\r\n - Documentation\r\n\r\n##### COMPONENT NAME\r\n - UI\r\n\r\n##### SUMMARY\r\nI am attempting to setup ldap integrations to our AD. With the setup there is no feature to allow me to get feedback in regards to errors when querying AD. I am only allowed to save. So far, I have been unsuccessful in configuring this auth mechanism. Do you have additional documentation with steps on setting this up?\r\n\r\n##### ENVIRONMENT\r\n* AWX version: AWX 1.0.1.93\r\n* AWX install method: docker on linux\r\n* Ansible version:  2.4.0.0\r\n* Operating System: CentOS Linux release 7.2.1511 \r\n* Web Browser: Chrome\r\n\r\n##### STEPS TO REPRODUCE\r\nLog into AWX, Settings, Authentication, LDAP\r\n\r\n##### EXPECTED RESULTS\r\n\r\nConnection to Active Directory\r\n\r\n##### ACTUAL RESULTS\r\n\r\n<!-- For bug reports, what actually happened? -->\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n<!-- Include any links to sosreport, database dumps, screenshots or other\r\ninformation. -->\r\n"},
{"text": "and beyond?\n\nWe need to document how to override the amp-img default placeholder and we really should make the mechanism more generic.\n\nCC @dvoytenko \n"},
{"text": "Create a documentation or wiki page detailing containing details about supporting or in-development AMP clients. Specify User-Agent components (header and robots.txt) and IP ranges to be used to identify incoming requests from these clients.\n\nExpected use: a publisher would like to allow or deny access to content for AMP clients.\n\nExample robotstxt database entry:\nhttp://www.robotstxt.org/db/googlebot.html\n"},
{"text": "documentation for creating an AMP HTML extension\n"},
{"text": "documentation For embedding AMP HTML content into an application\n"},
{"text": "A list of known apps that integrate AMP HTML pages, and links to specific details\n"},
{"text": "The AMP HTML format specification at https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md states:\n\n```\nThe following properties of CreativeWork must be present:\n* headline\n```\n\nPlease use http://schema.org/name as the required property instead of http://schema.org/headline. schema:name better matches the broad categories of all CreativeWorks, versus the much more article-oriented schema:headline.\n\nSimilarly, `alternativeHeadline` is recommended. It would be better to recommend the broader http://schema.org/alternateName.\n\nSee also https://github.com/schemaorg/schemaorg/issues/205\n"},
{"text": "https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#svg groups elements into a couple of different categories. Some of these categories exist in the svg specification, but with different meaning. I'd suggest using the categories as they're defined in the svg spec, for clarity.\n\nhttps://svgwg.org/svg2-draft/struct.html#TermContainerElement\nhttps://svgwg.org/svg2-draft/struct.html#TermStructuralElement\nhttps://svgwg.org/svg2-draft/struct.html#TermGraphicsElement\nhttps://svgwg.org/svg2-draft/text.html#TermTextContentElement\nhttps://svgwg.org/svg2-draft/painting.html#TermPaintServerElement\nhttps://svgwg.org/svg2-draft/struct.html#TermDescriptiveElement\n"},
{"text": "https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#svg\n\nAre there any svg attributes that are disallowed, or handled in a limited way (apart from the already listed \"xlink:href\")?\n"},
{"text": "It's possible that I've just missed this detail in the sea of documentation, but it seems to me that there's no clear way to signal the existence of an AMP page from a canonical/ordinary webpage.\n\nThe `<link rel=\"canonical\">` tag exists for an AMP page to reference its canonical page. How should the canonical page reference its AMP page?\n\nThis seems like an important feature for search-engine discovery and optimistic redirection to AMP pages on mobile browsers. I would expect a tag like `<link rel=\"alternate\" type=\"application/amp+html\">` or similar to do the trick (though I'm not sure what the AMP MIME type is, if one has been established).\n"},
{"text": "I followed a link to ampproject.org and it took me a long time to discover actually what it is. I think that putting a link to the [technical introduction](https://www.ampproject.org/how-it-works/) near the top of the page would be helpful, as currently it is at the bottom (on mobile).\n"},
{"text": "It would be helpful to clarify the allowed attributes for whitelisted elements.  This would help publishers avoid validation errors when transforming content from their content management systems.\n\nThere is already a HTML5 element whitelist in the [AMP Tag Addendum](https://github.com/ampproject/amphtml/blob/master/spec/amp-tag-addendum.md), but there isn't a clear reference for the attributes that are allowed vs. prohibited by the AMP spec.\n\nThere is some documentation in the form of the [validator](https://github.com/ampproject/amphtml/blob/master/validator/validator.protoascii), but the validator might not be the most accessible format for all users.\n"},
{"text": "Spec is up to date: https://github.com/ampproject/amphtml/blob/master/builtins/amp-pixel.md\n\nCC @jmadler \n"},
{"text": "There should be a page that provides best practices and guidelines relating to how to work with features that are partially built and under development and those that can be enabled via experiments. (These may be one and the same!)\n- What are the common flows and use cases related to turning on experiments to test a new feature?\n- Should you do testing only locally or is it ok to deploy to prod in some limited ways? How would that look?\n- To test or develop something, should you bring up the whole stack, or target against the prod runtime version? How do you know which to do?\n- How should you approach validation (see #1064)?\n- What is the cycle for runtime releases, experiments being launched (going to on by default), validation being caught up to features? Does feature being launched == validation is in place?\n"},
{"text": "documentatino - as part of CI\n- or as part of CMS publishing\n\nCC @pbakaus @Meggin \n"},
{"text": "Waiting for the user notification work to land, to document this.\n"},
{"text": "It appears that several substitutions are default-defined to be used in the amp-analytics variable syntax.\n\nFor instance, this ping to send a page view ID can be constructed in two ways: `?pvid=PAGE_VIEW_ID` or `?pvid=${pageViewId}` since the variable `pageViewId` inherits the value of `PAGE_VIEW_ID` by default.\n\nThe documentation should clarify how variables can be introduced and used in amp-analytics and talk about any default values inherited from the more general substitution scheme.\n\ncc @avimehta \n"},
{"text": "What the properties and guarantees are.\n\nThis would be helpful to have in many discussions and a good doc for new AMP developers (people working on AMP, not with AMP).\n"},
{"text": "Add a simple intro table to each AMP tag that includes: Description, Availability, Validation, Examples, and optional, Learn More (not all AMP tags will have more information at the start of it's release).\n- Description will be a brief sentence introducing the tag.\n- Availability will be Stable or Experimental for now, but could change to proper version control should this happen in the future. Experimental status will link to new experimental page that will include complete table of experimental APIs, and what it means to be experimental, how to work with experimental tags, how to move a tag out of experimental state.\n- Validation will list any validations that apply to the tag. I really like this one. Thanks, Erwin, for the inspiration.\n- Examples will link to any examples out there specifically for the tag. Examples could also link up to use cases, once available.\n- Optional Learn More will link to related docs. I'm sort of tempted to combine examples and learn more together into Related Docs.\n\nFYI... check out similar table in chrome.extensions APIs: https://developer.chrome.com/extensions/alarms\n"},
{"text": "Probably a good place is in https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md, under the table of elements.\n\n/cc @rudygalfi @Meggin \n"},
{"text": "Publishers often have deals with platforms where clicks from those platforms are not paywalled. There is already a way to pass the referrer down to the doc, but we should explicitly explain how it can be used.\n"},
{"text": "When I try and run the `facebook.amp.html` example I only see blank boxes where the `<amp-facebook>` embeds should be.\n\nSome documentation for `<amp-facebook>` would be very helpful as well.\n"},
{"text": "I've been displaying images from a feed where the size is not currently known. From the docs, it states that width and height are required.\n\n> amp-img components, like all externally fetched AMP resources, must be given an explicit size (as in width / height) in advance, so that the aspect ratio can be known without fetching the image.\n\nTo avoid validation errors the only solution I can think of is a fixed container that scales responsively and contains the image. Is there any official documentation on proposals for handling unknown sized images? Or is there a better solution than this?\n\nExample:\nhttp://jsbin.com/dogosa/edit?html,output\n\nThanks,\nByron\n"},
{"text": "https://github.com/ampproject/amphtml/blob/master/extensions/amp-accordion/amp-accordion.md needs a table listing basic information and validator errors, which would align it with the presentation of the other extension .md files.\n\ncc @Meggin @Gregable (to advise on validation errors to call out)\n"},
{"text": "In implementing Chartbeat and Omniture, I found it impossible to find docs from the individual vendors on the provided support links ([Chartbeat's link](http://support.chartbeat.com/docs/), [Adobe's link](https://helpx.adobe.com/marketing-cloud/analytics.html)). The docs might be on the support sites but I couldn't find them.\n\nHowever, looking at [vendor.js](https://github.com/ampproject/amphtml/blob/master/extensions/amp-analytics/0.1/vendors.js) in `amp-analytics` answered all my questions about implementation. I'm wondering if there is a better way to document individual vendors even when the vendors themselves don't have public documentation.\n\nThis could be something like:\n- Documenting specific and finite custom variables in the vendors section of the `amp-analytics` docs.\n- Linking the vendors section of the docs to individual sections in `vendors.js`.\n"},
{"text": "We are publishing a new Use-Cases doc for analytics.\n\nAnd so far, the use cases only include no vendor or googleanalytics.\n\nSetting a goal to create at least one use case that showcases each vendor and add them to the documentation.\n\nPart of this issue is creating good examples (perhaps using https://amp-by-example.appspot.com/).\n\nAnd the other part is documenting them in the Use-Cases doc.\n"},
{"text": "There's currently no mention of vendors in amp-pixel, but believe at least one is using it?\n\nWe should document any vendors who've integrated with pixel and link off to their docs. Eventually, we may want to centralize vendors into one doc for both amp-pixel and amp-analytics, if there's cross-over?\n"},
{"text": "Would be good to have an advanced topic doc showing developers techniques on how to use variables and populate them with server-side data.\n\nEspecially handy for things like remote configs in analytics.\n"},
{"text": "The [New PR](https://github.com/blog/2111-issue-and-pull-request-templates) template should include the steps from CONTRIBUTING.md\n"},
{"text": "Document which iframe resizing rules apply to Ad resizing. If they are the same - we can consider merging this part of the doc or simply referencing one from the other.\n\n/cc @Meggin @coreybyrnes\n"},
{"text": "How/Where to document Resize Status PostMessages i.e, the exact message names - 'embed-size-changed', 'embed-size-denied' etc -\n\nDo we need to have iframe resize status callbacks( 'embed-size-changed', 'embed-size-denied') for amp-iframe?\n"},
{"text": "How does CORS impact the [amp-iframe origin policy](../tree/master/spec/amp-iframe-origin-policy.md)?\n"},
{"text": "A colleague of mine noticed that it's unclear how to add ''google_ad_host'' inside `<amp-ad type=\"adsense\">`.  \r\n\r\nIt might make sense to look at ways to improve the amp-ad ref docs for specific ad networks to be more concise.\r\n"},
{"text": "Please add Media Impact to the list of supported Ad Networks:\n\nhttps://www.ampproject.org/docs/reference/amp-ad.html\n"},
{"text": "Looks like the [many extensions contributed so far](https://github.com/ampproject/amphtml/tree/master/extensions) have been developed by copy/paste/modify, since the [components spec](https://github.com/ampproject/amphtml/blob/master/spec/amp-html-components.md) doesn't point to a getting started guide, there's no guide in the [spec directory](https://github.com/ampproject/amphtml/tree/master/spec) and the [DEVELOPING](https://github.com/ampproject/amphtml/blob/master/DEVELOPING.md) doc doesn't touch on developing components.\r\n\r\nWould be great to have a guide explaining the build process, the API calls such as `AMP.registerElement`, imports, the `validator-amp-*.protoascii files`, [integration with tests/examples](https://github.com/ampproject/amphtml/pull/2017#issuecomment-184412965) etc.. Thanks!\r\n"},
{"text": "Hey Greg\u2014I believe you or someone on your team set up validation for amp-sidebar. Could whoever implemented also add a validation table into the amp-sidebar .md file?\n\nThanks!\n"},
{"text": "When validation rules are in place for amp-social-share, we'll want to clearly communicate these rules in the .md file for debugging. @powdercloud: are you the best person to get these in when ready?\n\nExample validation table can be found at the bottom of the [amp-accordion.md](https://github.com/ampproject/amphtml/blob/master/extensions/amp-accordion/amp-accordion.md)\n\n/cc @mkhatib \n"},
{"text": "It would be great to have a place where I can search all the AMP components, I am more than happy to do a PR ;)\n"},
{"text": "Update [3P/embed guidance](https://github.com/ampproject/amphtml/blob/master/3p/README.md) docs, specifically to:\r\n- Use more concise language\r\n- Clarify that semantic AMP tags for embeds are preferred over amp-iframe\r\n- Provide guidance on creating an AMP tag API\r\n- `<please suggest others>`\r\n"},
{"text": "When replacing `<img>` tags with `<amp-img>` tags is fraught with a specific type of mistake. `<img>` tags are void tags in HTML5, meaning that they are self-closing. `<amp-img>` cannot be made to be self-closing without changing a browser's HTML parser.\n\nChanging `<img ...>` to `<amp-img ...>` is a mistake.\nChanging `<img ...>` to `<amp-img ...></amp-img ...>` is correct.\n\nThe mistake may not cause rendering issues in most cases, but can cause them in some subtle cases, where CSS depends on the DOM structure or in cases like `<amp-live-list>` children as described [here](https://github.com/ampproject/amphtml/issues/4074). Since this is very subtle, it could easily go unnoticed. We can imagine hypothetical cases where this would result in a broken looking document, but we haven't actually spotted any in the wild yet.\n\nValidating that `<amp-img>` tags are followed by `</amp-img>` is not feasible at this stage due to the large fallout.\n\nAfter discussing this with @powdercloud, we came up with two suggested recommendations:\n1. Explicitly call out this potential issue in `<amp-img>` documentation.\n2. AMP runtime should detect child nodes of `<amp-img>` without the `fallback` attribute and:\n   1. Emit a runtime error to the dev console.\n   2. Move all children of `<amp-img>` to be immediate siblings in the DOM instead.\n"},
{"text": "The idea is to illuminate how to use existing amp-analytics features to track certain components.\n\nI see this being implemented as an \"Analytics\" section in the .md file of various amp-\\* components (some but most certainly not all). There will be use-cases listed as subheadings with a code sample.\n\nHere are the components where I think this applies today:\n- amp-accordion\n  - Track click on header to open\n  - Track click on header to close\n  - Track visibility of accordion\n- amp-lightbox\n  - Track click opening lightbox\n  - Track click closing lightbox\n  - Track visibility of thing that will show in lightbox\n- amp-sidebar\n  - Track open of sidebar\n  - Track closing of sidebar\n- amp-social-share\n  - Track click on social share provider\n- amp-sticky-ad\n  - Track ad being displayed / visible\n  - Track closing ad\n- amp-user-notification\n  - Track user notification being displayed / visible\n  - Track dismissing user notification\n\nNote that amp-carousel should likely have its own events, so #4292 will track that.\n\ncc @avimehta @jmadler @sebastianbenz @ericlindley-g \n"},
{"text": "It would be helpful to add a README.md to tests/ that explains a bit about text.csv/.txt\n"},
{"text": "Currently developers have issues where the wrong MIME type is specified, leading to errors fetching fonts (and subsequent delays in rendering)\n\nOpening this issue to document accepted types\u2014I think the right place for this is a list in `amp-font.md`, but it may be the cache docs instead.\n"},
{"text": "We checked in custom validation in #5006. We need to update the `amp-form.md` docs to reflect that.\n\nITI: #3343 \n"},
{"text": "I found that the file is getting super long. Also we are not documenting it anywhere so 3p ad developer don't know if they should update the file. \nI think the file is added to enable type check @cramforce Can you explain why we need the file?\nAnd as @lannka suggest, we can at least get rid of all the `data.xxx` from this file and instead enforce  using `data['xxx']`. \nIf we still need this file, we should also document it in ad readme.\n"},
{"text": "Arrived at https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#on from https://github.com/ampproject/amphtml/blob/master/extensions/amp-form/amp-form.md#events and was unable to figure out how to use events on the form submission.\n\nSome questions:\n1. What is the targetId? The id of a DOM element?\n2. How are methods defined on the target?\n\nLinking to [amp-lightbox](https://github.com/ampproject/amphtml/blob/master/extensions/amp-lightbox/amp-lightbox.md) would be a small step forward, though I don't see a `showLightbox` method there.\n"},
{"text": "We should prepare for outside core committers and add transparency to GOVERNANCE.md by disclosing the employer after each committer name. This was an idea from a third party, and I think it's a small, but important change.\n\n/cc @cramforce \n"},
{"text": "The [amp-live-list docs](https://github.com/ampproject/amphtml/blob/master/extensions/amp-live-list/amp-live-list.md) (or the [server filtering section](https://github.com/ampproject/amphtml/blob/master/extensions/amp-live-list/amp-live-list-server-side-filtering.md)) should specify if the Google AMP cache stays involved when the server is polled for updates.\n\nScenario:\n1. The AMP document is served from the cache\n2. The publisher would like to serve geo-tailored live updates based on the location of the user (IP-geolocated [for now](https://github.com/ampproject/amphtml/issues/826))\n3. It's unclear if the original Google AMP cache URL is polled, or if the publisher's endpoint is.\n"},
{"text": "Thanks for the previous work on this.\r\n\r\nTwo more points:\r\n\r\n1. Templates are often referred to as \"extended templates\". This may create the impression that there's a regular (or built-in) kind of templates as well. Should we clarify this terminology? What do \"extended\" templates actually mean? That the templating system is designed to run arbitrary templating languages (with Mustache being the only one currently implemented)? Can we drop \"extended\" or do we need to keep it to parallel [\"extended components\"](https://github.com/ampproject/amphtml/blob/master/extensions/README.md), which do have a built-in counterpart?\r\n\r\n2. The [templates spec](https://github.com/ampproject/amphtml/blob/master/spec/amp-html-templates.md) is actually less detailed than the [templates section in the HTML format spec](https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#extended-templates). I was [improving the templates documentation](https://github.com/ampproject/amphtml/pull/6017) when I realized this."},
{"text": "Follow up to #5868 to update docs when this is in."},
{"text": "## Short description of your issue:\r\n\r\nAt the very least, we need a document saying what version of Java is needed.\r\n\r\n## How do we reproduce the issue?\r\n\r\nOn OS X 10.11.6 El Capitan, with JDK 1.7.0_45, download the main project, and attempt `gulp dist` or `gulp test`. You get a bunch of errors and warnings. Upgrade to JDK 1.8 and they go away.\r\n\r\nThe point here is not that it should all work on JDK 1.7, but that `DEVELOPING.md` should give a proper list of any development tool dependencies. At the moment, it doesn't even list that you need Java at all."},
{"text": "We should have documentation for guidelines and recommendations for viewers similar to our cache guidelines.\r\n\r\nOne item would be: \"Share the canonical URL of a document when technically possible\".\r\n\r\nRelated to #6210 and #6087"},
{"text": "Once new release is out with experiment launched the docs should reflect stable for custom validation. \r\n\r\nLaunch PR #7185 "},
{"text": "In PR #7281 I am adding a new section on design reviews to CONTRIBUTING.md.  We should provide some example design docs in this section."},
{"text": "We landed support for input change event and exposing form.submit action that can be used together to support submit-on-change kind of behavior. \r\n\r\nWe need to update AMP-form docs to add a section about `form.submit` action and `input change` event. See the `forms.amp.html` example for more details on syntax and usage.\r\n\r\n```html\r\n<form method=\"post\"\r\n      id=\"poll1\"\r\n      action-xhr=\"/form/json/poll1\"\r\n      target=\"_blank\">\r\n    <fieldset>\r\n        <p>What is your favorite flightless bird?</p>\r\n                <label>\r\n                    <input name=\"question1\" value=\"Penguins\" type=\"radio\" on=\"change:poll1.submit\">\r\n                    Penguins\r\n                </label>\r\n                <label>\r\n                    <input name=\"question1\" value=\"Ostriches\" type=\"radio\" on=\"change:poll1.submit\">\r\n                    Ostriches\r\n                </label>\r\n                <label>\r\n                    <input name=\"question1\" value=\"Kiwis\" type=\"radio\" on=\"change:poll1.submit\">\r\n                    Kiwis\r\n                </label>\r\n                <label>\r\n                    <input name=\"question1\" value=\"Wekas\" type=\"radio\" on=\"change:poll1.submit\">\r\n                    Wekas\r\n                </label>\r\n    </fieldset>\r\n</form>\r\n```\r\n\r\nReference bug: #4272\r\n\r\n"},
{"text": "With PR #7568 we'll have at least four files that someone wanting to contribute to the AMP Project might look at--CONTRIBUTING.md, DEVELOPING.md and the two new guides.  We should clean up the redundancies between these files and make it more clear to new and ongoing contributors where they should look for information.\r\n\r\nI've also found that DEVELOPING.md has a lot of details that may only be relevant in certain situations without clearly indicating when it would be useful (e.g. SauceLabs, A4A Envelope).  We should figure out the best way of presenting this information."},
{"text": "I think this file could likely use the same updates we've made throughout our other docs, althogh maybe @bpaduch has kept this one a bit more up to date. While this file is mostly not directed at contributors, I think we can use it as marketing surfaces to get contributors :)"},
{"text": "https://github.com/ampproject/amphtml/pull/7693 exposes the `allowpaymentrequest` attribute on `amp-iframe`, we should update the documentation in `amp-iframe` reflecting this when:\r\n- [x] Validator changes for https://github.com/ampproject/amphtml/pull/7693 have made it to prod (~2 weeks or so)\r\n- [x] Viewer also allows `allowpaymentrequest` b/35582805\r\n\r\n@alanorozco Let's check on these in couple of weeks and see if we can update the documentation at that point.\r\n\r\n/cc @ericlindley-g "},
{"text": "We should clearly explain all syntax options and nature of state update (shallow vs. recursive merge). "},
{"text": "We've added tables of content to some files (e.g. CONTRIBUTING.md and getting-started-e2e-md).  We should programmatically keep these up to date as the files change."},
{"text": "It would be useful to have developer documentation that outlines what exactly happens (e.g., reporting to a server, console messaging, UI indications) when the various methods of the `user()` and `dev()` objects are called, when a window-level `error` or `unhandledrejection` event occurs, or when an AMP lifecycle method throws or rejects."},
{"text": "Document the features implemented in #7562\r\n"},
{"text": "Anyone that owns an API should create public documentation for that API (example: history api, etc). I'm currently writing the documentation for the amp-viewer-integration and, after the handshake, want to link people off to different API's but I need the docs to link them to."},
{"text": "We documented the [allowed `meta` tags for A4A](https://github.com/ampproject/amphtml/blob/master/extensions/amp-a4a/amp-a4a-format.md#42-document-metadata) but not normal AMP. \r\n\r\nAlso, should we only list the differences comparing to normal AMP in `amp-a4a-format.md`? That might be easier for maintenance?"},
{"text": "- We should get `amp-bind.md` good enough such that cross-referencing ampbyexample.com isn't necessary for new users.\r\n- Restore bindable properties/attributes matrix in light of #8272.\r\n\r\n/cc @ericlindley-g @kmh287 "},
{"text": "I've gotten the question a couple times as to whether AMP supports CSRF tokens \u2014 it would be great to clearly articulate in the amp-form documentation how CSRF is handled, and why tokens are not needed.\r\n\r\n/cc @aghassemi "},
{"text": "We should \r\n\r\n- explain how people can get access to percy (for now: google form to submit email?)\r\n- what to do if there is a failure."},
{"text": "GitHub now has support for a [SUPPORT.md file](https://help.github.com/articles/adding-support-resources-to-your-project/); we should add one."},
{"text": "Facebook pixel support was added in #9449. We need to also include documentation here: https://github.com/ampproject/docs/blob/master/content/docs/guides/analytics_amp/analytics-vendors.md\r\n\r\ncc @eduardogoncalves, who authored the pull request cited above."},
{"text": "## What's the issue?\r\n\r\nhttps://github.com/ampproject/amphtml/blob/master/spec/amp-cors-requests.md\r\n\r\nIn reading this documentation there isn't a full example and it is not fully clear what headers need to be implemented to ensure CORS compatibility consistently.\r\n\r\nA full example of headers used would make this much more comprehensible.\r\n\r\nfor example looking in ampbyexample amp-list example looks like these are set:\r\n\r\naccess-control-allow-credentials:    true\r\naccess-control-allow-headers:    Content-Type, Content-Length, Accept-Encoding, X-CSRF-Token\r\naccess-control-allow-methods:     POST, GET, OPTIONS\r\naccess-control-allow-origin:     https://ampbyexample.com\r\naccess-control-expose-headers:   AMP-Access-Control-Allow-Source-Origin\r\namp-access-control-allow-source-origin:\r\n\r\nWhy is the last one empty and what do the other things mean and which are generally required and which are just used in this context?\r\n\r\nThe docs (https://www.ampproject.org/docs/guides/amp-cors-requests) talk about:\r\n\r\n```\r\nThe CORS Origin header\r\nThe AMP-Same-Origin custom header\r\nSource origin restrictions via __amp_source_origin\r\n```\r\n\r\nand suggests\r\n```\r\nAMP-Same-Origin: true\r\n```\r\nAnd then talks about\r\n\r\n> \r\n\r\nIf the Origin header is set:\r\n```\r\nIf the origin does not match one of the following values, stop and return an error response:\r\n\r\n*.ampproject.org\r\n*.amp.cloudflare.com\r\nthe publisher's origin (aka yours)\r\nwhere * represents a wildcard match, and not an actual asterisk ( * ).\r\nIf the value of the __amp_source_origin query parameter is not the publisher's origin, stop and return an error response.\r\nIf the two checks above pass, process the request.\r\nIf the Origin header is NOT set:\r\n\r\nVerify that the request contains the AMP-Same-Origin: true header. If the request does not contain this header, stop and return an error response.\r\nOtherwise, process the request.\r\n```\r\nBut in the example code none of these seem to be set.\r\n\r\nAnd finally it says:\r\n\r\nEnsuring secure responses\r\nThe resulting HTTP response to a CORS request must contain the following headers:\r\n\r\n> Access-Control-Allow-Origin: origin\r\nhttps://ampbyexample.com\r\n\r\n> This header is a W3 CORS Spec requirement, where origin refers to the requesting origin that was allowed via the CORS Origin request header (for example, \"https://cdn.ampproject.org\"). Although the W3 CORS spec allows the value of * to be returned in the response, for improved security, you should validate and echo the value of the \"Origin\" header.\r\n\r\n> AMP-Access-Control-Allow-Source-Origin: source-origin\r\nseems to be empty???? how is source-origin different from origin?\r\n\r\n> This header allows the specified source-origin to read the authorization response. The source-origin is the value specified and verified in the \"__amp_source_origin\" URL parameter (for example, \"https://publisher1.com\").\r\n\r\n> Access-Control-Expose-Headers: AMP-Access-Control-Allow-Source-Origin\r\naccess-control-expose-headers:   AMP-Access-Control-Allow-Source-Origin\r\n\r\nthat one seems to be the same.\r\n\r\n> This header simply allows the CORS response to contain the AMP-Access-Control-Allow-Source-Origin header.\r\n\r\nOverall the documentation left me wondering what was actually required.\r\n\r\nWhat adds to the confusion is when I follow the above I get \r\n\r\nurl.js:345 Response must contain the AMP-Access-Control-Allow-Source-Origin header\r\n\r\nBut when I include the same host as the host that is calling the page all is fine.\r\n\r\nTo my view there should be a number of examples to demosntrate the various different combinations of options that are possible for example what are the headers needed for:\r\n\r\n1) page at example.com loads a JSON endpoint from example.com\r\n2) page at example.com loads a JSON endpoint from othersite.com\r\n3) page at google cache/cloudflare cache loads a JSON endpoint from example.com\r\n4) page at google cache/cloudflare cache loads a JSON endpoint from othersite.com \r\n\r\nI think those are all the potential examples that one might have(?)"},
{"text": "Hi,\r\n\r\nThe AMP Validator currently contains instructions inside an HTML comment.\r\n<https://validator.ampproject.org/>\r\n\r\nThe comment reads:\r\n\r\n> This is the minimum valid AMP HTML document. **Just** type away here and the AMP Validator will re-check your document on the fly.\r\n\r\nSuggested change\u2014remove the word \"just\":\r\n\r\n> This is the minimum valid AMP HTML document. Type away here and the AMP Validator will re-check your document on the fly.\r\n\r\nThe reasoning is outlined here: <http://bradfrost.com/blog/post/just/>\r\n\r\n> \"Just\" makes me feel like an idiot. \"Just\" presumes I come from a specific background, studied certain courses in university, am fluent in certain technologies, and have read all the right books, articles, and resources. \"Just\" is a dangerous word.\r\n\r\nCheers,\r\n\r\nJeremy"},
{"text": "## What's the issue?\r\n\r\nThe amp head tag validation is very strict and results in invalid AMP pages for formerly valid AMP pages. Limits the use of certain tags. Also missing documentation about the allowed tags. Only states that a certain tag is not allowed in that configuration without reference to fixing the problem. Providing a generic link to https://www.ampproject.org/docs/reference/spec#html-tags is not helpful. \r\n\r\n## How do we reproduce the issue?\r\n\r\n1. open https://ampbyexample.com/components/amp-fx-flying-carpet/#development=1\r\n2. see validation error: The attribute 'crossorigin' may not appear in tag 'link rel='. \r\n\r\n## What browsers are affected?\r\n\r\nAll browsers.\r\n\r\n## Which AMP version is affected?\r\n\r\nPowered by AMP HTML \u2013 Version 1512677960104 https://ampbyexample.com/components/amp-fx-flying-carpet/#development=1\r\n"},
{"text": "We have a [doc](https://docs.google.com/document/d/1BS_GR1nmZQPrtaGKmAV3nSicJFRhqhC2WhvhCxBZav8/edit) that describes new variables and events for analytics in `amp-story`.  We should fold that into the [amp-analytics documentation](https://www.ampproject.org/docs/reference/components/amp-analytics) or the [amp-story documentation](https://github.com/ampproject/amphtml/blob/master/extensions/amp-story/amp-story.md).\r\n\r\n/cc @bpaduch "},
{"text": "Went to read an article in the AMP blog and it's so hard to read, see screenshot:\r\n\r\n![1](https://user-images.githubusercontent.com/17784082/37107406-2bd29cc8-222c-11e8-8a88-be916b3010cc.png)\r\n\r\nCan't you just add some nicer CSS to make it readable, for example see here:\r\n\r\n![2](https://user-images.githubusercontent.com/17784082/37107451-408009f8-222c-11e8-8bb8-9c14502c57f9.png)\r\n\r\nAll I did was do this for example:\r\n\r\nOLD Style:\r\n\r\n```\r\nbody, .card, .post-detail .post-content p em, .post-detail .post-content p em a {\r\n    font-size: 20px;\r\n    line-height: 28px;\r\n}\r\n.post-detail .post-content p, .post-detail .post-content li {\r\n    letter-spacing: 0;\r\n}\r\n```\r\n\r\n\r\nNEW Style:\r\n\r\n```\r\nbody, .card, .post-detail .post-content p em, .post-detail .post-content p em a {\r\n    font-size: 1rem;\r\n    line-height: 28px;\r\n}\r\n.post-detail .post-content p, .post-detail .post-content li {\r\n    letter-spacing: 0;\r\n    text-align: justify;\r\n}\r\ntd {\r\n\tpadding: 10px;\r\n}\r\n```\r\n\r\nBlog page I am talking about is found here: https://www.ampproject.org/latest/blog/faster-ads-with-render-on-idle/\r\n\r\n(I'm just giving a brief example, just suggesting someone go over the blog and update all the responsive css and make it better)"},
{"text": "This feature will be available for use soon; we should write some documentation in amp-story.md on how to use it (similar to the [existing documentation for amp-story-grid-layer](https://github.com/ampproject/amphtml/blob/master/extensions/amp-story/amp-story.md#layers-amp-story-grid-layer))."},
{"text": "There will be format specification differences between versions 0.1 and 1.0 of amp-story.  Although [amp-story.md](https://github.com/ampproject/amphtml/blob/master/extensions/amp-story/amp-story.md) should always have complete documentation for the latest version, we should also consider writing documentation specifically targeted at those already using 0.1 who wish to upgrade to 1.0."},
{"text": "update amp-mustache docs in regards to tags supported for triple-mustache once #14447 is checked in, as support has been broadened to not just text formatting tags."},
{"text": "We've received feedback that the code review process is a bit opaque.  We should improve the documentation to make this better including things like:\r\n- how to know the right people are reviewing the PRs\r\n- what kinds of things reviewers look for\r\n- unit test best practices/when integrations tests are needed\r\n\r\nI'd propose creating new docs:\r\n- Getting your code reviewed\r\n- Testing best practices\r\n\r\nAnd then linking existing docs to these (e.g. \"[Building an AMP extension](https://github.com/ampproject/amphtml/blob/master/contributing/building-an-amp-extension.md)\" and \"[Getting Started End-to-end](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md)\").  While doing this we should also be conscious of the fact that we have a lot of documentation split among multiple files, so we should make sure the purpose of each doc is clear and make it as easy as possible for a contributor to find the information they need.\r\n\r\n/cc @aghassemi @choumx @lannka @rsimha "},
{"text": "We don't currently do a good job of documenting some common workflows, e.g.\r\n\r\n- gulp lint\r\n- gulp check-types\r\n- test describe.only versus --files (--files is recommended by the documentation but can produce spurious errors)\r\n- gulp test --watch\r\n- gulp test --nobuild --watch --files=test/functional/test-viewer.js\r\n- common git patterns (like pulling in changes a reviewer makes on a branch, resolving conflicts\r\n\r\nIn some cases documentation is spread around among 5 different docs--CONTRIBUTING, DEVELOPING, Getting Started E2E, Getting Started Quick Start, Building an AMP Extension.  As we're adding additional documentation we should also make the purposes of these docs more clear so it's easier to find the information people need.\r\n\r\n/cc @aghassemi @choumx @lannka @rsimha  \r\n\r\n\r\n"},
{"text": "We've heard feedback from people in the community that there are some ways the AMP release process could be improved.  The [AMP Release Process Proposal](https://docs.google.com/document/d/1izZF_vQQUBWCbHifo89g9CYIlfUFLcpQ9nnLPApV67Q/edit) documents a set of changes to our release process to address these concerns.\r\n\r\nMajor highlights from the proposal:\r\n\r\n- Each release will be tracked with a GitHub issue, with significant updates to the release documented in that issue (cherry picks, delays, push to production).\r\n- The canary cut will happen at a consistent time to add predictability to the release process for contributors.\r\n- The bar for getting a cherry pick (a one-off change added to the canary or release) approved will be higher to avoid disrupting our release process.  The process for proposing a cherry pick and getting it approved will be made consistent and well-documented."},
{"text": "# Plan\r\n\r\nAfter the design review (#15347) we have the following plan to resolve this issue:\r\n\r\n- Undelete the origin experiments code, adjusting it to use @jridgewell's suggestion to make it not async. (Issue #16183)\r\n- Until that is done, use document-level experiments sparingly.  We should not make breaking changes to document-level experiments (including the existing set).\r\n- After the origin experiments code is launched, no longer add any document-level experiments (with a comment in [prod-config.json](https://github.com/ampproject/amphtml/blob/master/build-system/global-configs/prod-config.json)).\r\n- After all existing document-level experiments are launched we'll remove the code supporting document-level experiments.\r\n\r\n# Proposal\r\nThe original proposal was to do one of the following (in order of preference):\r\n\r\n- removing support for document-level experiments in favor of origin trial experiments\r\n- not allowing breaking changes for features with document-level experiments (with the same policy we use for launched policies), and adding this to our [experiment documentation](https://www.ampproject.org/docs/reference/experimental)\r\n- providing very clear documentation that document-level experiments may break a page at any time, and should not be used on non-experimental production traffic\r\n\r\n# Background\r\n\r\nAMP currently supports [document-level experiments](https://www.ampproject.org/docs/reference/experimental), i.e. by adding a meta \"amp-experiments-opt-in\" on a page the page author can enable experiments that are whitelisted for document level experiments (specified in [allow-doc-opt-in in prod-config.json](https://github.com/ampproject/amphtml/blob/master/build-system/global-configs/prod-config.json)).\r\n\r\nThere are three main types of experiments we support that a developer can opt into:\r\n\r\n- Experiments enabled in the browser (via cookies or AMP.toggleExperiment), which allows developers to test experimental features but not to push these features to production.\r\n- Document-level experiments, which allows developers to use experimental features in a production setting by adding the meta tag to their document (ideally behind the developers own experiment).\r\n- Origin trial experiments, which allows us to whitelist domains who can enable the experiment features in a production setting (ideally behind the publisher's own experiment).\r\n\r\n# Document-level vs. origin trial\r\n\r\nThe biggest issue with document-level experiments is that we have no good way to know everyone who is using the document-level experiment or to notify them of upcoming breaking changes.  This means that a breaking change in the experiment carries the risk that a page in production breaks.  In contrast, with origin trials the whitelisting process allows us to (a) require developers using the experiment to acknowledge the risks involved in using an experimental feature and (b) collect information that we could use to notify developers using the experimental feature in the case a breaking change will be made.\r\n\r\nThe main use case we've had for document-level experiments thus far has been cases where a few developers are working closely with the AMP community to refine an upcoming feature.  This case is served well by the origin-trial case, and the number of these cases is not very large.\r\n\r\n# Other use cases for document-level experiments\r\n\r\nThe document-level experiment I2I (#6869) has an additional use case for the amp-inabox.  I don't have complete context for that use case so I'd like some input from @lannka whether origin trials is sufficient for that case.  If not we may still be able to move to the \"document-level experiments don't allow breaking changes\" policy.\r\n\r\n/cc @aghassemi @cathyxz @choumx @cramforce @jridgewell @kristoferbaxter\r\n\r\n\r\n\r\n"},
{"text": "Publishers can enable AMP-to-AMP linking by specifying `amphtml: true` on their links in the bookend.  We should document this feature in amp-story.md."},
{"text": "**(Reserved as a starter issue)**\r\n\r\nCurrently `amp-twitter` [documentation](https://github.com/ampproject/amphtml/blob/master/extensions/amp-twitter/amp-twitter.md) mentions that it supports displaying a `Tweet` in couple of places ( description and first line of the summary). \r\n\r\nWe should change it say it supports Tweet or Moment.\r\n\r\n### What you will need to know\r\n\r\nNot much!  This issue involves fixing some of our documentation which will get you familiar with using git & GitHub.\r\n\r\n### Motivation\r\n\r\nWe have a lot of documentation to help AMP contributors and users, but sometimes it gets out of date.  This issue will update some of our out-of-date documentation (and make it more resilient to future changes).\r\n\r\n### The bug\r\n\r\nOur Getting Started guides ([end-to-end](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#see-your-changes-in-production) and [quick start](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-quick.md#see-your-changes-in-production)) are intended for people getting started with contributions to AMP.  These guides have brief sections describing our release process to let people know when their contributions will be live in production, but this information is out of date.  We would like to update this documentation to be more current and resilient to future changes.\r\n\r\n### Step by step\r\n- [ ] Claim this issue by adding a comment below.  Please only claim this issue if you plan on starting work in the next day or so.  (If you [join the AMP Project](https://goo.gl/forms/T65peVtfQfEoDWeD3) we'll be able to assign this issue to you after you've claimed it.)\r\n- [ ] See the [Getting Started End-to-End Guide](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md) for [an intro to Git & GitHub](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#intro-to-git-and-github) and [how to get a copy of the code](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#get-a-copy-of-the-amphtml-code).  You can also refer to the [Quick Start Guide](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-quick.md) for the necessary setup steps with less explanation than the End-to-End guide.  For this bug it is not necessary to [build AMP](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#building-amp-and-starting-a-local-server) (but you may want to do that anyway just to see what it's like).\r\n- [ ] [Create a Git branch](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#create-a-git-branch) for making your changes.\r\n- [ ] Using the [instructions in the Getting Started guide](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#edit-files-and-commit-them), edit `contributing/getting-started-e2e.md` and `contributing/getting-started-quick.md`.  *Note that it is possible to make changes to these files using the GitHub UI, but for this issue please go through the exercise of getting and editing the code via git.*\r\n- [ ] Edit the \"See your changes in production\" sections of both guides.\r\n  - [ ] In the End-to-end guide, update the paragraph that starts \"In general we cut a release.\"  Instead of listing specific days we now want to convey the more general point that AMP is pushed to production after undergoing some testing and that it takes about 1-2 weeks for a change to be live for all users.  Link to the [release schedule](https://github.com/ampproject/amphtml/blob/master/contributing/release-schedule.md) docs for people who want to know more specific details.\r\n  - [ ] In the Quick Start guide make a similar change to the bullet point that starts \"Barring any issues\"\r\n- [ ] [Commit your changes](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#edit-files-and-commit-them) frequently.\r\n- [ ] [Push your changes to GitHub](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#push-your-changes-to-your-github-fork).\r\n- [ ] [Sign the Contributor License Agreement](https://github.com/ampproject/amphtml/blob/master/CONTRIBUTING.md#contributor-license-agreement) before creating a Pull Request.  (If you are contributing code on behalf of a corporation start this process as early as possible.)\r\n- [ ] [Create a Pull Request](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#send-a-pull-request-ie-request-a-code-review) and mention @aghassemi in a comment.  Mention `closes Issue 16310 ` in the description.\r\n- [ ] [Respond to your reviewer's comments](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#respond-to-pull-request-comments) (if any).\r\n\r\nOnce approved, your changes will be merged and it will be immediately live for all AMP contributors to benefit from.  **Congrats on making your first contribution to the AMP Project!**  \r\n\r\nThanks, and we hope to see more contributions from you soon.\r\n\r\n### Questions?\r\n\r\nIf you have questions ask in this issue or on your Pull Request (if you've created one) or see the [How to get help](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#how-to-get-help) section of the Getting Started guide.\r\n"},
{"text": "This a feature to capture the documentation work necessary to onboard different participants of the email ecosystem.\r\n\r\nWork includes:\r\n\r\n- [x] Document value prop of AMPHTML Email on ampproject.org\r\n- [x] Technical documentation for how AMPHTML Email would work and how it'll be backwards compatible\r\n- [x] Technical documentation outlining the requirements for a generic Mail provider to become compatible\r\n- [x] Document open-source components that the mail providers can reuse \r\n- [x] Technical documentation outlining requirements for an ESP to become compatible \r\n- [x] Enhance ampproject.org to receive feedback about AMPHTML Email."},
{"text": "Various [attributes of `amp-story`](https://github.com/ampproject/amphtml/blob/master/extensions/amp-story/amp-story.md#new-metadata-requirements) are used to specify images used for generating previews, etc. The docs specify the expected aspect ratios of each, but they should also include:\r\n\r\n- supported/recommended image formats (e.g. is SVG allowed?)\r\n- supported/recommended minimum dimensions\r\n- supported/recommended file sizes\r\n\r\n/cc @newmuis @bpaduch \r\n"},
{"text": "I seeing emojis used in commit message, but for a newcomer to the project, it's a bit intimidating. What does each one mean in the context of the AMP project?\r\n\r\nIt would be great to have the most common ones listed (with their definition) in [CONTRIBUTING.md](https://github.com/ampproject/amphtml/blob/master/CONTRIBUTING.md)."},
{"text": "There isn't any mention of Google Tag Manager on amproject.org. Why is this? Is there a legal reason? We have a lot of clients asking about GTM support in AMP, what tags are supported, etc. There should be at least an acknowledgement of the integration somewhere. [This section](https://developers.google.com/google-ads/amp/landing-pages#google_tag_manager) contains relevant links to both the [setup](https://support.google.com/tagmanager/answer/6103696) of GTM to work with AMP, and the list of supported [tags](https://support.google.com/tagmanager/answer/6106924). I think these should be linked somewhere, they are quite difficult to find even with a Google Search with 'GTM and AMP'."},
{"text": "As noted in the [documentation](https://www.ampproject.org/docs/design/responsive/style_pages#disallowed-styles), amp-custom styles aren't allowed to use the \"!important\" qualifier.\r\n\r\nThe validator seems to be picking up false positives in comments such as this one:\r\n\r\n    /**\r\n     * Some comment about `!important`\r\n     */\r\n    div{\r\n          color: white;\r\n    }\r\n\r\nThe style itself doesn't use important, but the validator complains:\r\n> \"text inside tag 'style amp-custom' contains 'CSS !important', which is disallowed.\"\r\n\r\nWhich, I guess, is an accurate representation of the situation, but isn't what the *[documentation](https://www.ampproject.org/docs/design/responsive/style_pages#disallowed-styles)* says is disallowed. Either the documentation or the validator should be updated to match one another.\r\n"},
{"text": "We want to make sure the [Getting Started Quick Start](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-quick.md) are still up to date and clear.\r\n\r\nWe should consider adding a summary checklist at the top linking to each section."},
{"text": "- [ ] Add instructions for creating \"OWNERS\" file for component.\r\n- [ ] Add a checklist at the top for a quick list of what to make sure is done sure to do.\r\n- [ ] Ensure that putting new components behind \"experiment\" flag is required.\r\n- [ ] Add references to existing implementation and special requirements for types of contributions. For instance, if implementing a video-player, we should include special documentation and point to a few existing components to use as model."},
{"text": "- [x] Consider moving the great details on \"Testing\" to the Testing.md guide.\r\n- [x] Add details on pre-testing your code with the \"prepush git hook\".\r\n- [ ] Add details on resolving merge conflicts (tips and gotchas).\r\n- [x] Review to ensure it's up-to-date, accurate and complete.\r\n\r\n"},
{"text": "Context: #13841\r\n\r\nWhen using an AMP element in the main document and using `shadow-v0.js`, AMP gives an error message like \"No ampdoc found for ...\". Instead, it would be better to say something like: \"AMP elements may not exist in the main document when using shadow-v0\". This will help developers find the source of the issue much more easily."},
{"text": "Blocked dates are a common use case, yet [the documentation is scant](https://www.ampproject.org/docs/reference/components/amp-date-picker#blocked), simply mentioning \"RRULE\". After reasonable effort (5 minutes), I wasn't able to figure out the correct syntax for the easiest pattern, blocking out X days starting on YYYY-MM-DD. The best I could come up with was\r\n\r\n    blocked=\"DTSTART;2018-10-10 RRULE:FREQ=DAILY;COUNT=10\"\r\n\r\nThis only disabled October 10 in the calendar.\r\n\r\nI understand the RRULES are [complex](https://www.nylas.com/blog/calendar-events-rrules/), but the simple example of a contiguous block of days would go a long way. I suspect the problem with the code above is rather minor, probably an incorrect separator or argument order."},
{"text": "What do you think about add a slide on [Codelabs: Creating your first AMP Component](https://codelabs.developers.google.com/codelabs/creating-your-first-amp-component/index.html#4) after **Slide 4**, about [Running all the Travis CI checks locally](https://github.com/ampproject/amphtml/blob/81262411085754eb3e978fb68e87b5e8fa307d80/contributing/getting-started-e2e.md#running-all-the-travis-ci-checks-locally) or running `lint`, `check-types` and `presubmit`, as this can be time saving when sending PRs?\r\n\r\n```\r\ngulp check-all --files extensions/amp-hello-world/0.1/amp-hello-world.js\r\n```\r\nor\r\n```\r\ngulp lint --files extensions/amp-hello-world/0.1/amp-hello-world.js\r\ngulp check-types --files extensions/amp-hello-world/0.1/amp-hello-world.js\r\ngulp presubmit --files extensions/amp-hello-world/0.1/amp-hello-world.js\r\n```"},
{"text": "## What's the issue?\r\n\r\nThere's no documentation on how to style the content fetched by amp-list.\r\n\r\n## How do we reproduce the issue?\r\n\r\nI wanted to put items fetched by amp-list into a grid, but found that styling the amp-mustache `<template>` itself does nothing. I couldn't find any information in the documentation, but I eventually found [this example on github](https://github.com/ampproject/samples/tree/master/amp-tube) that solves the problem using the following code:\r\n\r\n```\r\namp-list div[role=\"list\"] {\r\n    display: grid;\r\n    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\r\n    grid-gap: 0.5em;\r\n} \r\n```\r\n\r\nIf this is the recommended method for styling amp-lists (into a grid or flexbox) it would make things easier for developers if it was in the documentation somewhere."},
{"text": "There are non-obvious differences between the three options `\"none\"`, `\"focus\"` and `\"select\"` values, but there are no examples in `amphtml/examples` or ABE where we compare them side-by-side. Creating these examples would help document authors understand how best to use `amp-selector`"},
{"text": "Now that we've introduced the ability to add inline JSON configs for the bookend (#19171), we have to add documentation to let amp-story developers know they can do this.\r\n"},
{"text": "We should have a documented policy for when we will freeze production releases.\r\n\r\nSo far we've decided this on an ad hoc basis, taking into account the needs of users of AMP and when we would have enough people to monitor/address any production issues that come up.  The ad hoc nature of this has resulted in some confusion for users and contributors."},
{"text": "Filing follow up for documentation review of https://github.com/ampproject/amphtml/pull/19399. \r\nReview / clarify documentation for `amp-list-load-more` experiment. "},
{"text": "Users want to open a date picker and see their selected date in a UI element, but without an input-box treatment. There is no existing code sample for document authors to easily implement this use-case. "},
{"text": "Currently this resides in https://github.com/ampproject/amphtml/blob/master/contributing/TESTING.md\r\n\r\nBut I think it would make more sense to have a contributing/DEMO.md , and have the testing link to that for testing (demoing) local build externally.\r\n\r\ncc @cathyxz "},
{"text": "Inspired by #20420 , the Validator should also have a release and freeze schedule documented. It can either become a part of the general [Release Schedule](https://github.com/ampproject/amphtml/blob/master/contributing/release-schedule.md) document or it's own under validator/."},
{"text": "Viewing documentation and issue #7368, the clear action cannot be invoked. Documentation needs improvement for amp-from clear action: https://www.ampproject.org/docs/reference/components/amp-form"},
{"text": "## Describe the new feature or change to an existing feature you'd like to see\r\n\r\nAs we launch version 2 of components, surfacing the documentation (and examples, guides, tutorials) will be hard given the way we structure documentation currently. \r\n\r\nDocumentation currently lives outside the `0.1` or `0.X` folder which contains the code. \r\n\r\n## Potential solutions\r\n\r\n1. Mass migrate documentation to the same folder as the code. New documentation should lie in the same folder as the code.\r\n1. Determine the commit which introduces the `0.X+1` folder and use the documentation from the commit before that as the version `X` documentation, the future documentation will be attributed to `0.X+1`. This is problematic since folks don't sync documentation and code changes. \r\n1. Meta files with the dates a component goes into maintenance/deprecation mode.  \r\n\r\n## Additional information\r\n\r\nExamples, guides and tutorials will also need to be kept in sync with versioning changes. \r\n\r\ncc @rsimha @ampproject/wg-infra \r\n\r\nThis is from a conversation between @pbakaus @CrystalOnScript and myself. "},
{"text": "I'd just like to see more WordPress related AMP tutorials or a beginner's guide if anyone can help or knows where to look.  The AMP website only has one decent video, but it only went over 3 items and didn't even finish the hamburger menu. \r\n\r\nThe AMP.org site has lots of good info and I've managed to sort a few issues out after installing the recommended plugin to my WordPress and also got a child theme going so definitely heading in the right direction.\r\n\r\nAny advice?\r\n\r\nMaybe a daft question, so please don't shoot me. :)"},
{"text": "##Are the current AMP tags friendly to the disabled?\r\n\r\nThe blind, color-blind and other people need access to the content in this pages, is it supported?\r\n"},
{"text": "amp-story documentation has some really large files, which makes the whole AMP repo take longer to clone.  We can instead use [LFS ](https://git-lfs.github.com/) to store the files."},
{"text": "## Describe the new feature or change to an existing feature you'd like to see\r\n\r\nFeature request for simple use case of the `amp-lightbox-gallery` component. Like in jQuery plugins:\r\n\r\n```html\r\n<a href=\"detail.jpg\">\r\n  <amp-img src=\"thumbnail.jpg\" lightbox>\r\n</a>\r\n```\r\n\r\n## Describe alternatives you've considered\r\n\r\nI've tried to do it with `srcset`/`sizes` as recommended in #14900 and other issues and I've ended with following code:\r\n\r\n```html\r\n<amp-img\r\n  src=\"foto_1_tb.jpg\"\r\n  srcset=\"\r\n    foto_1_tb.jpg 200w,\r\n    foto_1.jpg 1024w\r\n    \"\r\n  sizes=\"\r\n    (max-width: 900px) calc((100vw - 8 * 0.25rem) / 4),\r\n    200px\r\n    \"\r\n  alt=\"\" layout=\"responsive\"\r\n  width=\"200\" height=\"200\" \r\n  lightbox role=\"button\" tabindex=\"0\">\r\n</amp-img>\r\n```\r\n\r\nLive website: https://www.vzhurudolu.cz/amp/kurzy/rychlost-nacitani\r\n\r\nIssues I can see here:\r\n\r\n- It impossible to use when thumbnail has different aspect ratio than detail image.\r\n- It will be hard to use for beginners. Especially the `sizes` attribute is difficult.\r\n- Thumbnail/Detail use case is undocumented: https://www.ampproject.org/docs/reference/components/amp-lightbox-gallery\r\n\r\n\r\n"},
{"text": "Should the G+ option now be removed from the amp-social-share documentation since G+ close?\r\n\r\nhttps://www.ampproject.org/docs/reference/components/amp-social-share"},
{"text": "Write developer documentation for the different story UI Types.\r\n\r\nContext: https://github.com/ampproject/amphtml/pull/21766#discussion_r278372743"},
{"text": "On [this documentation about CORS](https://github.com/ampproject/amphtml/blob/master/spec/amp-cors-requests.md), the example with the line:\r\n`// If same origin\r\n  if (req.headers['amp-same-origin'] == 'true') {`\r\n\r\nIf some malicious site (non-AMP) evil.com inject amp-same-origin=true into the request header, the code would bypass other checks and result in setting \r\nres.setHeader('Access-Control-Allow-Origin', origin _**evil.com**_); into the response header\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nVersion notes states `0.1` will be deprecated and removed on March 3rd of 2019. This date has passed and should be updated. \r\n\r\n**Which part of the site would that content live at?**\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#version-notes"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nSimilar to [amp-carousel](https://amp.dev/documentation/components/amp-carousel/?format=websites#attributes), amp-story attributes should be organized into a table and given code samples where needed. \r\n\r\n**Which part of the site would that content live at?**\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#attributes\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nUpdate documentation to reflect these features are no longer new, but now standard. \r\n\r\n**Which part of the site would that content live at?**\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#migrating-from-0.1-to-1.0\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nThe overview should be the first thing developers read to understand context of an AMP component. This section should come after the table. \r\n\r\n**Which part of the site would that content live at?**\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#overview\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nWe explain what a poster is and what its used for, but we do not include a code sample or instructions on how to use it. \r\n\r\n**Which part of the site would that content live at?**\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#posters\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\n`amp-story-page` is a separate amp component from `amp-story`. It should be migrated to its own page and include a code samples and a top table with description, required script, ect. \r\n\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#children-(of-amp-story)\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nUpdate documentation to state `0.1` is invalid AMP and will be deleted in the near future. If possible, quantify what \"near future\" is.  At that point it will be aliased to `1.0` and explain that that means. \r\n\r\n**Which part of the site would that content live at?**\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#migrating-from-0.1-to-1.0\r\n\r\n\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\n`amp-story-grid-layer` is a separate amp component from `amp-story`. It should be migrated to its own page and include a code samples and a top table with description, required script, ect.\r\n\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#layers\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\n`amp-story-cta-layer` is a separate amp component from `amp-story`. It should be migrated to its own page and include a code samples and a top table with description, required script, ect. \r\n\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#amp-story-cta-layer"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\n`amp-story-page-attachmente` is a separate amp component from `amp-story`. It should be migrated to its own page and include a code samples and a top table with description, required script, ect. \r\n\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#page-attachments\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nThe new bookend capabilities are no longer new and should be under the larger [bookend header](https://amp.dev/documentation/components/amp-story/?format=websites#bookend:-amp-story-bookend). Language should be updated to remove instances of \"new\". \r\n\r\nhttps://amp.dev/documentation/components/amp-story/?format=websites#new-bookend-capabilities\r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nWhen filtering component documentation [`amp-sidebar`](https://amp.dev/documentation/components/amp-sidebar/?format=websites#sidebar-for-stories) does not appear for stories in the sidebar. The frontmatter should be updated so when developers filter for stories they see the component is usable. \r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nWhen filtering component documentation [`amp-twitter`](https://amp.dev/documentation/components/amp-twitter/?format=websites) does not appear for stories in the sidebar. The frontmatter should be updated so when developers filter for stories they see the component is usable. \r\n"},
{"text": "## Missing or out-of-date documentation\r\n\r\n**Describe the content that is missing or should be up dated**\r\nWhen filtering component documentation [`amp-consent`](https://amp.dev/documentation/components/amp-consent/?format=websites#prompt-ui-for-stories) does not appear for stories in the sidebar. The frontmatter should be updated so when developers filter for stories they see the component is usable. \r\n"},
{"text": "\r\n\r\n## What's the issue?\r\n\r\n`<amp-social-share type=\"system\" ` doesn't work on HTTP sites, yet this is not [documented and can cause confusion](https://stackoverflow.com/questions/57605848/solved-amp-social-share-type-system-doesnt-seem-to-work-on-chrome-android/57628117?noredirect=1)\r\n\r\n## How do we reproduce the issue?\r\n\r\nNo SSL requirements written in the docs  https://amp.dev/documentation/components/amp-social-share/ even tho if you test your site on HTTP, the system icon won't show. \r\n\r\n\r\n## What browsers are affected?\r\n\r\nAndroid devices with Chrome\r\n\r\n"},
{"text": "Follow up to https://github.com/ampproject/amphtml/pull/24290. \r\n\r\nWe have automatic truncation of elements with overflowing contents and an optional overflow element that allows people to provide a fallback (a button that resizes elements on click). The heuristics for when the element gets truncated (and overflow button is shown) is not clear. We should document somewhere that resizing is allowed when: \r\n1. The bottom of the element is below the viewport, or\r\n2. If the bottom of the element is within 15% of the height of the entire page AND within 1000px of the end of the page."},
{"text": "## What's the issue?\r\n\r\n[amp-script reference](https://amp.dev/documentation/components/amp-script/?format=websites) documentation needs to be updated. Reference documentation is the source of truth for components, amp-script documentation is missing vital information. \r\n\r\n\r\n## Update requests\r\n\r\n- A simple example: copy should be more clear. \"An `amp-script` element can load JavaScript in two ways.\" With subheadings \"Load file from a URL\" and \"Reference local script\" with further explanation on referencing `id`. \r\n\r\n- Use of inline script for amp-script: we either need to add a warning that it currently isn't valid AMP, or we need to allow it. \r\n\r\n- Include section on CSP. We need to explain when a CSP is appropriate and how to hash it (an example of how to do this would be appreciated). Should probably be a top level header or update \"Security features\" to include this information. \r\n\r\n- Remove \"Interested in using amp-script?\" - amp-script is live, this information doesn't seem relevant. \r\n\r\n- Remove the FAQ: if it is asked frequently enough it should have a permeant place in the documentation. \r\n    - \"Which JavaScript APIs can I use?\" This should be moved under \"Restrictions\" with a subheading of \"Allowed APIs\" . Combine with \"Can you support ____ API?\" to include instructions on how to request APIs. \r\n    - Remove all other FAQ - these should be answered/addressed under \"Size of JavaScript code\" and CSP information. \r\n\r\nPlease let me know when these changes are ready to be reviewed! \r\n"},
{"text": "Context: I'm waiting for this [worker-dom PR](https://github.com/ampproject/worker-dom/pull/629) to make it into amp-script. I don't know how to track the release.\r\n\r\nIt'd be great to have two things:\r\n\r\n1. Add an FAQ entry in the amp-script documentation explaining how long it usually takes for a worker-doc feature to end in amp-script.\r\n2. Document the currently used worker-dom version (ideally with a link to version specific API docs).\r\n\r\n//cc @kristoferbaxter @choumx "},
{"text": "Add some documentation and maybe a runtime warning to discourage `on=\"tap:AMP.navigateTo(...)\"` in favor of `<a>` tags. The former has no crawler support."},
{"text": "`amp-story-cta-layer` should not be on the first page and is ignored by the runtime if present. This should be documented at: https://amp.dev/documentation/components/amp-story-cta-layer/?format=stories"},
{"text": "Today, we use [Prettier](https://prettier.io/) to [format and fix](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#code-quality-and-style) all our JS files and several non-JS files. The format checker has [supported](https://prettier.io/blog/2017/11/07/1.8.0.html) Markdown files for a while now.\r\n\r\n**Question:** Shall we start auto-formatting our `.md` files with Prettier?\r\n\r\n**Pros:**\r\n- One [single standard](https://prettier.io/docs/en/index.html) for the internal structure of documentation files\r\n- IDE support for [auto-fix-on-save](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#workflow-for-visual-studio-code) while editing code\r\n- Command line support for [auto-fixing local changes](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#manually-fixing-code)\r\n- Possible to [override](https://prettier.io/docs/en/ignore.html#markdown) Prettier's auto-format behavior for sections of code\r\n- Possible to [auto-wrap](https://prettier.io/docs/en/options.html#prose-wrap) text for easy editing\r\n\r\n**Cons:**\r\n- There might be some small changes to how `.md` files look when they are rendered\r\n    - Attribute lists are changed to [all-in-one-line](https://github.com/ampproject/amphtml/compare/master...rsimha:2019-10-18-PrettifyMarkdown#diff-8e253a89bfca066fd8ca624c28468261R295) if they all fit on one line\r\n![image](https://user-images.githubusercontent.com/26553114/67242799-42e7d380-f424-11e9-8596-7d001c5644a8.png)\r\n    - Attribute lists are changed to [one-per-line](https://github.com/ampproject/amphtml/compare/master...rsimha:2019-10-18-PrettifyMarkdown#diff-5b53438fd782012c7289a8bb5107d86b) if they don't all fit on one line\r\n![image](https://user-images.githubusercontent.com/26553114/67242852-601ca200-f424-11e9-93d6-1d73690ff7cf.png)\r\n\r\n- Failing `gulp prettify` checks can be annoying if an occasional contributor makes a badly formatted change\r\n- No easy way to auto-format code with Prettier if you edit code with the GitHub web-UI\r\n\r\n@CrystalOnScript @mrjoro @pbakaus @ampproject/wg-outreach Thoughts?\r\n"},
{"text": "## Overview\r\n\r\nThis is a proposal for removing the overview tables in component docs as they are hard to maintain. Instead, we should maintain the data in the frontmatter and create a better doc UX on amp.dev using this data. \r\n\r\n### Problem\r\n\r\nProblems with the tables:\r\n\r\n* hard to read on mobile\r\n* tedious to write in markdown due to html tables\r\n* inconsistent formatting\r\n* hard to parse and automatically update\r\n* required script row should be auto generated as it is potentially format specific (e.g. emails support a different version than websites).\r\n\r\n### Proposal\r\n\r\nRemove the tables from the component reference docs. Auto-generate content if possible, put all other information into the frontmatter.\r\n\r\nOn Github:\r\n\r\n1. Remove examples list\r\n2. Remove import script\r\n3. Move status  (`PROD`, `EXPERIMENT`)  into frontmatter\r\n4. Move supported layouts into frontmatter\r\n\r\nOn amp.dev:\r\n\r\n1. Auto-generate and inject import script specific to the selected format.\r\n2. Indicate status (`PROD`, `EXPERIMENT`) via a label behind the title.\r\n3. Auto-generate and inject list of related samples.\r\n4. Inject list of supported formats based \r\n\r\n### Open questions\r\n\r\n* On Github, is it a problem if **import script** and **supported layouts** are no longer displayed in the markdown version of the component doc? (e.g. [amp-carousel](https://github.com/ampproject/amphtml/blob/master/extensions/amp-carousel/amp-carousel.md))\r\n\r\n//cc @ampproject/wg-outreach @CrystalOnScript @pbakaus @matthiasrohmer @tharders "},
{"text": "[AMP Story and Analytics](https://github.com/ampproject/amphtml/blob/master/extensions/amp-story/amp-story-analytics.md) should be moved into [amp-story.md](https://github.com/ampproject/amphtml/blob/master/extensions/amp-story/amp-story.md) under an `Analytics` header. Components that support analytics specify analytic information in the reference documentation. \r\n"},
{"text": "## What's the issue?\r\n\r\nComponent specific actions and events should be included on the component reference document. \r\n\r\nSuch as [amp-sidebar](https://amp.dev/documentation/guides-and-tutorials/learn/amp-actions-and-events/#amp-sidebar)\r\n\r\n"},
{"text": "We currently have an issue where by default, the `tap` action does not trigger on keyboard enter. In fact, tap actions can be registered on `div` or `amp-img` or other elements that don't have a tabindex and are therefore unaccessible. \r\n\r\nWe already have a feature where we automatically register key triggers on enter and space for tap actions if a `TAPPABLE_ARIA_ROLE` is applied to the element. \r\nhttps://github.com/ampproject/amphtml/blob/af35a6eae86e38933dff9f1d2c9f24a30e915a0d/src/service/action-impl.js#L69-L85\r\n\r\nWe should highlight and recommend this in our documentation. In particular, if a tap action is applied to an element that isn't a button, we should recommend that it include one of the `TAPPABLE_ARIA_ROLES`, as well as have a tabindex so that the tap action is accessible. \r\n\r\nOur current documentation example uses a button. We should add an example that does not use a button and instead includes a tappable aria role. \r\n\r\n/cc @CrystalOnScript "},
{"text": "**summary**\r\nAfter landing prefix stripping for amp-list (https://github.com/ampproject/amphtml/pull/25489), we should also add it's documentation. On top of that we should also figure out which other components to add it to.\r\n\r\n**conditions of satisfaction**\r\n- [x] `amp-list.md` documentation updated to explain usage of the new strip-prefix attribute.\r\n- [x] determined list of json fetching components to add `strip-prefix` to."},
{"text": "## Summary\r\n\r\nI propose we add a YAML file to each component's folder that'll be used as a \"spec\" for the actions and events it supports.\r\n\r\nFor example: `extensions/amp-carousel/amp-carousel.yaml` could be:\r\n```yaml\r\namp-carousel:\r\n  actions:\r\n    goToSlide:\r\n      parameters:\r\n        index:\r\n          type: INTEGER\r\n          description: slide index to go to\r\n          required: true\r\n      description: Advances the carousel to a specified slide index.\r\n    toggleAutoplay:\r\n      parameters:\r\n        toggleOn:\r\n          type: BOOLEAN\r\n          description: value to toggle to\r\n          required: false\r\n      description: Toggle the carousel's autoplay status.\r\n  events:\r\n    slideChange:\r\n      description: Fired when the carousel's current slide changes.\r\n      data:\r\n        event.index:\r\n          type: INTEGER\r\n          description: index of slide changed to\r\n```\r\n\r\n## Reasoning\r\n\r\nCurrently, all actions and events are documented here: [Actions and Events](https://amp.dev/documentation/guides-and-tutorials/learn/amp-actions-and-events/?format=websites). However, there are a few issues with this:\r\n\r\n- That page has low discoverability. If we used a YAML file as a source of truth, we could also include actions and events on each component page easily.\r\n- That page is also manually maintained and is centralized. Having the YAML file in each component's directory helps mitigate that, similar to how we have validator specs separated.\r\n- Having an easy-to-process actions and events index would allow us to build better tooling and more documentation automation, which is cool.\r\n- AMP for Email only allows a limited subset of actions and we decided in the last WG meeting to standardize it in some form. Having YAML files makes this easier, as we could just have a `formats` field for each action, similar to how the validator has different rules for different formats.\r\n\r\n## Corner cases\r\n\r\n- Global actions (part of `AMP`) are provided by the runtime. This means no component would have them. Instead, this requires another YAML file for the runtime.\r\n- In additon, there are actions that are not added to any component, but are global: `AMP` and `amp-access`. To handle these, there should be a syntax that indicates the action is on a global object:\r\n```yaml\r\nglobal:\r\n  AMP:\r\n    actions:\r\n      ...\r\n  amp-carousel:\r\n    actions:\r\n      ...\r\n```\r\n- There are events and actions that apply to all elements. We need a special syntax for this too:\r\n```yaml\r\n\"*\":\r\n  events:\r\n    tap:\r\n      ...\r\n```\r\n- Some elements that aren't component (e.g. `input`) may have events. In this case, they could either be part of the component YAML for the most relevant component (`amp-form`) or part of the runtime YAML.\r\n\r\n## Initial creation\r\n\r\nTo initially create these YAML files, we could just parse [amp-actions-and-events.md](https://github.com/ampproject/amp.dev/blob/future/pages/content/amp-dev/documentation/guides-and-tutorials/learn/amp-actions-and-events.md).\r\n\r\n## Keeping it updated\r\n\r\nIdeas welcome. I wonder if there's a way to make a unit test that ensures a YAML line is in line with what the component provides?\r\n\r\n//cc @sebastianbenz "},
{"text": "## What's the issue?\r\n\r\nNormally when wanting to install a service worker, the way to do so is to use the aptly-named [`amp-install-serviceworker`](https://amp.dev/documentation/components/amp-install-serviceworker) component.\r\n\r\nHowever, if wanting to then add push notifications, the [`amp-web-push`](https://amp.dev/documentation/components/amp-web-push/) component needs to be used. \r\n\r\nThis introduces a problem because the `amp-web-push` component _also_ takes the responsibility of installing a service worker (apparently). Because only one service worker can be active on a given page, this introduces a conflict.\r\n\r\nThis being said, in testing if I add both components to the page, it doesn't seem to result in the service worker attempting to be installed twice. In fact, only the service worker referenced in `amp-install-serviceworker` is being added, even when I put `<amp-web-push>` _before_ `<amp-install-serviceworker>`. I'm not seeing `<amp-web-push>` install any service worker at all, so maybe I am not correctly understanding how it is supposed to work. However, if it is supposed to install a service worker it could create major confusions when a user tries to supply a service worker script that is specific to push notifications on `<amp-web-push>` but another script entirely for `<amp-install-serviceworker>`. This should get raised by the component as a user error.\r\n\r\nAlso, ideally the `amp-web-push` component would allow omission of the `service-worker-url` attribute if there is an `<amp-install-serviceworker>` element on the page. This would also allow `amp-web-push` to take advantage of the capabilities of `amp-install-serviceworker`, such as installing the origin service worker while accessing a page loaded from an AMP Cache.\r\n\r\nAll of this to say, I am not clear if there is actually a conflict. But since the documentation for the components does not refer to each other, it is not clear if they can work together. If not, they should. And the docs should be updated to clarify.\r\n\r\n## How do we reproduce the issue?\r\n\r\nInclude both `amp-service-worker` and `amp-web-push` on a page."},
{"text": "The  `amp-3d-gltf` component specifies that it defaults to a black background under the documentation for the `alpha` attribute. Per [PR #17085](https://github.com/ampproject/amphtml/pull/17085), this behavior was changed to white by default. \r\n\r\nThe documentation should explain clearly that:\r\n\r\n- by default, the background color of `amp-3d-gltf` is white\r\n- the `alpha` attribute specifies transparency \r\n- the `clearColor` attribute specifies the color of the background if not transparent\r\n\r\nSome of this should probably be added under the [`### Styling` header ](https://github.com/ampproject/amphtml/blob/master/extensions/amp-accordion/amp-accordion.md#styling)"},
{"text": "## What's the issue?\r\n\r\nNo one should use `amp-font` now that `font-display` exists.\r\n\r\nWhile we are removing documentation from amp.dev, it will still have documentation on GitHub since the extension is not deprecated/removed. \r\n\r\nWe should add a blurb explaining to not use the component and point users towards appropriate documentation. \r\n\r\n"},
{"text": "The attributes section is currently empty. It's probably worth to revisit the structure of the whole doc as well. Might be worth explaining how the viewport-margin attribute works, and how it's different from IntersectionObserver's rootMargin. \r\n\r\n//cc @CrystalOnScript @ampproject/wg-ui-and-a11y \r\n"},
{"text": "The `clear` action described on `amp-selector` [documentation page](https://amp.dev/documentation/components/amp-selector/#clearing-selections) should also be listed among its actions in the [AMP Actions and Events documentation](https://amp.dev/documentation/guides-and-tutorials/learn/amp-actions-and-events/?referrer=ampproject.org#amp-selector_1)."},
{"text": "## Describe the new feature or change to an existing feature you'd like to see\r\nI am having a hard time understanding what value I am supposed to give for the `url` property in the manifest.json based off the example given here.\r\n\r\n```\r\n{\r\n  \"prefer_related_applications\": true, // This is not necessary for <amp-app-banner>, but signals a preference on non-AMP pages using the same manifest.json file for the native app over a web app if available\r\n  \"related_applications\": [\r\n    {\r\n      \"platform\": \"play\",\r\n      \"id\": \"com.app.path\",\r\n      \"url\": \"android-app://com.app.path/https/host.com/path/to/app-content\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nhttps://amp.dev/documentation/components/amp-app-banner/\r\n\r\nTo an extent the same for the meta tag where I link to the IOS app. This may partly be due to my inexperience with deep linking, but I think it could really help clarify the docs if an example app was given with a fake ID and article to link to. Something like \"The Burgers2Go app has an ID of xzy, and have pages with the url structure https://www.burgers2go.com/help/our-burgers, so the manifest.json would look like this:\"\r\n\r\nOtherwise it is hard for me to tell in some of the stuff in the docs for this component what must always be present, what is placeholder for content dependent on the app details, and what the content I replace the placholder with should look like. \r\n\r\n## Describe alternatives you've considered\r\n\r\nI looked for more full examples of the manifest.json file like the full page article example [found here \r\n](https://github.com/ampproject/amphtml/blob/master/examples/article.amp.html) and the [examples page for the amp-app-banner ](https://amp.dev/documentation/examples/components/amp-app-banner/?format=websites) and couldn't find much to help me. It is also worth noting that the [manifest file that is linked from that examples page doesn't have the url property](https://amp.dev/amp-app-banner-manifest.json).\r\n\r\n## Additional context\r\nIf I knew the answers to these questions I would put out a PR for the docs myself, but sadly I don't. I am sorry if this is an unconventional issue that I have created here. \r\n"},
{"text": "## Describe the new feature or change to an existing feature you'd like to see\r\n\r\nI'm adapting my website to be AMP ready. I've faced with the problem that a little amount of services support embedding as amp-iframe https://amp.dev/documentation/components/amp-iframe/\r\n\r\nI mean iframe resizing to be true responsive https://amp.dev/documentation/components/amp-iframe/#iframe-resizing\r\n\r\n## Describe alternatives you've considered\r\n\r\nCould you please write good instructions for **website/service/iframe owners**: how to adapt their iframe to be AMP ready. This is not difficult and this instruction will help developers to implement it in correct way.\r\n\r\n## Additional context\r\n\r\nIt will be easy for me as website owner send this instruction to iframe owner to show what iframe should supports to be responsive on my website.\r\n"},
{"text": "document everything from `amp-analytics/0.1/variables.js`"},
{"text": "Original Launchpad bug 502798: https://bugs.launchpad.net/ipython/+bug/502798\nReported by: ellisonbg (Brian Granger).\n\nThe ipcluster program has been completely refactored to use the new config system.  The Sphinx docs for ipcluster need to be updated to reflect this.\n"},
{"text": "Original Launchpad bug 502799: https://bugs.launchpad.net/ipython/+bug/502799\nReported by: ellisonbg (Brian Granger).\n\nIPython.kernel was refactored in the Fall of 2009 to use the new config system.  We need to write the changes for this merge in our Sphinx docs.\n"},
{"text": "Original Launchpad bug 507078: https://bugs.launchpad.net/ipython/+bug/507078\nReported by: fdo.perez (Fernando Perez).\n\nExplain better how to test an uninstalled IPython (via pythonpath manipulations).\n\nIf possible, make the package testable in-place before installation, that would be ideal.  If not, explain better.\n"},
{"text": "Original Launchpad bug 420123: https://bugs.launchpad.net/ipython/+bug/420123\nReported by: timmie (Tim).\n\nIt would be good to have the Sphinx docs for download or offline reading at:\nhttp://ipython.scipy.org/moin/Download\n\nThis could be offered for the latest stable release.\n\nThanks.\n"},
{"text": "Original Launchpad bug 371041: https://bugs.launchpad.net/ipython/+bug/371041\nReported by: fdo.perez (Fernando Perez).\n\nOne unintended side effect of using a DVCS is that often, private commit messages aren't terribly informative (you're in a very fast-paced, experimental mode) and yet they show up in the final tree from which we later need to reconstruct good global  changelogs for the release notes.\n\nHere's an interesting discussion on the topic in the context of linux:\n\nhttp://lwn.net/Articles/328761/\n\nThis is mostly a note to myself to spend some time writing a bit about this in the docs, anyone is welcome to contribute ideas.\n"},
{"text": "In master, IPython.kernel has been removed in preparation for newparallel to be merged.  However, the old docs and examples are stil in place.  Once newparallel has been merged, we need to update the docs and examples and remove the Twisted ones.\n"},
{"text": "these instructions are out of date and wrong:\n\nhttp://ipython.github.com/ipython-doc/dev/install/install.html#installing-the-development-version\n\nmore details:\nhttp://mail.scipy.org/pipermail/ipython-dev/2011-April/007477\n"},
{"text": "The section of the messaging spec describing the history protocol is outdated.\n\nAre we dropping 'history_request' in favor of the (less flexible) 'history_tail_request'? If so, we need to change the spec. If not, we should implement the full protocol using Thomas' new history manager.\n"},
{"text": "the manpages in docs/man are not up to date with 0.11\ne.g. man ipython still uses the single hyphen for options which is so far I know deprecated\nalso it mentions some options in which don't seem to work anymore, like -autocall\nit should also include an explanation of the new configuration system.\n"},
{"text": "By popular demand, traditional '--args=values' is restored in master.  The docs should be updated to reflect this, because we should probably remove support for the 'args=values' in 0.12.  The only reason it will be supported in 0.11 is rc1 made it into EPD this week, but our docs should not reflect this pattern that won't stick.\n"},
{"text": "As pointed out on-list, we have dev docs for the config system, but not really a user-guide for how to configure IPython.\n\nWe have approximately zero help for users who want to migrate their old config to 0.11.\n\nA great place to start would be for people to post their `ipy_user_conf.py` or `ipythonrc` somewhere, so people familiar with the new config can provide translations as examples, and see what common patterns are for writing a simple doc.  This looks like a job for the [Wiki](http://wiki.ipython.org).\n"},
{"text": "I was unable to get the ipython notebook working in firefox (works fine in chrome). Turns out \nthat a newer version of tornado than 2.0 is needed for firefox. I used a fresh checkout from \ngit from yesterday and that works fine. I guess the websockets in tornado was updated to reflect that websockets in\nfirefox 6 has a moz prefix. It would be great if the 0.12 documentation could be updated to reflect this. \n"},
{"text": "need to add the following info to the docs.\n\nindeed there is vary basic authentication available and is highly recommended to do this over an SSL connection. for security reasons this password cannot be provided on the command line and has to be encoded in the notebook config file.\n\nhere is the relevant section from my ipython_notebook_config.py\n\nthe documentation for creating the certificate file is on the web.\n\nhttp://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html\n\n<pre>\n# The full path to an SSL/TLS certificate file.\nc.IPythonNotebookApp.certfile = u'/users/satra/mycert.pem'\n\n# The FQDN or IP for WebSocket connections. The default will work fine when the\n# server is listening on localhost, but this needs to be set if the ip option is\n# used. It will be used as the hostname part of the WebSocket url:\n# ws://hostname/path.\nc.IPythonNotebookApp.ws_hostname = 'fullname.mit.edu'\n\n# The IP address the notebook server will listen on.\nc.IPythonNotebookApp.ip = '*'\n\nc.IPythonNotebookApp.password = u'password'\n</pre>\n"},
{"text": "@minrk, pinning it on you since you mentioned you'd add this to the docs, this is just a reminder to keep it on the radar.\n\nThread started [here](http://mail.scipy.org/pipermail/ipython-user/2011-November/008726.html) on the user list and [ended here](http://mail.scipy.org/pipermail/ipython-user/2011-November/008730.html).\n"},
{"text": "Hi, \nJust had to test the notebook on a ubuntu machine today, which is on the same side of a proxy than the machine on which the ipython notebook server is. \nWebsocket connexion failed if the ip of the server is not blacklisted in the firefox preferences to not use the proxy.\n\nIt seems not to be a proxy configuration issues as a windows machine with the same version of FF and also autodetect proxy works fine.\n\nSo this is just an issue we should put  the workaroud somewhere in the doc. Don't know if it's ubuntu specific, and will try to get the proxy configuration to see if it come from there.\n"},
{"text": "The docs for the HTML notebook password still describe a plaintext password in the config file. I remember this was updated to use a salted and hashed password.\n"},
{"text": "The docs have [a space](http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html#notebook-document-format) for a description of the notebook file format, but so far there's nothing in it.\n"},
{"text": "The notebook is known to work well with current Chrome and Firefox.  Safari and apparently Opera appear to work reasonably well, but it seems that Safari doesn't work with secure (wss) connections, and the same may be true of Opera.  This may be a certificate issue, and the same thing may be affecting Opera.  Once we better understand the causes of these issues, notes should be added to docs.\n\n[This SO question](http://stackoverflow.com/questions/4014055/how-to-debug-safari-silently-failing-to-connect-to-a-secure-websocket) may be relevant for the Safari wss issue.\n"},
{"text": "The documentation page could quickly remind that \"%load_ext rmagic\" is required before anything.\nhttp://ipython.org/ipython-doc/dev/config/extensions/rmagic.html\n\n(it is the case on the page for the extension autoreload)\n"},
{"text": "The octave magic docs/examples should update this  information.\n"},
{"text": "I'm not sure when this started happening, I haven't used store in a while, but just ran into this. Basically, storing works, but restores don't automatically happen anymore. We should either:\n1. changes this behavior so that the auto-restores start to happen again (preferred), or \n2. just update the docs to show how to make stored things available (via `store -r`). See this session\n\n```\nIn [1]: pi = 3.1415926\n\nIn [2]: pi\nOut[2]: 3.1415926\n\nIn [3]: store pi\nStored 'pi' (float)\n\nIn [4]: store\nStored variables and their in-db values:\npi                  -> 3.1415926\n\nIn [5]: \nDo you really want to exit ([y]/n)? y\n15:44@~(HbIOTOH)$ ipython\nIn [1]: pi\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n<ipython-input-1-68f7b1e53523> in <module>()\n----> 1 pi\n\nNameError: name 'pi' is not defined\n\nIn [2]: store\nStored variables and their in-db values:\npi                  -> 3.1415926\n\nIn [3]: store -r\n\nIn [4]: pi\nOut[4]: 3.1415926\n```\n\nwhereas the docs suggest one shouldn't need to do `store -r`:\n\n```\nExample::\n\n  In [1]: l = ['hello',10,'world']\n  In [2]: %store l\n  In [3]: exit\n\n  (IPython session is closed and started again...)\n\n  ville@badger:~$ ipython\n  In [1]: l\n  Out[1]: ['hello', 10, 'world']\n```\n"},
{"text": "I suggest using jsdoc 3\n\nhttp://usejsdoc.org/\n"},
{"text": "Currently organizing code in different modules or different functions results in errors when such code is run remotely (see https://github.com/ipython/ipython/issues/2473#issuecomment-9200719 for an example).\n\nIt would be nice if the documentation would show how to work around such issues and maybe implement a simple way to decorate such functions so that running them remotely is easy.\n"},
{"text": "It would be a great help if the html documentation can be offered for downloading. Many people like offline use of html docs much better than pdf ones.\n\nPython, Django and pocoo's packages offer html docs for downloading. I hope ipython can join this best practice. Thanks.\n"},
{"text": "I think it'd be worthwhile to document (or provide a pointer to) the common shortcuts inside notebook cells. I remember that `Ctrl-Z` does an undo, but to do a redo, I end up trying `Ctrl-Shift-Z` first, before I get to the proper `Ctrl-Y`.\n"},
{"text": "Docs for all common notebook (web interface) keyboard shortcuts?\n\nAs in simply creating a simple doc page online for them? ~ Relatively trivial\n"},
{"labels":[null,"documentation"],"text":"[`ActiveRecord::Base.create_or_find_by/!`](https://apidock.com/rails/v6.0.0/ActiveRecord/Relation/create_or_find_by) attempts to improve on [`ActiveRecord::Base.find_or_create_by/!`](https://apidock.com/rails/v6.0.0/ActiveRecord/Relation/find_or_create_by) by avoiding race conditions that the previous implementation was susceptible to.\r\n\r\n`create_or_find_by` attempts to first create the record while relying on the database uniqueness constraint to inform us if it already exists.\r\nWhen the record does exist, it catches an exception and finds the record using the attributes provided. \r\n\r\nMy initial expectation of the method was that if used properly, it's expected to always return a record, even in concurrent situations.\r\n\r\nThe new strategy is still susceptible to race conditions which are not mentioned in the docs, the behavior in these cases is dependent on the database isolation level. \r\n\r\nMore concretely, `create_or_find_by` could raise a `RecordNotFound` if called from within an existing transaction in a repeatable reads isolation level.\r\n\r\nGiven how common it is for application code to run within an existing transaction (For example, code inside AR callbacks), and the fact that repeatable reads is the default isolation level for mysql, I think this is a fairly common use case.\r\n\r\n### Reproduction\r\n\r\nRepeatable read guarantees a consistent snapshot of the database, in InnoDB, the snapshot is established during the [first read](https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html). This means that if our transaction performed a read before a record is created by another transaction, we won't \"see\" it. In respect to uniqueness constraints, if we try inserting a record that violates it, the database would still protect us, but if we try querying for the record, it'll seem like it doesn't exist.\r\n\r\nIn the context of [`ActiveRecord::Base.create_or_find_by/!`](https://apidock.com/rails/v6.0.0/ActiveRecord/Relation/create_or_find_by), this would manifest as a uniqueness error being raised, but the record not existing.\r\n\r\n```ruby\r\ndef create_or_find_by(attributes, &block)\r\n  transaction(requires_new: true) { create(attributes, &block) }\r\nrescue ActiveRecord::RecordNotUnique \r\n  # exception caught, but we can't see the record because it's not present in our snapshot \r\n  find_by!(attributes)\r\nend\r\n```\r\n\r\nI created a few [test cases](https://gist.github.com/mzruya/603f722ede1615fd7957cfd95a4d466c#file-test_create_or_find_by_race_condition-rb-L101-L127) that demonstrate the issue.\r\n\r\n### Suggestions\r\nIt doesn't seem like it's possible to fix this behavior, the database isolation level would always prevent us from returning something that another connection committed. \r\n\r\nI do think that there's a misconception (at least I had it) that `ActiveRecord::Base.create_or_find_by` is meant to be atomic and work correctly in concurrent situations. \r\n\r\nTherefore it probably makes sense to document this behavior a bit more thoroughly and mention these caveats in the [docs](https://apidock.com/rails/v6.0.0/ActiveRecord/Relation/create_or_find_by).\r\n\r\nI'll gladly take on improving the current documentation if that seems reasonable.\r\nBut I'd also love to hear if anyone has any other thoughts on this behavior.\r\n\r\nThank you!"},{"labels":[null,"documentation"],"text":"### Steps to reproduce\r\nexecutable test case:\r\nhttps://gist.github.com/patrickneugebauer/6cd48c71835cf03aa2a332871f26efdf\r\n\r\n### Expected behavior\r\n`validates_presence_of :record` will not allow save if `record` is not persisted and cannot be persisted.\r\n\r\n### Actual behavior\r\n`validates_presence_of :record` allows save if `record` is not persisted and cannot be persisted.\r\n\r\n### System configuration\r\n**Rails version**: Rails 6.0.3.1\r\n\r\n**Ruby version**: ruby 2.7.1p83 (2020-03-31 revision a0c7c23c9c) [x86_64-linux]\r\n\r\n### Did you write a patch that fixes a bug?\r\nNo, wondering if this is expected before I dig in further.\r\nShould people just know to use `validated_presence_of :record, :record_id` together to ensure integrity of connections?\r\n\r\nI checked the documentation and it is not technically wrong, but I feel it could be improved by adding detail about what is expected.\r\n\r\nhttps://guides.rubyonrails.org/active_record_validations.html#presence\r\n\r\n> If you want to be sure that an association is present, you'll need to test whether the associated object itself is present, and not the foreign key used to map the association. This way, it is not only checked that the foreign key is not empty but also that the referenced object exists.\r\n```\r\nclass LineItem < ApplicationRecord\r\n  belongs_to :order\r\n  validates :order, presence: true\r\nend\r\n```\r\n"},{"labels":[null,"documentation"],"text":"The `ActiveRecord::Base.connection_handler.while_preventing_writes` method does not prevent all writes as the name suggests.\r\n\r\nThis should be mentioned in the method documentation, particularly because in the [original Changelog](https://github.com/eileencodes/rails/blob/f39d72d5267baed1000932831cda98503d1e1047/activerecord/CHANGELOG.md) the author @eileencodes does mention this.\r\n\r\n### Description\r\n\r\nWhile looking for a way to safely expose a sql query parameter I stumbled upon the `while_preventing_writes` method. I was surprised that I wouldn't need a another readonly DB user. Not quite sure if this would really work, I did some digging and found that, at least in Postgresql, this method is not sufficient to block all writes. \r\n\r\n### Steps to reproduce\r\n\r\nAll of the following queries do write to a Postgresql database in a `while_preventing_writes` block:\r\n\r\n#### Explain analyze\r\n\r\nPostgresql let's you not only explain a query, you can also [explain and analyze](https://www.postgresql.org/docs/current/sql-explain.html) a query which actually **runs** it and also analyzes it:\r\n\r\n```ruby\r\nActiveRecord::Base.connection_handler.while_preventing_writes do\r\n  ActiveRecord::Base.connection.execute \"EXPLAIN ANALYZE INSERT INTO roles_users (role_id, user_id) VALUES (1, 1234)\"\r\n  ActiveRecord::Base.connection.execute \"EXPLAIN ANALYZE UPDATE users SET role = 'admin' WHERE id = 10\"\r\n  ActiveRecord::Base.connection.execute \"EXPLAIN ANALYZE DELETE FROM users WHERE id = 26\"\r\nend\r\n```\r\n\r\n#### Nested comments\r\n\r\nPostgres has a [peculiar comment syntax](https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-COMMENTS) which lets you nest comments within other comments:\r\n\r\n```\r\n/* comment with nesting: /* nested comment */ */\r\n```\r\n\r\nThat is why these comments look like SELECT queries to the regular expression [READ_QUERY](https://github.com/rails/rails/blob/7cd59448ba7b841b47cf8fdbdab55053e56ba787/activerecord/lib/active_record/connection_adapters/postgresql/database_statements.rb#L23) which is supposed to guard against writes, but to the SQL parser they are comments:\r\n\r\n```ruby\r\nActiveRecord::Base.connection_handler.while_preventing_writes do\r\n  ActiveRecord::Base.connection.execute \"/*/**/SELECT*/ INSERT INTO roles_users (role_id, user_id) VALUES (1, 1234)\"\r\n  ActiveRecord::Base.connection.execute \"/*/**/SELECT*/ UPDATE users SET role = 'admin' WHERE id = 10\"\r\n  ActiveRecord::Base.connection.execute \"/*/**/SELECT*/ DELETE FROM users WHERE id = 26\"\r\nend\r\n```\r\n\r\n#### Select into\r\n\r\nFinally postgres let's you create tables using the [`SELECT INTO` statement](https://www.postgresql.org/docs/12/sql-selectinto.html), which starts the query with a `SELECT` but then goes on to write the result into a new table. While it cannot write into an existing table, technically it's still a write query:\r\n\r\n```ruby\r\nActiveRecord::Base.connection_handler.while_preventing_writes do\r\n  ActiveRecord::Base.connection.execute \"SELECT * INTO users_new FROM users\"\r\nend\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe writing queries are not applied to the database.\r\n\r\n### Actual behavior\r\n\r\nThe queries are written to the database.  The [regular](https://github.com/rails/rails/blob/7cd59448ba7b841b47cf8fdbdab55053e56ba787/activerecord/lib/active_record/connection_adapters/postgresql/database_statements.rb#L23) [expression](https://github.com/rails/rails/blob/7cd59448ba7b841b47cf8fdbdab55053e56ba787/activerecord/lib/active_record/connection_adapters/abstract_adapter.rb#L69) which is supposed to guard against write queries does not catch the write statements.\r\n\r\n### Discussion\r\n\r\nI suspect that parsing SQL with regexes is like parsing HTML with regexes: it cannot work. I suspect that @eileencodes was aware of the limitations of this regular expression approach and mentioned them in the Changelog at the time.\r\n\r\nI did spend a fair amount of time trying to fix these issues and I believe that a somewhat more complex Regex will be able to catch some of these issues, but not all.\r\n\r\nConsidering that the nested comments can have any level of depth, I don't think that they can be solved with Regexes. You will need to parse the SQL and strip out the comments entirely to safeguard against those.\r\n\r\nThe `SELECT INTO` issue is complex as well, as there can be a lot of statements between the `SELECT` and the `INTO` some of which may be strings or comments, so the regular expression approach is probably not going to work here as well. \r\n\r\nI have tests and a fix for the `EXPLAIN ANALYZE` queries, if you want me to I can open a pull request for those. I will add a pull request for the documentation change.\r\n\r\n### System configuration\r\n\r\n* Rails master\r\n* ruby 2.7.0p0\r\n* vagrant@rails-dev-box\r\n"},{"labels":[null,"documentation"],"text":"### Steps to reproduce\r\nSearch through the testing portion of the guide [https://edgeguides.rubyonrails.org/testing.html](https://edgeguides.rubyonrails.org/testing.html) for an example on how to test a post with a json body.\r\n\r\nThe result will be that you won't find an example that makes it clear on how to do this. When initializing a rails app, you have an option to build it as an api. So, I'd assume that I'd also learn how to test json body posts.\r\n\r\n### Expected behavior\r\nHave realistic documentation on posting JSON data to a Rails 6 API. I can't find anything at all on how to do this in the guide: [https://edgeguides.rubyonrails.org/testing.html](https://edgeguides.rubyonrails.org/testing.html)\r\n\r\n### Actual behavior\r\nNot able to write a post test that uses a json post body because I simply cannot see how to do it.\r\n\r\n### System configuration\r\n**Rails version**:\r\nlatest\r\n**Ruby version**:\r\nany ruby version"},{"labels":["documentation",null],"text":"For now, there is no mention of Webpack in the official guides  neither on edge guides  except for Ruby on Rails 6 release notes and upgrade guide. The documentation of the Webpacker gem is quite good, but a newcomer to Rails wouldn't know at first that he needs to have a look to it.\r\n\r\nSpecifically, it would be good to have a mention of Webpack in the following guides:\r\n* The Asset Pipeline\r\n* Working with JavaScript in Rails\r\n\r\nIn case of more than a simple mention, there could be a quite thorough explanation of the current state of the recommended use of the Rails asset pipeline together with Webpack  e.g. use asset pipeline for normal CSS and images, and use Webpack for JS. It would be good also to mention that Turbolinks and UJS are (now) just Yarn packages.\r\n\r\nAlso, because of Webpack, Rails now depend on having a JS runtime on your computer and Yarn. This is briefly mentionned in the Getting Started guide, section Starting up the Web Server, but it could be more explicit in the Installing Rails part."},{"labels":["documentation",null,null],"text":"I'm experimenting with some custom ajax functionality in a Rails 5 project.\r\n\r\nI've been struggling and struggling to post some data to the server using `Rails.ajax`, and after reading guide after guide, I eventually understood where I was going wrong from a two-year old [Github issue comment](https://github.com/rails/rails/issues/31507#issuecomment-354680347).\r\n\r\nLots of recent third party guides talk about `Rails.ajax` and recommend using it now that `jquery` isn't included by default any more ([for instance](https://www.rubyguides.com/2019/03/rails-ajax/)). If this isn't recommended, to save others the hassle I just went through, I would really appreciate a note in [these docs](https://guides.rubyonrails.org/working_with_javascript_in_rails.html)?"},{"labels":[null,null,"documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nI know there is `config` export for pages and api routes. They are partially described in [api-middlewares#custom-config](https://nextjs.org/docs/api-routes/api-middlewares#custom-config) or in [amp-support](https://nextjs.org/docs/advanced-features/amp-support/introduction) for example.\r\n\r\nI was searching for documentation on that `config` export but I couldn't find it.\r\nAlso: is a typescript type available for the `config` export?\r\n\r\n## To Reproduce\r\n\r\nLook at the next.js docs and search for documetation on `config` export for pages or api routes\r\n\r\n## Expected behavior\r\n\r\nThere is no such documentation on the `config` export"},{"labels":[null,"documentation"],"text":"API Routes also have access to the preview data added by Preview Mode in `req.previewData`. Our docs should be updated to reflect that."},{"labels":[null,null,"documentation",null],"text":"Add a new example called `fast-refresh` to our examples folder that showcases the different features and behaviors of [Fast Refresh](https://nextjs.org/docs/basic-features/fast-refresh).\r\n\r\nUse this removed example for inspiration: https://github.com/vercel/next.js/pull/16497 \r\n\r\nOnce the example is ready, add a link to it in our docs."},{"labels":[null,null,"documentation"],"text":"A migration guide for a common application with detailed steps of what has to be done if you want to move your current app in create-next-app to Next.js would be very useful for a lot of new users."},{"labels":[null,null,null,"documentation"],"text":"We should add a note to the headers documentation mentioning that if a header is already set successive ones with the same key will override the value "},{"labels":[null,"documentation"],"text":"We have multiple CSS examples, like tailwind, emotion, styled-components, e.t.c, but currently we don't mention them in the documentation. Adding this examples should help people that go to our docs looking for one of the popular libraries but fail in the process."},{"labels":[null,null,"documentation"],"text":"It has been mentioned multiple times on feedback that in the [documentation for Built-In CSS Support](https://nextjs.org/docs/basic-features/built-in-css-support) we don't really recommend CSS Modules over CSS-in-JS, therefore we create confusion about what should be used if both methods handle the same thing.\r\n\r\nWe should update the page to be clear about the advantages of CSS Modules over css-in-js, mainly being that you don't need JS to add the CSS as CSS Modules are concatenated into many minified and code-split `.css` files. Ideally users should end up thinking that CSS Modules are the preferred solution, but any CSS-in-JS library is also an option. "},{"labels":[null,null,null,"documentation"],"text":"We need to add documentation for the new ISR feature (`revalidate` property) for the `getStaticProps` lifecycle."},{"labels":[null,"documentation"],"text":"We currently don't have documentation about what the Next.js build output looks like from an architectural standpoint, this might be useful to add sometime. (it's not a public API that people should use)"},{"labels":[null,null,"documentation"],"text":"Our docs on catch all routes say the following:\r\n\r\n> Catch all routes can be made optional by including the parameter in double brackets ([[...slug]]).\r\n>\r\n>For example, pages/post/[[...slug]].js will match /post, /post/a, /post/a/b, and so on.\r\n\r\nWhich is very similar to what catch all routes already does, with the difference that it now includes the `/post` example, which is the only difference, but we aren't clear on that.\r\n\r\nTodo: update the section to be clear about the difference so you don't have to figure it out yourself."},{"labels":[null,null,"documentation",null],"text":"Documentation for custom routes was released some days ago (https://github.com/vercel/next.js/pull/14887). We should also add one example for every documentation page to better show how it works.\r\n\r\nThe following three examples are required:\r\n\r\n- [ ] `rewrites` For [docs/api-reference/next.config.js/rewrites.md](https://github.com/vercel/next.js/blob/canary/docs/api-reference/next.config.js/rewrites.md)\r\n- [ ] `redirects` For [docs/api-reference/next.config.js/redirects.md](https://github.com/vercel/next.js/blob/canary/docs/api-reference/next.config.js/redirects.md)\r\n- [ ] `headers` For [docs/api-reference/next.config.js/headers.md](https://github.com/vercel/next.js/blob/canary/docs/api-reference/next.config.js/headers.md)\r\n\r\nEvery example should contain very basic content and focus only on the feature it's about, showing how it works for multiple use cases that are currently not covered by the documentation. After the example is complete, add a link to it at the beginning of the page in docs.\r\n\r\nFeel free to open a PR and I'll review it asap."},{"labels":[null,"documentation",null],"text":"We should have more examples about our core features, one of those is [Environment Variables](https://nextjs.org/docs/basic-features/environment-variables).\r\n\r\nThe example should be added to the examples folder and linked by our docs"},{"labels":[null,"documentation"],"text":"Our current `next/router` docs (https://nextjs.org/docs/api-reference/next/router) use a combination between the `useRouter` hook and the `Router` object, ideally most examples should show the usage with `useRouter` as that's usually what you would need."},{"labels":[null,"documentation"],"text":"# Feature request\r\n\r\n## Is your feature request related to a problem? Please describe.\r\n\r\nWe released the `reactStrictMode` option to enable React Strict Mode quite a bit ago but I noticed the docs are currently missing.\r\n\r\n## Describe the solution you'd like\r\n\r\nAdd documentation for this options, more information on what it is / how it works can be found here: https://nextjs.org/blog/next-9-1-7#react-strict-mode-compliance-and-opt-in\r\n"},{"labels":["documentation"],"text":"In Next.JS 9.1.7, the CLI output has been redesigned and it shows '_size_' and '_first load size_' for each page now. It would be really helpful if someone can throw some light on what are these values, how are they calculated and what all modules are included while calculating these sizes. I cannot find documentation around this. "},{"labels":[null,"documentation"],"text":"## Describe the bug\r\n\r\nThere are no docs to be found on Logging. Also, the default configuration for production doesn't output access logs.\r\n\r\n## To Reproduce\r\n\r\nSearch official docs and observe there is nothing about logging.\r\n\r\n![Screen Shot 2019-12-29 at 1 00 47 PM](https://user-images.githubusercontent.com/487897/71561398-891c6d00-2a3b-11ea-85a7-33ed7f7788ef.png)\r\n\r\n\r\n## Expected behavior\r\n\r\nI expect to see extensive docs about logging.\r\n\r\nAlso, I expect to see access logs in my docker/kubernetes pod logs by default.\r\n\r\n## Screenshots\r\n\r\n![Screen Shot 2019-12-29 at 1 01 16 PM](https://user-images.githubusercontent.com/487897/71561425-e2849c00-2a3b-11ea-9303-626cd9a63c7f.png)\r\n\r\n\r\n## System information\r\n- Version of Next.js: 9.1.6\r\n\r\n## Additional context\r\n\r\nRelated issue #5512 \r\n"},{"labels":["documentation"],"text":"Looking at many SSG specific features being built into Nextjs (especially https://github.com/zeit/next.js/issues/9524) , here's what I think is the biggest issue in using Nextjs for SSG.\r\n\r\nThe problem is that Nextjs entire documentation and manuals are written from the perspective of a server-side rendering framework. They are not geared around the usecase of someone wanting to either quickly get started..or someone who is working with a Nextjs SSG and wanted help on some SSG contextual aspect. \r\n\r\nFor example, the entire section of \"Prefetching Pages\", \"Custom server and routing\", and many more is not relevant for a SSG usecase. In fact on the left side - there is no section for \"static site generation\". The documentation page of Nextjs lets people think that this is not a usecase that Nextjs supports. \r\n\r\nI dont believe there is any real way the two usecases can be merged in docs without massive cognitive overload for the user. \r\n\r\nThe documentation subsites should be entirely different for SSG and SSR usecases. Its ok if section are duplicated (which can be included in both sites from \"common docs\"). "},{"labels":["documentation",null],"text":"# Feature request\r\n\r\n## Is your feature request related to a problem? Please describe.\r\n\r\n`next export` allows me to export static HTML files. What's missing is a way to also export other dynamic content referenced by those static HTML files like images.\r\n\r\n## Describe the solution you'd like\r\n\r\nAPI routes allow me to render dynamic content like images. Next.js should support static export for API routes to make this content available for static sites.\r\n\r\n## Describe alternatives you've considered\r\n\r\nBoth alternatives I've considered don't seem practical to me:\r\n\r\n- Implement a custom export script which calls the API routes and stores the content in the exported directory.\r\n- Call the API route lamda function in exportPathMap.\r\n\r\n## Additional context\r\n\r\nMaybe there is a reason why this is not possible. In this case a short explanation in https://err.sh/zeit/next.js/api-routes-static-export would be great."},{"labels":["documentation"],"text":"Hey folks  \r\n\r\nI've seen a couple of cases of words like \"simple\", \"easily\" and \"just\" (and some other variations) in your docs. @carolstran explained in her blog post [How to remove condescending language from documentation](https://dev.to/meeshkan/how-to-remove-condescending-language-from-documentation-4a5p) why that can make some people feel frustrated and isolated while reading documentation with these words. \r\n\r\nIf you agree and allow me to proceed I can open a PR adapting the docs by removing these words or rephrasing the sentences where they are used.\r\n\r\nFor more reference on this topics you can also check [Jim Fisher's Don't Say Simply talk from Write the Docs Prague 2018](https://www.youtube.com/watch?v=gsT2BBWBVmM)\r\n\r\nThank you in advance and please let me know your thoughts about this  "},{"labels":["documentation"],"text":"I recently needed a feature allowing me to remove `if...else statement` from build output via a`webpack.DefinePlugin` ([DOCS](https://webpack.js.org/plugins/define-plugin/)) server vs browser condition\r\n\r\nI looked into Next.js source and happily found a `process.browser` in [webpack-config](https://github.com/zeit/next.js/blob/canary/packages/next/build/webpack-config.ts)\r\n\r\n```typescript\r\nlet webpackConfig: webpack.Configuration = {\r\n  // ...\r\n  plugins: [\r\n    // ...\r\n    new webpack.DefinePlugin({\r\n      // ...\r\n      'process.browser': JSON.stringify(!isServer)\r\n    })\r\n  ]\r\n};\r\n```\r\n\r\nI have done the tutorial and tried searching for this `process.browser`in the docs.  \r\nI found a lot of \"injectable\" `process.env[...]` related documentation, but nothing related to default build targets related variables.\r\n\r\nI tested it and it's doing exactly what I needed.\r\n\r\nDemo:  \r\n```typescript\r\nconst api: API = process.browser ? require(\"../api/proxy\").api : require(\"../api/auth\").api;\r\n```\r\n`api/proxy.ts`\r\n```typescript\r\nexport const api: API = {\r\n  find: (id: string) => fetch(`/api/resource/${id}`), // regexp \\/api\\/resource\\/\r\n  // ...\r\n};\r\n```\r\n\r\n`api/auth.ts`\r\n```typescript\r\nconst backendUrl = `${process.env.SECRET_BACKEND}/api/${process.env.SECRET_BACKEND_VERSION}`;\r\n\r\nexport const api: API = {\r\n  find: async (id: string) => {\r\n    const refs = await fetch(\r\n      // regexp \\/refs\\?access_token=\r\n      `${backendUrl}/refs?access_token=${process.env.SECRET_BACKEND_ACCESS_TOKEN}`\r\n    );\r\n    return fetch(`${backendUrl}/resource/${id}`, {\r\n      headers: {\r\n        Authorization: `Token ${process.env.SECRET_BACKEND_ACCESS_TOKEN}`,\r\n        ref: (await refs.json()).master\r\n      }\r\n    });\r\n  }\r\n  // ...\r\n};\r\n```\r\n\r\n`rm -rf .next`\r\n`npm run build`\r\n`cd next`\r\n\r\n```sh\r\ngrep \\/api\\/resource\\/ . -rl --exclude-dir=cache \r\n\r\n./static/OQ9_IPyl2LC8X7moS4k5r/pages/index.js\r\n```\r\n\r\n```sh\r\ngrep \\/refs\\?access_token= . -rl --exclude-dir=cache \r\n\r\n./serverless/pages/index.js\r\n```\r\n\r\n* is the process.browser meant to be private ?\r\n* is there a less hacky or more default way to implement this kind of build-target-toggle that I missed in the documentation ?\r\n* If no, and no, should this make its way to documentation ?"},{"labels":[null,null,"documentation"],"text":"# Feature request\r\n\r\n## Is your feature request related to a problem? Please describe.\r\n\r\nI see `Head.rewind()` being used in several examples (e.g. [the apollo example](https://github.com/zeit/next.js/blob/ba246446ef8069e160c3a9a010549e89e222dd1e/examples/with-apollo/lib/apollo.js)) but I don't really understand what it does and can't find documentation about it.\r\n\r\n## Describe the solution you'd like\r\n\r\nIf possible, I would like some documentation about it. What happens if I omit it? Could it be possible to have a concrete example?\r\n"},{"labels":["documentation"],"text":"[API routes](https://github.com/zeit/next.js#api-routes) were introduced some time ago and they are a very important feature especially for the people building their API in custom servers. This new lesson will be focused in teaching their usage as APIs for Next.js pages.\r\n\r\nThe lesson will be included alongside the lessons in `basics`, and it should show the usage of multiple api routes with at least a `get` and a `post` request."},{"labels":["documentation"],"text":"We should explain the need for this file.\n\nCloses [a Spectrum thread](https://spectrum.chat/next-js/general/what-is-the-purpose-of-next-env-d-ts~ac4c7ca0-aacb-4bfe-ad23-81315fa040af)."},{"labels":["documentation"],"text":"Users have trouble understand how to handle HTTP methods, even though this is normal Node.js HTTP request handling we should document it for ergonomics.\r\n\r\nRef:\r\nhttps://spectrum.chat/next-js/general/restrict-methods-with-next-api-routes~534c1e77-3f93-4bf5-a2ec-74cc3f2d2a6a\r\nhttps://spectrum.chat/next-js/general/post-api-routes~abf984ab-3d6a-46b3-9173-7f1a074f6e0f"},{"labels":[null,"documentation"],"text":"# Bug report\r\n\r\nIt was time-consuming and little frustrating to figure out minor adjustments to get Next 9's new routing to play nice with now and now-cli locally.  \r\n\r\n## To Reproduce\r\n\r\nUpdate a production app deployed on now 2 to v9+ and then attempt to use the new dynamic routing.\r\n\r\n## Expected behaviour\r\n\r\nGiven that Now and Next are both published by Zeit, tighter documentation updates, or references with regards to upgrading.\r\n\r\n## Additional context\r\n\r\nWhile I eventually managed, by going through the updated docs, the blog post, the upgrade  / migrate notes, and then the updated examples folder, which was the key.\r\n\r\n1. for others looking for an answer, using @now/next@canary as per the example, and removing your custom routes object, now automatically handles dynamic routes defined in the new way introduced in Next 9.  (ie:  `post/[id]/index.js` `post/[id]/comment.js`)\r\n\r\n2. This isn't just a moan, but a suggestion to sett a policy for updating docs, examples and deployment specifics together with the release.  \r\n\r\nTotally Love NEXT and after a tumultuous 6 months with the upgrade to now 2,  having just got NEXT 9 setup and working, I'm feeling the same zen and hopefulness about js that I felt when I first found NEXT.\r\n"},{"labels":["documentation"],"text":"Reported here: https://twitter.com/sudhirmith/status/1149706325854576640\r\n\r\nhttps://nextjs.org/docs#amp-behavior-with-next-export\r\n\r\nThis section needs to be updated to mention it outputs without the subdirectory as per the new behavior: https://github.com/zeit/next.js/blob/canary/UPGRADING.md#next-export-no-longer-exports-pages-as-indexhtml"},{"labels":[null,"documentation"],"text":"Hi,\r\nWould like to request for documentation using breakpoint/debugging. \r\nSeems some of the recommended ways on the internets are out of date now (on Spectrum / SO, etc..). \r\nSome documentation from Zeit would be awesome.."},{"labels":["documentation"],"text":"Could you please add api reference to the documentation with a detailed description of each method for each entity? To go through the documentation without the ability to see immediately available api, spoils the blood.\r\nAs here https://reactjs.org/docs/react-api.html or here https://vuejs.org/v2/api/\r\nAttempting to sort out and collect piece of information from the documentation, learn, and source code and types, spends a lot of time"},{"labels":[null,"documentation"],"text":"# Feature request\r\n\r\n## Is your feature request related to a problem? Please describe.\r\n\r\nThere's quite a bit of confusion on how Next.js handles routing as is slightly different from most routing frameworks out there. The reason for this is that Next.js by design has a constraint that we won't ship a routes manifest of every possible route in the application.\r\n\r\n## Describe the solution you'd like\r\n\r\nI've written a pretty complete comment about how `<Link>` works with custom parameters etc. It can be found here: https://github.com/zeit/next.js/issues/2833#issuecomment-414919347\r\n\r\nMost of that comment can be used in the docs.\r\n"},{"labels":["documentation"],"text":"Filed using the bug report template as there is no issue template for documentation/website issues (maybe there should be an issue for that)\r\n\r\n## Describe the bug\r\nThere is currently no real documentation for `next/router`, the closest thing is a brief section that is for some reason located under https://nextjs.org/docs/#intercepting-popstate \r\n\r\n## To Reproduce\r\n1. Look for documentation on usage of `next/router`\r\n2. Notice there is no documentation on the full API and usage of `next/router`\r\n\r\n## Expected behavior\r\nThere is documentation on the full API and usage of `next/router`\r\n\r\n"},{"labels":["documentation"],"text":"> Currently it doesn't explain what Component and Container do, and what props they take, if any.\r\n\r\n#4145"},{"labels":[null,"documentation"],"text":"It is not clear how to effectively optimise the usage and bundling of many assets within a next.js application. Typically webpack offers great power around this as you can use various loader chains to reference, load and optimise assets in place. Since the docs say to avoid adding webpack loaders when manipulating the configuration it isn't so obvious how we should go about managing assets and optimisation of those assets in an efficient manner.\r\n\r\nThere are two main parts that I see which I would love to see solutions and documentation for:\r\n\r\n1. How do we avoid having to maintain a large global assets folder as opposed to keeping assets close to where they are actually needed? I've seen the babel-plugin suggestion however it strikes me as counter-intuitive that a babel plugin is hitting the filesystem and especially emitting new files. This seems absolutely like the job for the bundler and/or framework (what I see next.js as). *This also has the caveat that it only works for imports from js modules whereas the power we get from webpack to have any file references (ie. css `url()`) detected and managed within the dependency graph is a huge win.*\r\n2. How should we optimise these assets when building or developing a next.js application?\r\n\r\nIs it on the roadmap to improve the asset management story of next.js? The more decisions about this stuff that can be moved away from user code (great example being needing to prefix CDN urls manually) the better."},{"labels":["documentation"],"text":"In Slack and in issues there comes repeatedly the question of how to do stuff differently server-side vs. client-side. I added a note in Wiki related to this: https://github.com/zeit/next.js/wiki/FAQ but it's obviously not clear enough.\r\n\r\nI think it would be nice to add to README:\r\n- that e.g in `getInitialProps` you can use `process.browser` var to check where you are (which AFAIK is better than `!!ctx.req` because this one still returns `true` on the first client-side render...?)\r\n- that `componentDidMount` only runs client-side, so can add client-only code there. More generally, precise which React lifecycle methods are client-only.\r\n- or suggest to use something like <NoSSR> component in render, or other libraries which render server-side only above-the-fold (link to examples, e.g https://github.com/zeit/next.js/tree/master/examples/progressive-render?)\r\n- note that these should hopefully be exceptions, and it's nice to strive for code which is as universal as possible"},{"labels":["documentation"],"text":"A quick brain dump of all the questions that arose while getting started with `next.js`. Some of these might be answered already and I just haven't looked hard enough. \r\n\r\n## How `next.js` improves performance\r\nAs I see it a major responsibility of the `next.js` build process and server is performance improvements.\r\nSome are documented and some not and there doesn't seem to be a central place to look up all that is done.\r\n\r\nI'd like to see a comprehensive list (possibly with reasoning and resources) of all the steps the build process and server takes inherently (e.g. compression, HTTP headers, minification, code splitting, etc.). And also steps it doesn't take (yet), pitfalls and configuration/extensions that the end user might have to implement by themselves. The Wiki might be a good place for such a list.\r\n\r\n## Configuration File\r\nThe `README.md`'s [custom configuration](https://github.com/zeit/next.js#custom-configuration) section seems incomplete in terms of describing what keys the exported object from `next.config.js` accepts. Based on present documentation there is only one key `webpack`. Is that all there is for the moment?\r\n\r\n## Programmatic API\r\nA start to document `next.js`' programmatic API is taken in section [Custom server and routing](https://github.com/zeit/next.js#custom-server-and-routing) of `README.md`. What I'm missing is proper documentation for instance methods like `getRequestHandler()`, `render()` etc.\r\n\r\n---\r\n\r\nI'm more than happy to help with improvements but would like some suggestions and pointers from somebody more familiar with the matter. Also maybe the community has more points to add to the list.\r\n\r\n> Documentation is like sex: when it is good, it is very, very good; and when it is bad, it is better than nothing."},{"labels":["documentation",null,null],"text":"Update the [main doc page for sparse](https://pytorch.org/docs/master/sparse.html). What is needed includes at least:\r\n\r\n- remove the legacy constructors (`torch.sparse.FloatTensor` et al.), use `torch.sparse_coo_tensor` instead and explain it's a regular `Tensor` instance:\r\n\r\n```\r\n>>> s = torch.sparse_coo_tensor(indices=[[0, 3]], \r\n...                             values=[[1, 2], [3, 4]], \r\n...                             size=(4, 2))                               \r\n>>> s                                                                      \r\ntensor(indices=tensor([[0, 3]]),\r\n       values=tensor([[1, 2],\r\n                      [3, 4]]),\r\n       size=(4, 2), nnz=2, layout=torch.sparse_coo)\r\n>>> type(s)                                                                \r\n<class 'torch.Tensor'>\r\n```\r\n\r\n- do not use any private methods (e.g. `._indices()` --> `.indices()`).\r\n- add docstrings to sparse-specific methods (e.g. `coalesce()`, `is_coalesced()`.\r\n- explain the main purpose(s) of sparse tensors. for example, the word \"memory\" is not present on the page right now.\r\n- explain the purpose of the `torch.sparse` namespace (which is, hold functions where the \"unspecified elements are 0\" assumption doesn't hold or somehow sparse/dense don't have the same behaviour).\r\n- explain that functions in the `torch` namespace _may_ support sparse tensors, and give a few representative code examples (e.g. using `torch.bmm`)\r\n- explain the purpose of a hybrid tensor and give more details on it\n\ncc @jlin27 @vincentqb @aocsa @nikitaved @pearu"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\nhttps://pytorch.org/docs/stable/quantization.html should have clear examples on how to customize `qconfig` objects for assigning custom qconfigs to layers or skipping layers from quantization.\n\ncc @jlin27 @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @vkuzo"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\nhttps://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/torch/_torch_docs.py#L4273-L4286\r\n\r\nThe following warning can be removed given that https://github.com/pytorch/pytorch/pull/42004 is in.\r\n\r\ncc: @mruberry \n\ncc @jlin27 @mruberry @VitalyFedyunin"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/91463487-70c65180-e859-11ea-90d6-3a83df442108.png)\r\n\r\nWe can change this to \"tensors that require grad\"\n\ncc @jlin27"},{"labels":["documentation",null],"text":"Keyword arguments should be put into a separate section `Keyword Arguments` instead of inside `Parameters`, just like:\r\n![image](https://user-images.githubusercontent.com/1032377/91362885-3c4a8b00-e7b0-11ea-87fa-84f11111760c.png)\r\ninstead of\r\n![image](https://user-images.githubusercontent.com/1032377/91362983-7025b080-e7b0-11ea-8446-bc7123109b23.png)\r\n\r\nI have three PRs fixing this issue: https://github.com/pytorch/pytorch/pull/43583 https://github.com/pytorch/pytorch/pull/43586 https://github.com/pytorch/pytorch/pull/43589\r\n\r\nBut some of them can not be fixed due to https://github.com/pytorch/pytorch/issues/43669: List of ops remaining to be fixed:\r\n- `torch.eye`\r\n- `torch.bmm`\r\n- `torch.gather`\r\n- `torch.linspace`\r\n- `torch.logspace`\r\n- `torch.ones`\r\n- `torch.ones_like`\r\n- The entire https://github.com/pytorch/pytorch/pull/43589\r\n\r\n\r\ncc @jlin27 @mruberry "},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.embedding_bag\r\n\r\n#### API\r\n\r\n`torch.nn.functional.embedding_bag`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input :\r\n\r\n**last element is the size of the input, or the ending index position of the last bag** (*The*) \r\n\r\nwhich should be part of the description for input argument `include_last_offset`\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91067883-788bb900-e601-11ea-9342-e699c095979d.png)\r\n\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve\r\n\r\n#### API\r\n\r\n`torch.lu_solve`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `b` , but it is not in the signature.\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91067635-2fd40000-e601-11ea-844c-651148a46bd6.png)\r\n\n\ncc @jlin27"},{"labels":[null,null,"documentation",null],"text":"##  Documentation\r\n\r\nAs reported at https://github.com/pytorch/pytorch/issues/32994, since pytorch-1.5 `nonzero` started to require an explicit `as_tuple` argument. The current docs don't reflect that. https://pytorch.org/docs/master/generated/torch.nonzero.html\r\n\r\n```\r\nimport torch\r\nprint(torch.__version__)\r\ntorch.nonzero(torch.tensor([1, 1, 1, 0, 1])) # current example\r\ntorch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=False) # should be instead\r\n```\r\noutput:\r\n```\r\n1.7.0.dev20200813\r\ntest2:4: UserWarning: This overload of nonzero is deprecated:\r\n        nonzero(Tensor input, *, Tensor out)\r\nConsider using one of the following signatures instead:\r\n        nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1597302504919/work/torch/csrc/utils/python_arg_parser.cpp:864.)\r\n  torch.nonzero(t)\r\n```\r\n\r\nShould the examples be modified to explicitly pass the `as_tuple` argument to match the code requirements? \r\n\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"As mentioned [here](https://github.com/pytorch/pytorch/pull/31125#issuecomment-673654283), the warning in LambdaLR should be removed and replaced by a simple note in the load/save state_dict documentation.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\nIt is not clear what I can pass as args in `torch.multiprocessing.spawn()`\r\nCan I pass the model, dataset, optimizer, scheduler, etc. through the args? Is it just primitives or perhaps objects which can be pickled?\r\n\r\nIt would be great if a more concrete explanation was available.\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIn [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding), it is stated: \r\n\r\nwith `mode=\"mean\"` is equivalent to `Embedding` followed by `torch.mean(dim=0)`\r\n\r\nHowever, I think the author intends `dim=1` instead. This is seen by comparing the following example ... \r\n```\r\n>>> # an Embedding module containing 10 tensors of size 3\r\n>>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')\r\n>>> # a batch of 2 samples of 4 indices each\r\n>>> input = torch.LongTensor([1,2,4,5,4,3,2,9])\r\n>>> offsets = torch.LongTensor([0,4])\r\n>>> embedding_sum(input, offsets)\r\ntensor([[-0.8861, -5.4350, -0.0523],\r\n        [ 1.1306, -2.5798, -1.0044]])\r\n```\r\n... with the corresponding example in [`Embedding`](https://pytorch.org/docs/stable/generated/torch.mean.html).\r\n\r\n```\r\n>>> # an Embedding module containing 10 tensors of size 3\r\n>>> embedding = nn.Embedding(10, 3)\r\n>>> # a batch of 2 samples of 4 indices each\r\n>>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\r\n>>> embedding(input)\r\ntensor([[[-0.0251, -1.6902,  0.7172],\r\n         [-0.6431,  0.0748,  0.6969],\r\n         [ 1.4970,  1.3448, -0.9685],\r\n         [-0.3677, -2.7265, -0.1685]],\r\n\r\n        [[ 1.4970,  1.3448, -0.9685],\r\n         [ 0.4362, -0.4004,  0.9400],\r\n         [-0.6431,  0.0748,  0.6969],\r\n         [ 0.9124, -2.3616,  1.1151]]])\r\n```\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\nThis issue refers to #42864, but the original issue was closed by the author while I was writing my response to it.\r\n\r\nThese are the docs at the moment:\r\n![loss1](https://user-images.githubusercontent.com/47462742/89954344-2f1a9180-dc31-11ea-9dc1-5e73ee5f1d88.PNG)\r\n\r\nBased on the [source](https://github.com/pytorch/pytorch/blob/0ff0fea42bf9721b87e01fe15445dfd3ea5f2093/aten/src/ATen/native/Loss.cpp#L74) the correct input/target shape is:\r\n- Input 1: (N, ) where  means, any number of additional dimensions\r\n- Input 2: (N, ) same shape as Input 1, or it has to be broadcastable to that shape\r\n- Target: (N, ), same shape as Input 1 or Input 2, or it has to be broadcastable to that shape\r\n\r\nSo I think  this part in the docs should be fixed and an example of how to use the loss would be helpful.\r\n\n\ncc @jlin27 @albanD @mruberry"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nOn master documentation: the quotes around \"mean\" and \"sum\" look a little weird but it isn't a big deal\r\n\r\n![image](https://user-images.githubusercontent.com/5652049/89908595-0efcbb00-dbbc-11ea-9a9a-642105579849.png)\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nIn the [docs](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) (the same applies to LSTM and GRU) in case of the reccurent **bidirectional** layer, it is not mentioned that there is an additional set of parameters suffixed with `_reverse` that are used in the backward pass:\r\n(k represents the layer)\r\n* `weight_hh_l[k]_reverse` \r\n* `weight_ih_l[k]_reverse` \r\n* `bias_hh_l[k]_reverse`\r\n* `bias_ih_l[k]_reverse`\r\n\r\nIn my opinion, it makes sense to have them documented so the user can easily access them and e.g. apply his own initialization procedure.\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nMemory leaks are reported relatively frequently but it's often a different issue, e.g. that there's a caching allocator and/or you are doing worst-case allocation (increasing the size each time).  Here's a recent example: https://github.com/pytorch/pytorch/issues/42557.\r\n\r\nWe should have a guide on how to debug these issues.\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"Great to see SWA as a part of `torch.optim`. It would be helpful to add the API reference for `SWALR` and `AveragedModel` in the `torch.optim` doc page (like the other schedulers and optimizers). Else, one has to look at the Github code to understand all the possible arguments for both of them. Thanks!\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nThe description on non_blocking should mention pinned memory, since `non_blocking` only affects copies between pinned memory (on the CPU) and GPU.\r\n\r\nThe current description for the non_blocking argument says \"if True and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.\" \r\n\r\nThe docs for `Tensor.cuda()` are better:\r\n\r\n\"If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. Default: False.\"\r\n\r\nAdditionally, the description of non_blocking in `Tenosr.to()` should be improved. \n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"##  Documentation\r\nStrange behavior in torch.distributions.negative_binomial:\r\nhttps://pytorch.org/docs/stable/distributions.html#negativebinomial\r\n\r\nWhen trying out the negative binomial distribution I found that the second input parameter prob (success probability) is actually the failure probability --> not consistent with documentation. This is also not consistent with other software such as scipy.\r\n\r\nExample:\r\nIn [1]: import torch \r\n   ...: from scipy.stats import nbinom \r\n   ...: import numpy as np \r\n   ...: n=2.0 \r\n   ...: x=0.0 \r\n   ...: p=.75 \r\n   ...: dis=torch.distributions.NegativeBinomial(n, 1-p) \r\n   ...: prob_torch=dis.log_prob(torch.tensor(x)).exp() \r\n   ...: prob_np=nbinom.pmf(x,n,p) \r\n   ...: np.isclose(prob_np,prob_torch.numpy())                                  \r\nOut[1]: True\r\n\r\n\n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw @jlin27"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nHi,\r\n\r\nOn the web page https://pytorch.org/docs/master/generated/torch.stft.htmlwhich documents torch.stft, normalization is defined as follows:\r\n\r\nf normalized is True (default is False), the function returns the normalized STFT results, i.e., multiplied by (frame_length)0.5(\\text{frame\\_length})^{-0.5}(frame_length)0.5 .\r\n\r\nBut frame_length is not defined anywhere on the page.  I think frame_length should be \"win_length\", which later on the page is defined to be the size of a window frame.\r\n\r\nBruce Maggs\r\n\n\ncc @jlin27"},{"labels":[null,null,"documentation",null,null],"text":"##  Documentation\r\n\r\nIn our [distributed docs](https://pytorch.org/docs/stable/distributed.html) under \"Synchronous and asynchronous collective operations\" we document `work.wait()` and `work.is_completed()`, but the documentation is not completely accurate in the case of NCCL.\r\n\r\nFor a NCCL Work object, `wait()` does not in general block the process and instead synchronizes the NCCL stream with the default stream. `is_completed` does check if the kernel execution is finished but can return false even after calling wait. It might also be useful to clarify the blocking behavior can be achieved with `NCCL_BLOCKING_WAIT=1`.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski @jlin27"},{"labels":["documentation",null],"text":"##  Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nThe following code:\r\n\r\n```\r\nimport torch\r\na = torch.arange(0, 4)\r\nb = torch.tensor(a)\r\n```\r\n\r\ngenerates a warning at the console:\r\n\r\n```UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).```\r\n\r\nThis warning should be eliminated.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nI often write code that is designed to accept any type that's convertible to a tensor as input, and I don't want my code to have any side effects on the input.  `torch.tensor()` provides the perfect semantics for this use-case since - as documented - it always copies its input.  For example:\r\n\r\n```\r\ndef foo(x):\r\n  # x might be a scalar, list, tuple, `numpy.ndarray`, `torch.Tensor`, etc.\r\n  x = torch.tensor(x)\r\n  # Return something based on x\r\n```\r\n\r\nIn this case, the warning is extremely annoying, since its suggestion to use `Tensor.clone().detach()` instead only works if the input is already a tensor.  Furthermore, it isn't clear why a warning is necessary here at all - it might as well be saying \"Warning: this function works as documented!\"\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nPlease eliminate this warning.  If necessary, incorporate its advice into the documentation for `torch.tensor()` instead.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nPerhaps this warning is suggesting that `torch.tensor` will be deprecated or modified in the future?  If so, it would be better to incorporate those plans into the documentation.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"Including creating the appropriate PyCapsule (pycapsule stuff seems to not be part of standard? or not mentioned explicitly?) and if its destructor is needed.\r\n\r\nI recently did this work to interop with ffmpeg and made two examples: \r\n\r\n1. https://github.com/vadimkantorov/pydlpack (relevant PyCapsule code here: https://github.com/vadimkantorov/pydlpack/blob/master/dlpack.py#L91)\r\n2. https://github.com/vadimkantorov/readaudio\r\n\r\nWhile memory management in this scenario is hard to do completely right, it still may be useful for the cases when full-blown PyTorch Extension would require learning Torch C++ API and working with C++.\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nConsider the documentation for Conv2d against 1.7.0:\r\n`\r\ntorch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')\r\n`\r\n\r\nThe \"T\" type is never defined and not particularly useful for documentation, even if it is the correct mypy type.  We should have a better strategy here.\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nIn the API docs, the function names should NOT be capitalized. For example, in https://pytorch.org/docs/master/generated/torch.set_flush_denormal.html#torch.set_flush_denormal\r\n\r\nwe should not write\r\n\r\n```\r\nTORCH.SET_FLUSH_DENORMAL\r\n```\r\n\r\nbut \r\n\r\n```\r\ntorch.set_flush_denormal\r\n```\r\n\r\nPython - like many other programming languages - is case sensitive.\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"##  Documentation\r\n\r\nWe occasionally see users post about performance issues when using NCCL for distributed training ([example](https://discuss.pytorch.org/t/distributeddataparallel-on-multiple-gpu-nodes-slower-than-one-gpu-node/75984), [example](https://github.com/pytorch/fairseq/issues/789)). Often tuning some NCCL environment variables such as `NCCL_MIN_NRINGS`, `NCCL_SOCKET_NTHREADS` can help with the performance depending on the network, so we should document these suggestions in our [distributed docs](https://pytorch.org/docs/stable/distributed.html) to help users self-troubleshoot performance issues. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski @jlin27"},{"labels":[null,"documentation",null],"text":"##  Documentation\r\n\r\n\"Distributed RPC Framework\" documentation (master version) contains the following note:\r\n\r\n> Please refer to [PyTorch Distributed Overview](https://pytorch.org/docs/master/rpc.html) for a brief introduction to all features related to distributed training.\r\n\r\n<img width=\"976\" alt=\"Screen Shot 2020-07-24 at 12 44 58\" src=\"https://user-images.githubusercontent.com/2459423/88383996-dd0deb00-cdab-11ea-86ed-89def1b30e58.png\">\r\n\r\nThe link behind \"PyTorch Distributed Overview\" is missing.\r\n\r\nI could send a PR with a quick fix, but I can not figure out where is the correct ressource...\r\n\r\ncc @mrshenli "},{"labels":[null,"documentation",null],"text":"It would be nice to have this in docs directly, since loops like `for device in range(torch.cuda.device_count())` are simpler if it is known to return 0 when no CUDA is compiled or it is not available\r\n\r\nIn https://discuss.pytorch.org/t/torch-cuda-device-count-with-no-cuda-compiled-available/90328 it seems so!\n\ncc @ngimel @jlin27"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\nThe original question was posted [here](https://discuss.pytorch.org/t/torch-exp-is-modified-by-an-inplace-operation/90216). As `torch.exp()` leverages its output for calculating its gradient while most other functions use their input, `torch.exp()` has a different behavior when its output is inplaced, which is not natural to users. I understand that this implementation is beneficial to the code efficiency. The description of `torch.exp()` in the document should emphasize this property.\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @jlin27"},{"labels":[null,null,"documentation",null,null,null],"text":"##  Bug\r\n\r\nA segmentation fault occurs when passing an array for `streams` argument.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n\r\nx = [[[3, 2, 3],[1, 3, 4]],\r\n       [[3, 1, 2],[3, 2, 4]],\r\n       [[4, 4, 2], [1, 1, 1]]] \r\ntorch.cuda.comm.scatter(torch.tensor(x), [1], chunk_sizes=None, dim=0, streams=[1, 2])\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo segfault. There is no documentation for `streams` argument, so it is not clear what the function expects for an input other than the default value(None). \r\n\r\n## Environment\r\nPyTorch version: 1.5.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.5.0\r\n[pip] torchvision==0.6.0a0+82fd1c8\r\n[conda] blas 1.0 mkl\r\n[conda] cudatoolkit 10.1.243 h6bb024c_0\r\n[conda] mkl 2020.1 217\r\n[conda] mkl-include 2020.1 217\r\n[conda] mkl-service 2.3.0 py37he904b0f_0\r\n[conda] mkl_fft 1.0.15 py37ha843d7b_0\r\n[conda] mkl_random 1.1.0 py37hd6b4f25_0\r\n[conda] numpy 1.18.1 py37h4f9e942_0\r\n[conda] numpy-base 1.18.1 py37hde5b4d6_1\r\n[conda] pytorch 1.5.0 py3.7_cuda10.1.243_cudnn7.6.3_0 pytorch\r\n[conda] torchvision 0.6.0 py37_cu101 pytorch\r\n\n\ncc @ezyang @gchanan @zou3519 @ngimel @jlin27"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/cuda.html :\r\n\r\nBoth reset_peak_stats and reset_peak_memory_stats are mentioned, but no docs are available.\n\ncc @jlin27"},{"labels":["documentation",null],"text":"https://pytorch.org/docs/master/tensors.html?highlight=narrow_copy#torch.Tensor.narrow_copy:\r\n\r\n![image](https://user-images.githubusercontent.com/1041752/87787192-83ea0880-c83b-11ea-9cfa-fc7b78c59591.png)\r\n\r\nMaybe worth having some assert/search that docs have no backticks (usually, it's an indicator of markdown problem)\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Bug\r\n\r\nI could not find this in the documentation, but it seems like `torch.cuda.BoolTensor` actually uses a byte for each element instead of a bit.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nx = torch.empty([], device=\"cuda\") # load something on GPU to get a baseline\r\n# running nvidia-smi, I see the GPU has 715 MiB of RAM in use\r\n# let's create a tensor which should take 10 MiB:\r\nx = torch.empty(10*8*1024**2, dtype=torch.bool, device=\"cuda\")\r\nassert x.element_size() == 1\r\n# running nvidia-smi, the GPU has 795 MiB of RAM used, or ~ 80 MiB additional (instead of the 10 MiB expected)\r\n```\r\n\r\nChecking repeatedly with different sizes of tensors indicated that a `torch.cuda.BoolTensor` takes 8 bits per element on GPU.\r\n\r\n## Expected behavior\r\n\r\nEither `torch.cuda.BoolTensor` should only take 1 bit per element (not sure if there is a GPU limitation here) or `x.element_size()` should return 8\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.5.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: CentOS Linux release 7.6.1810 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 418.87.00\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.5\r\n[pip3] pytorch-utils==0.1\r\n[pip3] torch==1.5.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] cudatoolkit               10.1.243             h6bb024c_0\r\n[conda] mkl                       2020.1                      217\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.1.0            py37h23d657b_0\r\n[conda] mkl_random                1.1.1            py37h0573a6f_0\r\n[conda] numpy                     1.18.5           py37ha1c710e_0\r\n[conda] numpy-base                1.18.5           py37hde5b4d6_0\r\n[conda] pytorch                   1.5.1           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-utils             0.1                       dev_0    <develop>\n\ncc @jlin27"},{"labels":[null,"documentation",null,null],"text":"##  Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nHello, I noticed this older issue https://github.com/pytorch/pytorch/issues/6662 is still open and looked through this PR https://github.com/pytorch/pytorch/pull/24435 about adding `doctest` to jit. If it would be helpful I can work on other parts of the docs to convert code blocks to use `doctest`. Currently it seems there are over 400 code blocks in the docs using the format `Example::` that are not being tested, e.g. a bunch are in this [file](https://github.com/pytorch/pytorch/blob/master/torch/_torch_docs.py). \n\ncc @jlin27 @mruberry @VitalyFedyunin"},{"labels":["documentation",null],"text":"##  Documentation\r\n![image](https://user-images.githubusercontent.com/33288114/87407914-60ce1780-c5f5-11ea-80eb-4fdb8c374463.png)\r\n\r\nThis formula confuses me because this formula does not look like a matrix formula. I can't determine whether W or x is a matrix or a vector.I think your formula does not consider the consistency of matrix dimensions.\r\n\r\nAccording to the relevant weights and input dimensions provided by the official documentation  I think the correct formula should be:\r\n![image](https://user-images.githubusercontent.com/33288114/87409139-1e0d3f00-c5f7-11ea-953e-3fe0ffcb727f.png)\r\n\r\n\r\n\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\nDocumentation for [ConvTranspose1d layer](https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose1d.html) does not contain even brief explanation of how channels are treated and how number of channels affect processing. \r\n\r\nSame goes for [Conv1d layer](https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html) but it was at least covered in [this discussion](https://discuss.pytorch.org/t/understanding-convolution-1d-output-and-input/30764/2) and is somewhat more obvious. With transposed convolution it is easy to find dozens of sites, books and animations explainig how it happens but I have not found a single mention that happens when there is multiple input channels and how they are reduced to single output channel. There are several questions on discuss.pytorch.org which seem relevant:\r\n1. https://discuss.pytorch.org/t/how-to-use-torch-nn-convtranspose1d-to-take-the-derivative-of-torch-nn-conv1d-function/32620\r\n2.  https://discuss.pytorch.org/t/upsampling-convtranspose2d-getting-the-original-tensor-size/51936\r\n3.  https://discuss.pytorch.org/t/understanding-conv2d-and-convtranspose2d/42142\r\n4. https://discuss.pytorch.org/t/convtranspose2d-weight-shape/52528\r\n\r\nIn the end, I just created ConvTranspose1d layer, feed it with constant values and hypothesized that it:\r\n1. applies transposed convolution to each input channel independently\r\n2. sum results for each channels, adds bias and push it to output channel\r\n3. multiple output channels just use different set of kernels and produce different values\r\n\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"##  Documentation\r\n\r\nhttps://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4itemEv\r\nand\r\nhttps://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4I0ENK2at6Tensor4itemE1Tv\r\n\r\nare both missing descriptions.\r\n\r\n--------\r\n\r\nI am trying to get a `float` out of a `Tensor` (that is a single element, kFloat32) on the GPU.\r\n\r\ne.g.\r\n`float x = x_tensor.item()`\r\nHowever, this gives a compile-time error:\r\n`error: cannot convert c10::Scalar to double in assignment`\r\n\r\nUsing `float x = x_tensor<float>.item()` compiles just fine, doesn't work either (program spins forever, probably waiting for CUDA kernel to end after a bad memory access or something).\r\n\r\nWhat is the proper way to get the float out of the tensor (and back into main memory)? It prints just fine, so it must be doable.\r\nI'd prefer to do it without printing and parsing or moving the entire tensor back to the CPU.\n\ncc @yf225 @glaringlee @jlin27"},{"labels":[null,"documentation",null,null],"text":"##  Documentation\r\n\r\n*Note*: I'm linking 1.5.0, not 1.5.1, b/c I can't get a permalink to 1.5.1 docs :(\r\n\r\nI was trying to get familiar with how BatchNorm stuff works:\r\nhttps://pytorch.org/docs/1.5.0/nn.html?highlight=batchnorm#torch.nn.BatchNorm1d\r\nThe docs and paper xref were great! But I didn't have a good grasp of how the running estimates were actually implemented, so I looked at the code:\r\nhttps://github.com/pytorch/pytorch/blob/480851ad2c21e8e39e336d849b8030a2f91718d7/torch/nn/modules/batchnorm.py#L41-L54\r\n\r\nLooked pretty straightforward, and I learned what \"buffers\" are in this context for `pytorch`; however, I was wondering what this did to the registration of buffers / parameters.\r\n\r\nLooking at the docs for both `.register_parameter` and `.register_buffer`:\r\nhttps://pytorch.org/docs/1.5.0/nn.html#torch.nn.Module.register_buffer\r\nhttps://pytorch.org/docs/1.5.0/nn.html#torch.nn.Module.register_parameter\r\n\r\nI notice that there are kinda code paths for handling the `None` case, but it's not documented (at least not in this immediate area).\r\n\r\nFrom what it looks like, registering something with `None` is tantamount to just setting the attribute to `None` - there's no real exposure of the parameter to users...\r\n\r\n## Suggestion Solutions\r\n\r\n* Document this behavior in `.register_parameter` and `.register_buffer` - if you register a buffer / parameter with `None`, it's basically just gonna be ignored\r\n* Have some brief exposition defining the terms \"parameter\" and \"buffer\" next to each other, and mention the possible equivalence of `Parameter.requires_grad=False` to a registered buffer? AFAICT, this comparison is done kinda implicitly, e.g.:\r\n  * <https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict>\r\n  * <https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html> - smells like a copy of the first?\r\n\r\n## Related\r\n\r\n* #8104 - all the `None` stuff\r\n* #16675 - understandable about `register_buffers` looking like a hack... but I guess it's pretty well used?\r\n* #39670 - behavior for `jit`\r\n* (kinda meta) <https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723/10> - asked my question - why buffers instead of parameters, and equivalence\r\n* #2018 - most recent PR that mentions `register_buffer` (Sep. 2017)\r\n* 7f4ff0e615e - introduction of the `Module.register_buffer` in Sep. 2016, I guess to help w/ impl. of `BatchNorm`\r\n\r\n## Example\r\n\r\nUsing Torch 1.5.1:\r\n```py\r\nfrom pprint import pformat\r\n\r\nimport torch.nn as nn\r\n\r\ndef show(code):\r\n    module = eval(code)\r\n    print(f\">>> {code}\")\r\n    print()\r\n    print(f\"state_dict:\\n{pformat(module.state_dict())}\\n\")\r\n    print(f\"parameters:\\n{pformat(list(module.named_parameters()))}\\n\")\r\n    print(f\"buffers:\\n{pformat(list(module.named_buffers()))}\\n\")\r\n    print(\"---\")\r\n\r\n# Everything.\r\nshow('nn.BatchNorm1d(1)')\r\n\r\n# No parameters.\r\nshow('nn.BatchNorm1d(1, affine=False)')\r\n\r\n# No buffers.\r\nshow('nn.BatchNorm1d(1, track_running_stats=False)')\r\n```\r\n\r\nOutput:\r\n```\r\n>>> nn.BatchNorm1d(1)\r\n\r\nstate_dict:\r\nOrderedDict([('weight', tensor([1.])),\r\n             ('bias', tensor([0.])),\r\n             ('running_mean', tensor([0.])),\r\n             ('running_var', tensor([1.])),\r\n             ('num_batches_tracked', tensor(0))])\r\n\r\nparameters:\r\n[('weight', Parameter containing:\r\ntensor([1.], requires_grad=True)),\r\n ('bias', Parameter containing:\r\ntensor([0.], requires_grad=True))]\r\n\r\nbuffers:\r\n[('running_mean', tensor([0.])),\r\n ('running_var', tensor([1.])),\r\n ('num_batches_tracked', tensor(0))]\r\n\r\n---\r\n>>> nn.BatchNorm1d(1, affine=False)\r\n\r\nstate_dict:\r\nOrderedDict([('running_mean', tensor([0.])),\r\n             ('running_var', tensor([1.])),\r\n             ('num_batches_tracked', tensor(0))])\r\n\r\nparameters:\r\n[]\r\n\r\nbuffers:\r\n[('running_mean', tensor([0.])),\r\n ('running_var', tensor([1.])),\r\n ('num_batches_tracked', tensor(0))]\r\n\r\n---\r\n>>> nn.BatchNorm1d(1, track_running_stats=False)\r\n\r\nstate_dict:\r\nOrderedDict([('weight', tensor([1.])), ('bias', tensor([0.]))])\r\n\r\nparameters:\r\n[('weight', Parameter containing:\r\ntensor([1.], requires_grad=True)),\r\n ('bias', Parameter containing:\r\ntensor([0.], requires_grad=True))]\r\n\r\nbuffers:\r\n[]\r\n\r\n---\r\n```\r\n\r\ncc @jlin27 @albanD @mruberry"},{"labels":["documentation",null],"text":"##  Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nA graphs page on site, to track the evolution of PyTorch, along with the time required to train neural networks, along with the data available.\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nWithout graph it become difficult to see, where things are headed.\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI suggest a few graphs, a few more could be added,\r\n1) Evolution of PyTorch\r\n2) Evolution of time required to train neural network, since the beginning of PyTorch\r\n3) Evolution of data available since the beginning of PyTorch in terms of size\r\n4) Evolution of number of parameters in neural network\r\n5) Evolution of FLOPS\n\ncc @jlin27"},{"labels":["documentation",null],"text":"PyTorch has accumulated a lot of lingo that are fuzzily-defined. Need a glossary table that gives a clear definition to commonly used terms to facilitate discussion and communication. \n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"This function has an input parameter add_histogram(), what does it mean? There is no documentation about this parameter. \r\n\r\nDocumentation link \r\nhttps://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_histogram\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\n#### Link\r\n\r\n[https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_)\r\n\r\n[https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_)\r\n\r\n#### API\r\n\r\n`torch.nn.init.kaiming_uniform_` \r\n\r\n`torch.nn.init.kaiming_normal_`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input :\r\n\r\n**with 'leaky_relu')** (*used*) \r\n\r\nwhich should be part of the description for input argument `a`\n\ncc @jlin27 @albanD @mruberry"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\nThe current documentation says it's an \"ordered dictionary that respects the order of insertion\", but it can, in fact, destroy its order. For example: \r\n\r\n![image](https://user-images.githubusercontent.com/38511765/85050917-aeb54400-b14b-11ea-9e61-5aa455df6019.png)\r\n\r\nWe should update our documentation to clarify its behavior, especially as users might try to enumerate the dict sequentially. \r\n\n\ncc @jlin27 @albanD @mruberry"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\nPR gh-37419 refactored the content of `docs/source/rpc/index.rst` into `docs/source/rpc.rst` but did not link to the latter from `doc/source/index.rst` so the top-level RPC documentation is missing from https://pytorch.org/docs/master/.\r\n\r\nThis should be fixed before the next release. I think it should be enough to replace `rpc/index.rst` with `rpc` on [`docs/source/index.rst`](https://github.com/pytorch/pytorch/blob/master/docs/source/index.rst) and make sure the top-level page is then visible.\r\n\r\nnoticed by @mrshenli \n\ncc @jlin27 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["documentation",null],"text":"## Background\r\nThis post is to gather feedback on the proposal to split up larger doc pages (e.g. torch.html) into subpages to reduce load time and help with content searchability. This is done by adding a summary table in the main page that links to subpages for each function/class.\r\n\r\nMore details can be found in this PR: https://github.com/pytorch/pytorch/issues/38010\r\n\r\n*Known Issue:* New format generates new urls which would break existing documentation links. *Proposed Fixes:*\r\nhttps://github.com/pytorch/pytorch/pull/39086\r\nhttps://github.com/pytorch/pytorch/pull/39032\r\n\r\n## Feedback Instructions\r\n1. Review the current view for torch compared to the newly proposed view: \r\n* Current: https://pytorch.org/docs/stable/torch.html\r\n* Proposed New: https://5591729-65600975-gh.circle-artifacts.com/0/docs/torch.html\r\n\r\n2. Leave a comment highlighting:\r\n* Pros\r\n* Cons\r\n* Which option do you prefer and why? \n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nIn the Tensor API documentation page.\r\n\r\nI saw \r\n\r\n> self, index and src should have same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim.\r\n\r\nBut when I look at size() API. \r\n\r\nhttps://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\r\nIt doesn't mention it can take an argument. Is something missing?\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"##  Feature\r\n\r\nCurrently, the [recommended approach to achieve reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) is setting global random seeds. I would like to propose that instead all functions which need a random source accept a local, non-global, `random_seed`/`random_state` argument to which one can pass a random state object, or numpy generator, or something else, which binds random source locally.\r\n\r\n## Motivation\r\n\r\nReproducibility if often needed in ML programs. But current approach of setting global random seed in various places in pytorch is suitable only for small programs which are using only pytorch. Now imagine that you have many libraries around, each trying to set global random seed at various times. This can lead to strange changes in random seeds just because you swapped two calls around. Moreover, if any of those libraries stand-alone produces a sequence of numbers, it might be that when used in combination with pytorch the sequence is not anymore the same. So if you are trying to compare two different implementations (one in pytorch one in some other library), the behavior of the implementation in the other library can change just because you also have pytorch around.\r\n\r\nOf course, the other library should also not be using global random source. But ideally pytorch should not either.\r\n\r\nIn our concrete case, we have an AutoML system generating many candidate pipelines using primitives to build those pipelines which in turn use many different libraries. Those pipelines are run and we hope to have full reproducibility (which helps with metalearning). But setting/resetting global random seed just because one primitive in one candidate pipeline we are exploring happens to use pytorch influences behavior of unrelated pipelines.\r\n\r\n## Pitch\r\n\r\nInstead of relying on global random sources, add `random_state` parameter to all functions which expect/use randomness. Do not ever set random seed of a global random source in pytorch code (the user of pytorch can do that themselves if they really want).\r\n\r\n## Alternatives\r\n\r\nUse context managers to set and reset random seeds, but this becomes tricky when using multi-threading.\r\n\r\n## Additional context\r\n\r\nSee example in [sklearn](https://scikit-learn.org/stable/glossary.html#term-random-state).\n\ncc @jlin27 @pbelevich"},{"labels":[null,"documentation",null],"text":"The only mention of this in the doc is \r\nhttps://github.com/pytorch/pytorch/blob/0251ba61089795a7a27c0473e4bb022805432c1f/torch/distributed/distributed_c10d.py#L337-L341\r\nwith no example. \n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jlin27"},{"labels":["documentation",null],"text":"##  Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nCurrently, it is a bit tricky to switch from CPU build to CUDA build. Users will sometimes forget to specify the cudatoolkit requirement and find out that they installed the CPU build. Then they will attempt to follow the steps on pytorch.org, that is `conda install -c pytorch cudatoolkit=x.x pytorch`. But actually that is not enough because `cpuonly` is still there.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. `conda install -c pytorch cpuonly pytorch`\r\n2. `conda install -c pytorch cudatoolkit=x.x pytorch`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nBetter user message. \r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"We should add the appropriate documentation, registration, and tests. See also https://github.com/pytorch/pytorch/issues/36403. \r\n\r\n@albanD \n\ncc @suo @ezyang @SsnL @albanD @zou3519 @gqchen"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/quantization.html#torch.quantization.add_quant_dequant\r\n\r\n#### API\r\n\r\n`torch.quantization.add_quant_dequant` \r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input :\r\n\r\n**we want to quantize** (*that*) \r\n\r\nwhich should be part of the description for input argument `module`\r\n\r\n"},{"labels":["documentation",null,null],"text":"When the PR that enables async user function for `rpc.remote` is added to the stack started from #39216, update the master `rpc.rst` doc to mention it in `rpc.rpc_sync`, `rpc.rpc_async`, and `rpc.remote` APIs.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nhttps://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html\r\n\r\nHere the var[x] of the denominator is population variance dividing by N. It should be precisely clarified here. Otherwise, the reader may use the sample variance by calling tensor.var() which is divided by N - 1.  "},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nAs title, `torch.backends` is undocumented\r\n\r\nRelated issue: https://github.com/pytorch/pytorch/issues/12468\r\n\r\nFYI, here is the search suggestions for `torch.b`. Seems that many people are searching for `torch.backends.cudnn.{benchmark, deterministic, enabled}`\r\n![image](https://user-images.githubusercontent.com/6421097/83131164-0b6d9380-a0a5-11ea-8a52-f4e340cf4e82.png)\r\n\r\n"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\nI am trying to understand what the function \"coalesce\" does for sparse tensors [here](https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor.coalesce). However, the API is not provided. \r\n\n\ncc @vincentqb"},{"labels":[null,"documentation",null,null],"text":"`torch.cuda.nccl` seems meant to be a set of low-level API (compared to `torch.cuda.comm` and `torch.distributed`). It would be great to document it.\n\ncc @ngimel"},{"labels":[null,null,"documentation",null],"text":"`torch.cuda.comm` and `torch.distributed` have similar but subtly different API sets. It would be great to make them consistent, and document the former.\n\ncc @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":["documentation",null],"text":"This is a summary of the potential doc improvements that I came up with. I can work on a pull request for these changes after review of what changes need to be made.\r\n\r\n1. Make clear that `torch.set_default_dtype` and `torch.get_default_dtype` only affect `torch.Tesor` and not `torch.tensor`.\r\n   \r\n    Suggestion:-\r\n    In [torch.set_default_dtype](https://pytorch.org/docs/stable/torch.html#torch.set_default_dtype) change:\r\n    The default floating point dtype is initially `torch.float32`. -> The default floating-point dtype used by `torch.Tensor` is initially `torch.float32`.\r\n\r\n    In [torch.get_default_dtype](https://pytorch.org/docs/stable/torch.html#torch.get_default_dtype) change:\r\n    Get the current default floating-point `torch.dtype` -> Get the current default floating point `torch.dtype` used by `torch.Tensor`.\r\n\r\n2. Specify the default value of **sci_mode** in [torch.set_printoptions](https://pytorch.org/docs/stable/torch.html#torch.set_printoptions). \r\n\r\n    Currently, it says to look for the default value defined by _Formatter. But to get to the default value a lot of work has to be done like:\r\n    ```python\r\n    torch.set_printoptions??  # Get location `torch/_tensor_str.py`\r\n    torch._tensor_str._Formatter??  # Default value defined in the __init__ method of this class\r\n    ```\r\n\r\n    Suggestion:-\r\n    Specify the default value of `False` in the docs or specify the exact path of `_Formatter` as `torch._tensor_str._Formatter`.\r\n\r\n3. The header of [torch.full_like](https://pytorch.org/docs/stable/torch.html#torch.full_like) is placed incorrectly.\r\n\r\n4. Add example for [torch.split](https://pytorch.org/docs/stable/torch.html#torch.split).\r\n\r\n5. Add a note/warning in [torch.squeeze](https://pytorch.org/docs/stable/torch.html#torch.squeeze). When using batch_size=1, using torch.squeeze can result in an error. So add a warning telling the users to be careful when using torch.squeeze.\r\n\r\n6. The header of [torch.randint(low=0, high, size](https://pytorch.org/docs/stable/torch.html#torch.randint) is placed incorrectly.\r\n\r\n7. The header of [torch.randint_like](https://pytorch.org/docs/stable/torch.html#torch.randint_like) is placed incorrectly.\r\n   \r\n8. Add documentation for `_use_new_zipfile_serialization=False` argument of [torch.save](https://pytorch.org/docs/stable/torch.html#torch.save) + Add example using the above argument and pickle_protocol=4/5.\r\n\r\n9. "},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\nMany classes, methods and functions lack docstrings. They should be added or clear reasons provided for not documenting them. xref gh-38244 which refactored the documentation coverage checks: the undocumented items are all listed in `docs/source/conf.py` in `coverage_ignore*` lists"},{"labels":["documentation",null],"text":"##  Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIt would be nice to have explicit statement in the documentation that `torch.optim.lr_scheduler.ReduceLROnPlateau` with `mode=min` and `threshold_mode='rel'` expects loss to be `> 0`\r\n\r\nThis is the excerpt from documentation about `threshold_mode`:\r\n```\r\nthreshold_mode (str): One of `rel`, `abs`. In `rel` mode,\r\ndynamic_threshold = best * ( 1 + threshold ) in 'max'\r\nmode or best * ( 1 - threshold ) in `min` mode.\r\nIn `abs` mode, dynamic_threshold = best + threshold in\r\n`max` mode or best - threshold in `min` mode. Default: 'rel'.\r\n```\r\n\r\nAnd this is the corresponding excerpt from the code:\r\n```python\r\ndef is_better(self, a, best):\r\n    if self.mode == 'min' and self.threshold_mode == 'rel':\r\n        rel_epsilon = 1. - self.threshold\r\n        return a < best * rel_epsilon\r\n\r\n    elif self.mode == 'min' and self.threshold_mode == 'abs':\r\n        return a < best - self.threshold\r\n\r\n    elif self.mode == 'max' and self.threshold_mode == 'rel':\r\n        rel_epsilon = self.threshold + 1.\r\n        return a > best * rel_epsilon\r\n\r\n    else:  # mode == 'max' and epsilon_mode == 'abs':\r\n        return a > best + self.threshold\r\n```\r\n\r\nI use `NegativeDiceLoss` in my experiments so the loss is negative and is expected to be minimized. The only correct mode of `ReduceLROnPlateau` I can use is `threshold_mode='abs'` because `threshold_mode='rel'` will not work for minimizing negative loss.\r\n\r\nImagine `mode='min', threshold_mode='rel', threshold=0.1`:\r\n* If we use loss that is expected to be > 0 all the time and current loss value is `10` than dynamic threshold will be `(1 - 0.1) * 10 == 9`. And the check will be `a < 9`. So there will be a margin between previous loss value and the new one to ensure that model improved.\r\n* If we use loss that is expected to be < 0 all the time (as in my case with `NegativeDiceLoss`) and current loss value is `-10` than dynamic threshold will be `(1 - 0.1) * -10 == -9`. And the check will be `a < -9`. __It does not correspond to logic of scheduler__ as it allows new loss value to be slightly worse than previous. As I understand the goal of the dynamic threshold is to guarantee that the loss improved __by some margin__ to deal with noise in loss calculations.\r\n\r\nIf my understanding is correct it is better to __explicitly state in documentation__ that for `threshold_mode='rel'` only positive loss functions are expexted. Probably negative loss functions are not common to use but sometimes they are required (as with dice score as loss function).\r\nPlease correct me if I'm wrong :)"},{"labels":["documentation",null,null],"text":"##  Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nMore often than not, integer quantization is performed by \"simulating\" quantisation during the forward pass and let the gradients flow unchanged during back propagation (i.e. using Straight Through Estimator, STE). \r\n\r\nFrom the description in [FakeQuantize](https://pytorch.org/docs/stable/_modules/torch/quantization/fake_quantize.html#FakeQuantize) I think it's fair to assume that STE is used since nothing is mentioned about what happens during backward pass. However, FakeQuantize does not use STE. (evidence [here](https://github.com/pytorch/pytorch/blob/fe44741dbac80c73139306f46da8b719d44444d3/aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu#L67) showing the CUDA implementation). The backward pass does gradient clipping. \r\n\r\nI think this should be mentioned (maybe as a \"Note\"?) in the DOCs since these two approaches for backward prop can lead to very different results in some cases. \n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"},{"labels":["documentation",null,null],"text":"##  Feature\r\nThe current official docs don't have specific examples regarding the use of seeds and worker_init_fn in order to attain a certain degree of reproducibility.\r\nFollowing the suggestion by @ptrblck in the [thread](https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097), I would like to add the solutions mentioned to the official docs.\r\n\r\n## Motivation\r\nWhile participating in a Kaggle competition I ended up spending countless hours of GPU compute on reproducing my results across followup runs of the pipeline but I realized it too late that the number of workers, the seeds **inside** these works all affect the outcome of the run. I searched a lot of discussions and docs to end up realizing how to use the worker_init_fn and what to expect from it. I would not want people to end up where I did so I would like to give them a heads up with whatever I have understood.\r\n\r\n## Pitch\r\nAs suggested by @ptrblck , I would like to update the [doc](https://pytorch.org/docs/stable/notes/randomness.html) with the code examples and give people a clear heads-up regarding the dependency of the results on the workers and seeds and how they may be able to reach an acceptable level of reproducibility.\r\n\r\n## Alternatives\r\nAs far as I can tell, this would be the most thorough example.\r\n\r\n## Additional context\r\nN/A\r\n\n\ncc @jlin27"},{"labels":[null,"documentation",null,null],"text":"##  Documentation MISSING\r\n\r\n Currently, the only documentation on the pages says \"The author of this package has not provided a project description\"\r\n\r\nCan I have permission or a contact person to call so I can help add more instructions, a README.md or work to implement existing pages that need documentation? \r\n\r\nThe <Project description> on website https://pypi.org/project/torch/ is where I am referencing and I would be happy to start adding documentation as Torch is on my list of things to learn. -->\r\n\r\n<!-- Currently, the only documentation on the pages says \"The author of this package has not provided a project description\"\r\n\r\nCan I have permission or a contact person to call so I can help add more instructions, a README.md or work to implement existing pages that need documentation? \r\n\r\nThe <Project description> on website https://pypi.org/project/torch/ is where I am referencing and I would be happy to start adding documentation as Torch is on my list of things to learn. -->\r\n\n\ncc @malfet"},{"labels":[null,"documentation",null,null],"text":"##  Documentation\r\n\r\nCurrently the build from source info references Anaconda since it is \"easier\". May be the case but as an experienced software packager I would really love information for the full build process that does not involve Anaconda training wheels. I do not want or plan to have anything to do with anaconda on servers I manage.\r\n\r\nIs there an old version of the build from source documentation still available somewhere? The old documentation described the entire process in detail.\r\n\r\nThanks.\n\ncc @malfet"},{"labels":[null,"documentation",null],"text":"##  Documentation\r\n\r\nAfter gh-37419, there are now [subtopic pages](https://pytorch.org/docs/master/generated/torch.is_tensor.html#torch.is_tensor) underneath the [main topic pages](https://pytorch.org/docs/master/torch.html). \r\n\r\nShould the subtopic pages have a right-navbar to indicate where they fall inside the topic pages?\r\n\r\nIn any case, the left- and right- navbars are quite long without alot of visual formatting to indicate where the current page is located. Indenting or color or other clues could help users see where they are.\n\ncc @ezyang @zou3519"},{"labels":[null,"documentation",null,null],"text":"##  Documentation\r\n\r\ngh-37419 split up the \"heaviest\" pages into a leading pages and subpages. More can be done to split up pages (these have the most lines of HTML):\r\n- [ ] [`docs/source/quantization.rst](https://pytorch.org/docs/stable/quantization.html)\r\n- [ ] [`docs/source/tensors.rst`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)\r\n- [ ] [`docs/source/nn.fuctional.rst`](https://pytorch.org/docs/stable/nn.functional.html)\r\n- [ ] [`docs/source/distributions.rst`](https://pytorch.org/docs/stable/distributions.html)\r\n- [ ] [`docs/source/autograd.rst`](https://pytorch.org/docs/stable/autograd.html)\r\n\r\nThe strategy would start with using `autosummary` like in gh-37419, but more work might be needed to split up classes with many methods. \n\ncc @ezyang @zou3519"},{"labels":["documentation"],"text":"Standalone documentation browsing apps like [Dash](https://kapeli.com/dash), [Zeal](https://zealdocs.org/), and [Velocity](https://velocity.silverlakesoftware.com/) use a documentation packaging format called [docset](https://kapeli.com/docsets)\r\n\r\nThe excellent Material UI docs would be even nicer if they were published in the `docset` format, available for local+offline search and perusal.\r\n\r\n- [x] I have searched the [issues](https://github.com/mui-org/material-ui/issues) of this repository and believe that this is not a duplicate.\r\n\r\n## Summary \r\n\r\nExisting docs should be published and made available on the [Kapeli/Dash-User-Contributions](https://github.com/Kapeli/Dash-User-Contributions) repository."},{"labels":["documentation",null],"text":"I feel like the documentation for withTheme is pretty dry and would benefit from having a better description for what the feature does, what problems does it solve and would benefit from having an example code.\r\n\r\n<!-- Checked checkbox should look like this: [x] -->\r\n- [x] This is a v1.x issue. <!-- (v0.x is no longer maintained) -->\r\n- [x] I have searched the [issues](https://github.com/mui-org/material-ui/issues) of this repository and believe that this is not a duplicate.\r\n"},{"labels":[null,"documentation",null,null],"text":"**Description**\r\n\r\nThe SwarmJoin API supports `Availability` (https://github.com/moby/moby/blob/master/daemon/cluster/swarm.go#L147-L188), which the CLI client also uses, but it is not documented in the API docs (https://github.com/moby/moby/blob/master/api/swagger.yaml#L10008).\r\n"},{"labels":[null,"documentation"],"text":"In https://github.com/moby/moby/pull/6909 the change was made that, if one specifies an ENTRYPOINT and the base image contains a CMD, then the CMD will be emptied.\r\n\r\nThis behavior is nowhere documented. (Neither in the Dockerfile Reference nor Best Practices)\r\n\r\nFor me this behavior was quite unexpected, because the ENTRYPOINT seems to be the point where one can push in start up scripts. For example nginx images specify a CMD (without ENTRYPOINT) to start the nginx. In my inheriting image I use an ENTRYPOINT script to do some environment dependant configuration and then want to do `exec \"$@\"` to \"resume\" with the base image's CMD. That does not work and I need to respecify the CMD (= copy over and maintain) from the nginx base image.\r\n\r\nThis behavior needs to be documented. From my point of view it could even be changed back, but I certainly miss the creators intented purpose of ENTRYPOINT."},{"labels":[null,"documentation",null,null],"text":"Hi !\r\n\r\nI'm writing a tool (in Python) to get the logs from any containers through the Docker Engine's HTTP API.\r\n\r\nAfter reading [the doc](https://docs.docker.com/engine/api/v1.30/), and trying to implement something, I founded that the protocol used seems to be different from the one for Attach, it seems to be :\r\n\r\n```{\\xFF}\\r\\n{\\xFF}{datas}```\r\n\r\nWhere the first hex number is the size of the message, and the second one seems to be (empirical deduction) the number of bytes that are supposed to be ignored before the start of the actual payload (so the log).\r\nAnd it seems to work, for some containers at least ...\r\n\r\n**EDIT** : The second seems to be the number of bytes that are supposed to be ignored IF the container has ```tty: false```, otherwise, it seems to be the start of the log **END of the edit**\r\n\r\nWith one strange deduction : if ```\\x07``` is found, then it overrides the second {\\xFF} size, and the message starts after this value...\r\n\r\nAm I missing a part of the documentation, or am I not using the HTTP API in a wrong way ?\r\n\r\nThanks for your time,\r\n\r\n**NB** : The protocol above seems to be specific for the logs endpoint, it's not working for the events' endpoint.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.05.0-ce\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n"},{"labels":["documentation",null,null],"text":"This PR https://github.com/moby/moby/pull/13165 allows duration strings as `--since/--until`, but I can't find any example of `docker events --until`. From the help infomation of `docker events`, I suppose that `docker events --until 10m` will stream events for 10 minutes then stop streaming. But actually, it returns immediately without any output.\r\n\r\nAfter diging into codes, I find that the duration will be subtracted from `time.Now()`, that is to say, docker waits for -10 minutes in this scenario, that's why it returns immediately without any output. Then I try `docker events --until -10m` and it works as what I expected.\r\n\r\nMaybe it's intended behavior because @ahmetb has already answered this question in his PR:\r\n![image](https://user-images.githubusercontent.com/20569488/29160624-902086b8-7de5-11e7-959b-4514b34075f0.png)\r\n\r\nBut I still think it's confusing, `until` means do something up to some time, right?\r\n"},{"labels":[null,"documentation",null],"text":"I was looking at [`dispatchers.go`](https://github.com/moby/moby/blob/f577caff19d486d8d01443507d891cb1b0891cdc/builder/dockerfile/dispatchers.go#L689-L693) today when I realized that you can actually specify `tcp` or `udp` when you use the `EXPOSE` instruction.\r\n\r\nThis isn't mentioned at all in the [Dockerfile reference for `EXPOSE`](https://docs.docker.com/engine/reference/builder/#expose). Is this intentional?"},{"labels":[null,"documentation"],"text":"**Description**\r\n\r\nLooking at the [Docker Engine API docs](https://docs.docker.com/engine/api/v1.26/#operation/ContainerCreate) (scroll down and expand HostConfig, then scroll down to find the Binds documentation), I don't see anything related to the SELinux Context, which is mentioned in the docker documentation [here](https://docs.docker.com/engine/tutorials/dockervolumes/#volume-labels).\r\n\r\nHowever, it seems that the `:Z` or `:z` which are SELinux flags are accepted as I'm able to pass them (See reproduction of the issue).\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nIf you have vagrant : \r\n1. create a `Vagrantfile` containing \r\n\r\n```\r\n# -*- mode: ruby -*-\r\n# vi: set ft=ruby :\r\n\r\nVagrant.configure(\"2\") do |config|\r\n  config.vm.box = \"fedora/25-cloud-base\"\r\n\r\n  config.vm.provision \"shell\", inline: <<-SHELL\r\n    set -eux\r\n\r\n    # prepare the system and install dependencies\r\n    dnf install -q -y docker\r\n    systemctl start docker\r\n    docker pull alpine:latest\r\n    \r\n    # Sanity check, to make sure unwanted argument do not pass\r\n    curl -s --unix-socket /var/run/docker.sock -H \"Content-Type: application/json\" \\\r\n    -d '{\"Image\": \"alpine\", \"Binds\": [\"/vagrant/Vagrantfile:/Vagrantfile:a\"]}' \\\r\n    -X POST http:/v1.24/containers/create\r\n\r\n    curl -s --unix-socket /var/run/docker.sock -H \"Content-Type: application/json\" \\\r\n    -d '{\"Image\": \"alpine\", \"Binds\": [\"/vagrant/Vagrantfile:/Vagrantfile:Z\"]}' \\\r\n    -X POST http:/v1.24/containers/create\r\n  SHELL\r\nend\r\n```\r\n2. run `vagrant up` in the same directory\r\n\r\nIf you don't use vagrant, make sure you have docker installed and run the two curl commands manually.\r\n\r\n**Describe the results you received:**\r\n\r\nThe first curl command failed which is normal, the api shouldn't accept invalid specifications, but the second succeeds, which is also normal as it is SELinux related. However, it is not specified in the documentation.\r\n\r\nI will work on a fix in the documentation as soon as possible. However, I have a question about it, should we add a link to the [documentation](https://docs.docker.com/engine/tutorials/dockervolumes/#volume-labels) for reference or should we just specify more examples ?\r\n\r\nThank you."},{"labels":["documentation"],"text":"From the release notes of v17.06.0-ce-rc2\r\n\r\n> Placement now also take platform in account moby/moby#33144\r\n\r\nWhile the PR that introduced the feature also updated the API docs, it didn't update the [user docs for service create](https://github.com/moby/moby/blob/master/docs/reference/commandline/service_create.md#specify-service-constraints---constraint), that also mention the placement constraints.\r\n\r\nI also noticed that the documentation here https://github.com/docker/swarmkit/#features talks about placement constrains, and mentions `node.ip` as another supported constraint. I'm not sure this made it to 17.06 yet, but if it did, we also need to update the user docs page with that.\r\n\r\n/ping @nishanttotla since he was the one who introduced #33144\r\nand @mstanleyjones for visibility\r\n\r\n---\r\n\r\nIf you're triaging this issue and are thinking if this issue should be filed against docker/docker.github.io instead, keep in mind that docker/docker.github.io will fetches https://github.com/moby/moby/blob/master/docs/reference/ to build the user-facing docs, so this issue needs to be addressed on this repo."},{"labels":[null,"documentation"],"text":"As seen there: https://github.com/moby/moby/blob/df4ca50805baa1d1488d811e82125f607c8daa09/integration-cli/docker_api_swarm_service_test.go#L63,\r\nthere is an `insertDefaults` query parameter not stated in the docs."},{"labels":["documentation"],"text":"**Description**\r\n\r\nDocker and Docker-Compose documentations do not make clear what are the valid values of ulimit options for Docker.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Read Docker documentation\r\n2. Struggle to find correct option names. \r\n\r\n\r\n**Describe the results you received:**\r\n\r\nGoogle finds just a few places where possible options are listed - https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_Ulimit.html \r\nLooking on code - turns out it is incomplete and [any valid `setrlimit` option](https://linux.die.net/man/2/setrlimit) can be passed into `ulimits`\r\n\r\n**Describe the results you expected:**\r\n\r\nList of possible options or reference to example (setting list of file descriptiors, max memory, max cpu usage, etc)"},{"labels":["documentation",null],"text":"**Description**\r\n\r\nin `moby/container/state.go` we have a struct that handles containers state, it looks like this : \r\n``` go\r\ntype State struct {\r\n\tsync.Mutex\r\n\t// FIXME: Why do we have both paused and running if a\r\n\t// container cannot be paused and running at the same time?\r\n\tRunning           bool\r\n\tPaused            bool\r\n\tRestarting        bool\r\n\tOOMKilled         bool\r\n\tRemovalInProgress bool // Not need for this to be persistent on disk.\r\n\tDead              bool\r\n\tPid               int\r\n\tExitCodeValue     int    `json:\"ExitCode\"`\r\n\tErrorMsg          string `json:\"Error\"` // contains last known error when starting the container\r\n\tStartedAt         time.Time\r\n\tFinishedAt        time.Time\r\n\tHealth            *Health\r\n\r\n\twaitStop   chan struct{}\r\n\twaitRemove chan struct{}\r\n}\r\n```\r\nthis is a bit misleading as we need to always update the two bools in order to avoid any inconsistant behaviour.\r\nthe state is embed in : \r\n```go\r\ntype CommonContainer struct {\r\n\tStreamConfig *stream.Config\r\n\t// embed for Container to support states directly.\r\n\t*State          `json:\"State\"` // Needed for Engine API version <= 1.11\r\n          .....\r\n     }\r\n```\r\nas states the comments to ensure compatibility with versions <= 1.11, i think we can refactor without breaking much of it."},{"labels":["documentation",null,null],"text":"Currently the plugin docs have a \"legacy\" doc which explains both the v1 discovery model as well as the RPC protocol for activation.\r\nThe v2 plugin doc doesn't really mention anything about how RPC is handled.\r\n\r\nBasically, getting started with plugins can be pretty confusing. Docs could use some love."},{"labels":["documentation",null,null],"text":"Consciously referring to Issue #29617, now Closed.\r\n\r\nThere *is* (in my humble opinion) an issue here.\r\n\r\nIt is a documentation issue.\r\n\r\nFollowing https://docs.docker.com/engine/getstarted/step_one/#step-3-verify-your-installation, at Step 3, sub-step 2, the command provided is\r\n\r\ndocker run hello-world\r\n\r\nNothing there indicates that there might be anything platform/architecture dependent.\r\n\r\nThis is in Verify your Installation. It is therefore likely to be followed verbatim by relatively uninformed potential users who might reasonably assume that a basic test should Just Work.\r\n\r\nIn fact the result is this:\r\npi@rpi2b ~ $ docker run hello-world\r\nstandard_init_linux.go:178: exec user process caused \"exec format error\"\r\n\r\nThe resulting error message is not handled in the linked 'Troubleshooting' section of the tutorial.\r\n\r\nThe workaround is\r\npi@rpi2b ~ $ docker run armhf/hello-world\r\n\r\nHello from Docker on armhf!\r\nThis message shows that your installation appears to be working correctly.\r\n\r\n\r\n\r\n"},{"labels":["documentation",null,null,null],"text":"There are several scripts in the [hack folder](https://github.com/docker/docker/tree/master/hack). It definitely deserves a good README  "},{"labels":["documentation",null,null],"text":"When using `docker deploy` with a 3.2 compose version, it fails with \"unsupported Compose file version: 3.2\". I could not find any docs (especially on https://docs.docker.com/compose/compose-file/compose-versioning/#compatibility-matrix) on the required docker version to use the 3.2 compose.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a compose file with version 3.2:\r\n```\r\n# compose-v32.yml\r\nversion: '3.2'\r\n  services:\r\n    myservice:\r\n    ...\r\n```\r\n2. Run `docker deploy -c compose-v32.yml mystack\r\n\r\n**Describe the results you received:**\r\nunsupported Compose file version: 3.2\r\n\r\n**Describe the results you expected:**\r\nCreated myservice\r\n\r\n**Output of `docker version`:**\r\n```\r\nClient:\r\nVersion:      17.03.1-ce\r\nAPI version:  1.27\r\nGo version:   go1.7.5\r\nGit commit:   c6d412e\r\nBuilt:        Fri Mar 24 00:00:50 2017\r\nOS/Arch:      linux/amd64\r\n\r\nServer:\r\nVersion:      17.03.1-ce\r\nAPI version:  1.27 (minimum version 1.12)\r\nGo version:   go1.7.5\r\nGit commit:   c6d412e\r\nBuilt:        Fri Mar 24 00:00:50 2017\r\nOS/Arch:      linux/amd64\r\nExperimental: true\r\n```\r\n**Output of `docker info`:**\r\n```\r\nContainers: 5\r\nRunning: 4\r\nPaused: 0\r\nStopped: 1\r\nImages: 6\r\nServer Version: 17.03.1-ce\r\nStorage Driver: overlay2\r\nBacking Filesystem: extfs\r\nSupports d_type: true\r\nNative Overlay Diff: true\r\nLogging Driver: syslog\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\nVolume: local\r\nNetwork: bridge host ipvlan macvlan null overlay\r\nSwarm: active\r\nNodeID: mlc7uzdsvcnxkzi32437xybuv\r\nIs Manager: true\r\nClusterID: 6xjp4en70m49rkgt1u2jzo1fe\r\nManagers: 1\r\nNodes: 2\r\nOrchestration:\r\n Task History Retention Limit: 5\r\nRaft:\r\n Snapshot Interval: 10000\r\n Number of Old Snapshots to Retain: 0\r\n Heartbeat Tick: 1\r\n Election Tick: 3\r\nDispatcher:\r\n Heartbeat Period: 5 seconds\r\nCA Configuration:\r\n Expiry Duration: 3 months\r\nNode Address: 10.99.0.9\r\nManager Addresses:\r\n 10.99.0.9:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 4ab9917febca54791c5f071a9d1f404867857fcc\r\nrunc version: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe\r\ninit version: N/A (expected: 949e6facb77383876aeff8a6944dde66b3089574)\r\nSecurity Options:\r\nseccomp\r\n Profile: default\r\nKernel Version: 4.9.13-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 3.354 GiB\r\nName: swarm-manager000000\r\nID: YLGP:ANHJ:ODKD:DZXG:YOG3:XA2Y:VVLK:LZ2M:N2NS:7PCF:STL5:DKVL\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\nFile Descriptors: 276\r\nGoroutines: 372\r\nSystem Time: 2017-04-04T14:33:05.635698193Z\r\nEventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nSwarm Mode running on Azure Container Services."},{"labels":[null,"documentation",null,null],"text":"**Description**\r\n\r\nThe `DiskUsage` call (which backs `docker system df`) can take an extremely long time to finish, since it walks through every container and volume's filesystem. In cases where you have lots of containers or volumes with lots of files, we've seen `DiskUsage` take minutes or even hours to complete, chewing up CPU and disk I/O the whole time. This isn't necessarily incorrect behavior, per se, but it is at least very surprising, especially since `ContainerStats` isn't expensive at all.\r\n\r\nI'm not sure how feasible it would be to improve the performance of `DiskUsage`, but if that's not possible, there should be more messaging around the costs of this API call.\r\n\r\nAlso, is there any way to get the size of an individual volume, or can you only get the size of every volume at once with `DiskUsage`?"},{"labels":[null,"documentation",null,null,null],"text":"The markdown API docs are missing any description for the `NetworkingConfig` object that is included in the config to create a container. This object is included in the example JSON that gets POSTed to `/containers/create` in the 1.22, 1.23, and 1.24 docs, but is not mentioned in the following \"JSON Parameters\" section. In particular, its child object `EndpointsConfig` is not fully defined.\r\n\r\nThis issue only affects the older markdown API docs. The objects mentioned above are defined in more detail in `api/swagger.yaml`. For the most part, the various places in the swagger description of the API that accept or produce information about networks refer to `#/definitions/EndpointSettings`.\r\n\r\n"},{"labels":["documentation"],"text":"\r\n**Description**\r\n\r\nThe placement constraints grammar is not documented.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker help create`\r\n2.  search for \"placement constraints\" on docker.com\r\n3. \r\n\r\n**Describe the results you received:**\r\n\r\nNo documentation.\r\n\r\n**Describe the results you expected:**\r\n\r\nDocumentation of the grammar, such as how not equal is written, whether and and or can be used, and what the grammar is etc.\r\n"},{"labels":["documentation",null,null,null],"text":"The [Load Balancing](https://docs.docker.com/engine/swarm/key-concepts/#load-balancing) section in the swarm docs don't make it clear if the internal loadbalancer also does health checks, and if it removes nodes that aren't running the service anymore (because it got killed or the node got rebooted).\r\n\r\nIn the following case I've got a service with replicas 3, 1 instance running on each of the 3 nodes.\r\n\r\nManager:\r\n```\r\n[root@centosvm ~]# docker ps\r\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS               NAMES\r\na593d485050a        ddewaele/springboot.crud.sample:latest   \"sh -c 'java $JAVA_OP\"   7 minutes ago       Up 7 minutes                            springbootcrudsample.1.5syc6j4c8i3bnerdqq4e1yelm\r\n```\r\nNode1:\r\n```\r\n[root@node1 ~]# docker ps\r\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS               NAMES\r\nd3b3fbc0f2c5        ddewaele/springboot.crud.sample:latest   \"sh -c 'java $JAVA_OP\"   4 minutes ago       Up 4 minutes                            springbootcrudsample.3.7y1oyjyrifgkmxlr20oai5ppl\r\n```\r\nNode 2:\r\n```\r\n[root@node2 ~]# docker ps\r\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS               NAMES\r\nebca8f24ec3a        ddewaele/springboot.crud.sample:latest   \"sh -c 'java $JAVA_OP\"   7 minutes ago       Up 7 minutes                            springbootcrudsample.2.4tqjad7od8ep047s55485na1t\r\n```\r\nNow, on node1, we kill the docker container. This node will be without a service (swarm will re-create it here after a couple of seconds to keep the replication=3 on the service)\r\n```\r\n[root@node1 ~]# docker kill d3b3fbc0f2c5\r\nd3b3fbc0f2c5\r\n```\r\nContainer gone\r\n```\r\n[root@node1 ~]# docker ps\r\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS               NAMES\r\n```\r\nNew container up\r\n```\r\n[root@node1 ~]# docker ps\r\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS               NAMES\r\nb8c9a7a5cf97        ddewaele/springboot.crud.sample:latest   \"sh -c 'java $JAVA_OP\"   11 seconds ago      Up 9 seconds                            springbootcrudsample.3.9v4cnhi8dvq7n8afb2kvp28sk\r\n```\r\n\r\nIn the output below however, when container `d3b3fbc0f2c5` was killed, the ingress  loadbalancer didn't detect this, and it was still sending traffic to the node (resulting in connection refused) ? \r\n\r\nHow should we handle such a scenario ? Do we still need an external loadbalancer for this scenario and how should we configure it ?\r\n\r\n```\r\n[root@centosvm ~]# while :; do curl http://localhost:8080/env/hostname ; echo \"\" ; sleep 1; done\r\n{\"hostname\":\"d3b3fbc0f2c5\"}\r\n{\"hostname\":\"a593d485050a\"}\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"d3b3fbc0f2c5\"}\r\n{\"hostname\":\"a593d485050a\"}\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"d3b3fbc0f2c5\"}\r\n{\"hostname\":\"a593d485050a\"}\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"a593d485050a\"}\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"a593d485050a\"}\r\ncurl: (7) Failed connect to localhost:8080; Connection refused\r\n\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"a593d485050a\"}\r\ncurl: (7) Failed connect to localhost:8080; Connection refused\r\n\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"a593d485050a\"}\r\ncurl: (7) Failed connect to localhost:8080; Connection refused\r\n\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"a593d485050a\"}\r\ncurl: (7) Failed connect to localhost:8080; Connection refused\r\n\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"a593d485050a\"}\r\ncurl: (7) Failed connect to localhost:8080; Connection refused\r\n\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"a593d485050a\"}\r\ncurl: (7) Failed connect to localhost:8080; Connection refused\r\n\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"a593d485050a\"}\r\n{\"hostname\":\"b8c9a7a5cf97\"}\r\n{\"hostname\":\"ebca8f24ec3a\"}\r\n{\"hostname\":\"a593d485050a\"}\r\n{\"hostname\":\"b8c9a7a5cf97\"}\r\n```"},{"labels":["documentation"],"text":"Hi,\r\nI was reading the docs (to get up-to-date between 1.11 and 1.13) and [here](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#/each-container-should-have-only-one-concern) I found this sentence:\r\n\r\n> In addition to the fact that containers can now be spawned with an init process, some programs might spawn additional processes of their own accord. \r\n\r\nSo I wonder, can we **really** specify an init process or does this page simply talk about the `CMD`?\r\n\r\nThe link brings nowhere hlpeful. So if it's just about the `CMD`, could we change the docs (should I click \"Edit this page\" or \"Request dos change\"?) because it is misleading :)\r\nI was really hoping that we could _finally_ have an init process :)\r\n\r\nThanks in advance :P"},{"labels":["documentation",null,null],"text":"`docker stack ps` docs are sparse in examples.\r\nhttps://github.com/docker/docker/blob/master/docs/reference/commandline/stack_ps.md#examples\r\n\r\nIt would be good to have concrete examples similar to `docker service ps` and use them in the filtering logic. https://github.com/docker/docker/blob/master/docs/reference/commandline/service_ps.md#examples"},{"labels":["documentation"],"text":"The `CopyToContainer` method's documentation simply states the following:\r\n> CopyToContainer copies content into the container filesystem.\r\n\r\nHowever, the [API endpoint's documentation](https://docs.docker.com/engine/api/v1.24/index.html#extract-an-archive-of-files-or-folders-to-a-directory-in-a-container) states the endpoint only accepts tarballs. The code in the `cli` package using `CopyToContainer` clearly shows files being archived to a tarball as well.\r\n\r\nI think `CopyToContainer`'s documentation should reflect that it expects the `content` argument to be a tarball.\r\n"},{"labels":[null,"documentation",null],"text":"from my personal experience using the tooling. it would be great if these were resolved. please, do not take these as criticism.\r\n\r\n#### output:\r\n\r\ni  wonder how everyone deals with the docker ouput, in particular the core team. i suspect, i could be wrong, that people are handcrafting scripts for common operations.\r\n\r\ndealing with the ouput in a *unix env could be easier:\r\n\r\n* requires extra work when using common tools, e.g., `docker history <id>` -> `SIZE` contains spaces so `sort -h` doesn't handle it that well. one needs to do deal with Go templates.\r\n* some commands don't take formatting options so they're harder to deal with, e.g., `docker history`.\r\n* is there a standard delimiter that is always used for all the commands? quick test: remove just the `COMMENT` column, note the space(s) in the fields themselves.\r\n\r\n```sh\r\n$ docker history golang_1.4-alpine:latest\r\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\r\nf687c7a9c8fd        22 hours ago        /bin/sh -c echo 'hello world from' 'golang...   0 B                 \r\n0deea0c51a1c        11 months ago       /bin/sh -c #(nop) WORKDIR /go                   0 B                 \r\n<missing>           11 months ago       /bin/sh -c mkdir -p \"$GOPATH/src\" \"$GOPATH...   0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV PATH=/go/bin:/usr/lo...   0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV GOPATH=/go                0 B                 \r\n<missing>           11 months ago       /bin/sh -c set -ex  && apk add --no-cache ...   138 MB              \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV GOLANG_SRC_SHA1=486d...   0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV GOLANG_SRC_URL=https...   0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ENV GOLANG_VERSION=1.4.3      0 B                 \r\n<missing>           11 months ago       /bin/sh -c #(nop) ADD file:0f9cfb2e848f093...   4.79 MB             \r\n```\r\n\r\nwhat i'm trying to get at is, it would nice if the output played nicely with the tools folks are familiar with, and some consistent behaviour in the flags commands take (some take `--format` other don't).\r\n\r\n#### documentation\r\n\r\n#### aliasing\r\n\r\nthe alias, specifically, at the bottom is confusing, e.g., https://docs.docker.com/engine/reference/commandline/images/ says `docker images` -> ` docker image ls`:\r\n\r\n* why not then just redirect to the page?\r\n* the alias should be at the top of the page, it is less confusing that way.\r\n\r\n#### links to docs\r\n\r\nit would _really_ nice to have links to the docs directly (not the alias) available on the command line. something like below, or whatever fits your bill.\r\n\r\n```sh\r\n$ docker images --help\r\n\r\nhttps://docs.docker.com/engine/reference/commandline/image_ls/\r\n\r\nUsage:\tdocker images [OPTIONS] [REPOSITORY[:TAG]]\r\n\r\n...\r\n```\r\n\r\n---\r\n\r\n```sh\r\n$ docker version\r\n```\r\n\r\n```sh\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\n```sh\r\n$ docker info\r\n```\r\n\r\n```sh\r\nContainers: 153\r\n Running: 0\r\n Paused: 0\r\n Stopped: 153\r\nImages: 1589\r\nServer Version: 1.13.0\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 1290\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.4-moby\r\nOperating System: Alpine Linux v3.5\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 1.952 GiB\r\nName: moby\r\nID: NRXU:ZFNJ:3AC2:WVYL:UODL:44GO:E7AA:AFWL:22FC:FJYR:URTX:SV2N\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 16\r\n Goroutines: 26\r\n System Time: 2017-02-09T19:26:55.917920388Z\r\n EventsListeners: 1\r\nNo Proxy: *.local, 169.254/16\r\nUsername: mohamedbana\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: true\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```"},{"labels":["documentation",null],"text":"On doc page: https://github.com/docker/docker/blob/1.13.x/docs/reference/commandline/service_create.md\r\n\r\nPlease add [this example](https://github.com/docker/docker/issues/24879#issuecomment-278044244) about how to use `--container-label`.\r\n\r\nCheers!"},{"labels":["documentation",null],"text":"The 1.13.0 docs do not describe the effect of using `--memory-swap=0`. The default is `-1` which means unlimited. Does `0` mean no swap will be allowed, or do I have to set `--memory` and `--memory-swap` to the same value to accomplish that?\r\n\r\nhttps://docs.docker.com/engine/admin/resource_constraints/#/memory-swap-details"},{"labels":["documentation",null,null],"text":"Hi, All,\r\n\r\nI found in swagger.yml and api doc, there are some missing status codes about network:\r\n\r\n```\r\nGET /networks/(id or name) misses status code of 500;\r\nDELETE /networks/(id or name) missed status code of 403;\r\n```\r\n\r\nHere is my environment, we can see that there are two network has a prefix network id of `a`:\r\n```\r\nroot@ubuntu:~# docker network ls\r\nNETWORK ID          NAME                DRIVER              SCOPE\r\nb80b14598d1f        allen               bridge              local\r\nabe5fa290b6d        allen2              bridge              local\r\ndb8dfef4e6de        allen3              bridge              local\r\na89bfaea7f72        allen4              bridge              local\r\nfd6a89aad8ff        bridge              bridge              local\r\n85ded5476b05        docker_gwbridge     bridge              local\r\nd909a9f88b4c        host                host                local\r\nz1sybtvc99hu        ingress             overlay             swarm\r\n615a39ff6c47        none                null                local\r\n```\r\nThen we send an http request like the following pic, we found the status code is 500 which is not versioned in the swagger.yml nor api docs:\r\n<img width=\"782\" alt=\"2017-02-03 1 25 57\" src=\"https://cloud.githubusercontent.com/assets/9465626/22580645/06843094-ea15-11e6-845d-92a959624c63.png\">\r\n\r\n\r\nSecond, we send a DELETE request for network `host`, then daemon will response a status code of 403 which is also not versioned in swagger.yml or api docs. Here is test pic:\r\n![wechatimg4](https://cloud.githubusercontent.com/assets/9465626/22580703/a0a774d8-ea15-11e6-8ee1-32d82da5fbd7.jpeg)\r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\nThere is no status code 500 for api endpoint `GET /networks/(id or name), nor 404 for `DELETE /networks/(id or name)`\r\n\r\n**Describe the results you expected:**\r\nrelated status codes are versioned in doc\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nroot@ubuntu:~# docker version\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:50:17 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:50:17 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nroot@ubuntu:~# docker info\r\nContainers: 8\r\n Running: 0\r\n Paused: 0\r\n Stopped: 8\r\nImages: 37\r\nServer Version: 1.13.0\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 202\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: gkdoxio9hz1b325t544ftpry4\r\n Is Manager: true\r\n ClusterID: 3bidcebapq2115pts6cik50ah\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.59.103\r\n Manager Addresses:\r\n  192.168.59.103:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.19.0-25-generic\r\nOperating System: Ubuntu 14.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.954 GiB\r\nName: ubuntu\r\nID: Q2ZC:GWDN:27OH:GRMH:G6QU:W7QP:4TIX:Q5F6:YEVK:45XP:EXHC:HOB5\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 32\r\n Goroutines: 132\r\n System Time: 2017-01-29T14:51:41.049720516+08:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n https://a.b.c/\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":[null,"documentation"],"text":"The [ARG section](https://docs.docker.com/engine/reference/builder/#/arg) says the following:\r\n\r\n>**Warning:** It is not recommended to use build-time variables for passing secrets like github keys, user credentials etc. Build-time variable values are visible to any user of the image with the docker history command.\r\n\r\nThis, however, is not very helpful. The warning should mention what to use instead when you need secrets during build time."},{"labels":[null,"documentation",null],"text":"I read the v1.25 document, but I can't find any related information.  I was wondering if \"docker stack deploy\" api was the on roadmap.  "},{"labels":["documentation"],"text":"The [configuration file documentation](https://docs.docker.com/engine/reference/commandline/cli/#/configuration-files) states\r\n\r\n> The property psFormat specifies the default format for docker ps output. When the --format flag is not provided with the docker ps command, Dockers client uses this property. If this property is not set, the client falls back to the default table format. For a list of supported formatting directives, see the [Formatting section in the docker ps documentation](https://docs.docker.com/engine/reference/commandline/ps/)\r\n\r\nThere's no such section in the docker ps documentation."},{"labels":["documentation"],"text":"A while ago, I started contributing to docker and was able to fix few doc and cli related issues.\r\nBut was unable to pick issues related to other areas like runtime, networking, etc. It is bit difficult to know how runc, containerd, etc are used by docker in its code.\r\n\r\nFor any beginner level contributor, it becomes difficult to understand the entire code flow or even a single docker component codebase.\r\nThere are documentation available for few of the tools but it don't mention anything about the tools used internally within docker.\r\nThere should be documentation explaining all tools and technologies that docker uses internally in the code like : `runc, libnetwork, containerd, docker-containerd-shim, ctr, cobra, rootfs, etc`\r\nShould also document : how docker creates namespaces, which all system calls do docker calls internally, how docker uses runc and so on.\r\nThere should be pointers to files and repository where the code resides for each of these tools like code for rootfs related stuff, image related stuff, etc.\r\nSuch documentation will really help beginner level contributor a lot."},{"labels":[null,"documentation",null],"text":"The docs at https://docs.docker.com/engine/reference/commandline/version/ say to get the template fields for `docker version` output, use `docker version --format '{{json .}}'` which gives me:\r\n\r\n```\r\n{\"Client\":{\"Version\":\"1.13.0\",\"ApiVersion\":\"1.25\",\"GitCommit\":\"49bf474\",\"GoVersion\":\"go1.7.3\",\"Os\":\"darwin\",\"Arch\":\"amd64\",\"BuildTime\":\"Wed Jan 18 16:20:26 2017\"},\"Server\":{\"Version\":\"1.13.0\",\"ApiVersion\":\"1.25\",\"MinAPIVersion\":\"1.12\",\"GitCommit\":\"49bf474\",\"GoVersion\":\"go1.7.3\",\"Os\":\"linux\",\"Arch\":\"amd64\",\"KernelVersion\":\"4.9.5-moby\",\"Experimental\":true,\"BuildTime\":\"Wed Jan 18 16:20:26 2017\"}}\r\n```\r\n\r\nSo I try\r\n```\r\ndocker version --format '{{.Server.ApiVersion}}'\r\n\r\ntemplate: :1:9: executing \"\" at <.Server.ApiVersion>: can't evaluate field ApiVersion in type *types.Version\r\n```\r\n\r\nEventually I guess and try\r\n```\r\ndocker version --format '{{.Server.APIVersion}}'\r\n1.25\r\n```\r\n\r\nSo the output on the JSON list has a different casing, making it hard to work out what the Go names are. I blame Go's weird JSON handling, but I wonder if it can be fixed? Or maybe the docs just could add a note?\r\n\r\nActually the JSON has the same name\r\n```\r\ndocker version --format '{{json .Server.ApiVersion}}'\r\n\r\ntemplate: :1:14: executing \"\" at <.Server.ApiVersion>: can't evaluate field ApiVersion in type *types.Version\r\ndocker version --format '{{json .Server.APIVersion}}'\r\n\"1.25\"\r\n```\r\n\r\n```\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\ncc @thaJeztah "},{"labels":["documentation",null,null],"text":"I am trying to understand the resource control related to the CPUs from Swarm Mode.\r\n\r\nFrom the docs I just see:\r\nhttps://docs.docker.com/engine/reference/commandline/service_create/\r\n--limit-cpu value                Limit CPUs (default 0.000)\r\n\r\nhttps://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#create-a-service\r\nNanoCPUs  CPU limit in units of 10-9 CPU shares.\r\n\r\nThe concept is completely different from the ones related to containers and I think there is a lack of documentation regarding the use of these parameters (limit-cpu and reserved-cpu)"},{"labels":["documentation",null],"text":"https://docs.docker.com/engine/reference/commandline/tag/\r\n\r\n````\r\nA tag name may contain lowercase and uppercase characters, digits, underscores, periods and dashes. A tag name may not start with a period or a dash and may contain a maximum of 128 characters.\r\n````\r\n\r\nWhat is a character? You mean ASCII Latin letters? The use of the word \"character\" is WRONG because a character refers to ANY code point! There is also more than one character referred to as dash and underscore in unicode. That needs to be specified properly too. I know you're probably referring to ASCII and not unicode, but you never actually write that."},{"labels":["documentation",null,null],"text":"**Description**\r\nI found that `docker service update` command document is not so detailed in https://github.com/docker/docker/blob/master/docs/reference/commandline/service_update.md\r\n\r\nI think there are lots of missing options in explanation, such as `--rollback`, and so on.\r\n\r\nIn addition, in the doc it says:\r\n```\r\nThe parameters are the same as [`docker service create`](service_create.md).\r\n``` \r\n\r\nI think that is not true.\r\n\r\nHere is the evidence:\r\n```\r\nroot@ubuntu:~# docker service create --help | grep \"\\-\\-\" | wc -l\r\n45\r\nroot@ubuntu:~# docker service update --help | grep \"\\-\\-\" | wc -l\r\n57\r\n```\r\n So, there is at least 12 parameters that are not the same.\r\n\r\nI think we need a detailed `docker service update` doc for the users. "},{"labels":["documentation",null,null,null],"text":"**Description**\r\n\r\nThere is very little information from API documentation that can explain how to use REST request to promote or demote swarm node from remote side. It seems I should post \"/nodes/id/update\" with parameters attached. However, some parameters like 'version' are vague. Can you provide an example to show promote/demote?\r\n\r\nSomething like:\r\n`curl --unix-socket /var/run/docker.sock -X POST -H 'Content-Type: application/json' -d '{\"version\": .., ..}' http:/nodes/id/update`\r\n\r\nThanks"},{"labels":["documentation",null],"text":"As far as I can tell all tutorials are for sandbox style swarm where multiple VMs run on a single physical host. I can't find a good resource that gives a simple tutorial on getting a swarm running across more than one physical machine. Maybe this would need to be done per OS, as my understanding is Windows and MacOS don't support this natively, but do when using docker-machine.\r\n\r\nIn an ideal tutorial, I think a user should be able to follow the commands on their OS, and get a swarm running across two physical hosts. \r\n\r\nFrom my Windows OS perspective, questions I'd like answered in such a tutorial (as these are areas where I'm stuck after looking at the current tutorials and also production swarm docs):\r\n* On Windows must I (or is it best practice to) use `docker-machine create --swarm etc`\r\n* What is the relationship between `docker-machine create --swarm etc`, `docker swarm etc`, `docker run swarm etc`. After trial and error I think I understand how the layers build upon each other, but an explicit description would be nice\r\n\r\nAs a final bit of context to this request, I'm trying to set up a two node swarm across two Windows hosts. I think I must use `docker-machine create --swarm etc` - and everything appears to work OK except the worker node is stuck in pending. I _think_ this is because TLS is enabled by default (and it can't be disabled, or disabling is too dangerous to provide any explanation of how to do it :P ) so that's my next thing to try... but it would be nice if there was a quick one pager that pointed out all the relevant steps."},{"labels":["documentation",null],"text":"Could we create a doc that has a table of plugins that are supported for 1.13?  Ideally in the future this should be in the hub / store but a doc to start would be great."},{"labels":["documentation",null,null,null],"text":"**Description**\r\n\r\nDocker volume command documentation files do not fully describe expected behavior for contributors writing volume drivers. In particular, volume management in swarm mode can be troublesome for clustered storage systems when determining which node to create volumes and where to report volume list. This can lead to cartesian bugs when creating volumes (every node creates a volume based on the volume create request). Some plugin authors have been confused as to where to report a volume as available depending on how their storage system manages volumes.\r\n\r\n@anusha-ragunathan is aware of this and will be following up with more info.\r\n\r\nCC'ing @thaJeztah @stevvooe @vieux per Anusha's request."},{"labels":["documentation"],"text":"While it's clear how to use the `--tmpfs` flag on `docker run`, it isn't clear how to accomplish the same thing via the Remote API because the `HostConfig.Tmpfs` option is undocumented [here](https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/create-a-container).\r\n\r\nIt took a fair amount of trial-and-error to figure out that it's meant to look something like this:\r\n\r\n```js\r\n'HostConfig': {\r\n  // ...\r\n  'Tmpfs': {\r\n    '/tmp': 'rw,exec' // mount options (your choice)\r\n  }\r\n}\r\n```\r\n\r\nHopefully documenting this here, and eventually in the actual API docs, will save someone else from diving through [this](https://github.com/docker/docker/blob/master/api/types/container/host_config.go)."},{"labels":["documentation",null],"text":"https://github.com/docker/docker/pull/27998 seems to have been merged with without any documentation.\r\n\r\nThere should be.\r\n\r\nShort version how to use these features to get the ball rolling:\r\n\r\n1. Make a new `docker-compose.yml` or port an existing one, with `version: '3'` explicitly specified.  Seems the \"schema\" (layout of `services`, `networks`, etc. keys) did not change much in this new version but some parameters for services are unsupported.  I'm guessing `build:` is likely not supported as well?\r\n2. Run `docker deploy --compose-file docker-compose.yml stackname`.\r\n3. `docker stack` can be used to view the generated services for the \"stack\" defined by the Docker Compose file.  e.g., `docker stack ls` to view created stacks, `docker stack services stackname` to view the services associated with that stack, etc.\r\n\r\nFYI @londoncalling @johndmulhausen "},{"labels":[null,"documentation"],"text":"## Reasoning\r\n\r\nAs part of [my work to improve the API documentation](https://github.com/docker/docker.github.io/pull/210) I realised that \"Docker Remote API\" is a bit of a confusing name. The Docker Registry serves the [Docker Registry API](https://docs.docker.com/registry/spec/api/), Docker Cloud serves the [Docker Cloud API](https://docs.docker.com/registry/spec/api/), Docker Hub has (had?) the [Docker Hub API](https://docs.docker.com/v1.7/reference/api/docker-io_api/), Docker Trusted Registry has the [Docker Trusted Registry API](https://docs.docker.com/apidocs/overview/), but Engine is the odd one out.\r\n\r\nAs far as I can see, it's a historical artefact from when \"Docker\" was just the Engine. We've since added other APIs, and the original was never given a distinctive name.\r\n\r\nBesides consistency, here are some reasons for the change:\r\n\r\n1) \"Remote\" is redundant all HTTP APIs are remote. \"Docker API\" would collide with other Docker APIs, hence \"Docker Engine API\".\r\n2) When you want to find the API for the Engine, it's not immediately clear that you should be looking for the \"Remote API\". (And vice versa, \"Remote API\" does not make it immediately obvious that it's the thing the Engine exposes.)\r\n3) The information architecture in the documentation is a bit weird, because \"Remote API\" doesn't comfortably sit under \"Engine\".\r\n\r\n## Implementation\r\n\r\nThe only place we reference it seems to be in the documentation and CLI docs, so looks like we can just do a big ol' search and replace.\r\n\r\nThe only breakage I can think of is if people search for \"Docker Remote API\" on Google and the documentation doesn't turn up. To counteract this, we can leave it as the \"Remote API\" in old versions of the API documentation, and add \"Remote API\" to the keywords of the latest API docs. Or, just a bit of copy like: \"In version X of Docker and below, the Engine API was called the Docker Remote API.\"\r\n\r\n/cc @dnephin @johndmulhausen @icecrime @shykes "},{"labels":["documentation",null,null,null],"text":"**Description**\r\n\r\nWhen I read documentation about live-restore option I supposed that it is supported by Docker for Windows. There is no one mention that it is not supported: https://docs.docker.com/engine/admin/live-restore/\r\n\r\nFurthermore, this documentation https://docs.docker.com/engine/reference/commandline/dockerd/ lists this option in \"Windows configuration file\" section. But I was not able to configure it in both ways (as an argument of dockerd and variable of daemon.json)\r\n\r\n**Steps to reproduce the issue:**\r\n1. When I try to run dockerd with --live-restore option I got a error\r\n```\r\nPS C:\\Users\\Administrator> dockerd --live-restore --register-service\r\nStatus: unknown flag: --live-restore\r\nSee 'dockerd --help'., Code: 125\r\n```\r\n2. When I added ``` \"live-restore\": true ``` to the daemon.json and started dockerd I got another error\r\n```\r\nPS C:\\Users\\Administrator> dockerd\r\nunable to configure the Docker daemon with file C:\\ProgramData\\docker\\config\\daemon.json: the following directives don't match any configuration option: live-restore\r\n```\r\n\r\n**Expected behaviour:**\r\nI expect that dockerd should run successfully with this option. Or at least have mention in the documentation that it is not supported\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.2-cs2-ws-beta\r\n API version:  1.25\r\n Go version:   go1.7.1\r\n Git commit:   050b611\r\n Built:        Tue Oct 11 02:35:40 2016\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      1.12.2-cs2-ws-beta\r\n API version:  1.25\r\n Go version:   go1.7.1\r\n Git commit:   050b611\r\n Built:        Tue Oct 11 02:35:40 2016\r\n OS/Arch:      windows/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 0\r\n Paused: 0\r\n Stopped: 1\r\nImages: 1\r\nServer Version: 1.12.2-cs2-ws-beta\r\nStorage Driver: windowsfilter\r\n Windows:\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: nat null overlay\r\nSwarm: inactive\r\nDefault Isolation: process\r\nKernel Version: 10.0 14393 (14393.321.amd64fre.rs1_release_inmarket.161004-2338)\r\nOperating System: Windows Server 2016 Datacenter\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 4 GiB\r\nName: EC2AMAZ-P7L1INT\r\nID: 6JSQ:WIFX:QJ25:TM5T:JBOG:CR7M:NSOW:X2AJ:QJXR:PRID:CLE2:6Q67\r\nDocker Root Dir: C:\\ProgramData\\docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nIt is a EC2 instance which was launched from \"Windows Server 2016 with containers image\""},{"labels":["documentation"],"text":"per https://github.com/docker/docker/pull/25820#issuecomment-256800737, we need to add documentation for the new metrics API, and probably some examples in other parts of the documentation to show how it's used.\n\nopening this issue for tracking, so that we unblock the PR that implements the feature\n\n/cc @mstanleyjones @crosbymichael  - assigning you both so that we don't forget :smile:\n"},{"labels":["documentation",null],"text":"nocopy volume bind option is not mentioned in the respective 1.23 remote API page, although it is implemented and mentioned in changelog as well.\n\nhttps://docs.docker.com/engine/reference/api/docker_remote_api/#v1-23-api-changes\nhttps://docs.docker.com/engine/reference/api/docker_remote_api_v1.23\n"},{"labels":["documentation",null,null],"text":"https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/#/for-a-direct-lvm-mode-configuration\n\nThis doc informs users to push all images to a remote registry and then informs to rm -f /var/lib/docker, thus removing everything including certs that may be stored under this directory tree.  For good reasons, we instruct users to change from the default devicemapper loopback configuration that we recommended in early releases, of which they maybe running UCP on those nodes.   Deleting the certs in /var/lib/docker/discovery_certs creates a far more complex recovery scenario than just copying the certs back after the lvm changes have been completed.  This is the error received when you try to start up the docker daemon after following the doc instructions on a UCP node.\n\nInitializing discovery with TLS\nError starting daemon: discovery initialization failed (Could not read CA certificate \\ \"/var/lib/docker/discovery_certs/ca.pem\\\" open /var/lib/docker/discovery_certs/ca.pem: no such file or directory\n\nCan additional steps be written into the referenced doc perhaps such as copy the /var/lib/docker directory to /tmp to enable the recovery of items such as certs, etc in order to  keep from  placing users into extensive recovery scenarios.  thnx\n"},{"labels":["documentation",null],"text":"On the `docker service ps` documentation page (https://docs.docker.com/engine/reference/commandline/service_ps/) only three filters are shown as supported: id, name and desired-state. However, I discovered that there is also the \"node\" filter which matches tasks by either the hostname of the node they are deployed on, or the NodeID of that node (confirmed on 1.12.1).\n\nThe API reference for the tasks API correctly mentions that the NodeID can be used as a parameter to the node filter (https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/list-tasks), but omits the fact that the hostname of the node can be also used as the value to that filter\n"},{"labels":["documentation",null],"text":"I failed to find in the documentation or in the issues any information regarding how many swarm managers I should declare. Sure, more than one if you want high availability, but how many? and why not make all nodes managers? what is the tradeoff of being a manager vs a worker?\n"},{"labels":["documentation",null,null],"text":"The restart-policies for `docker run` had intuitive names like `no, on-failure, always` and we [have docs that explain what they mean](https://docs.docker.com/engine/reference/run/#restart-policies-restart).\n\nWith `docker service`, the restart policies are a bit different. You have options like `none, on-failure, or any`. It's not obvious what the `restart-policy=any` means, so we need to have some documentation for it.\n"},{"labels":["documentation",null],"text":"In reading https://docs.docker.com/engine/reference/builder/#/entrypoint, I find it a bit confusing to follow and understand with confidence.\n\nHere are a few questions which result:\n- Is there any way to pass the list form of `--entrypoint`, so when using `docker run` on the cli, your command will receive `SIGTERM` or other signals?\n- If you want to ensure your commands receive those signals, you _must_ use the list form for `--entrypoint` or `ENTRYPOINT`, correct?\n- When no `--entrypoint` is specified, what is the behavior of docker's signal passing?\n- Does the `--command` parameter (in either the list form or not) affect the behavior of signal passing?\n"},{"labels":["documentation"],"text":"The content of https://docs.docker.com/registry/insecure/#using-self-signed-certificates says \"be sure to use the name 'myregistrydomain.com' as a CN\". I think the intent of the overall page is for the developer to fill in their own domain. Perhaps it could be a bit clearer.\n"},{"labels":["documentation",null],"text":"It is probably the case that the recommended way of working with images and a swarm of nodes is to use a private image registry or docker cloud solution. However, I feel that the current design (and documentation) is a bit incomplete. The problem is that if you are a newcomer and start creating your swarm, when you build your image on one of the swarm leaders, and then starts a service using that image, if the service is scheduled to run on other nodes wheere the image is not available then it will just silently fail. The error claiming that an image is missing is often not complete displayed either when using `docker service ps` since the message is too long, which requires the user to login into the machine where the docker daemon is running and checking the logs there... not very friendly.\nI have not found in the documentation that an image that is custom built must be pushed to a image registry in order to make the swarm work, which for me at least, took some precious time to discover the hard way. \nSo finally, I think the best solution would be if every swarm node could act as a private image registry out of the box, but only for the nodes part of that swarm. By doing that images will just be created once and everything will work magically as so many other things in docker. The other alternative I can think of is that when building an image you could pass a --swarm flag to tell docker that you want to build that same image on all the nodes part of the swarm. The problem with this solution is of course that if the swarm grows, the new nodes will not have that image and fail again silently...\n"},{"labels":["documentation",null],"text":"Just putting this down here so we don't forget;\n\nWe don't have clear documentation on how \"daemon shutdown\" works; i.e. timeout of containers, stop, kill. Also what happens if a container fails to be killed.\n\nAnother point brought up by @mlaventure is that `systemd` also uses a timeout, so the daemon itself may be killed before it finishes shutdown.\n"},{"labels":["documentation",null,null],"text":"https://github.com/docker/docker/pull/24987 was merged without documentation changes. This issue is for tracking that\n\n/cc @ripcurld00d \n"},{"labels":["documentation"],"text":"Rename / move existing docs where needed, and update completion scripts for #26025 and #26716\n"},{"labels":["documentation"],"text":"Since the Docker API client and types have been moved form [docker/engine-api](https://github.com/docker/engine-api) to the main docker repo [docker/client](https://github.com/docker/docker/tree/master/client), no README.md with a simple usage example is available anymore.\nIt would be helpful to add a simple README.md file with at least the same usage example from the old [README.md](https://github.com/docker/engine-api/tree/6c72c24563a7a266e91c8445811f74f138973457).\nI'll be glad to send a PR for this.\n"},{"labels":["documentation",null],"text":"[VENDORING.md](https://github.com/docker/docker/blob/master/VENDORING.md) starts out with the following:\n\n> This document outlines recommended Vendoring policies for Docker repositories. (Example, libnetwork is a Docker repo and logrus is not.)\n\nThen, it goes on about some semantic versioning requirements for no apparent reason and requirements about a release process, including a changelog (even for dependencies that have version control). Most of this is a waste of time for package-level projects (go-events, go-connections, etc.) when we have commit-level changelog.\n\nThis document SHOULD cover the following:\n1. Structure of our vendoring system.\n2. How to update an existing dependency.\n3. How to add a new dependency.\n4. Any helpful incantations to make vendoring work more smoothly.\n\nThis will be much more helpful to contributors than a bunch of nonsense about semantic versioning.\n"},{"labels":["documentation",null],"text":"In `docker node` you can see that there is a `docker node update --label-add` option to add labels.\n\nI was confused about the difference between this and Engine labels (i.e., added with `--label` to `dockerd`) and asked the Swarm team to explain.  The answer was insightful and should be documented.\n\nEssentially:\n1. A compromised worker node could change its own Engine labels, but not node labels.  Only managers can change node labels.\n2. Therefore, node labels could be useful to e.g. schedule only on machines where certain special workloads should be run, for instance machines where [PCI-SS compliance](https://www.pcisecuritystandards.org/) is met.  A compromised worker could not compromise these special workloads because it cannot change node labels.\n3. Engine labels, however, are still useful because some features which do not affect secure orchestration of containers might be better off set in a decentralized manner.  For instance, an engine could have a label to indicate that it has a certain type of disk device, which may not be relevant to security directly.  These labels are more easily \"trusted\" by the orchestrator.\n\nhttps://docs.docker.com/engine/swarm/manage-nodes/#/add-or-remove-label-metadata explains that you _can_ do this, but not why you might want to.\n\nFYI @londoncalling and thanks @diogomonica @stevvooe @cpuguy83 for explanation\n"},{"labels":["documentation",null,null],"text":"Someone probably needs to start a roadmap issue for getting a solution to current problems with \"docker deploy\" (ie. where the feature is planned to go) because so far, I cannot find anything, let alone an issue, describing current progress.\n\nIncluding:\n- `docker deploy`\n- `docker stacks`\n- The planned relation between Swarm Mode / Docker Compose / `docker deploy`\n- The planned file formats\n"},{"labels":["documentation",null],"text":"The CLI reference docs say this about `swarm leave --force`.\n\n```\n      --force   Force leave ignoring warnings.\n```\n\nThey should also mention that `swarm leave --force` allows you to remove a manager, but does not gracefully remove the manager from the swarm. Doing this may cause the swarm to lose its quorum. Unless you don't care about the impact on the existing cluster, you should always demote managers to workers before removing them.\n\ncc @sfsmithcha\n"},{"labels":["documentation",null],"text":"Feedback on finding Node state and availability in the docs:\n\nDoc is here: https://docs.docker.com/engine/swarm/manage-nodes/#manage-nodes-in-a-swarm\n\n I searched for \"status\", \"node status\", \"node state\", \"availability\", \"node availability\", \"swarm availability\". I assumed there was no such page so I didn't search for \"Reachable\", which would have been my next keyword, but it looks like that one correctly points to that page. Also, I went to the swarm-mode overview page looking for some sort of \"concepts\" or \"keywords\" page, followed the url to the \"key-concepts\" page which did not mention any details node status and noticed that the only follow-up URL at the bottom redirected me back to swarm-mode overview. I didn't suspect to go to the \"manage nodes\" headline as I imagined that would cover the various leave/rm/demote/promote flows, rather than document the possible node states\n"},{"labels":["documentation"],"text":"This is an issue with the documentation for setting up TLS in a swarm which can be found [here](https://docs.docker.com/swarm/configure-tls/). In [Step 3](https://docs.docker.com/swarm/configure-tls/#/step-3-create-and-sign-keys) it says that a file called `node-priv.key` will be created which will contain the private key. However, no reference to a `.key` file is found anywhere else in the document. I'm not really sure what changes need to be made, as far as I can tell it crates the private key in a file called `node-priv-key.pem`, but I'm not sure if anything else needed to be done.\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nI'm not able to find the actual markdown file in the `/docs` folder in the repo which contains the guide.\n\n**Steps to reproduce the issue:**\n1. Go through the [swarm/configure-tls](https://docs.docker.com/swarm/configure-tls) guide\n2. Expect a `.key` file to be made at some point\n3. It isn't\n\n**Describe the results you received:**\nThe resulting keys two `.pem` files are created, with one of them suggesting it's the private key.\n\n**Describe the results you expected:**\nDocs to either refer to the private key consistently or have steps for creating the `.key` file\n"},{"labels":["documentation",null],"text":"**Steps to reproduce the issue:**\n1. Go to https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/display-system-wide-information and search for /swarm to look for how to get swarm join token \n2.\n3.\n\n**Describe the results you received:**\n\nNo results on how to retrieve swarm join token. \n\n**Describe the results you expected:**\n\nThe API to retrieve the swarm join token should be available on the API docs\n\n**Additional information you deem important (e.g. issue happens only occasionally):**\n"},{"labels":["documentation",null,null,null],"text":"I would like to put together a quick guide for Docker on ARM - especially Raspberry Pi. There is a `curl | sh` method newly available which makes the process quicker and easier. From my perspective this should feel as close as possible to the experience of Docker on x86_64. \n\nI'd like to put a short/simple guide together:\n- Recommended (default) OS to be Raspbian (Jessie Lite)\n- `curl | sh` recommended installation method\n- A mention to the problem with multi-arch - i.e. `docker run busybox` won't work\n- A recommended base image: `resin/rpi-raspbian`\n- Give short example of Dockerfile for Node.js, Python\n- Short mention of GPIO (accessing the input output headers on the device) and privilege mode.\n- A disclaimer / buyer beware.. few words on limitations.\n\nPotential follow-up guide:\n- Building Docker on ARM\n- Adding a swap file\n- Expected build duration\n- Limitations\n\nRef: \nhttps://github.com/docker/docker/issues/24561\n\nInput especially welcome from @DieterReuter , @estesp , @crosbymichael @StefanScherer as follow-up. @thaJeztah what are your thoughts?\n"},{"labels":["documentation",null,null],"text":"The `/swarm/leave` api supports a `?force=true` parameter and it should be documented in the [api docs](https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/leave-a-swarm). I'll be glad to send a PR for this.\n"},{"labels":["documentation",null,null,null],"text":"Currently, the Swarm Init api (/swarm/init) returns the node id when the swarm is successfully initialized:\n\n```\ncurl -XPOST --unix-socket /var/run/docker.sock http:/swarm/init -d '{\"ListenAddr\": \"127.0.0.1\"}'\n\"ejdf53mtjiqp8feyf1907si12\"\n```\n\nThis behavior is not documented in the [api docs](https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/initialize-a-new-swarm).\n\nI believe this should be documented and I can send a PR for that.\n"},{"labels":["documentation"],"text":"Current man pages are not all consistent in term of content, placement and sections. There is also a lot of _duplication_ between man pages and cli reference documentation as they are maintained separately. This issues is there to discuss on those subjects and follow evolutions of our generation process.\n\nThe main goals are the following and take most of the inspiration from `git` man pages and reference documentation :\n- Have consistent man page across commands.\n- Have cli reference documentation be in sync (or the same) as man page (just a web page for it).\n- Have (or not) man pages generated from source (part of it or all of them).\n### Man page style guide\n\nWe currently don't have any style guide for man pages, i.e. what information to put where and in what order. As code style guide, we might want to define a _man page style guide_ document that would define what makes a good man page for `docker` commands. This is related to #16219.\n\nThat could look something like :\n\n```\n## NAME\n\ncommand - short description of the command\n\n## SYNOPSIS\n\ncommand --with --some --flags --and [OPTIONS] \n\n##  DESCRIPTION\n\nA description is more detailed than what's in the NAME section, explaining more in depth what the command does (but that's all)\n\n## OPTIONS\n\n-a --all\n\nFlags descriptions \n\n## ADDITIONAL SECTION 1\n\nAdditional section are possible. An example would be `FILTERING` and `FORMATTING` for the `docker-ps` command, or something about the reference format for command that allow us to set them (like `docker-commit`, `docker-tag`, ).\n\n## ADDITIONAL SECTION 2\n\nNo limit in the number of section though \n\n## EXAMPLES\n\nHere, we just gives example on how to do this on that ; this is not where we explain how to filter container (it's done in its own section), just a working example of it.\n\n## ENVIRONMENT AND CONFIGURATION VARIABLES\n\nIf there is any environment variable or configuration (in the client/daemon config file) related to the command, we'll talk about it here ; like `ImagesFormat` for `docker-images` formatting.\n\n## SEE ALSO\n\n```\n### Man page and cli ref doc maintainability\n\nMaintaining separately man pages and cli reference documentation is error prone, it's easy to forget some flags or description somewhere. It's related to #19088. The basic idea would be to generate man pages and cli reference documentation from the same source.\n\nGiven that we are trying to generate man page from the code (at least partially), we should also study a way to generate cli reference documentation at the same time  shouldn't be difficult to convert markdown/man page to hmtl .\n#23825 introduced a way to generate man page directly from code  from a small subset of command right now, but goal is to do that for all commands.\n\nCurrent limitations / questions are the following (making the assumption that cli reference documentation is generated from the same source) :\n- Cobra does not seems to allow add any custom section (it only supports `EXAMPLE`).  It's possible to add `## Some Section` in the description part, but this would make the `OPTIONS` section be really far down in the man pages (which can be fine :angel:).\n- It makes \"documentation\" PR's becomes more difficult; to make changes to this part of the documentation now requires contributors to touch the actual code.\n- Should we keep around some part of the man pages/documentation in markdown files (that would be merge with generated content from cobra) ?\n\n/cc @thaJeztah @dnephin @icecrime \n"},{"labels":["documentation"],"text":"Is it intentional to have 1.12 `/swarm/leave` to have a query param of `?force=true`? If so, can we have the documentation updated (happy to do it if you point me in the right direction)? If not, should it be in the body instead and possibly JSON like the rest of the `/swarm/*` endpoints?\n\nThanks.\n"},{"labels":["documentation",null],"text":"https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.25.md needs to be updated. @thaJeztah kindly agreed to take care of this.\n"},{"labels":["documentation",null],"text":"Instead of adding users to the **_docker_** group as the [Installation manuals](https://docs.docker.com/engine/installation/#installation) suggest, which is a security hole by definition and antonomasia, I think it's better to add a password to the **_docker_** group and use the **sg** and **newgrp** commands as follows:\n\nAdd a password to **_docker_** group:\n`gpaswd docker`\n\nCreate a session with the **_docker_** group added to the supplementary group ID's:\n`sg docker newgrp $(id -gn)`\n\nAt this point the user may run **docker** commands...\n\nLogout:\n`exit`\n\nNote: The **sg** command is needed to avoid running the sequence: `newgrp docker` and then `exec newgrp $USER`\n\nOn the plus side, there's no need to use **sudo** anymore with its side-effects of resetting environment variables, etc.\n"},{"labels":["documentation",null],"text":"documentation  fail, can't find reason, for PR #25203, are there anyone meeting this:\n\nBuild Errors:\nFix the path under windows for dockerimages.md: https://github.com/docker/docker/pull/25203\n# Filtered (engine) Summary:\n\n```\nFound: 852 files\nFound: 0 errors\n```\n"},{"labels":["documentation",null],"text":"The API docs do not cover DELETE or POST on `/nodes/<id>`. Only GET is documented.\n"},{"labels":["documentation"],"text":"Reference to `docker daemon` v/s `dockerd` is ambiguous in \"Configuring and running Docker on various distributions\" (docs/admin/index.md).  See @mlaventure 's comment on https://github.com/docker/docker/pull/24970/files/88ebfc701238dcfacfaf1d3f0a363848460df452#r71992335\n\nThis article looks like it needs to be reworked for clarity in general. \n"},{"labels":["documentation",null],"text":"The command line reference documentation for `docker service create` and `docker service updates` lists the supported flag, but does not explain each one in detail. When possible, we should add detailed descriptions of the flags.\n\ncc @sfsmithcha\n"},{"labels":["documentation"],"text":"Current docs/tutorial refer to listen address only. When https://github.com/docker/docker/pull/24237 goes in, need to clarify the distinction between listen address and advertise address and the address auto-detection behavior.\n"},{"labels":["documentation"],"text":"per https://github.com/docker/docker/issues/23710#issuecomment-233751568, we don't have a CLI command (yet) to view logs for a service, so for 1.12 we should document this. Related issue for 1.13; https://github.com/docker/docker/issues/24812\n\n/cc @sfsmithcha should I assign this for you? Or keep it unassigned?\n"},{"labels":["documentation",null],"text":"cfr: https://github.com/docker/docker/issues/23931#issuecomment-233035772\n\nThis is supported now but the usage is not clear to users.  Hopefully my comment will help to explain or maybe I could submit a patch if I somehow miraculously find some time.\n"},{"labels":["documentation",null,null],"text":"The API uses an \"open schema\", which means that new properties _may_ be added to older API versions if a newer daemon is used. However, it should still be compatible with older clients.\n\nThis is described in the documentation; https://github.com/docker/docker/blob/v1.11.2/docs/reference/api/docker_remote_api.md\n\n> Docker's Remote API uses an open schema model. In this model, unknown properties in incoming messages are ignored. Client applications need to take this behavior into account to ensure they do not break when talking to newer Docker daemons.\n\nThis wording needs some improvement, because it doesn't clearly describe that older API versions can introduce \"unknown\" properties\n"},{"labels":["documentation",null,null],"text":"Swarm integration tests have a lot of helper functions and utilities that are really useful, but really poorly organized and documented. Most of the tests themselves are in `integration-cli/docker_api_swarm_test.go`, but the helpers are spread out across files like `integration-cli/docker_utils.go`. These helper functions need doc comments describing their usage, and there needs to be a short guide written on how to use the swarm integration helpers.\n\nFor example,\n\n```\nfunc waitAndAssert(c *check.C, timeout time.Duration, f checkF, checker check.Checker, args ...interface{})\n```\n\nhas no documentation explaining usage. \n\nFurther, there are things like:\n\n```\ntype checkF func(*check.C) (interface{}, check.CommentInterface)\ntype reducer func(...interface{}) interface{}\n```\n\nIt's totally unclear what a `checkF` or a `reducer` does or how to implement one.\n\nAdding this documentation would make writing new swarm integration tests way easier. Further than just adding documentation, a good refactoring of the testing suite might be in order, to improve clarity. Breaking the swarm suite into multiple smaller suites might also be good. Pretty much the whole integration suite is a mess and it needs work, but documentation and light-touch refactoring is a really great place to start on improving it.\n"},{"labels":["documentation",null],"text":"`docker run --pid=...` was implemented in https://github.com/docker/docker/pull/10080, but only the CLI docs were updated, but not the related API changes. Support for `--pid=container:...` was added in https://github.com/docker/docker/pull/22481, which also was not documented in the API\n"},{"labels":["documentation",null],"text":"Sorry if this is a duplicate.  I feel like I'm asking a dumb question but I couldn't figure this out even with all the documentation and tutorials.  I think it would be great if the answer went into another page of [this tutorial](https://docs.docker.com/engine/swarm/swarm-tutorial/).\n\nIn docker 1.12.0-rc3 I set up 3 nodes in swarm mode and an overlay network.  \n\nI notice the task names are predictably networktest.1, .2, .3:\n\n```\n$ docker service tasks networktest\nID                         NAME           SERVICE      IMAGE   LAST STATE         DESIRED STATE  NODE\nd6naj58s8izkb822fpd4b6tl0  networktest.1  networktest  alpine  Running 4 seconds  Running        ip-172-31-47-32.us-west-2.compute.internal\n7sglm20hhbmn1ws5fiqvqfgx5  networktest.2  networktest  alpine  Running 4 seconds  Running        ip-172-31-43-158.us-west-2.compute.internal\nau4unlfyavw5y6vhodaw2dwe4  networktest.3  networktest  alpine  Running 4 seconds  Running        ip-172-31-43-158.us-west-2.compute.internal\n```\n\nbut the container names have a random suffix:\n\n```\n[ec2-user@ip-172-31-47-32 ~]$ docker ps\nCONTAINER ID        IMAGE                           COMMAND                  CREATED             STATUS              PORTS                                  NAMES\ne67110a596b0        alpine:latest                   \"sleep 36000\"            5 minutes ago       Up 5 minutes                                               networktest.1.d6naj58s8izkb822fpd4b6tl0\n```\n\nand\n\n```\n[ec2-user@ip-172-31-43-158 ~]$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n412ed90a1cd7        alpine:latest       \"sleep 36000\"       4 minutes ago       Up 3 minutes                            networktest.2.7sglm20hhbmn1ws5fiqvqfgx5\n44ea273ba889        alpine:latest       \"sleep 36000\"       4 minutes ago       Up 3 minutes                            networktest.3.au4unlfyavw5y6vhodaw2dwe4\n```\n\nHow are we then supposed to have container-to-container communication?\n\nSo giving that full randomly-generated name allows one container to ping another:\n\n```\n[ec2-user@ip-172-31-47-32 ~]$ docker exec -it e67110a596b0 ping networktest.2.7sglm20hhbmn1ws5fiqvqfgx5\nPING networktest.2.7sglm20hhbmn1ws5fiqvqfgx5 (10.0.0.4): 56 data bytes\n64 bytes from 10.0.0.4: seq=0 ttl=64 time=0.319 ms\n64 bytes from 10.0.0.4: seq=1 ttl=64 time=0.236 ms\n64 bytes from 10.0.0.4: seq=2 ttl=64 time=0.217 ms\n```\n\n_But_ I really wish this worked too:\n\n```\n[ec2-user@ip-172-31-47-32 ~]$ docker exec -it e67110a596b0 ping networktest.2\nping: bad address 'networktest.2'\n```\n\nI didn't see any parameters to `docker service create` for `links`, so how do I configure one container to communicate with another? \n\nIn [this blog](https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/) it implied the `frontend` and `redis` database would be connected with just the following commands. Was that misleading?\n\n```\ndocker network create -d overlay mynet\ndocker service create --name frontend --replicas 5 -p 80:80/tcp --network mynet mywebapp\ndocker service create --name redis --network mynet redis:latest\n```\n"},{"labels":["documentation",null],"text":"The \"--privileged\" flag does a lot \"under the hood\"; apart from enabling all capabilities, it also disables apparmor profiles, seccomp profiles, etc.\n\nWe should document exactly what the flag does, so that people can make better decisions if they need it (or not)\n"},{"labels":["documentation",null],"text":"I have been unable to find any documentation or examples for the mount configuration option on the [docker service create](https://docs.docker.com/engine/reference/commandline/service_create/) command.  How do you express the source, target, driver and driver plugin specific options?\n"},{"labels":["documentation"],"text":"It would be nice if the documentation contains best practices for operating swarm managers.\n\nFor example:\n- Are swarm managers like mesos masters which needs to be operated in odd numbers to maintain quorum?\n- What is the maximum number of managers you should have in a swarm? Are they like Zookeeper ensembles where as the number of members in the ensemble increases, the write performance decreases?\n- Is it advisable to make every node a manager, so that we can administrate the swarm from any node?\n- Can swarm be run in high availability mode? If so, is there an example?\n"},{"labels":["documentation"],"text":"The use of constraints should be documented in the reference. While https://github.com/docker/docker/blob/master/docs/reference/commandline/service_create.md says that the `--constraint` flag exists when creating a service, it is unclear how this is to be used and whether the same usage as docker/swarm applies.\n\nIt would also be nice to have a few examples showing how this is to be used.\n"},{"labels":["documentation",null],"text":"According to the engine-api Docs, resource settings are \"NanoCPUs\" and \"MemoryBytes\" - https://godoc.org/github.com/docker/engine-api/types/swarm#Resources\n\nThe corresponding docs for the remote API show simply \"CPU\" and \"Memory\" https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.24.md#create-a-service\n"},{"labels":["documentation"],"text":"#23790 added support for this as an environment variable `DOCKER_SERVICE_PREFER_OFFLINE_IMAGE` to support our integration tests.\n\nWe should figure out if we want to:\n- document this\n- make it configurable in some other way\n- use digest or image IDs to avoid pulling if local image is present.\n\nI'd prefer not to rush this for 1.12 timeframe.\n\n@vdemeester \n"},{"labels":["documentation"],"text":"Per @thaJeztah , verify correct version of api docs post-1.12 release. Ref https://github.com/docker/docker/pull/23777 .\n"},{"labels":["documentation"],"text":"Review consistency of CLI reference with the code on the new commands:\n- `docker node`\n- `docker service`\n- `docker swarm`\n"},{"labels":["documentation"],"text":"Review consistency of API reference with the code on the new endpoints:\n- Node endpoints\n- Service endpoints\n- Swarm endpoints\n"},{"labels":["documentation"],"text":"Given there are apparently duplicate properties at the options level and HostConfig level the lack of documentation for the containers/create endpoint in the [Remote API](https://docs.docker.com/engine/reference/api/docker_remote_api_v1.23/#create-a-container) is a blocker.\n\nIn particular, without deep knowledge of the system, how are adopters supposed to differentiate between...\n\n`options.Volumes` vs. `options.HostConfig.Binds`\n\n...or...\n\n`options.ExposedPorts` vs. `options.HostConfig.PortBindings`\n\n...as they appear to be doing the same thing. Probably they are not, but really as [noted before](https://github.com/docker/docker/issues/2949) in an issue not properly closed according to those in the thread, these things are NOT self-describing. If the conventions correspond to equivalent operations documented elsewhere, then this link should be explicit.\n\nHostConfig is not even labelled - what does it do? Isn't the whole JSON-ish structure a host configuration? This is particularly galling since the /run endpoint describes that it 'takes a HostConfig' for backwards compatibility, and links to the documentation for containers/create to clarify, where there is no information describing what the HostConfig object is for, (versus the whole options object, which contains a bunch of host-specific config).\n\nBACKGROUND\n\nI am trying to execute through the Remote API a self-removing (--rm) container configuration already verified interactively through a command line `docker run` invocation. I can't be the only person who works in this way, starting with docker run, and then trying to use an API to automate invocation. \n\nMy attempt to code this through dockerode (backed by the Remote API) very challenging simply because the documentation is so poor (dockerode points to the Remote API documentation as documentation for its own API). \n"},{"labels":["documentation",null,null],"text":"Kube's is somehow generating docs, I didn't look into the details but it's prettty cool, even if its slow as eff to run (maybe we can make it better, I didn't really dig deep). Then maybe we don't have to think about so many things :)\n\nhttps://github.com/kubernetes/kubernetes/blob/master/hack/update-generated-docs.sh\n"},{"labels":["documentation",null],"text":"Referring to #22166\n\nThe [last commit of the device-mapper documentation](https://github.com/docker/docker/commit/a7b2f87b0637a3711e90c0f1d0a99dd5d32bd60f#diff-52525eec69924c13d098f5d19b2d79adL254) by @chenchun clarified the overall use.\nBut as I go through the steps, I'm missing the part where the thinpool configuration on /data and /metadata is done.\n\nThe [diff shows that the previous version ](https://github.com/docker/docker/pull/22662/commits/3cf8c53515ff8144a63c6d4418738b5d4f4416a7)had this covered in lines 254 and following\n"},{"labels":[null,"documentation",null,null],"text":"Currently it seems very hard to find any documentation on what kind of security is promised by Dockerfile execution, especially if building Dockerfiles from untrusted sources.\n\nAs far as I can gather, the security is as follows:\n- context directory contents are available, but there is no access to files outside it\n- full access to network is available as if running an untrusted docker image\n- escalating access to host is prevented with the same strength as normal untrusted docker images\n- resource constraints are given in docker build command line\n\nThis means that if these limitations are acceptable, it is possible to allow building Dockerfiles from untrusted sources without creating a separate VM or similar to contain the build.\n\nI assume something like this is already done by Docker Hub when builds are submitted to it, as it probably does not use a separate VM for each build, so the priviledge separation probably has been tested quite rigorously.\n\nIs my assessment correct? Should this be explicitly documented somewhere?\n"},{"labels":["documentation",null],"text":"The current text:\n\n> when that variables current value is passed through (i.e. `$MYVAR1` from the host is set to `$MYVAR1` in the container).\n\ncould be usefully be changed to:\n\n> when that variables current value, set via `export`, is passed through (i.e. `$MYVAR1` from the host is set to `$MYVAR1` in the container, where `export MYVAR1=\"foo\"` was used).\n"},{"labels":["documentation",null,null],"text":"The list of allowed configuration options in [this document](https://docs.docker.com/engine/reference/commandline/daemon/#daemon-configuration-file) is not applicable to Windows. Several of the options do not work on Windows. I think this requires minor clarification, perhaps a sample that is applicable to Windows:\n\nWhen used as is, the following is returned:\n\n``` none\nPS C:\\> dockerd\ntime=\"2016-05-12T15:40:59-07:00\" level=fatal msg=\"unable to configure the Docker daemon with file C:\\\\ProgramData\\\\docker\\\\config\\\\daemon.json: the following directives don't match any configuration option: iptables, bip, icc, api-cors-headers, exec-root, default-gateway-v6, ipv6, cgroup-parent, ip-mask, ip, userland-proxy, default-gateway, ip-forward, selinux-enabled, userns-remap, fixed-cidr-v6\\n\"\n```\n\nHere is a working sample:\n\n``` json\n{\n    \"authorization-plugins\": [],\n    \"dns\": [],\n    \"dns-opts\": [],\n    \"dns-search\": [],\n    \"exec-opts\": [],\n    \"storage-driver\": \"\",\n    \"storage-opts\": [],\n    \"labels\": [],\n    \"log-driver\": \"\", \n    \"mtu\": 0,\n    \"pidfile\": \"\",\n    \"graph\": \"\",\n    \"cluster-store\": \"\",\n    \"cluster-advertise\": \"\",\n    \"debug\": true,\n    \"hosts\": [\"tcp://0.0.0.0:2376\", \"npipe://\"],\n    \"log-level\": \"\",\n    \"tlsverify\": true,\n    \"tlscacert\": \"C:\\\\ProgramData\\\\docker\\\\certs.d\\\\ca.pem\",\n    \"tlscert\": \"C:\\\\ProgramData\\\\docker\\\\certs.d\\\\server-cert.pem\",\n    \"tlskey\": \"C:\\\\ProgramData\\\\docker\\\\certs.d\\\\server-key.pem\",\n    \"group\": \"\",\n    \"default-ulimits\": {},\n    \"bridge\": \"\",\n    \"fixed-cidr\": \"\",\n    \"raw-logs\": false,\n    \"registry-mirrors\": [],\n    \"insecure-registries\": [],\n    \"disable-legacy-registry\": false\n}\n\n```\n\nIf this makes sense, and is in line with the docs scope / purpose, I am happy write up and propose the changes.\n\nneilp\n"},{"labels":["documentation",null,null],"text":"Hi fellow volume plugin developers!\n\nI'm currenty writing my first volume plugin and I have some questions that are not really answered in the API documentation. I will send a pull request that clarifies these points because I think it will also help others:\n\nOn .Create I create a special device, let's call it /dev/specialdev_name and put a file system on top of it.\nOn .Mount I mount it and reference count for the name. The first .Mount really mounts the file system, further .Mounts increase the refcount.\nOn .Umount I decrease the refcount and if 0, I umount the /dev/specialdev_name\nOn .Remove I remove the /dev/specialdev_name if the refcount is 0/is not mounted, otherwise I set Err.\n\nSo far so easy, the problem are .Path (and .Get and .List). To me it is not clear how to react if a volume is known to the driver (i.e., it was created with .Create), but is currently _not_ mounted. Should I report the \"Mountpoint\" where it will be available after a .Mount, or set \"Err\"? The same question applies for .Get. For .List it seams that I should include volumes known but not mounted, otherwise they don't show up in \"docker volume ls\". But yeah, it all boils down to this \"known to the plugin and created, but currently not mounted\" situation. Set Mountpoint to where it will be available after .Mount? Or set Mountpoint to \"\"? Or even set Err?\n"},{"labels":["documentation",null],"text":"I had some trouble working out the correct way to use `docker run` with the sha256 digest. I found the documentation a little confusing with regard to this. For example the [run reference page](https://docs.docker.com/engine/reference/run) has the following example:\n\n```\n$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\n```\n\nHowever I had problems trying to run my image this way. For example:\n\n``` bash\ndocker run namespace/image@digest # Wrong - parse error\ndocker run namespace/image@sha256:digest # Correct\n```\n\nI had to work this out from trial and error. The link to the [Image[@digest]](https://docs.docker.com/engine/reference/run/#image-digest) section anchor doesn't have an example, which I think might have been useful. I believe therefore it might be beneficial to update the documentation regarding using digests.\n\nI suggest the **General Form** section to be:\n\n```\n$ docker run [OPTIONS] IMAGE[:TAG|@sha256:DIGEST] [COMMAND] [ARG...]\n```\n\nIf there are other possible digests, then sha256 could be changed to DIGEST_TYPE or something similar.\n\nSecondly I suggest that the [Image[@digest]](https://docs.docker.com/engine/reference/run/#image-digest) anchor have the following example of using the digest:\n\n```\ndocker run alpine@sha256:9cacb71397b640eca97488cf08582ae4e4068513101088e9f96c9814bfda95e0 date\n```\n\nThis is my feedback from initially trying to get digests to run with `docker run`. If this was confusing to me, it might possibly be confusing to other users.\n"},{"labels":["documentation"],"text":"The the Docker Remote API v1.24 (https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.24.md), the REST function,\n\nTag an image into a repository\n\ndoes not really explain what the function does.\n"},{"labels":["documentation"],"text":"Tutorial  [Work with a development container](https://docs.docker.com/opensource/project/set-up-dev-env/) talks about the **binary** directory. Current development source makes **binary-client** and **binary-daemon**. This modifies a few steps, like the one where we copy the binary file.\n\nThe documentation can benefit from updates also.\n"}
]