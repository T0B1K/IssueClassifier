[{"text": "Right now `SparseArray.astype(numpydtype)` is sparse:\r\n\r\n```python\r\nIn [6]: a = pd.SparseArray([0, 1, 0, 1])\r\n\r\nIn [7]: a.astype(np.dtype('float'))\r\nOut[7]:\r\n[0, 1.0, 0, 1.0]\r\nFill: 0\r\nIntIndex\r\nIndices: array([1, 3], dtype=int32)\r\n```\r\n\r\nThis is potentially confusing. I did it to match the behavior of SparseSeries, but we may not want that."},
{"text": "The name of the function `islistlike` from `pandas.core.dtypes.common` suggests that whatever gets `True` returned will be \"like a list\". As such, it's used in a handful of places - e.g. I got asked to use it in #20347, and now again in #22486. What I found out in the latter one is that it's true for sets:\r\n\r\n```\r\n>>> from pandas.core.dtypes.common import islistlike\r\n>>> islistlike({1, 2, 3})\r\nTrue\r\n```\r\n\r\nThis has some uncomfortable consequences - for `str.cat` it's a bug (#23009), and for `df.setindex` it would be too.\r\n\r\n@jreback asked me to try out removing `set` from `islistlike` (https://github.com/pandas-dev/pandas/pull/22486#issuecomment-428159769), so this issue is following up on that.\r\n> @h-vetinari why don't you try (separate PR) excluding set from islistlike and see what the implications of that are.\r\n\r\nThere are some rare cases like `.isin`, where sets should also be included. I'd say to have a looser definition `issetlike` that's basically `islistlike or set`. Alternatively, one could think of `islistlike(strict=False)` to include sets.\r\n\r\nAnother question is if this needs a deprecation cycle and how?"},
{"text": "This supersedes #22721.\r\n\r\nPandas is trying to straddle many different chasms, which leads to undesirable behaviour on the fringes. For the purpose of this issue, I'm talking mainly about\r\n1. supporting python 2/3 (will be over soon...)\r\n1. being largely based on numpy's type system\r\n\r\nFrom the first point, we have the inconsistent handling of str vs. bytes, so having the Series-concatenator work with bytes is a necessity in Python 2.\r\n\r\nMostly due to the second point, there's no proper string dtype, it's just hiding in the `object` dtype. I started #22721 as a side issue which came up while refactoring in #22725. Then I got told that:\r\n> We do NOT handle bytes in `.str` if you want to add tests and raise, pls do so, but not going to 'make it work better'. It is amazingly confusing and causes all sorts of errors. We probably don't have explicit checks on this (though I *thought* that we always infer on the strings that must be string/unicode and *never* bytes).\r\n\r\nHowever, it works already -- the `Series.str`-accessor already checks that it can only be called on an object column, but there's not much more it can do (not least because inspecting every element of a Series would be very performance-intense). Consequently, `.str.cat` currently *does* work on bytes data, and easily at that:\r\n```\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> s = pd.Series(np.array(list('abc'), 'S1').astype(object))\r\n>>> t = pd.Series(np.array(list('def'), 'S1').astype(object))\r\n>>> s.str.cat(t, sep=b'')\r\n0    b'ad'\r\n1    b'be'\r\n2    b'cf'\r\ndtype: object\r\n>>> s.str.cat(t, sep=b',')\r\n0    b'a,d'\r\n1    b'b,e'\r\n2    b'c,f'\r\ndtype: object\r\n```\r\n\r\nLong story short - this issue supersedes #22721, and should serve as a long term goal to disable `.str` once Python 2 gets dropped and/or there is a string dtype."},
{"text": "In #20347, I was asked to allow list-like inputs for the `.str.cat` methods for Series. However, I didn't know that `islistlike({'a', 'b', 'c'})` is True (for sets) until I stumbled over it in #22486.\r\n\r\nThese objects should imo clearly be disallowed, lest users shoot themselves in the foot massively:\r\n```\r\n>>> s = pd.Series(['a', 'b', 'c'])\r\n>>> s.str.cat({'a', 'b', 'c'})\r\n0    ac\r\n1    bb\r\n2    ca\r\ndtype: object\r\n```\r\n\r\nI'm not even sure this should go through a deprecation cycle - IMO it's just a plain ol' bug that should be fixed ASAP."},
{"text": "I frequently come into situations where I have to break up piping, because piping doesn't work on attributes:\r\n\r\n```python\r\n>>> newdf = (df.groupby(...)\r\n...             .pipe(...)\r\n>>> newdf.index.name = 'indexname'\r\n>>> newdf.columns = pd.CategoricalIndex(newdf.columns)\r\n>>> newdf = newdf.pipe(...)  # and so on...\r\n```\r\n\r\nI think it would be cleaner if NDFrame had a ``setattr`` method that returns ``self``. Then the above would become:\r\n\r\n```python\r\n>>> newdf = (df.groupby(...)\r\n...             .pipe(...)\r\n...             .setattr('index.name', 'indexname')\r\n...             .setattr('columns', lambda x: pd.CategoricalIndex(x.columns))\r\n...             .pipe(...)  # and so on...\r\n...             )\r\n```\r\n\r\nThe first argument to ``.setattr`` is a string and can be dot-seperated, and the second parameter is called with ``self`` as its first argument if a callable.\r\n\r\nI think this would make piping cleaner in many cases.\r\n\r\nOpinions?"},
{"text": "Hello,\r\nInitially the string methods, like replace, lower, zfill, strip etc etc.. are restricted to Series use only.\r\nIt would be good if a parameter is put to use it on data frames too. Methods like strip won't affect numeric columns since they wount be having spaces already. But if there is a method which can affect a numeric column, it can be excluded using exclude parameter (which should be added). \r\nA simple way of doing it is demonstrated below. \r\n\r\n```\r\nimport pandas as pd\r\ndata=pd.readcsv(\"nba.csv\")\r\ndata\r\ndtypes=data.dtypes.astype(str)\r\nfor columns in data.columns:\r\n    data[columns]=data[columns].astype(str)\r\n    data[columns]=data[columns].str.replace(\" \",\"\")\r\n    data[columns]=data[columns].astype(dtypes[columns])\r\ndata\r\n```\r\nIn this example, the method is working fine with Series of all dtypes. And after successfully applying method, the columns are converted back to their original dtype.\r\n\r\nIf this issue is approved, I would like to work and contribute to this feature."},
{"text": "From the dev-chat https://github.com/pandas-dev/pandas/issues/22274#issuecomment-425211774 and a rehash of https://github.com/pandas-dev/pandas/issues/20633\r\n\r\nConclusion: \r\n\r\n- Make the offset alias `'D'` and `offsets.Day` always operate as a calendar day.\r\n\r\nTechnical implication:\r\n* `Day` will need to subclass `DateOffset` instead of `Tick`\r\n\r\nOperations that will change behavior (may be missing some):\r\n1. `Day` arithmetic with Timestamp/DTI/datetime Series/DataFrame\r\n2. `DatetimeIndex.shift`\r\n3. Any use of `'D'` with Timedelta/TDI/timedelta Series/DataFrame\r\n4. Tick arithmetic with `Day`\r\n\r\nDeprecation Procedures (could use some input here)\r\n1. None\r\n2. None\r\n3. `DeprecationWarning` that users should use `'24 H'` instead of `'D'` and allow users to use `'D'` for v0.24.0\r\n4.  None\r\n\r\ncc @pandas-dev/pandas-core "},
{"text": "#### Problem description\r\n\r\nInitializing a dataset using a `set` type of object results in pandas building a cartesian product. Which isn't great b/c:\r\n\r\n- It's totally unexpected\r\n- it's extremely inefficient and at first will just block the console until you figure out why\r\n- it's not mentioned in the docs, it's not part of the pandas cheatsheet, most of the SO answers, etc. E.g. it's **astounishing**.\r\n\r\n#### Example\r\n```python\r\nimport pandas as pd\r\n\r\ncol1 = set([1, 2, 3])\r\ncol2 = [2, 3, 4]\r\npd.DataFrame({\r\n    'col1': set(col1),\r\n    'col2': col2\r\n})\r\n#         col1  col2\r\n# 0  {1, 2, 3}     2\r\n# 1  {1, 2, 3}     3\r\n# 2  {1, 2, 3}     4\r\n#\r\n# ^ wait, what?...\r\n```\r\n\r\n#### Expected Behaviour:\r\n```python\r\npd.DataFrame({\r\n    'col1': list(col1),\r\n    'col2': col2\r\n})\r\n#    col1  col2\r\n# 0     1     2\r\n# 1     2     3\r\n# 2     3     4\r\n```\r\n\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.showversions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.15.0-24-generic\r\nmachine: x8664\r\nprocessor: x8664\r\nbyteorder: little\r\nLCALL: None\r\nLANG: enUS.UTF-8\r\nLOCALE: enUS.UTF-8\r\npandas: 0.23.1\r\npytest: 3.7.1\r\npip: 18.0\r\nsetuptools: 39.1.0\r\nCython: 0.28.3\r\nnumpy: 1.14.5\r\nscipy: 1.1.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.4.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.4.4\r\nnumexpr: 2.6.5\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: 1.2.8\r\npymysql: None\r\npsycopg2: 2.7.5 (dt dec pq3 ext lo64)\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n</details>\r\n"},
{"text": "The state of the various flavours of `.unique` as of `v0.23`:\r\n- `[pd/Series/Index].unique` does not have `keep`-kwarg\r\n- `Series.unique` returns array, `Series.dropduplicates` returns `Series`. Returning a plain `np.ndarray` is quite unusual for a `Series` method, and furthermore the differences between these closely-related methods are confusing from a user perspective, IMO\r\n- same point for `Index`\r\n- `DataFrame.unique` does not exist, but is a much more natural candidate (from the behaviour of numpy, resp. `Series/Index`) than `.dropduplicates`\r\n- `pd.unique` chokes on 2-dimensional data\r\n- no `returninverse`-kwarg for any of the `.unique` variants; see #4087 (milestoned since 0.14), #21357\r\n\r\nI originally wanted to add `df.unique(..., returninverse=True|False)` for #21357, but got directed to add it to `duplicated` instead. After slow progress over 3 months in #21645 (PR essentially finished since 2), @jorisvandenbossche brought up the - justified (IMO) - feedback that: \r\n> I think my main worry is that we are adding a `returninverse` keyword which actually does not return the inverse for that function (it does return the inverse for another function), and that it is in name similar to numpy's keyword, but in usage also different.\r\n\r\nand\r\n> [...] it might make sense to add this to `pd.unique` / `Series.unique` as well? (not necessarily at the same time; or might actually be an easier starter)\r\n\r\nThis prompted me to have another look at the situation with `.unique`, and I found the list of the above inconsistencies. To resolve them, I suggest to:\r\n- [ ] Change return type for `[Series/Index].unique` to be same as caller (deprecation cycle by introducing `raw=None` which at first defaults to True?)\r\n- [ ] Add `keep`-kwarg to `[Series/Index].unique` (make `.unique` a wrapper around `.dropduplicates`?)\r\n- [ ] Add `df.unique` (as thin wrapper around `.dropduplicates`?)\r\n- [ ] Add `keep`-kwarg to `pd.unique` and dispatch to `DataFrame/Series/Index` as necessary\r\n- [ ] Add `returninverse`-kwarg to all of them (and add to EA interface); under the hood by exposing the same kwarg to `duplicated` and `dropduplicates` as well\r\n- [ ] (something for later) solve #21720 (treatment of `np.nan/None` in `df.duplicated` inconsistent vs. Series behaviour)\r\n\r\nEach point is essentially self-contained and independent of the others, but of course they make more sense together.\r\n"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```\r\ncurrent     -> queryconfig[\"aggregation\"] = \"5min\"\r\nproposed -> queryconfig[\"aggregation\"] = \"PT5M\"\r\n\r\nresampleddataframe = dataframe.resample(queryconfig[\"aggregation\"], how = queryconfig[\"aggregationtype\"], closed = queryconfig[\"closed\"])\r\n```\r\n\r\n#### Problem description\r\n\r\nhttps://en.wikipedia.org/wiki/ISO8601#Durations\r\n\r\nCurrent behavior uses non standardized markup for time duration, wich can cause errors and confusion. Implementing ISO8601 duration grantees standardized solution for time duration in PANDAS. It does not have to replace the current system rather extend it.\r\n\r\n\r\n"},
{"text": "Sometimes we want to apply backgroundgradient to some columns according to values in another column. There's no easy way to do this currently. It can be made possible by adding an optional values parameter. And when the values parameter is specified, it is used to generate background colors. \r\n"},
{"text": "From discussion started in #22639:\r\n\r\nCurrently, `pd.readcsv` has two booleans\r\n\r\n* **errorbadlines** : boolean, default True\r\n  Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these \"bad lines\" will dropped from the DataFrame that is returned.\r\n* **warnbadlines** : boolean, default True\r\n  If errorbadlines is False, and warnbadlines is True, a warning for each \"bad line\" will be output.\r\n\r\nThis is confusing (what happens if both are `True`), and not in line with other `errors`-kwargs that are all around the place. Clearer would be something like: `errorbadlines = {'raise'|'warn'|'ignore'}`, and removing `warnbadlines`."},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pytest # Not run via pytest -- just used for exception testing\r\n\r\ndef createdf(total, index=None):\r\n  dma = [501, 501, 501, 501, 501, 501, 502, 502, 502, 502, 502, 502]\r\n  size = [1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2]\r\n  age = ['20-25', '30-35', '40-45', '20-25', '30-35', '40-45',\r\n         '20-25', '30-35', '40-45', '20-25', '30-35', '40-45']\r\n  df = pd.DataFrame()\r\n  df['dma'] = dma\r\n  df['size'] = size\r\n  df['age'] = age\r\n  df['total'] = total\r\n\r\n  df10 = df.copy()\r\n  df10.total = 10 * df.total\r\n\r\n  df.setindex(index, inplace=True)\r\n  return df\r\n\r\ndef runtest(index, value, usedfindex, expectedexception=None, expecteddf=None):\r\n  total = np.array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],)\r\n  df = createdf(total, index)\r\n  df10 = createdf(10 * total, index)\r\n\r\n  def run():\r\n    if usedfindex:\r\n      df.loc[df.index==value, 'total'] = df10.loc[df10.index==value, 'total']\r\n    else:\r\n      df.loc[value, 'total'] = df10.loc[value, 'total']\r\n\r\n  if expectedexception:\r\n    with pytest.raises(expectedexception):\r\n      run()\r\n  else:\r\n    run()\r\n    pd.testing.assertframeequal(df, expecteddf)\r\n\r\n\r\ntotal = np.array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],)\r\n\r\nexpecteddf = createdf(\r\n  np.array([10., 10., 10., 1., 1., 1., 10., 10., 10., 1., 1., 1.],),\r\n  'size')\r\nruntest('size', 1, False, expectedexception=ValueError)  # A1\r\nruntest('size', 1, True, expecteddf=expecteddf)         # A2 *\r\nruntest('size', (1,), False, expecteddf=expecteddf)     # B1 *\r\nruntest('size', (1,), True, expecteddf=expecteddf)      # B2 *\r\n\r\nexpecteddf = createdf(\r\n  np.array([10., 1., 1., 10., 1., 1., 10., 1., 1., 10., 1., 1.],),\r\n  'age')\r\nWRONGDF = createdf(total, 'age')\r\nruntest('age', '20-25', False, expectedexception=ValueError)     # A1\r\nruntest('age', '20-25', True, expecteddf=expecteddf)            # A2 *\r\nruntest('age', ('20-25',), False, expectedexception=ValueError)  # B1\r\nruntest('age', ('20-25',), True, expecteddf=WRONGDF)            # B2\r\n\r\nexpecteddf = createdf(\r\n  np.array([10., 1., 1., 1., 1., 1., 10., 1., 1., 1., 1., 1.],),\r\n  ['size', 'age'])\r\nruntest(['size', 'age'], (1, '20-25'), False, expecteddf=expecteddf)    # B1 *\r\nruntest(['size', 'age'], (1, '20-25'), True, expectedexception=KeyError) # B2\r\n```\r\n#### Problem description\r\n\r\nWhen assigning via the loc parameter, I'm running into issues with using a string index.  The example shows various attempts at using loc to assign with different indices: a single int column, a single string column, and a two-column index.\r\n\r\nThe variations of attempts are commented as:\r\n- for single column indexes, use a flat value (A) or a tuple (B).  Multi-column indexes only use tuple (B).\r\n- use `df.loc[df.index==value, column]` (1) vs `df.loc[value, column]` (2)\r\n\r\n#### Expected Output\r\n\r\nI'd like to use a single variation for all index types (but it seems no single method works).  Ideally, it would be 'B2', but that does not work for a string-based index. \r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.0.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.7.0\r\nmachine: x8664\r\nprocessor: i386\r\nbyteorder: little\r\nLCALL: None\r\nLANG: enUS.UTF-8\r\nLOCALE: enUS.UTF-8\r\n\r\npandas: 0.23.4\r\npytest: 3.7.2\r\npip: 18.0\r\nsetuptools: 40.2.0\r\nCython: None\r\nnumpy: 1.15.1\r\nscipy: None\r\npyarrow: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.5\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: 1.2.11\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n\r\n</details>\r\n"},
{"text": "Using this for sparse in https://github.com/pandas-dev/pandas/pull/22325, it'd be good to have generally."},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n# two frames w/similar indexes\r\ndf1 = pd.DataFrame(np.eye(4))\r\ndf2 = pd.DataFrame(np.eye(4)*-1)\r\n\r\n# reverse, sort -> indexes are kept reversed\r\ndf1.iloc[::-1].align(df2.iloc[::-1], axis=0, join=\"outer\")\r\n\r\n# one frame has a different index\r\ndf2 = pd.DataFrame(np.eye(4) * -1, index=range(1, 5))\r\n\r\n# reverse, sort -> indexes are sorted\r\ndf1.iloc[::-1].align(df2.iloc[::-1], axis=0, join=\"outer\")\r\n\r\n```\r\n#### Problem description\r\n\r\nInconsistent behavior, makes it impossible to use structures of the form:\r\n```\r\ndef func(x, arg):\r\n    if arg < 0:\r\n        return func(arg.iloc[::-1])\r\n    return ...\r\n```\r\nas the order gets switched without user's consent.\r\n\r\n#### Expected Output\r\nindexes are kept in the original order \r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.showversions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLCALL: None\r\nLANG: None\r\nLOCALE: None.None\r\npandas: 0.23.1\r\npytest: 2.9.2\r\npip: 9.0.1\r\nsetuptools: 23.0.0\r\nCython: 0.24\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\npyarrow: None\r\nxarray: 0.9.3\r\nIPython: 4.2.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.2.2\r\nnumexpr: 2.6.1\r\nfeather: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.2\r\nlxml: 3.6.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nsqlalchemy: 1.1.13\r\npymysql: 0.7.9.None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n</details>\r\n"},
{"text": "The actual implementation of ``pandas.Index.shift`` for a datetime-like index takes a differently named parameter for the number of shifts than the base method.\r\n\r\nIt should be named ``periods``, which would also be consistent with the ``pandas.DataFrame.shift`` or ``pandas.Series.shift`` methods. But for a DatetimeIndex it's ``n``.\r\n\r\nSee:\r\nhttps://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/indexes/base.py#L2586\r\nvs.\r\nhttps://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/indexes/datetimelike.py#L1021\r\n\r\nWith a DatetimeIndex, following the documentation leads to a\r\n```\r\nTypeError: shift() got an unexpected keyword argument 'periods'\r\n```\r\n\r\nIf this is somehow intended, then it should be stated so in the docstring for ``pandas.Index.shift``.\r\n\r\npandas version is 0.23.4\r\n"},
{"text": "Right now, `Series.shift(0)` will just return the series. Shifting for all other periods induces a copy:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: a = pd.Series([1, 2])\r\n\r\nIn [3]: a.shift(1) is a\r\nOut[3]: False\r\n\r\nIn [4]: a.shift(0) is a\r\nOut[4]: True\r\n```\r\n\r\nShould we defensively copy on `0` as well, for a consistent user experience?\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/e669fae0762d901e61f7af84fc3b5181848d257d/pandas/core/generic.py#L8084-L8086"},
{"text": "Currently, there's\r\n\r\n```DataFrame.update(other, join='left', overwrite=True, filterfunc=None, raiseconflict=False)```\r\nand \r\n```Series.update(other)```\r\n\r\nI think Series should have the same keywords/capabilities, and ideally share the implementation, probably in `generic.py`. Relevant also in light of #22286 and #21855."},
{"text": "I would like to request a `pandas.Series.query()` method that works identically to [`pandas.DataFrame.query()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html).\r\n\r\nI have a large multi-indexed series that I would like to split into training and testing data for an ML research project. A minimal working example would be:\r\n```python\r\nimport pandas as pd\r\n\r\nyears = range(2002, 2018)\r\nfields = range(1, 5)\r\n\r\nindex = pd.MultiIndex.fromproduct(\r\n    [years, fields], names=['year', 'field'])\r\n\r\nseries = pd.Series(index=index)\r\n```\r\nWhat I would like to be able to do is split this series into 2010 data and not 2010 data. Accessing 2010 data is very easy:\r\n```python\r\ntestdata = series[2010]\r\n```\r\nAccessing not 2010 data is very hard. This is the shortest method I've found so far:\r\n```python\r\ntraindata = series.toframe().query('year != 2010')[0]\r\n```\r\nSince `pandas.Series` doesn't support `query()`, I have to convert it to a DataFrame and then back into a Series. Is there any reason why `pandas.Series` doesn't support `query` directly?"},
{"text": "#### Code Sample\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnp.random.seed(2357)\r\n\r\ndf = pd.DataFrame(np.random.normal(size=(100, 3)))\r\ns = pd.Series(np.random.randn(50))\r\n\r\ndf.corrwith(s)\r\n```\r\n#### Problem description\r\n\r\nIt might be desirable to have `corrwith` return a warning in situations like the above where the alignment of the two data sets is suspect (the two objects only happen to have an overlapping index).  Perhaps the warning should be triggered unless the two objects have the right shape *and* the indexes are equal?\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.showversions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.5.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.6.0\r\nmachine: x8664\r\nprocessor: i386\r\nbyteorder: little\r\nLCALL: None\r\nLANG: enUS.UTF-8\r\nLOCALE: enUS.UTF-8\r\n\r\npandas: 0.23.0\r\npytest: 3.5.1\r\npip: 18.0\r\nsetuptools: 39.1.0\r\nCython: 0.28.2\r\nnumpy: 1.14.4\r\nscipy: 1.1.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.4.0\r\nsphinx: 1.7.4\r\npatsy: 0.5.0\r\ndateutil: 2.7.3\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.3\r\nnumexpr: 2.6.5\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: 2.5.3\r\nxlrd: 1.1.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 1.0.4\r\nlxml: 4.2.1\r\nbs4: 4.6.0\r\nhtml5lib: 1.0.1\r\nsqlalchemy: 1.2.7\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n\r\n</details>\r\n"},
{"text": "#### Problem description\r\n\r\nThe docs for ``DataFrame.groupby`` signature start with:\r\n```\r\nby : mapping, function, label, or list of labels\r\n    Used to determine the groups for the groupby.\r\n```\r\n... but the code assumes that lists of mappings or functions can also be passed, and this is also tested, although with limited enthusiasm:\r\nhttps://github.com/pandas-dev/pandas/blob/0370740034978d3a63d4b8e5e2c96ff54e7e08ba/pandas/tests/groupby/testgrouping.py#L667\r\n... and consistency (apparently that code path is used somewhere else):\r\nhttps://github.com/pandas-dev/pandas/blob/0370740034978d3a63d4b8e5e2c96ff54e7e08ba/pandas/tests/groupby/testgrouping.py#L732\r\n\r\n#### Expected Output\r\n\r\nEither we disable/deprecate the possibility of passing lists of mappings, ore we document it.\r\n\r\nI guess the latter is the desired outcome, since the code does not support the feature \"by chance\". Still I wanted to double check with @pandas-dev/pandas-core because\r\n- it is not a killer feature, as it is really easy to pass a single lambda that does the same job of a list of mappings (and more, like applying different mappings to specific levels of the index)\r\n- removing it would allow us to simplify the code quite a bit (e.g. #22257 wouldn't have happened)\r\n- it is probably not much used\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-6-amd64\r\nmachine: x8664\r\nprocessor: \r\nbyteorder: little\r\nLCALL: None\r\nLANG: itIT.UTF-8\r\nLOCALE: itIT.UTF-8\r\n\r\npandas: 0.24.0.dev0+437.g33d70efb5\r\npytest: 3.5.0\r\npip: 9.0.1\r\nsetuptools: 39.2.0\r\nCython: 0.28.4\r\nnumpy: 1.14.3\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.5.0\r\ndateutil: 2.7.3\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.2.2.post1634.dev0+ge8120cf6d\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: 0.2.1\r\ngcsfs: None\r\n\r\n\r\n</details>\r\n"},
{"text": "This is a proposal to replace #20633.\r\n\r\n`offsets.Day` (`'D'`) is documented to represent calendar day; however, `Day` arithmetic and usage of the `'D'` offset alias (e.g. `df.resample('D')`, `timedeltarange(..., freq='D')`, etc.) currently respects absolute time (i.e. `Day` acts like `Timedelta(days=1)`). This usage is ingrained pretty deeply in a lot of methods and operations and is fairly difficult to walk back to respect the notion of calendar day as stated in the docs.\r\n\r\nInstead, I propose to keep `Day` as is (a timedelta-like frequency like `Hour` (`'H'`)) and add a new frequency `CalendarDay`, `'CD'`. It would act very similarly to `DateOffset(day=1)` but act like a frequency. Thoughts?\r\n"},
{"text": "To pivot, there's currently:\r\n\r\n`pd.pivottable / df.pivottable`\r\nhttps://github.com/pandas-dev/pandas/blob/d30c4a0696d5fbdc3c7ce36a9b9b19224a557e09/pandas/core/reshape/pivot.py#L28\r\n\r\n`df.pivot`\r\nhttps://github.com/pandas-dev/pandas/blob/d30c4a0696d5fbdc3c7ce36a9b9b19224a557e09/pandas/core/reshape/reshape.py#L386\r\n\r\n`pivotsimple` (Importable at the top level as `pd.pivot`)\r\nhttps://github.com/pandas-dev/pandas/blob/d30c4a0696d5fbdc3c7ce36a9b9b19224a557e09/pandas/core/reshape/reshape.py#L411\r\n\r\nI'm not sure why `pd.pivot` currently doesn't mirror `df.pivot` like how `pd.pivottable` mirrors `df.pivottable`, but it would clearer if these were in sync. "},
{"text": "#### Proposal  description\r\n\r\nDataclasses were added in Python 3.7.\r\n\r\nIt would be nice for pandas to support dataclasses. For example could be possible to construct dataframe from by calling `.fromdataclasses` or just `.DataFrame(data=dataclasslist)`. There should be also possibility to do `.todataclasses`.\r\n\r\n#### Expected Behaviour\r\n\r\n```python\r\nfrom dataclasses import dataclass\r\nimport pandas as pd\r\n\r\n@dataclass\r\nclass SimpleDataObject(object):\r\n  fielda: int\r\n  fieldb: str\r\n\r\ndataclassobject1 = SimpleDataObject(1, 'a')\r\ndataclassobject2 = SimpleDataObject(2, 'b')\r\n>>> asd\r\n\r\n# Dataclasses to DataFrame\r\ndf = pd.fromdataclasses([dataclassobject1, dataclassobject2])\r\ndf.dtypes == ['fielda', 'fieldb']\r\n>>> True\r\ndf.dtypes == ['int', 'str']\r\n>>> True\r\n\r\n# Dataclasses to DataFrame\r\ndf = pd.DataFrame(data=[dataclassobject1, dataclassobject2])\r\ndf.dtypes == ['fielda', 'fieldb']\r\n>>> True\r\ndf.dtypes == ['int', 'str']\r\n>>> True\r\n\r\n# DataFrame to Dataclasses\r\ndf = pd.DataFrame(columns=['fielda', 'fieldb'], data=[[1, 'a'], [2, 'b']])\r\ndataclasslist = df.todataclasses()\r\ndataclasslist == [dataclassobject1, dataclassobject2]\r\n>>> True\r\n```"},
{"text": "I always found the mechanics of `combinefirst` very unintuitive, and constantly need to look into the docs to see what's happening. I haven't checked the git history, but it seems that the method was a direct response from wesm to a SO question (https://stackoverflow.com/a/9794891). In particular, I think this would be much more intuitive to do with `df.update`, which is a subset of what #21855 proposes -- it introduces `join='outer'` for `DataFrame.update` (currently, only `'left'` is supported, but even the source code notes `# TODO: Support other joins`).\r\n\r\nWith that new option, `df1.combinefirst(df2)` would be the same as `df1.update(df2, join='outer', overwrite=False)`, only that `combinefirst` has much fewer options and controls (i.e. `filterfunc` and `raiseconflict)`. The only difference is that `df.update` currently returns None, see #21858.\r\n\r\nSince it's quite a well-established function, the deprecation cycle would maybe have to be longer than usual, but I think the `update` variant is much cleaner, as well as more versatile, than this single-purpose function."},
{"text": "Currently, `df.update` follows the convention of `dict.update` to return None and update inplace. This is against the prevailing trend (and philosopy?) of pandas to move away from `inplace`. See for example (one among many...) @TomAugspurger's response in #21841:\r\n> Generally, we're moving away from inplace operations. It's confusing whether inplace means no copy or not. Reindex, by definition, can't be inplace unless the index is the same.\r\nWe recommend chaining your method calls, and hope to provide better memory control in the future.\r\n\r\nThe `update`-method of perfoms an important function regardless of whether it returns the object or None, and so should IMO be enabled to work in chained operations as well.\r\n\r\nFirst step there would be adding an `inplace`-argument with default `True`, and then -- *potentially* -- transitioning with a longish deprecation cycle towards `inplace=False`.\r\n\r\nRelevant xrefs: #21855 #21859"},
{"text": "Currently we don't specify what the behaviour should be for `ExtensionArray.argsort` when there are missing values. \r\nThis is not a huge problem because the `Series.sortvalues` deals with the missing values itself (only argsorts the non-missing data), but still we should pin down and test the behaviour.\r\n\r\nI suppose we should follow numpy's example here and put them last (which is also consistent with the default behaviour of `sortvalues`):\r\n\r\n```\r\nIn [114]: a = np.array([1, 3, 2, np.nan, 4, np.nan])\r\n\r\nIn [115]: a.argsort()\r\nOut[115]: array([0, 2, 1, 4, 3, 5])\r\n\r\n```\r\n\r\n\r\n\r\n"},
{"text": "xref https://github.com/pandas-dev/pandas/pull/21584#discussionr198317876\r\n\r\nThis method would allow for changing the `closed` value of an existing `IntervalIndex`.\r\n\r\nExample usage:\r\n```python\r\nIn [2]: index = pd.intervalrange(0, 3, closed='both')\r\n\r\nIn [3]: index\r\nOut[3]:\r\nIntervalIndex([[0, 1], [1, 2], [2, 3]]\r\n              closed='both',\r\n              dtype='interval[int64]')\r\n\r\nIn [4]: index.setclosed('neither')\r\nOut[4]:\r\nIntervalIndex([(0, 1), (1, 2), (2, 3)]\r\n              closed='neither',\r\n              dtype='interval[int64]')\r\n```"},
{"text": "The string detection routines are a bit odd, they work with arrays & not list-likes. We should change the impl to ``inferdtype`` which already does the correct things.\r\n\r\n```\r\n[5]: from pandas.core.dtypes.common import isstringlikedtype, isstringdtype\r\n\r\nIn [2]: isstringlikedtype(np.array([b'foo']))                  \r\nOut[2]: True\r\n\r\n# should work\r\nIn [3]: isstringlikedtype([b'foo'])          \r\nOut[3]: False\r\n\r\n# should work\r\nIn [4]: isstringlikedtype(np.array(['foo', 'bar'], dtype=object))\r\nOut[4]: False\r\n\r\nIn [6]: isstringdtype(np.array(['foo', 'bar'], dtype=object))                    \r\nOut[6]: True\r\n\r\n# should work\r\nIn [7]: isstringdtype(['foo', 'bar'])                        \r\nOut[7]: False\r\n```\r\nWe should also consolidate these into a single ``isstringdtype`` if possible (this might be tricky though).\r\n\r\n"},
{"text": "https://github.com/pandas-dev/pandas/pull/21486 is implementing support for `axis=None` for compatibility with NumPy 1.15's `all` / `any`.\r\n\r\nThe basic idea is to have `df.op(axis=None)` be the same as `np.op(df, axis=None)` for reductions like `sum`, `mean`, etc.\r\n\r\nGetting there poses a backwards-compatibility challenge. Currently for some reduction functions like `np.sum` we interpret `axis=None` as\r\n\r\n```python\r\nIn [11]: df = pd.DataFrame({\"A\": [1, 2]})\r\n\r\nIn [12]: np.sum(df)\r\nOut[12]:\r\nA    3\r\ndtype: int64\r\n```\r\n\r\nThe best I option I see is to just warn that the behavior will change in the future, without providing a way to achieve the behavior today. Then users can update things like `np.sum(df)` to `np.sum(df, axis=0)`. In a later version we'll make the actual change to interpret `axis=None` like NumPy."},
{"text": "\r\n\r\n```python\r\n    # in lexsortindexers() which is called in sortindex()\r\n    # the original codes\r\n    # if isinstance(orders, bool):\r\n    #     orders = [orders] * len(keys)\r\n    # elif orders is None:\r\n    #     orders = [True] * len(keys)\r\n    # the changed codes\r\n    if orders:\r\n        orders = [True] * len(keys)\r\n    elif orders is None:\r\n        orders = [True] * len(keys)\r\n    else:\r\n        orders = [False] * len(keys)\r\n\r\n```\r\n#### Problem description\r\n\r\nSince there is \"1==True\" and \"0==False\" in Python,  I usually use 0/1 to replace False/True which is more convenient (Maybe this is not a good habit). In most case, it works as expected because the judge sentence is usually \r\n```python\r\n       if bool:\r\n              \u2026\u2026\r\n       else:\r\n             \u2026\u2026\r\n```\r\n\r\nAs usual, I wrote \"dataframe.sortindex(level=0, axis=1, ascending=0)\" in my codes. However, there came an error this time. I checked the code and then found that the value of 'ascending' was passed to a variable named 'orders' which is a parameter of function \"lexsortindexer()\" in sorting.py. \r\nThe codes in lexsortindexer() : \r\n```python\r\ndef lexsortindexer(keys, orders=None, naposition='last'):\r\n    from pandas.core.categorical import Categorical\r\n    labels = []\r\n    shape = []\r\n    if isinstance(orders, bool):\r\n        orders = [orders] * len(keys)\r\n    elif orders is None:\r\n        orders = [True] * len(keys)\r\n    \u2026\u2026\r\n```\r\nSince 0 is neither bool type or None, th orders was unchanged and then there is an error. In order to make the code more compatible, I change the judge sentence as follows:\r\n```python\r\n    if orders:\r\n        orders = [True] * len(keys)\r\n    elif orders is None:\r\n        orders = [True] * len(keys)\r\n    else:\r\n        orders = [False] * len(keys)\r\n```\r\nWill this suggestion be accepted in the next version of pandas?\r\n\r\n#### Expected Output\r\ndataframe.sortindex(level=0, axis=1, ascending=0)\r\n0 error\r\n\r\n"},
{"text": "My understanding is that `Index[object]` was originally written with strings in mind, which is why ops like `sub` and `neg` don't make sense.  Since we've got more non-string cases (e.g. #21314), what if instead these were defined something like:\r\n\r\n```\r\ndef neg(self):\r\n    return Index([-x for x in self])\r\n\r\ndef sub(self, other):  # hand-wave appropriate handling for scalar vs vector other\r\n    return Index([x - other for x in self])\r\n```\r\n\r\nThe string cases would still raise, but for things like Decimal and DateOffset we could get the \"natural\" behavior.  Thoughts?"},
{"text": "During the implementation of non-numpy backed ExtensionArrays I quite often run into the case where it is simpler for me to write a complete re-implementation of the method defined on `pd.Series` instead of using the current implementation that only delegates part of the work. It would probably make sense to introduce some sort of delegation mechanism, either we continue the delegation like in https://github.com/pandas-dev/pandas/blob/4274b840e64374a39a0285c2174968588753ec35/pandas/core/base.py#L1041 or we could possibly add really general interface like NumPy's `arrayufunc`: https://docs.scipy.org/doc/numpy/reference/arrays.classes.html#numpy.class.arrayufunc\r\n\r\nMy use case where this arises currently is coming from https://github.com/pandas-dev/pandas/issues/21296 and `pd.Series.argsort` but I expect that there will be much more cases in this direction while I continue to implement the ExtensionArray interface for Arrow Arrays."},
{"text": "Now that we support merging `on` combination of column names and index levels, this should work\r\n\r\n```python\r\nIn [40]: a = pd.DataFrame({\"A\": [1, 2, 3, 4]}, index=pd.MultiIndex.fromproduct([['a', 'b'], [0, 1]], names=['outer', 'inner']))\r\n\r\nIn [41]: b = pd.Series([1, 2, 3, 4], index=pd.MultiIndex.fromproduct([['a', 'b'], [1, 2]], names=['outer', 'inner']), name='B')\r\n\r\nIn [42]: pd.merge(a, b, on=['outer', 'inner'])\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-42-337c5a9e9f8f> in <module>()\r\n----> 1 pd.merge(a, b, on=['outer', 'inner'])\r\n\r\n~/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/reshape/merge.py in merge(left, right, how, on, lefton, righton, leftindex, rightindex, sort, suffixes, copy, indicator, validate)\r\n     58                          rightindex=rightindex, sort=sort, suffixes=suffixes,\r\n     59                          copy=copy, indicator=indicator,\r\n---> 60                          validate=validate)\r\n     61     return op.getresult()\r\n     62\r\n\r\n~/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/reshape/merge.py in init(self, left, right, how, on, lefton, righton, axis, leftindex, rightindex, sort, suffixes, copy, indicator, validate)\r\n    524         if not isinstance(right, DataFrame):\r\n    525             raise ValueError('can not merge DataFrame with instance of '\r\n--> 526                              'type {right}'.format(right=type(right)))\r\n    527\r\n    528         if not isbool(leftindex):\r\n\r\nValueError: can not merge DataFrame with instance of type <class 'pandas.core.series.Series'>\r\n```\r\n\r\nShould be the same as\r\n\r\n```python\r\nIn [39]: pd.merge(a, b.toframe(), on=['outer', 'inner'])\r\nOut[39]:\r\n             A  B\r\nouter inner\r\na     1      2  1\r\nb     1      4  3\r\n```"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.Index([1, 2, 3]).droplevel([])\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-9172aa27c42a> in <module>()\r\n----> 1 pd.Index([1, 2, 3]).droplevel([])\r\n\r\nAttributeError: 'Int64Index' object has no attribute 'droplevel'\r\n```\r\n\r\n#### Problem description\r\n\r\nClearly the call above doesn't make too much sense, but there is no reason not to support it, and can help [write cleaner code](https://github.com/pandas-dev/pandas/pull/21016#issuecomment-389572566).\r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [2]: pd.Index([1, 2, 3]).droplevel([])\r\nOut[2]: Int64Index([1, 2, 3], dtype='int64')\r\n```\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: e033c0616158d3ba974456b4f84810492936b1fe\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-6-amd64\r\nmachine: x8664\r\nprocessor: \r\nbyteorder: little\r\nLCALL: None\r\nLANG: itIT.UTF-8\r\nLOCALE: itIT.UTF-8\r\n\r\npandas: 0.24.0.dev0+10.ge033c0616.dirty\r\npytest: 3.5.0\r\npip: 9.0.1\r\nsetuptools: 39.0.1\r\nCython: 0.25.2\r\nnumpy: 1.14.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.5.0\r\ndateutil: 2.7.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},
{"text": "Here are 2 ways to drop rows from a pandas data-frame based on a condition:\r\n\r\n1. `df = df[condition]`\r\n\r\n2. `df.drop(df[condition].index, axis=0, inplace=True)`\r\n\r\nThe first one does not do it *inplace*, right?\r\n\r\nThe second one does not work as expected when the index is not unique, so the user would need to `resetindex()` then `setindex()` back. \r\n\r\n\r\n**Question**\r\nWould it be possible to have column dropping based directly on the condition?\r\ne.g.\r\n`df.drop(condition, axis=0, inplace=True)`\r\n"},
{"text": "I haven't quite replaced cyberpandas' take with pandas' yet. I think one reason is that I need to be able to pass axis through to `pandas.core.algorithms.take`\r\n\r\nWe'll have to be careful. The default for `takend` is 0, while for `ndarray.take` it's None. I think we want the default to be 0.\r\n\r\nI still have to figure out one other thing, but then hopefully things will work.\r\n"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.DataFrame([1, 2], columns=range(3))\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in createblockmanagerfromblocks(blocks, axes)\r\n   4844                 blocks = [makeblock(values=blocks[0],\r\n-> 4845                                      placement=slice(0, len(axes[0])))]\r\n   4846 \r\n\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in makeblock(values, placement, klass, ndim, dtype, fastpath)\r\n   3192 \r\n-> 3193     return klass(values, ndim=ndim, placement=placement)\r\n   3194 \r\n\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in init(self, values, placement, ndim)\r\n    124                 'Wrong number of items passed {val}, placement implies '\r\n--> 125                 '{mgr}'.format(val=len(self.values), mgr=len(self.mgrlocs)))\r\n    126 \r\n\r\nValueError: Wrong number of items passed 1, placement implies 3\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-4ad51ebcfae4> in <module>()\r\n----> 1 pd.DataFrame([1, 2], columns=range(3))\r\n\r\n/home/nobackup/repo/pandas/pandas/core/frame.py in init(self, data, index, columns, dtype, copy)\r\n    403                 else:\r\n    404                     mgr = self.initndarray(data, index, columns, dtype=dtype,\r\n--> 405                                              copy=copy)\r\n    406             else:\r\n    407                 mgr = self.initdict({}, index, columns, dtype=dtype)\r\n\r\n/home/nobackup/repo/pandas/pandas/core/frame.py in initndarray(self, values, index, columns, dtype, copy)\r\n    536             values = maybeinfertodatetimelike(values)\r\n    537 \r\n--> 538         return createblockmanagerfromblocks([values], [columns, index])\r\n    539 \r\n    540     @property\r\n\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in createblockmanagerfromblocks(blocks, axes)\r\n   4852         blocks = [getattr(b, 'values', b) for b in blocks]\r\n   4853         totitems = sum(b.shape[0] for b in blocks)\r\n-> 4854         constructionerror(totitems, blocks[0].shape[1:], axes, e)\r\n   4855 \r\n   4856 \r\n\r\n/home/nobackup/repo/pandas/pandas/core/internals.py in constructionerror(totitems, blockshape, axes, e)\r\n   4829         raise ValueError(\"Empty data passed with indices specified.\")\r\n   4830     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\r\n-> 4831         passed, implied))\r\n   4832 \r\n   4833 \r\n\r\nValueError: Shape of passed values is (1, 2), indices imply (3, 2)\r\n```\r\n\r\n#### Problem description\r\n\r\n(From https://github.com/pandas-dev/pandas/pull/18626#issuecomment-378068742 )\r\n\r\n#18819 (now fixed) disabled a call such as ``pd.Series([1], index=range(3))`` - the same result can be obtained with ``pd.Series(1, index=range(3)``, which is less ambiguous.\r\n\r\nIn principle, the same reasoning should lead us to disable ``pd.DataFrame([[1, 2]], index=range(3))``. But that can't be replaced as comfortably, because ``pd.DataFrame([1, 2], index=range(3))`` aligns vertically - and this couldn't be otherwise, as 1d objects are treated as ``Series``, and ``Series`` in ``DataFrames`` are mainly columns, not rows. Moreover, this is probably quite used in existing code, and also in tests:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/6cacdde5630c593999059833b516e1fec60aaf72/pandas/tests/frame/testapply.py#L139\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/6cacdde5630c593999059833b516e1fec60aaf72/pandas/tests/indexes/testmulti.py#L3248\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/6cacdde5630c593999059833b516e1fec60aaf72/pandas/tests/reshape/testreshape.py#L499\r\n\r\nSo I think the best way to proceed is:\r\n- allow 1d objects to be broadcasted horizontally (not just aligned vertically)\r\n- clearly document the above, and the fact that 2d objects of length 1 are broadcasted vertically instead\r\n\r\n#### Expected Output\r\n\r\n``` python\r\nIn [3]: pd.DataFrame([[1]*3, [2]*3], columns=range(3))\r\nOut[3]: \r\n   0  1  2\r\n0  1  1  1\r\n1  2  2  2\r\n```\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\nIn [3]: pd.showversions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 7ec74e5f7b1f9a379b318153da88092cccb855cc\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-6-amd64\r\nmachine: x8664\r\nprocessor: \r\nbyteorder: little\r\nLCALL: None\r\nLANG: itIT.UTF-8\r\nLOCALE: itIT.UTF-8\r\n\r\npandas: 0.23.0.dev0+798.g7ec74e5f7\r\npytest: 3.5.0\r\npip: 9.0.1\r\nsetuptools: 39.0.1\r\nCython: 0.25.2\r\nnumpy: 1.14.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.5.0\r\ndateutil: 2.7.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},
{"text": "Hi,\r\n\r\nI would like to map a function expecting N arguments to a DataFrame of N columns. Today to do that I do:\r\n\r\n```python\r\ndf = pd.DataFrame({\"A\": [1, 2, 3, 4], \"B\": [5, 6, 7, 8]})\r\ndef f(x,y):\r\n     return x*y\r\n\r\ndf[['A', 'B']].apply(lambda x: f(*x), axis=1) \r\n```\r\nSo the `map` method for DataFrame would be:\r\n\r\n```python\r\ndef map(self, f):\r\n    return self.apply(lambda x: f(*x), axis=1)\r\n```\r\nif I am right. Then `df.map(f)` would produce a Serie.\r\n\r\nNote that with 1 colummn and a 1 argument function `map` would work as `numpy.map` does.\r\n\r\nThanks,\r\n\r\nOlivier.\r\n\r\nps: I edited this post after the talk I had with Tom, so it must seem strange now :smiley: "},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.Series([1,2,3]).loc[[i for i in (4,5)]]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-2-aba9eb9eea33> in <module>()\r\n----> 1 pd.Series([1,2,3]).loc[[i for i in (4,5)]]\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in getitem(self, key)\r\n   1370 \r\n   1371             maybecallable = com.applyifcallable(key, self.obj)\r\n-> 1372             return self.getitemaxis(maybecallable, axis=axis)\r\n   1373 \r\n   1374     def isscalaraccess(self, key):\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in getitemaxis(self, key, axis)\r\n   1829                     raise ValueError('Cannot index with multidimensional key')\r\n   1830 \r\n-> 1831                 return self.getitemiterable(key, axis=axis)\r\n   1832 \r\n   1833             # nested tuple slicing\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in getitemiterable(self, key, axis)\r\n   1109 \r\n   1110         if self.shouldvalidateiterable(axis):\r\n-> 1111             self.hasvalidtype(key, axis)\r\n   1112 \r\n   1113         labels = self.obj.getaxis(axis)\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in hasvalidtype(self, key, axis)\r\n   1683                         raise KeyError(\r\n   1684                             u\"None of [{key}] are in the [{axis}]\".format(\r\n-> 1685                                 key=key, axis=self.obj.getaxisname(axis)))\r\n   1686                     else:\r\n   1687 \r\n\r\nKeyError: 'None of [[4, 5]] are in the [index]'\r\n\r\nIn [3]: pd.Series([1,2,3]).loc[(i for i in (4,5))]\r\nOut[3]: \r\n4   NaN\r\n5   NaN\r\ndtype: float64\r\n\r\n\r\n```\r\n#### Problem description\r\n\r\nSince we convert iterators to lists anyway...\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/1e4e04bf47417aadaf11c7d55c206508f2899fa5/pandas/core/indexing.py#L1124\r\n\r\n... we might as well do the conversion as soon as possible (i.e., in ``getitem``), and simplify the code by only handling list-likes which have a length. I would also consider changing ``islistlike`` to return ``False`` for iterators, or provide it with a ``haslen=False`` argument.\r\n\r\nIt would also solve this other, less important, difference:\r\n```\r\nIn [2]: pd.Series([1,2,3]).loc[[i for i in (2,5)]]\r\n/usr/bin/ipython3:1: FutureWarning: \r\nPassing list-likes to .loc or [] with any missing label will raise\r\nKeyError in the future, you can use .reindex() as an alternative.\r\n\r\nSee the documentation here:\r\nhttps://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\r\n  #! /bin/sh\r\nOut[2]: \r\n2    3.0\r\n5    NaN\r\ndtype: float64\r\n\r\nIn [3]: pd.Series([1,2,3]).loc[(i for i in (2,5))]\r\nOut[3]: \r\n2    3.0\r\n5    NaN\r\ndtype: float64\r\n```\r\n\r\n... and probably others.\r\n\r\n#### Expected Output\r\n\r\nExactly the same for lists and iterators.\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: d04b7464dcc20051ef38ac2acda580de854d3e01\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-6-amd64\r\nmachine: x8664\r\nprocessor: \r\nbyteorder: little\r\nLCALL: None\r\nLANG: itIT.UTF-8\r\nLOCALE: itIT.UTF-8\r\n\r\npandas: 0.23.0.dev0+754.gd04b7464d.dirty\r\npytest: 3.5.0\r\npip: 9.0.1\r\nsetuptools: 39.0.1\r\nCython: 0.25.2\r\nnumpy: 1.14.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.5.0\r\ndateutil: 2.7.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},
{"text": "`DatetimeIndex.offset` and `DatimeIndex.freq` refer to the same thing, and are used interchangeably throughout the code for `DatetimeIndex`:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/4a344972722cc3c27250cbc8e382472b13e66bde/pandas/core/indexes/datetimes.py#L1729-L1732\r\n\r\nIs there a reason for having both?  \r\n\r\n`TimedeltaIndex` and `PeriodIndex` only have `freq`, so it seems like we could keep `freq` for consistency, deprecate `DatetimeIndex.offset`, and modify the `DatetimeIndex` code to only use `freq`."},
{"text": "xref issues\r\n\r\n- [ ] #20596\r\n- [ ] #16980 \r\n- [ ] #8774\r\n\r\nAs I understand the offset classes, they are supposed to respect transitions in \"wall time\" instead of absolute time. Currently the `Day` offset is currently defined to be 24 hours instead of 1 calendar day which is problematic to respect \"wall time\" wrt DST transitions.\r\n\r\n```\r\nIn [25]: foo\r\nOut[25]: Timestamp('2016-10-30 00:00:00+0300', tz='Europe/Helsinki') # DST change on this day\r\n\r\nIn [26]: foo + pd.tseries.offsets.DateOffset(weeks=1)\r\nOut[26]: Timestamp('2016-11-06 00:00:00+0200', tz='Europe/Helsinki')\r\n\r\nIn [27]: foo + pd.tseries.offsets.Week()\r\nOut[27]: Timestamp('2016-11-06 00:00:00+0200', tz='Europe/Helsinki') # respects calendar transition\r\n\r\nIn [28]: foo + pd.Timedelta(weeks=1)\r\nOut[28]: Timestamp('2016-11-05 23:00:00+0200', tz='Europe/Helsinki')\r\n\r\nIn [29]: foo + pd.tseries.offsets.DateOffset(days=1)\r\nOut[29]: Timestamp('2016-10-31 00:00:00+0200', tz='Europe/Helsinki')\r\n\r\nIn [30]: foo + pd.tseries.offsets.Day()\r\nOut[30]: Timestamp('2016-10-30 23:00:00+0200', tz='Europe/Helsinki') # does not respects calendar transition\r\n\r\nIn [31]: foo + pd.Timedelta(days=1)\r\nOut[31]: Timestamp('2016-10-30 23:00:00+0200', tz='Europe/Helsinki')\r\n```\r\n\r\nFrom prior issues, #20596, #16980, #8774, it seems like many are confused that 1 day = 24 hours when the DST transition comes up and defining 1 day = 1 calendar day would be more intuitive and consistent with other offsets. @"},
{"text": "As a way to make dropping an index level possible in an method chain.\r\n\r\n```python\r\n>>> df = pd.DataFrame(np.arange(12).reshape(4, 3), columns=['a', 'b', 'c']).setindex(['a', 'b'])\r\n>>> df.droplevel(0)\r\n```\r\n\r\nroughly equivalent to \r\n\r\n```python\r\ndf2 = df.copy()\r\ndf2.index = df.index.droplevel(0)\r\ndf2\r\n```\r\n\r\ncc @twiecki "},
{"text": "Dask would like to inherit the accessors registered by `register*acessor`. This would be much easier if the `regiser*accessor` methods added the name of the accessor to a (private) class variable.\r\n\r\nIt seems like we have use of it in pandas as well: https://github.com/pandas-dev/pandas/pull/19960/files#r171815351"},
{"text": "I don't think that pandas should make any assumptions on how subclasses `init` method. Currently we assume that the first positional argument accepts an instance of the class or a sequence of the class's scalar type.\r\n\r\nI'd rather break that into two distinct methods.\r\n\r\n```python\r\n@classmedthod\r\ndef fromextensionarray(cls, extensionarray: ExtensionArray) -> ExtensionArray:\r\n    \"\"\"Construct a new ExtensionArray from an instance of the same type\r\n\r\n    Parameters\r\n    ----------\r\n    extensionarray : ExtensionArray\r\n        An instance of 'cls'\r\n\r\n    Returns\r\n    -------\r\n    ExtensionArray\r\n    \"\"\"\r\n\r\n\r\nScalarType = ExtensionArray.dtype.type\r\n\r\n@classmethod\r\ndef fromscalars(cls, scalars: Sequence[ScalarType]) -> ExtensionArray:\r\n    \"\"\"Construct a new ExtensionArray from a sequence of the scalar type\r\n\r\n    Parameters\r\n    ----------\r\n    scalars : Sequence[ScalarType]\r\n        A sequence of cls.dtype.type, the scalar type for this array\r\n\r\n    Returns\r\n    -------\r\n    ExtensionArray\r\n    \"\"\"\r\n```\r\n\r\nI think these should be abstract. For many subclasses, a simple `cls(arg)` should suffice."},
{"text": "As a follow-up to the changes in DataFrame.apply (https://github.com/pandas-dev/pandas/pull/18577), we should make sure `Series.apply` behaves consistently with that as well.\r\n\r\nI think the default behaviour is OK (it already did what we now do by default for DataFrame: expand Series, keep lists or arrays as scalars, see below). \r\n\r\nBut we could *in principle* also add the same control over the output type by adding a similar `resulttype` keyword.\r\n\r\nCurrent behaviour:\r\n\r\n```\r\nIn [42]: s = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\r\n\r\nIn [44]: s.apply(lambda x: [1, 2])\r\nOut[44]: \r\na    [1, 2]\r\nb    [1, 2]\r\nc    [1, 2]\r\nd    [1, 2]\r\ndtype: object\r\n\r\nIn [45]: s.apply(lambda x: pd.Series([1, 2]))\r\nOut[45]: \r\n   0  1\r\na  1  2\r\nb  1  2\r\nc  1  2\r\nd  1  2\r\n\r\nIn [46]: s.apply(lambda x: pd.Series([1, 2], index=['A', 'B']))\r\nOut[46]: \r\n   A  B\r\na  1  2\r\nb  1  2\r\nc  1  2\r\nd  1  2\r\n\r\nIn [47]: s.apply(lambda x: np.array([1, 2]))\r\nOut[47]: \r\na    [1, 2]\r\nb    [1, 2]\r\nc    [1, 2]\r\nd    [1, 2]\r\ndtype: object\r\n```"},
{"text": "Returning a categorical feels more natural to me\r\n\r\n```python\r\nIn [11]: pd.factorize(pd.Categorical(['a', 'a', 'c']))\r\nOut[11]: (array([0, 0, 1]), array([0, 1]))\r\n```\r\n\r\nThat's kind of what we do for a `DatetimeIndex` with TZ:\r\n\r\n```python\r\nIn [10]: pd.factorize(pd.Series(pd.DatetimeIndex(['2017', '2017'], tz='US/Eastern')))\r\nOut[10]:\r\n(array([0, 0]),\r\n DatetimeIndex(['2017-01-01 00:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None))\r\n```"},
{"text": "In pandas 0.22 and earlier, we passing multiple values for `y` worked by accident. The code clearly assumed that `y` was a scalar, but things usually worked, perhaps with an incidental warning:\r\n\r\n```python\r\nIn [12]: df = pd.DataFrame(np.random.uniform(size=(10, 2)), columns=['a', 'b'])\r\n\r\nIn [15]: df.plot(x='c', y=['a', 'b'])\r\n/Users/taugspurger/miniconda3/envs/pandas-0.21.0/lib/python3.6/site-packages/pandas/plotting/core.py:1714: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\r\n  series.name = label\r\nOut[15]: <matplotlib.axes.subplots.AxesSubplot at 0x10f83b198>\r\n```\r\n\r\nOn master, this will currently raise. I think that we should explicitly support list-likes for `y`.\r\n\r\nTagging for 0.23 since I think this should be a blocker.\r\n\r\nxref https://github.com/pandas-dev/pandas/pull/18695 where we made the change."},
{"text": "This would let us provide a default `constructfromstring` method.\r\n\r\n> It seems we could have a default implementation for ExtensionDtype.constructfromstring ? (I now just copy pasted from the decimal example, and I think json example also has the same basic one)\r\n\r\nThe default would have to rely on `ExtensionDtype()` being constructable with no arguments, or an `ExtensionDtype.empty` method. For many that'll be\r\n\r\n```python\r\n@classmethod\r\ndef empty(cls):\r\n    return cls()\r\n```\r\n\r\nwhich is easier than implementing `constructfromstring`. Maybe call it `fromempty`."},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> import pandas as pd; \r\n>>> idx1d = pd.MultiIndex(levels=[[1.0]], labels=[[0]], names=['x'])\r\n>>> idx2d = pd.MultiIndex(levels=[[1.], [2.]], labels=[[0], [0]], names=['x', 'y'])\r\n\r\n# DataFrame\r\n>>> print(pd.DataFrame({'data': [1]}, idx2d).tocsv())\r\nx,y,data\r\n1.0,2.0,1\r\n\r\n>>> print(pd.DataFrame({'data': [1]}, idx1d).tocsv())\r\nx,data\r\n\"('1.0',)\",1\r\n\r\n# Series\r\n>>> print(pd.Series([1], idx2d).tocsv())\r\n1.0,2.0,1\r\n\r\n>>> print(pd.Series([1], idx1d).tocsv())\r\n\"('1.0',)\",1\r\n\r\n```\r\n#### Problem description\r\nThe output for `index.nlevels==1` should be the same as for `index.nlevels==2`, with one column less. That matches the output from other index types and was the output produced by pandas up to 0.20.3.\r\n\r\n#### Expected Output\r\n```python\r\n>>> print(pd.DataFrame({'data': [1]}, idx1d).tocsv())\r\nx,data\r\n1.0,1\r\n\r\n>>> print(pd.Series([1], idx1d).tocsv())\r\n1.0,1\r\n```\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLCALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.22.0\r\npytest: 3.2.1\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.11.3\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.4.2\r\nnumexpr: 2.6.1\r\nfeather: None\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: None\r\nsqlalchemy: 1.1.5\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n</details>\r\n"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(\r\n    {'col1': [1, 2], 'col2': [0.5, 0.75]}, index=[400, 800])\r\n\r\ndf.todict('index', header=False)\r\n# {400: [1, 0.5], 800: [2, 0.75]}\r\n```\r\n#### Problem description\r\n\r\nIt would be cool if the `DataFrame` [`.todict()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.todict.html) functionality would support a new option `header=` just as the [`.tocsv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tocsv.html) export.\r\n\r\nIn my specific case, the dataframe as above is a matrix with a time series as index and columns of bins.\r\nI would love to export it to a dict with only the `index` as key but a plain 1D array as values.\r\n\r\nCurrently, the `index` export will put a dict of dict, namely the first index being the `index` and the inner index to the `column names`:\r\n\r\n```python\r\ndf = pd.DataFrame(\r\n    {'col1': [1, 2], 'col2': [0.5, 0.75]}, index=[400, 800])\r\n\r\ndf.todict('index')\r\n# {400: {'col1': 1.0, 'col2': 0.5}, 800: {'col1': 2.0, 'col2': 0.75}}\r\n```\r\n\r\n#### Expected Output\r\n\r\nMy current work-around looks like this:\r\n```python\r\niteration=df.index.values\r\ndict(zip(\r\n    iteration,\r\n    df.loc[iteration].asmatrix()\r\n))\r\n# {400: array([1. , 0.5]), 800: array([2.  , 0.75])}\r\n```\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-5-amd64\r\nmachine: x8664\r\nprocessor: \r\nbyteorder: little\r\nLCALL: None\r\nLANG: enUS.utf8\r\nLOCALE: deDE.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.2.7\r\nCython: None\r\nnumpy: 1.14.0\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n```"},
{"text": "Since `Period` objects are inherently intervals of time, it seems like there should be a way to access `Interval` properties and methods.  Inheritance has been suggested in passing in other issues, and seems reasonable.\r\n\r\nAs an example benefit, #9089 suggests implementing `Period.duration`.  If `Period` were to inherit from `Interval`, this would already be implemented via the `Interval.length` property:\r\n\r\n```python\r\nIn [2]: p = pd.Period('2018Q1', freq='Q')\r\n\r\nIn [3]: piv = pd.Interval(p.starttime, (p + 1).starttime, closed='left')\r\n\r\nIn [4]: str(piv)\r\nOut[4]: '[2018-01-01, 2018-04-01)'\r\n\r\nIn [5]: piv.length\r\nOut[5]: Timedelta('90 days 00:00:00')\r\n```\r\n\r\nLikewise for `PeriodIndex` inheriting from `IntervalIndex`.  Would be nice to have cross compatibility between the two, e.g. `IntervalIndex.overlaps(Period)` or `PeriodIndex.getloc(Interval)`."},
{"text": "This was brought up by @shoyer in a different issue, and it makes sense to me\r\n\r\nAdding `closed` to `IntervalDtype` makes sense to me, as in my mind `closed` is inherently part of the dtype.  Since differing `closed` makes two `IntervalIndex` incompatible, I'd expect their dtypes to not be equal, but they currently are, which seems a little strange:\r\n```python\r\nIn [2]: ii1 = pd.intervalrange(0, 3, closed='left')\r\n\r\nIn [3]: ii2 = pd.intervalrange(0, 3, closed='right')\r\n\r\nIn [4]: ii1.dtype == ii2.dtype\r\nOut[4]: True\r\n```\r\n\r\nThere's also a larger discussion as to if adding `closed` to `IntervalDtype` would allow us to remove `closed` as a parameter to the `IntervalIndex` constructor.  My preference would be to keep the `closed` parameter for user convenience, similar to how `CategoricalIndex` accepts `categories` and `ordered` parameters despite also accepting `CategoricalDtype` (though maybe this is just for legacy reasons).  Willing to be convinced otherwise though.\r\n\r\nxref https://github.com/pandas-dev/pandas/issues/19263#issuecomment-359986104 (original comment)\r\nxref #19370 (might be rendered moot)\r\n\r\ncc @shoyer"},
{"text": "I've seen code like\r\n\r\n```python\r\nIn [36]: s = pd.Series(range(10))\r\n\r\nIn [37]: pd.concat([s.diff(i).rename(f\"L{i}\") for i in range(10)], axis=1)\r\nOut[37]:\r\n    L0   L1   L2   L3   L4   L5   L6   L7   L8   L9\r\n0  0.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n1  0.0  1.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n2  0.0  1.0  2.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN\r\n3  0.0  1.0  2.0  3.0  NaN  NaN  NaN  NaN  NaN  NaN\r\n4  0.0  1.0  2.0  3.0  4.0  NaN  NaN  NaN  NaN  NaN\r\n5  0.0  1.0  2.0  3.0  4.0  5.0  NaN  NaN  NaN  NaN\r\n6  0.0  1.0  2.0  3.0  4.0  5.0  6.0  NaN  NaN  NaN\r\n7  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  NaN  NaN\r\n8  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  NaN\r\n9  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0\r\n```\r\n\r\nCould we simplify that by allowing `s.diff(range(10), prefix=\"L\")`?\r\n\r\nHow would we handle DataFrames? Prefix with the column name and a separator, like `getdummies`?"},
{"text": "- ``pd.Index`` accepts ``name`` - great\r\n- ``pd.MultiIndex`` accepts ``names`` - after all, they are multiple ``name``s\r\n- ``pd.Index`` accepts ``names`` for compatibility with ``pd.MultiIndex`` - well, OK, after all, that constructor can result in a ``MultiIndex``\r\n- ``pd.Index`` should accept ( #19082 ) ``names`` even when it results in a flat index - well, OK, still for compatibility\r\n- ``pd.MultiIndex`` accepts ``name`` for compatibility  - wait, wasn't ``names`` already provided for compatibility?! OK, forget it, go for ``name``.\r\n- ``pd.Index`` accepts ``name`` even for multiple levels' names, for compatibility - with ``MultiIndex``, which accepts it for compatibility with ``pd.Index`` - aaaaalright\r\n- All ``pd.Index`` subclasses (which will never result in multiple levels, and which currently in some cases discard ``names``, in other cases raise an error) [should accept](https://github.com/pandas-dev/pandas/pull/19168/files#r161731210) ``names`` for compatibility - no, wait.\r\n\r\n####  Proposal 1\r\n\r\n1. There is one way to have compatibility across any kind of ``Index`` (sub)class constructor, and it is ``name``. When the constructor results in a ``MultiIndex`` (which can happen with ``pd.Index`` or ``pd.MultiIndex``), then ``name`` should be list-like, and each level will \"receive\" the respective component\r\n\r\n2. in those cases - and only in those cases - in which a constructor actually results in a ``MultiIndex``, ``names`` can be used as an alias for ``name``. In all other cases, it is not accepted.\r\n\r\nor alternatively:\r\n\r\n2. (b) (my preference) ``names`` is deprecated (in all cases in which it is currently supported, and remains unsupported in other constructors/cases)\r\n\r\n3. (c) (tradeoff between mental health and backward compatibility) ``names`` is supported in ``pd.MultiIndex``, still supported but deprecated in ``pd.Index`` when resulting in ``MultiIndex`` (and remains unsupported in other constructors/cases)\r\n\r\nCorollary:\r\n\r\n3. ``names`` will not be supported by any constructor that is not ``pd.Index`` or ``pd.MultiIndex``.\r\n\r\nNotice that a 1-level ``MultiIndex`` is still a ``MultiIndex``. That is,\r\n\r\n- ``pd.Index([('a',), ('b',)], name=('onlyonename',))`` will still work\r\n- ``pd.Index([('a',), ('b',)], names=('onlyonename',))`` will still work (at least as long as we don't deprecate ``names`` entirely)\r\n- ``pd.Index([('a',), ('b',)], name='onlyonename')`` will still not work\r\n\r\n####  Proposal 2\r\n\r\n1. There is one way to have compatibility across any kind of ``Index`` (sub)class constructor, and it is ``names``, which must always be list-like.\r\n\r\n2. In those cases in which the constructor results in a flat index, ``name`` is also accepted, and interpreted as ``names=(name,)``; instead ``name`` is deprecated for ``pd.MultiIndex``, and for ``pd.Index`` when resulting in a ``MultiIndex`` (even if with 1 level)\r\n\r\nCorollary:\r\n\r\n3. implementation-wise, we will want to decorate all ``new`` of non-``MultiIndex`` subclasses to convert ``names`` to ``name``"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# create ordinary example data frame\r\nmyDf = pd.DataFrame(data={'myCol': pd.Series(np.random.randn(1000)).cumsum()})\r\n\r\n# rolling + apply example 1: this works as expected because wintype=None returns a core.window.Rolling which provides apply functionality\r\nmyDf2 = myDf.rolling(window=50, center=True, wintype=None).apply(func=lambda x2: np.percentile(a=x2, q=50, interpolation='nearest'))\r\n# rolling + apply example 2: this does not work because as soon as users change wintype to be anything else than None a core.window.Window is returned instead - which does *not* provide apply functionality yet\r\nmyDf3 = myDf.rolling(window=50, center=True, wintype='hamming').apply(func=lambda x2: np.percentile(a=x2, q=50, interpolation='nearest'))\r\n```\r\n#### Problem description\r\n\r\n`core.window.Window` does not yet provide apply functionality like `core.window.Rolling` does, hence functions other than the readily provided ones (like `sum()`, `mean()`, etc) cannot be applied to it.\r\n\r\n(I'm not to deep into the details, but IMHO returning different object types for just changing the window type from rectangular to some other window form seems rather unintuitive altogether. What was the reason for it being designed this way? I couldn't quickly come up with any. It leads to e.g. example 1 from above working fine as long as the `wintype` parameter is untouched - but breaks as soon as `wintype` is changed to a different-than-rectangular window, which does not semantically seem to make sense.)\r\n\r\n#### Expected Output\r\n\r\n`core.window.Window` should provide apply functionality, probably via `.apply()` like `core.window.Rolling`, so that example 2 from above works fine too.\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.showversions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.13.0-26-generic\r\nmachine: x8664\r\nprocessor: x8664\r\nbyteorder: little\r\nLCALL: enUS.UTF-8\r\nLANG: enUS.UTF-8\r\nLOCALE: enUS.UTF-8\r\n\r\npandas: 0.22.0\r\npytest: 3.2.1\r\npip: 9.0.1\r\nsetuptools: 36.5.0.post20170921\r\nCython: 0.26.1\r\nnumpy: 1.13.3\r\nscipy: 1.0.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.0\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n"},
{"text": "when I originally added first class timedelta support I allowed ``.astype('m8[unit]')`` to return a float, except if unit==ns. IOW it was de-facto the same a dividing by ``Timedelta(1, unit=unit)``\r\n\r\n```\r\nIn [1]: s = Series(pd.totimedelta(['1 day', '1 minutes', '1 second']))\r\n\r\nIn [2]: s\r\nOut[2]: \r\n0   1 days 00:00:00\r\n1   0 days 00:01:00\r\n2   0 days 00:00:01\r\ndtype: timedelta64[ns]\r\n\r\nIn [3]: s.astype('timedelta64[s]')\r\nOut[3]: \r\n0    86400.0\r\n1       60.0\r\n2        1.0\r\ndtype: float64\r\n\r\nIn [4]: s / pd.Timedelta('1 s')\r\nOut[4]: \r\n0    86400.0\r\n1       60.0\r\n2        1.0\r\ndtype: float64\r\n\r\nIn [5]: s.astype('timedelta64[ns]')\r\nOut[5]: \r\n0   1 days 00:00:00\r\n1   0 days 00:01:00\r\n2   0 days 00:00:01\r\ndtype: timedelta64[ns]\r\n\r\n```\r\n\r\nnote that for [5], however we just return a timedelta64[ns] (which happens to be the underlying data representation).\r\n\r\n#19224 fixes construction of non-ns (and preserves the astype freq conversions. I think that this is confusing to the user and we should revert this and have a ``m8[unit]`` just work the same as it does for ``M8[unit]`` namely that you get back a timedelta dtype (and not a float), **that is the same**\r\n\r\nSo [3] would be the same as [5]\r\n\r\nWe do this for datetime types now\r\n```\r\nIn [13]: Series(pd.daterange('20170101', periods=3)).astype('M8[s]')\r\nOut[13]: \r\n0   2017-01-01\r\n1   2017-01-02\r\n2   2017-01-03\r\ndtype: datetime64[ns]\r\n```\r\n\r\nsure this is slightly convenient but a bit confusing.\r\n\r\nI would propose that we provide a deprecation warning and change it in the next version."},
{"text": "#### Problem description\r\n\r\nCurrently `IntervalIndex.astype` doesn't do anything when passed an `IntervalDtype`:\r\n```python\r\nIn [2]: ii = pd.intervalrange(0.0, 3.0)\r\n\r\nIn [3]: ii\r\nOut[3]:\r\nIntervalIndex([(0.0, 1.0], (1.0, 2.0], (2.0, 3.0]]\r\n              closed='right',\r\n              dtype='interval[float64]')\r\n\r\nIn [4]: dtype = IntervalDtype('int64')\r\n\r\nIn [5]: ii.astype(dtype)\r\nOut[5]:\r\nIntervalIndex([(0.0, 1.0], (1.0, 2.0], (2.0, 3.0]]\r\n              closed='right',\r\n              dtype='interval[float64]')\r\n```\r\n\r\nThis is because the current implementation of `IntervalIndex.astype` doesn't distinguish between different `IntervalDtype`, and treats them all as equivalent to the existing dtype:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/8acdf801c501a0ce2ac14ddd676cd248f0f32180/pandas/core/indexes/interval.py#L700-L702\r\n\r\n\r\n#### Expected Output\r\nI'd expect the subtype to be converted to `'int64'`:\r\n```python\r\nOut[5]:\r\nIntervalIndex([(0, 1], (1, 2], (2, 3]]\r\n              closed='right',\r\n              dtype='interval[int64]')\r\n```"},
{"text": "xref #16079 (adding lowercase `n/a` to default missing values) - I have a case where I want to treat `'n/a'` as non-missing, but keep the rest of the default behavior.  It seems this is about the best I can do, which involves an internal value.\r\n```python\r\nfrom io import StringIO\r\ndata = '''a,b\r\n1,2\r\n3,n/a'''\r\npd.readcsv(StringIO(data), \r\n    navalues=pd.io.common.NAVALUES.difference(['n/a']), \r\n    keepdefaultna=False)\r\n```\r\nNot suggesting reverting #16079, but wonder if if there might be a better way to expose through some kind of option?  Initial idea was something like `pd.options.navalues` (set to the current `NAVALUES`), though might be issues using a mutable value as an option.   Also could expose something like `pd.io.DEFAULTNAVALUES`\r\n\r\n"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: df = pd.DataFrame(np.arange(16).reshape(4, 4), index=pd.MultiIndex.fromproduct([[1, 2], ['a', 'b']]), columns=['a', 'b', 'c', 'd'])\r\n\r\nIn [3]: df.loc[2, 'a'] # select a row: good\r\nOut[3]: \r\na     8\r\nb     9\r\nc    10\r\nd    11\r\nName: (2, a), dtype: int64\r\n\r\nIn [4]: df.loc[2, 'c'] # select a (part of) col: guessing game, but I understand it is a feature\r\nOut[4]: \r\na    10\r\nb    14\r\nName: c, dtype: int64\r\n\r\nIn [5]: df.loc[2, 'e'] = -1 # now there is no column: add a row?\r\n\r\nIn [6]: df # ... nope, still adds a column\r\nOut[6]: \r\n      a   b   c   d    e\r\n1 a   0   1   2   3  NaN\r\n  b   4   5   6   7  NaN\r\n2 a   8   9  10  11 -1.0\r\n  b  12  13  14  15 -1.0\r\n\r\nIn [7]: df.loc[3, 'f'] = -2 # what if the row label is entirely missing?\r\n\r\nIn [8]: df # sitll adds a row and a col\r\nOut[8]: \r\n        a     b     c     d    e    f\r\n1 a   0.0   1.0   2.0   3.0  NaN  NaN\r\n  b   4.0   5.0   6.0   7.0  NaN  NaN\r\n2 a   8.0   9.0  10.0  11.0 -1.0  NaN\r\n  b  12.0  13.0  14.0  15.0 -1.0  NaN\r\n3     NaN   NaN   NaN   NaN  NaN -2.0\r\n```\r\n#### Problem description\r\n\r\nIn general, if ``df.index`` is a ``MultiIndex``, pandas interprets the syntax ``df.loc[a, b]`` as ``df.loc[(a,b),:]``.\r\n\r\n``Out[4]:`` is (debatable, but) understandable: in absence of the desired row, and in presence of a column with the same name, it interprets as ``df.loc[(a,), b]``.\r\n\r\nHowever, there is no reason why ``Out[5]:`` and ``Out[6]:`` should add a column: since priority when labels are present goes to the index, the same should happen when labels are absent.\r\n\r\nSomewhat related to #17024 .\r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [8]: df\r\nOut[8]: \r\n        a     b     c     d\r\n1 a   0.0   1.0   2.0   3.0\r\n  b   4.0   5.0   6.0   7.0\r\n2 a   8.0   9.0  10.0  11.0\r\n  b  12.0  13.0  14.0  15.0\r\n  e  -1.0  -1.0  -1.0  -1.0\r\n3 f  -2.0  -2.0  -2.0  -2.0\r\n```\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-4-amd64\r\nmachine: x8664\r\nprocessor: \r\nbyteorder: little\r\nLCALL: None\r\nLANG: itIT.UTF-8\r\nLOCALE: itIT.UTF-8\r\n\r\npandas: 0.23.0.dev0+42.g93033151a\r\npytest: 3.2.3\r\npip: 9.0.1\r\nsetuptools: 36.7.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: 0.2.1\r\n\r\n\r\n</details>\r\n\r\n  "},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn : pd.Timedelta('1 day').isoformat()\r\nOut: 'P1DT0H0M0S'\r\n\r\nIn : pd.Timedelta(pd.Timedelta('1 day').isoformat())\r\nOut: ValueError: invalid abbreviation: P\r\n```\r\n#### Problem description\r\n\r\nA ``Timedelta`` can be formatted as an ISO 8601 Duration, but cannot be constructed from one. This makes it difficult to natively parse and convert any ISO 8601 Durations into their respective ``Timedelta`` objects. \r\n\r\nFor a specific example, ``writejson`` takes a ``Timedelta`` and writes it out in the ISO format, but ``readjson`` cannot reasonably parse that format back into a ``Timedelta`` object without such a constructor (see #19039)\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 4319335aadd7355e54236bde2ab6dc130eedf9ad\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.3.0\r\nmachine: x8664\r\nprocessor: i386\r\nbyteorder: little\r\nLCALL: None\r\nLANG: enUS.UTF-8\r\nLOCALE: enUS.UTF-8\r\n\r\npandas: 0.23.0.dev0+31.g4319335aa\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.5\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n</details>\r\n"},
{"text": "Opening a new issue so this isn't lost.\r\n\r\nIn https://github.com/pandas-dev/pandas/pull/18882 banned duplicate names in a MultiIndex. I think this is a good change since allowing duplicates hit a lot of edge cases when you went to actually do something. I want to make sure we understand all the cases that actually produce duplicate names in the MI though, specifically groupby.apply.\r\n\r\n```python\r\nIn [1]: import dask.dataframe as dd\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]:     pdf = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9],\r\n   ...:                         'b': [4, 5, 6, 3, 2, 1, 0, 0, 0]},\r\n   ...:                        index=[0, 1, 3, 5, 6, 8, 9, 9, 9]).setindex(\"a\")\r\n   ...:\r\n   ...:\r\n\r\nIn [4]: pdf.groupby(pdf.index).apply(lambda x: x.b)\r\n```\r\n\r\nAnother, more realistic example: groupwise dropduplicates:\r\n\r\n\r\n```python\r\nIn [18]: df = pd.DataFrame({\"B\": [0, 0, 0, 1, 1, 1, 2, 2, 2]}, index=pd.Index([0, 1, 1, 2, 2, 2, 0, 0, 1], name='a'))\r\n\r\nIn [19]: df\r\nOut[19]:\r\n   B\r\na\r\n0  0\r\n1  0\r\n1  0\r\n2  1\r\n2  1\r\n2  1\r\n0  2\r\n0  2\r\n1  2\r\n\r\nIn [20]: df.groupby('a').apply(pd.DataFrame.dropduplicates)\r\nOut[20]:\r\n     B\r\na a\r\n0 0  0\r\n  0  2\r\n1 1  0\r\n  1  2\r\n2 2  1\r\n```\r\n\r\nIs it possible to throw a warning on this for now, in case duplicate names are more common than we thought?"},
{"text": "Since Python 3.6 dicts have for practical purposes been ordered by insertion order in CPython, and from Cpython 3.7 dicts will be formally ordered by insertion order in the language specs.\r\n\r\nPandas currently orders series/dataframe comstructed from dict alphabetically. I propose to make the sort order dependent on python version, so Python <3.6 will keep the current behavior, while Python >= 3.6 will use insertion order.\r\n\r\nI have played a bit with this and the code changes seem to be minimal (change a line in ``initdict`` in each of ``series.py``/``frame.py``). After making the changes only 36 tests are failing, which is good. Updating the tests will mainly consists of making the columns order fixed in places (i.e. add a ``columns=[...]`` for dataframes and ``index=[...]`` for series), so I predict this would not be a huge PR.\r\n\r\nI could take this on, if there's agreement.\r\n\r\nEDIT: Ok, ``sparse/frame.py`` will also have its ``initdict`` changed, but still I think this will be a small code change."},
{"text": "### Code demonstrating my issue with the 'inplace' argument of df.setindex().\r\n\r\n```python\r\n\r\nimport pandas as pd\r\n\r\n#start from a 2-dim list\r\nalist = [[1,2], [4,5]]\r\n\r\n#create a dataframe from the list\r\ndf = pd.DataFrame(alist, columns=['time', 'temp'])\r\nprint(df.head())\r\n\r\n#change the index of the dataframe to be the time column\r\ndf.setindex('time')\r\n\r\n#print the \"changed\" dataframe\r\nprint(df.head())\r\n\r\n#....but it's the same\r\n\r\n```\r\n#### Problem description\r\n\r\ndf.setindex() has an 'inplace' argument, with a default value of 'False'.  In the above example, the change made to your dataframe is thrown away.  This choice of convention is at the root of many pandas beginners' problems.  When you call .setindex(), one's mental approach is to modify your existing dataframe, not generate a new one.  Use of this function is outrageously counterintuitive.\r\n\r\n#### Expected Output\r\n\r\n'                temp\r\ntime      \r\n1                2\r\n4               5\r\n'\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 17.2.0\r\nmachine: x8664\r\nprocessor: i386\r\nbyteorder: little\r\nLCALL: None\r\nLANG: enUS.UTF-8\r\nLOCALE: enUS.UTF-8\r\n\r\npandas: 0.21.1\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 38.2.4\r\nCython: None\r\nnumpy: 1.13.3\r\nscipy: None\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.1.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.6.0\r\nhtml5lib: 1.0b10\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n</details>\r\n"},
{"text": "#### Problem description\r\nI really like the assign function and it's ability to be applied in pipelines.\r\nHowever, if you pass a dictionary via prefixed by **, the dictionary must only contain columns that already exist in the preceeding dataframe. So in a dataframe, that contains column 'A', and I want to construct column B as f(A) and column C = g(A, B), im forced to do\r\n```python\r\n# Your code here\r\npd.DataFrame({'A':[1, 2]}).assign(**{'B': lambda x:f(x['A'])}).assign(**{'C': lambda x:g(x['A'], x['B])})\r\n\r\n       A    B     C\r\n0      1    f(1)  g(1, f(1))\r\n1      2    f(2)  g(2, f(2))\r\n```\r\nfor some f and g and obtain a result like seen above. In extreme cases, this can lead to a lot of chained assign statements.\r\n\r\nFor convenience we could change the signature slightly to accept also *args, but every element in args should be such that the original assign function could be applied. In particular args could be a list of dictionaries.\r\n\r\nWith this, we could write the previous code as\r\n```python\r\n# Your code here\r\npd.DataFrame({'A':[1, 2]}).assign(*[{'B': lambda x:f(x['A']), {C': lambda x:g(x['A'], x['B])}])\r\n```\r\nOf course (also as it is right now) the user is responsible to construct a correct \"computational graph\" here. Additionally, the implementation I currently think of would use len(args) (-1 intermediate) copies of the original dataframe. However, using the stacked procedure above, this also happens.\r\n\r\nThus, we obtain a simpler syntactic way of using assign and we don't break the original implementation.\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLCALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: 3.2.1\r\npip: None\r\nsetuptools: 36.5.0.post20170921\r\nCython: 0.26.1\r\nnumpy: 1.13.3\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.0\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandasgbq: None\r\npandasdatareader: None\r\n</details>\r\n"},
{"text": "#### Problem description\r\nI really like the assign function and it's ability to be applied in pipelines.\r\nHowever, if you pass a dictionary via prefixed by **, the dictionary must only contain columns that already exist in the preceeding dataframe. So in a dataframe, that contains column 'A', and I want to construct column B as f(A) and column C = g(A, B), im forced to do\r\n```python\r\n# Your code here\r\npd.DataFrame({'A':[1, 2]}).assign(**{'B': lambda x:f(x['A'])}).assign(**{'C': lambda x:g(x['A'], x['B])})\r\n\r\n       A    B     C\r\n0      1    f(1)  g(1, f(1))\r\n1      2    f(2)  g(2, f(2))\r\n```\r\nfor some f and g and obtain a result like seen above. In extreme cases, this can lead to a lot of chained assign statements.\r\n\r\nFor convenience we could change the signature slightly to accept also *args, but every element in args should be such that the original assign function could be applied. In particular args could be a list of dictionaries.\r\n\r\nWith this, we could write the previous code as\r\n```python\r\n# Your code here\r\npd.DataFrame({'A':[1, 2]}).assign(*[{'B': lambda x:f(x['A']),   {'C': lambda x:g(x['A'], x['B])}])\r\n```\r\nOf course (also as it is right now) the user is responsible to construct a correct \"computational graph\" here. Additionally, the implementation I currently think of would use len(args) (-1 intermediate) copies of the original dataframe. However, using the stacked procedure above, this also happens.\r\n\r\nThus, we obtain a simpler syntactic way of using assign and we don't break the original implementation.\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLCALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: 3.2.1\r\npip: None\r\nsetuptools: 36.5.0.post20170921\r\nCython: 0.26.1\r\nnumpy: 1.13.3\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.0\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandasgbq: None\r\npandasdatareader: None\r\n</details>\r\n"},
{"text": "### Background\r\nFollow-up from this specfic chain of comments: https://github.com/pandas-dev/pandas/pull/18710#discussionr155950020\r\nAnd these PR's in general: #18677, #18710\r\n\r\n### Issue\r\nFor the context of this discussion, I'm only referring to data that is already categorical; I don't think there was any ambiguity with converting non-categorical to categorical.  This applies using `.astype('category')` on `Categorical`, `CategoricalIndex`, and `Series`.\r\n\r\nThe crux of the issue comes down to whether `.astype('category')` should ever change data that is already categorical.  An argument that it shouldn't is that `.astype('category')` doesn't explicitly specify any changes, so nothing should be changed, and it's the existing behavior.\r\n\r\nThe other argument is that  `.astype('category')` should be equivalent to `.astype(CategoricalDtype())`.  Note that `CategoricalDtype()` is the same as `CategoricalDtype(categories=None, ordered=False)`:\r\n```python\r\nIn [2]: CategoricalDtype()\r\nOut[2]: CategoricalDtype(categories=None, ordered=False)\r\n```\r\n\r\nThis means that if the existing categorical data is ordered, then `.astype(CategoricalDtype())` would change the categorical data from having `ordered=True` to `ordered=False`, and so  `.astype('category')` should do the same.\r\n\r\nI don't think there are any scenarios where the categories themselves would change; the only potential thing that could change is `ordered=True` to `ordered=False`.  See below for a summary of some potential options.  Feel free to modify any of the pro/cons listed below, or suggest any other potential options.\r\n\r\n### Option 1: `.astype('category')` does not change anything\r\n\r\nThis would not require any additional code changes, as it's the current behavior.\r\n\r\n**Pros:**\r\n- Maintains current behavior `.astype('category')`\r\n- Less likely to cause user confusion due to unforeseen changes\r\n    - At least in my mind, but I could be convinced otherwise\r\n    - Forces the user to be explicit when making potentially unintended changes\r\n\r\n**Cons:**\r\n- Inconsistent with `.astype(CategoricalDtype())`\r\n\r\n### Option 2: `.astype('category')` changes `ordered=True` to `ordered=False`\r\n\r\nThis would require some additional code changes, but is relatively minor.\r\n\r\n**Pros:**\r\n- Makes `.astype('category')` consistent with `.astype(CategoricalDtype())`\r\n- A bit cleaner/more maintainable in terms of code\r\n     - No special case checking for the string 'category'\r\n\r\n**Cons:**\r\n- Changes current behavior of `.astype('category')`\r\n\r\n### Option 3: Allow  `ordered=None` in `CategoricalDtype`\r\nBasically, make `CategoricalDtype()` return `CategoricalDtype(categories=None, ordered=None)`.  I should preface this by saying that I have not scoped out the amount of code that would need to be changed for this, nor the potential ramifications.  This may not be a good idea.\r\n\r\n**Pros:**\r\n - Maintains current behavior `.astype('category')`\r\n - Makes  `.astype('category')` consistent with `.astype(CategoricalDtype())`\r\n\r\n**Cons:**\r\n - Changes the default behavior of `CategoricalDtype`\r\n - Could potentially involve a lot of code change and unseen ramifications"},
{"text": "Hi all, this is a proposal to add a new block and type for representing IP Addresses.\r\nThere are still some details that need ironing out, but I wanted to gauge reactions to\r\nincluding this in pandas before spending too much more time on it.\r\n\r\nHere's a notebook demonstrating the basics: http://nbviewer.jupyter.org/gist/TomAugspurger/3ba2bc273edfec809b61b5030fd278b9\r\n\r\n## Abstract\r\n\r\nProposal to add support for storing and operating on IP Address data.\r\nAdds a new block type for ip address data and an `ip` accessor to\r\n`Series` and `Index`.\r\n\r\n## Rationale\r\n\r\nFor some communities, IP and MAC addresses are a common data format. The data\r\nformat was deemed important enough to add the `ipaddress` module to the standard\r\nlibrary (see `PEP 3144`). At Anaconda, we hear from customers who would use a\r\nfirst-class IP address array container if it existed in pandas.\r\n\r\nI turned to StackOverflow to gauge interest in this topic. A search for \"IP\" on\r\nthe [pandas stackoverflow\r\ntag](https://stackoverflow.com/search?q=%5Bpandas%5D+IP) turns up 300 results.\r\nUnder the NumPy tag there are another 80. For comparison, I ran a few other\r\nsearches to see what interest there is in other \"specialized\" data types (this\r\nis a very rough, probably incorrect, way of estimating interest):\r\n\r\n| term      | results |\r\n| --------- | ------- |\r\n| financial | 251     |\r\n| geo       | 120     |\r\n| ip        | 300     |\r\n| logs      | 590     |\r\n\r\n\r\nCategorical, which is already in pandas, turned up 1,089 items.\r\n\r\nOverall, I think there's enough interest relative to the implementation /\r\nmaintenance burden to warrant adding the support for IP Addresses. I don't\r\nanticipate this causing any issues for the arrow transition, once ARROW-1587 is\r\nin place. We can be careful which parts of the storage layer are implementation\r\ndetails.\r\n\r\n## Specification\r\n\r\nThe proposal is to add\r\n\r\n1.  A type and container for IPAddress and MACAddress (similar to\r\n    `CategoricalDtype` and `Categorical`).\r\n2.  A block for IPAddress and MACAddress (similar to `CategoricalBlock`).\r\n3.  A new accessor for Series and Indexes, `.ip`, for operating on IP\r\n    addresses and MAC addresses (similar to `.cat`).\r\n\r\nThe type and block should be generic IP address blocks, with no\r\ndistinction between IPv4 and IPv6 addresses. In our experience, it's\r\ncommon to work with data from multiple sources, some of which may be\r\nIPv4, and some of which may be IPv6. This also matches the semantics\r\nof the default `ipaddress.ipaddress` factory function, which returns\r\nan `IPv4Address` or `IPv6Address` as needed. Being able to deal with\r\nip addresses in an IPv4 vs. IPv6 agnostic fashion is useful.\r\n\r\n### Data Layout\r\n\r\nSince IPv6 addresses are 128 bits, they do not fit into a standard NumPy uint64\r\nspace. This complicates the implementation (but, gives weight to accepting the\r\nproposal, since doing this on your own can be tricky).\r\n\r\nEach record will be composed of two uint64s. The first element \r\ncontains the first 64 bits, and the second array contains the second 64\r\nbits. As a NumPy structured dtype, that's\r\n\r\n```python\r\nbase = np.dtype([('lo', '>u8'), ('hi', '>u8')])\r\n```\r\n\r\nThis is a common format for handling IPv4 and IPv6 data:\r\n\r\n> Hybrid dual-stack IPv6/IPv4 implementations recognize a special class of\r\n> addresses, the IPv4-mapped IPv6 addresses. These addresses consist of an\r\n> 80-bit prefix of zeros, the next 16 bits are one, and the remaining,\r\n> least-significant 32 bits contain the IPv4 address.\r\n\r\nFrom [here](https://en.wikipedia.org/wiki/IPv6#Software)\r\n\r\n### Missing Data\r\n\r\nUse the lowest possible IP address as a marker. According to RFC2373,\r\n\r\n> The address 0:0:0:0:0:0:0:0 is called the unspecified address. It must\r\n> never be assigned to any node. It indicates the absence of an address.\r\n\r\nSee [here](https://tools.ietf.org/html/rfc2373.html#section-2.5.2).\r\n\r\n### Methods\r\n\r\nThe new user-facing `IPAddress` (analogous to a `Categorical`) will have\r\na few methods for easily constructing arrays of IP addresses.\r\n\r\n```python\r\nIPAddress.frompyints(cls, values: Sequence[int]) -> 'IPAddress':\r\n    \"\"\"Construct an IPAddress array from a sequence of python integers.\r\n\r\n    >>> IPAddress.frompyints([10, 18446744073709551616])\r\n    <IPAddress(['0.0.0.10', '::1'])>\r\n    \"\"\"\r\n\r\nIPAddress.fromstr(cls, values: Sequence[str]) -> 'IPAddress':\r\n    \"\"\"Construct an IPAddress from a sequence of strings.\"\"\"\r\n```\r\n\r\nThe methods in the new `.ip` namespace should follow the standard\r\nlibrary's design.\r\n\r\n**Properties**\r\n\r\n-   `ismulticast`\r\n-   `isprivate`\r\n-   `isglobal`\r\n-   `isunspecificed`\r\n-   `isreserved`\r\n-   `isloopback`\r\n-   `islinklocal`\r\n\r\n### Reference Implementation\r\n\r\nAn implementation of the types and block is available at\r\n[pandas-ip](https://github.com/ContinuumIO/pandas-ip/) (at the moment\r\nit's a proof of concept).\r\n\r\n### Alternatives\r\n\r\nAdding a new block type to pandas is a major change. Downstream libraries may\r\nhave special-cased handling for pandas' extension types, so this shouldn't be\r\nadopted without careful consideration.\r\n\r\nSome alternatives to this that exist outside of pandas:\r\n\r\n1.  Store `ipaddress.IPv4Address` or `ipaddress.IPv6Address` objects in\r\n    an `object` dtype array. The `.ip` namespace could still be included\r\n    with an extension decorator. The drawback here is the poor\r\n    performance, as every operation would be done element-wise.\r\n2.  A separate library that provides a container and methods. The\r\n    downside here is that the library would need to subclass `Series`,\r\n    `DataFrame`, and `Index` so that the custom blocks and types are\r\n    interpreted correctly. Users would need to use the custom\r\n    `IPSeries`, `IPDataFrame`, etc., which increases friction when working\r\n    with other libraries that may expect / coerce to pandas objects.\r\n\r\nTo expand a bit on the (current) downside of alternative 2,  when the pandas constructors\r\nsee an \"unknown\" object, they falls back to `object` dtype and stuffs the actual Python object\r\ninto whatever container is being created:\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import pandasip as ip\r\n\r\nIn [3]: arr = ip.IPAddress.frompyints([1, 2])\r\n\r\nIn [4]: arr\r\nOut[4]: <IPAddress(['0.0.0.1', '0.0.0.2'])>\r\n\r\nIn [5]: pd.Series(arr)\r\nOut[5]:\r\n0    <IPAddress(['0.0.0.1', '0.0.0.2'])>\r\ndtype: object\r\n```\r\n\r\nI'd rather not have to make a subclass of Series, just to stick an array-like thing into a Series.\r\n\r\nIf pandas could provide an interface such that objects satisfying that interface\r\nare treated as array-like, and not a simple python object, then I'll gladly close\r\nthis issue and develop the IP-address specific functionality in another package.\r\nThat might be the best possible outcome to all this.\r\n\r\n### References\r\n\r\n-   [pandas-ip](https://github.com/ContinuumIO/pandas-ip/)\r\n-   [PEP 3144](https://www.python.org/dev/peps/pep-3144/)\r\n-   [RFC 2373](https://tools.ietf.org/html/rfc2373.html#section-2.5.2)\r\n-   [ipaddress howto](https://docs.python.org/3/howto/ipaddress.html)\r\n-   [ipaddress](https://docs.python.org/3/library/ipaddress.html)\r\n\r\n"},
{"text": "And the `prod` of those should be 1.\r\n\r\nxref: https://github.com/pandas-dev/pandas/issues/9422, https://github.com/pandas-dev/pandas/pull/17630, https://mail.python.org/pipermail/pandas-dev/2017-November/000657.html\r\n\r\nWe need to implement that and design and implement the alternative (either a new method, or a keyword-argument to sum and prod)."},
{"text": "Series.replace() should potentially raise an exception if an invalid argument is given? I am new to pandas and this is more of a question. **This is a user error on my part, but possible improvement to replace.**\r\n\r\n```python\r\ndf = pd.DataFrame(\r\n{\r\n    'one': ['1','1 ','10'],\r\n    'two': ['1 ', '20 ', '30 ']\r\n})\r\n\r\n# creates\r\n     one  two\r\n0    1    1\r\n1    1    20\r\n2    10    30\r\n\r\n# I intentionally added a space in df.one. So df.one.valuecounts() results in\r\n1     1\r\n10    1\r\n1     1\r\n\r\n# I want to strip the spaces around all values in df.one \r\n# so that valuecounts() only has 1 and 10 with values 2 and 1 respectively. \r\n# The following does not work.\r\n\r\ndf.one.replace(lambda x: x.strip(), inplace=True)\r\n\r\n```\r\n#### Problem description\r\n\r\nSeries.replace() accepts `\"str, regex, list, dict, Series, numeric, or None\"`. So I understand why the above does not work. But as @toobaz pointed out on gitter, when a lambda is passed, replace should probably raise an error?\r\n\r\n#### Expected Output\r\nRaises TypeError\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.showversions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.7.0\r\nmachine: x8664\r\nprocessor: i386\r\nbyteorder: little\r\nLCALL: None\r\nLANG: enUS.UTF-8\r\nLOCALE: enUS.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.3.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.8\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.8.0\r\nbs4: None\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.14\r\npymysql: 0.7.11.None\r\npsycopg2: None\r\njinja2: 2.8.1\r\ns3fs: None\r\npandasgbq: None\r\npandasdatareader: None\r\n</details>\r\n"},
{"text": "I've got a use case where I have a data frame with a ton of columns and I want to add a level to the column index to help in groupby operations.  My first instinct was to create a dictionary to illustrate the grouping, and use that pretty directly in indexing the dataframe. I.e.:\r\n\r\n```python\r\nhierarchy = {'g0':['c0','c1'], 'g1':['c2', 'c3', 'c4']}\r\nnewindex = MultiIndex.fromdict(hierarchy)\r\n```\r\n\r\nThis just strikes me as a pretty natural way to think about hierarchical structure of data.\r\n\r\nSo something like this, but with support for nested dictionaries, added to the MultiIndex class:\r\n\r\n```python\r\ndef fromdict(cls, data, names=None):\r\n    '''\r\n    Construct a two level MultiIndex from a dictionary-like object.\r\n\r\n    Parameters\r\n    ----------\r\n    data : dict \r\n        {group : array-like}\r\n        Here the array-like represents the group members\r\n    names : optional names for the levels.\r\n\r\n    Returns\r\n    -------\r\n    index : MultiIndex\r\n\t\r\n    Notes\r\n    -----\r\n    Respects grouping order if dict is an OrderedDict\r\n    '''\r\n    lvl1 = []\r\n    lvl2 = []\r\n    for name, cols in data.items():\r\n        lvl1 += [name] * len(cols)\r\n        lvl2 += cols\r\n\r\n    return MultiIndex.fromarrays([lvl1, lvl2], names=names)\r\n\r\n```\r\n"},
{"text": "I find the following counter-intuitive:\r\n\r\n```\r\n>>> per = pd.Period('2016')\r\n>>> per\r\nPeriod('2016', 'A-DEC')\r\n>>> (per.month, per.day, per.hour)\r\n(12, 31, 0)\r\n```\r\n\r\nSince this Period represents the span from Jan1-Dec 31, it seems that month, day, hour, etc are not meaningful, should be NaN.  "},
{"text": "I need a way to apply a custom function on a rolling dataframe where the centered value is not included.  \r\n\r\nThis code below works well, but it does include the center value:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nmad = lambda x: np.mean(np.fabs(x - np.median(x)))\r\n\r\ndf = pd.DataFrame([1,1,1,10,1,1,1,2,2,2,2,2,20,2,2,2,2])\r\n\r\ndf.rolling(3, minperiods=3, center=True).apply(mad)\r\n```\r\n\r\nAnd this code below does not include the center, but only allows me to do mean or sum, no custom function:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame([1,1,1,10,1,1,1,2,2,2,2,2,20,2,2,2,2])\r\n\r\ndf.rolling(window=[1,0,1], wintype='boxcar', center=True, minperiods=3).mean()\r\n```\r\n\r\nIs there a way I am missing that would allow me to compute my custom function (mad) which does not include the center value?\r\n"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: pd.Categorical([1,3,2,4], categories=[4,2,3,1,5]).unique()\r\nOut[2]: \r\n[1, 3, 2, 4]\r\nCategories (4, int64): [1, 3, 2, 4]\r\n\r\n```\r\n\r\nsame happens with ``CategoricalIndex`` and ``Series`` of ``dtype=category`` (as long as ``ordered=False``).\r\n\r\n#### Problem description\r\n\r\nThe general pandas approach to categoricals seems to change the ``.categories`` field as seldom as possible. So I see no reason why ``unique()``, which by definition does not affect the set of distinct values, should change the categories (reordering them based on appearance, and dropping unused ones).\r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [2]: pd.Categorical([1,3,2,4], categories=[4,2,3,1,5]).unique()\r\nOut[2]: \r\n[1, 3, 2, 4]\r\nCategories (4, int64): [4, 2, 3, 1, 5]\r\n\r\n```\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\nIn [3]: pd.showversions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 63e8527d32aaf6afe1cd4b2a7b3bfadb088c9a72\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x8664\r\nprocessor: \r\nbyteorder: little\r\nLCALL: None\r\nLANG: itIT.UTF-8\r\nLOCALE: itIT.UTF-8\r\n\r\npandas: 0.22.0.dev0+131.g63e8527d3\r\npytest: 3.2.3\r\npip: 9.0.1\r\nsetuptools: 36.7.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: 0.2.1\r\n\r\n\r\n</details>\r\n"},
{"text": "There are a bunch of GH issues relating to arithmetic ops with datetimelike objects behaving unexpectedly when boxed in a `(Series|DataFrame)`.  For the issues I have in mind (I'll collect a list later), the operation works as expected when done on a `(DatetimeIndex|TimedeltaIndex|PeriodIndex)`.\r\n\r\nThe relevant `(Series|DataFrame)` operations are defined in core.ops.  The `DatetimeIndex` (et al) ops are defined in `core.indexes.datetimelike.DatetimeIndexOpsMixin`.  With a little gumption the relevant ops can be taken out of `DatetimeIndexOpsMixin` and made into `(Index|Series|DataFrame)`-agnostic classes `TimestampVector`, `TimedeltaVector`, `PeriodVector`.\r\n\r\nWould it be feasible/desirable to have appropriately-dtyped `(Series|DataFrame)` methods dispatch to these implementations?"},
{"text": "#### Code Sample\r\n\r\n```python\r\nA = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\r\nB = pd.DataFrame({'a': [1, 2, 3], 'c': [7, 8, 9]})\r\n\r\ndef foo(A, B):\r\n    C = A.merge(B, on=['a'])\r\n    C['c']  # Where does c originate from?\r\n\r\n```\r\n#### Problem description\r\n\r\nThere is no way to explicitely specificy whether 'c' originates from A or B. This makes code harder to maintain. Better would be if I could write something like:\r\n\r\n```python\r\nC = A.merge(B, on=['a'], alwayssuffix=True)\r\nC['cy]  # Obvious that C comes from B, because suffixes are always created\r\n```\r\n"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame.fromdict({'a': [1, 2], 'f':[3, 4]})\r\nprint(df.min(axis=None))\r\nprint(np.min(df, axis=None))\r\n```\r\n#### Problem description\r\nSimilar to numpy I would expect the function to return the total min or max for ```axis=None``` (a scalar).\r\nThe function returns a Series with one value per column instead. \r\n\r\nAs the same happens with np.min, the problem could also be connected to numpy.\r\n\r\nIf this behavior is intended, I would suggest changing the default of axis to 0.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\ncommit: None\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLCALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.4.0\r\nCython: None\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.9999999\r\nsqlalchemy: 1.1.13\r\npymysql: 0.7.9.None\r\npsycopg2: 2.7.1 (dt dec pq3 ext lo64)\r\njinja2: 2.9.6\r\ns3fs: None\r\npandasgbq: None\r\npandasdatareader: None\r\n\r\n</details>\r\n"},
{"text": "```\r\nIn [13]: pd.Series.asobject?\r\nType:        property\r\nString form: <property object at 0x10b674548>\r\nDocstring:\r\nreturn object Series which contains boxed values\r\n\r\n*this is an internal non-public method*\r\n```\r\n\r\nLet's make it private then."},
{"text": "When running pandas in AWS, The following works perfectly fine:\r\n```python\r\npd.readcsv(\"s3://mybucket/data.csv\")\r\n```\r\nBut running the following, does not:\r\n```python\r\npd.readcsv(\"hdfs:///tmp/data.csv\")\r\n```\r\n\r\nIt would be a good user experience to allow for the hdfs:// schema too similar to how http, ftp, s3, and file are valid schemas right now."},
{"text": "```python\r\n\r\nfrom collections import OrderedDict\r\nimport pandas as pd\r\ndata = OrderedDict([('ele2', OrderedDict([('b', 1), ('a', 2)])),\r\n                    ('ele1', OrderedDict([('b', 2), ('a', 5)]))])             \r\npd.DataFrame(data)\r\n```\r\n![capture](https://user-images.githubusercontent.com/832380/32543581-53bb0d4c-c476-11e7-8b7d-c9c9089663da.PNG)\r\n#### Problem description\r\nThe output shows the rows get sorted. \r\n\r\n#### Expected Output\r\n\r\nWhen constructing a df with OrderedDicts I would expect the order in columns *and* rows to stay the same. \r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\nIn [6]: pd.showversions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.10.0-514.21.1.el7.x8664\r\nmachine: x8664\r\nprocessor:\r\nbyteorder: little\r\nLCALL: C\r\nLANG: enUS.UTF-8A\r\nLOCALE: None.None\r\n\r\npandas: 0.21.0\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: 1.6.2\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.3.0\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.8.0\r\nbs4: None\r\nhtml5lib: 0.999\r\nsqlalchemy: 1.1.10\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandasgbq: None\r\npandasdatareader: None\r\n\r\n</details>\r\n"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nind = pd.Series(['first', 'second', 'third'], name='count')\r\ns = pd.Series([1, 2, 3], index=ind, name='i should be a y label')\r\ns.plot()\r\nplt.show()\r\n```\r\n#### Problem description\r\n\r\nSo when you call Series.plot, xlabel is set to the name of the index, but ylabel is not set to the name of the series itself. To me it seems asymmetric and believe it should set ylabel automatically.\r\nAlso, if I plot it with `s.plot(kind='hist')`, I get ylabel \"Frequency\" but no xlabel, which I would expect to be the name of the series as well.\r\n\r\n#### Expected Output\r\n\r\nylabel set to name of the series for usual plot, and xlabel set to name of the series for hist plot\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.showversions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: AMD64 Family 16 Model 4 Stepping 3, AuthenticAMD\r\nbyteorder: little\r\nLCALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: 3.2.1\r\npip: 9.0.1\r\nsetuptools: 36.5.0.post20170921\r\nCython: 0.26.1\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.8\r\nlxml: 3.8.0\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandasgbq: None\r\npandasdatareader: None\r\n\r\n</details>\r\n"},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\npropose something like this:\r\n\r\n```python\r\npd.setoption('mode.inplaceoperationsdefault', True)\r\n```\r\n#### ~Problem~ Proposal description\r\n\r\nMy team routinely works with DataFrames of 50GB and up where forgetting `inplace=True` on mutating operations quickly becomes detrimental to shared resources.  We would like to be able to default all operations with an inplace argument to True and be forced to explicitly allow copies if needed.  The proposal is to use a global option to determine the default inplace value vs. making it false in each function/method.  It seems like this should be transparent to the overall Pandas community which expects `inplace=False` semantics (still the default) but gives flexibility to \"advanced\" users.  From what we can tell in the docs, all inplace arguments are currently False, there are no divergent cases which would prevent a single global default.  Even if there were (perhaps in the future), function authors could override the global default with their opposing local default preference (not great for consistency though).\r\n\r\n#### Expected Output\r\n\r\nIf set to True, all functions/methods which can operate inplace will unless explicitly overriden with an `inplace=False` argument.\r\n"},
{"text": "The implementation of `Series.get` currenlty simply is:\r\n\r\n```python\r\ndef get(self, key, default=None):\r\n    try:\r\n        return self[key]\r\n    except (KeyError, ValueError, IndexError):\r\n        return default\r\n```\r\n\r\nSo it is simply using `[]` / `getitem` under the hood.\r\n\r\nSomehow I think this is the expected thing, but the consequence is that it brings along all complexities of `getitem` in pandas (whether to fallback to positional or not ..). \r\n\r\nSo, if we want, we could make `get` more strict by using `loc` under the hood (and it should be possible to do that with a deprecation in a first phase for those cases that `loc` raises but `[]` returns a value).\r\n"},
{"text": "## Context\r\nDuring the checkout process, if the payment fails because the charge is rejected by the gateway, the API responds with a 500 status code and an HTML error page. This happens with Stripe gateways (tested with `StripeGateway` and `StripeElementsGateway`), I don't have accounts with other gateways to test them.\r\n\r\n## Expected Behavior\r\nThe API should answer with a 422 status code and a JSON body explaining the error like specified in the docs.\r\n\r\n## Actual Behavior\r\nThe API answers with a 500 status code and an HTML error page.\r\n\r\n## Possible Fix\r\nThe base controller for API v1 `Spree::Api::BaseController` has a line to `rescuefrom Spree::Core::GatewayError, with: :gatewayerror`. The API v2 base controller `Spree::Api::V2::BaseController` should probably have a similar logic.\r\n\r\n## Steps to Reproduce\r\n1. Create a basic rails app and install spree\r\n2. In the Admin, setup a payment method with `Spree::Gateway::StripeGateway` or `Spree::Gateway::StripeElementsGateway`\r\n3. Through the API v2, create a cart, add some items, and go through the checkout steps until the `payment` step.\r\n4. Try to update the checkout with one of Stripe's [special test tokens](https://stripe.com/docs/testing#cards-responses) that result in a charge failure, like `tokchargeDeclined` or `tokchargeDeclinedInsufficientFunds`\r\n\r\n## Your Environment\r\n* Version used:\r\nTested with 2 different setups:\r\n  - Rails 5.2.4.3 and Spree 3.7.10\r\n  - Rails 6.0.3.1 and Spree 4.1.6\r\n* Gemfile, Gemfile.lock and stack trace:\r\nhttps://gist.github.com/johannboutet/0e4774bfeedcc86bc72402081e95ed1c"},
{"text": "<!--- Provide a general summary of the issue in the Title above -->\r\n\r\n## Context\r\nWhen an admin creates new products in the backend it's optional to set an available date for the product. In some cases the product is not yet in stock and therefore the admin might set the available date to a future date. Spree's frontend does not show a product unless it has a set date and that date is current or in the past. However the storefront api seems to leak out products that don't yet have an available on date or have a future date.\r\n<!--- Provide a more detailed introduction to the issue itself -->\r\n<!--- How has this issue affected you? What were you trying to accomplish? -->\r\n\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nProducts that have available date set in the future or null should not be returned from the storefront api. At least not to an unauthenticated user or non-admins. \r\n\r\n## Actual Behavior\r\n<!--- Tell us what happens instead -->\r\nProducts that have available date set in the future or null are returned from the storefront api.\r\n\r\n## Possible Fix\r\n<!--- Not obligatory, but suggest a fix or reason for the issue -->\r\nI\u2019m guessing that product finder class is missing an available check in the scope.\r\n\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this issue include code to reproduce, if relevant -->\r\n1. Deploy the lates 4.0 stable sanbox version of Spree to Heroku.\r\n2. Log in to admin and change one product's available on date to a future date. For example Ruby on Rails Tote.\r\n3. Call the Products endpoint (http://sandkassi4.herokuapp.com/api/v2/storefront/products).\r\n4. Observe that the product Roby on Rails Tote is returned even though it has an available date set in the future.\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the issue in -->\r\n* Version used: 4.0\r\n* Gemfile and Gemfile.lock as text in a Gist:\r\n* Any relevant stack traces (\"Full trace\" preferred):\r\n\r\n<!--- Please remember to format code using triple backticks (`)\r\n\t  so that it is neatly formatted when the issue is posted. -->\r\n\r\n<!--- In 99% of cases, this information is enough to determine the cause and\r\n\t  solution to the problem that is being described.\r\n\r\n\t  Any issue that is open for 14 days without actionable information or\r\n\t  activity will be marked as \"stalled\" and then closed. Stalled issues\r\n\t  can be re-opened if the information requested is provided. -->\r\n"},
{"text": "Spree API when hitting  `/products?include=images` path gives the response in following format.\r\n\r\n````\r\n{\r\n  \"data\": [...],\r\n  \"included\": [\r\n    {\r\n      \"id\": \"4\",\r\n      \"type\": \"image\",\r\n      \"attributes\": {\r\n      \"viewabletype\": \"Spree::Variant\",\r\n      \"viewableid\": 3,\r\n      \"styles\": [\r\n        {\r\n          \"url\": \"/rails/activestorage/representations/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBDUT09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--bd6df012d992b43227344ecb03dc0ab6e13b1555/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaDdCam9MY21WemFYcGxTU0lMTkRoNE5EZytCam9HUlZRPSIsImV4cCI6bnVsbCwicHVyIjoidmFyaWF0aW9uIn19--504f95c059f17243e0d7d1312e2b387f26a40aff/bg.jpg\",\r\n          \"width\": \"48\",\r\n          \"height\": \"48\"\r\n        },\r\n        {\r\n          \"url\": \"/rails/activestorage/representations/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBDUT09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--bd6df012d992b43227344ecb03dc0ab6e13b1555/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaDdCam9MY21WemFYcGxTU0lOTVRBd2VERXdNRDRHT2daRlZBPT0iLCJleHAiOm51bGwsInB1ciI6InZhcmlhdGlvbiJ9fQ==--594f543b42e0bfafbe9d09df814e1d60837f9ef6/bg.jpg\",\r\n          \"width\": \"100\",\r\n          \"height\": \"100\"\r\n        },\r\n        {\r\n          \"url\": \"/rails/activestorage/representations/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBDUT09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--bd6df012d992b43227344ecb03dc0ab6e13b1555/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaDdCam9MY21WemFYcGxTU0lOTWpRd2VESTBNRDRHT2daRlZBPT0iLCJleHAiOm51bGwsInB1ciI6InZhcmlhdGlvbiJ9fQ==--46583de60196cb1eee5b4a704d67209eee40c09a/bg.jpg\",\r\n          \"width\": \"240\",\r\n          \"height\": \"240\"\r\n        },\r\n        {\r\n          \"url\": \"/rails/activestorage/representations/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBDUT09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--bd6df012d992b43227344ecb03dc0ab6e13b1555/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaDdCam9MY21WemFYcGxTU0lOTmpBd2VEWXdNRDRHT2daRlZBPT0iLCJleHAiOm51bGwsInB1ciI6InZhcmlhdGlvbiJ9fQ==--3b3e65d1d9c712d2d540e15aa84ca541eb12b365/bg.jpg\",\r\n          \"width\": \"600\",\r\n          \"height\": \"600\"\r\n        }\r\n      ]\r\n    }\r\n  }],\r\n  \r\n  \"meta\": {\r\n    \"count\": 14,\r\n    \"totalcount\": 14,\r\n    \"totalpages\": 1\r\n    },\r\n    \"links\": {\r\n      \"self\": \"https://galific.herokuapp.com/api/v2/storefront/products?include=variants.images\",\r\n      \"next\": \"https://galific.herokuapp.com/api/v2/storefront/products?include=variants.images&page=1\",\r\n      \"prev\": \"https://galific.herokuapp.com/api/v2/storefront/products?include=variants.images&page=1\",\r\n      \"last\": \"https://galific.herokuapp.com/api/v2/storefront/products?include=variants.images&page=1\",\r\n      \"first\": \"https://galific.herokuapp.com/api/v2/storefront/products?include=variants.images&page=1\"\r\n    }\r\n  }\r\n}\r\n````\r\nHere you can see the image URL's are coming as included object. While developing for mobile its not a good idea to put the very crucial element (product image path in this case) in separate object. This require the mobile dev to loop unnecessary just to get the image path which can be provided within the image relationships object like this: \r\n\r\n````\r\n\"images\": {\r\n  \"data\": [\r\n  {\r\n    \"id\": \"2\",\r\n    \"type\": \"image\",\r\n    \"url\" : 'image-path.jpg'\r\n  }]\r\n}\r\n\r\n````\r\n\r\n \r\n## Expected Behavior\r\nThe image path should be included in data object itself rather than separated include object.\r\n "},
{"text": "**Is your feature request related to a problem? Please describe.**\r\nI've been trying to integrate react with it by having backend as spree and frontend as react. I've tried to figure out on how to I can let users login to the store using Spree API, but I couldn't find any API related to it. \r\n\r\n**Describe the solution you'd like**\r\nIt would be great if there's a feature where we can let our user to login/signup in our store using Spree API\r\n\r\n\r\n"},{"text": "## Context\r\n<!--- Provide a more detailed introduction to the issue itself -->\r\nUsing the **Storefront API v2** I am trying to add a payment using the **Spree::Gateway::Eway** payment method but I get an payment method is unsupported error.\r\n\r\nIt works fine through the frontend with the payments showing up in my EWay sandbox.\r\n\r\nSeems the same as [issue #8051](https://github.com/spree/spree/issues/8051) which was closed.\r\n\r\nAs per [docs](https://guides.spreecommerce.org/api/v2/storefront#operation/Update%20Checkout) I add address and shipping method to `PATCH /checkout` and then `PATCH /checkout/next` returns an updated cart state of \"payment\".\r\n\r\nI then add the payment sources as per docs to the `PATCH /checkout` endpoint:\r\n\r\n```\r\n{\r\n    \"order\": {\r\n        \"paymentsattributes\": [\r\n            {\r\n                \"paymentmethodid\": 4\r\n            },\r\n        ]\r\n    },\r\n    \"paymentsource\": {\r\n      \"4\": {\r\n        \"number\": \"4444333322221111\",\r\n        \"month\": \"01\",\r\n        \"year\": \"2022\",\r\n        \"verificationvalue\": \"123\",\r\n        \"name\": \"John Doe\"\r\n      }\r\n    }\r\n}\r\n```\r\nwhere the paymentmethodid: 4 is the Eway method.\r\n\r\nThen try to update the checkout with `PATCH /checkout/next`\r\n\r\n<!--- How has this issue affected you? What were you trying to accomplish? -->\r\n\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nRespond with updated cart with:\r\n```\r\n{\r\n...\r\n\"state\": \"confirm\"\r\n...\r\n}\r\n```\r\n`\r\nStatus: 200\r\n`\r\nAs mentioned above, it works as expected through the frontend.\r\n\r\n## Actual Behavior\r\n<!--- Tell us what happens instead -->\r\nI get the following response from the `PATCH /checkout/next` call\r\n\r\n`Status:422`\r\n```\r\n{\r\n    \"error\": \"That payment method is unsupported. Please choose another one.\",\r\n    \"errors\": {\r\n        \"base\": [\r\n            \"That payment method is unsupported. Please choose another one.\"\r\n        ]\r\n    }\r\n}\r\n```\r\n\r\n\r\n## Possible Fix\r\n<!--- Not obligatory, but suggest a fix or reason for the issue -->\r\nN/A\r\n\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this issue include code to reproduce, if relevant -->\r\n1. Create a new Spree project\r\n2. Add Spree::Gateway::Eway payment method\r\n3. Follow as per above\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the issue in -->\r\n* Version used: Spree 3.7.4, Rails 5.2.3\r\n[Gemfile](https://gist.github.com/JasonKroll/6b63bb91f4814853bebdca48c8bcc1cd)\r\n[Gemfile.lock](https://gist.github.com/JasonKroll/aa6a5b574864183bcb7a30698367812e)\r\n\r\n<!--- Please remember to format code using triple backticks (`)\r\n\t  so that it is neatly formatted when the issue is posted. -->\r\n\r\n<!--- In 99% of cases, this information is enough to determine the cause and\r\n\t  solution to the problem that is being described.\r\n\r\n\t  Any issue that is open for 14 days without actionable information or\r\n\t  activity will be marked as \"stalled\" and then closed. Stalled issues\r\n\t  can be re-opened if the information requested is provided. -->\r\n"},
{"text": "When integrating [Vue Storefront](https://github.com/spark-solutions/spree2vuestorefront) with Spree we came across an issue - we cannot pass the applied coupon code when removing the code itself from the Order. \r\n\r\nHence we need to change the logic a bit - when there's no coupon code passed it should remove all Coupon codes Promotions from the Order.\r\n\r\nTODO:\r\n\r\n- [ ] update the API endpoint and service to make `couponcode` optional\r\n- [ ] add additional request specs\r\n- [ ] update swagger doc\r\n\r\nhttps://guides.spreecommerce.org/api/v2/storefront#operation/Remove%20Coupon%20Code"},
{"text": "An endpoint for fetching saved Credit Cards\r\n\r\n## Expected Behavior\r\n`GET /account/creditcards`\r\nshould return a list of all Credit Cards (no pagination) in JSON API format:\r\n```\r\n{\r\n   id: 1,\r\n   type: 'creditcard',\r\n   attributes: {\r\n      type: 'visa',\r\n      lastdigits: '4111',\r\n      month: 12,\r\n      year: 2020,\r\n      name: 'John Doe'\r\n   },\r\n   relationships: {\r\n      ... needs an association to Payment Method - TBD...\r\n   }\r\n}\r\n```\r\n\r\n`GET /account/creditcards?filter[paymentmethodid]=1`\r\nwill return only CCs for selected payment method\r\n\r\n`GET /account/creditcards/default`\r\nwill return a singular resource - the default CC for the current user\r\n\r\n## Actual Behavior\r\n<!--- Tell us what happens instead -->\r\n\r\n## Possible Fix\r\n`api/v2/storefront/account/creditcardscontroller` (`index` and `show` actions)\r\n\r\nsource of inspiration: https://github.com/spree/spree/blob/master/api/app/controllers/spree/api/v1/creditcardscontroller.rb"},
{"text": "For all storefronts, we have build we always needed some kind of tax estimate for given address. If the address isn't specified let's use `Spree::Address.default`"},
{"text": "For all storefronts, we have build we always needed some kind of shipping cost (and method) estimate for given address. If address isn't specified let's use `Spree::Address.default`"},
{"text": "As per discussion in Riot, ApiPromise currently exposes WsProvider.disconnect() via ApiPromise.disconnect(), but it does not expose WsProvider.connect() via the ApiPromise interface. In order to make an intuitive interface and not cause any breaking changes, ApiPromise could expose a connect() method.\r\n\r\nI don't see this as urgent at all, but just nice to have in the long term in order to avoid confusion. I would be happy to take a stab at it myself."},
{"text": "I would like to query historical account balances for all accounts at a particular block, something like\r\n\r\n```\r\nconst accounts = await api.query.system.account.at(blockHash).entries()\r\n```\r\n\r\nBut looks like not possible with current polkadot.js.\r\n\r\nOne way is add `blockHash` to `.entries()` and pass it to `getKeysPaged`"},
{"text": "Currently, the metadata API containing items like `api.consts.system.extrinsicBaseWeight` always returns the value extracted from the latest metadata. For some applications like calculating fees of historical transactions it is necessary to access metadata of the block (or rather its parent) where the transaction resides in.\r\n\r\nIt would be nice to have an API similar to the one provided for accessing storage using the `at` function:\r\n```\r\napi.query.transactionPayment.nextFeeMultiplier.at(blockHash);\r\n```"},
{"text": "https://github.com/paritytech/substrate/pull/6300\r\n\r\nRPC `systemdryRun` allow to dry run an extrinsic before submit it."},
{"text": "We have some breaking changes between our old and new testnet and we would like to have a single API that support both testnet.\r\n\r\nBut seems like the filterVersions is not been called on user provided types and we cannot know which types without knowing the spec version first.\r\n\r\nhttps://github.com/polkadot-js/api/blob/e6beb6554dff7ac61fd2e9d16b2631978e6ec626/packages/types-known/src/index.ts#L59-L62\r\n\r\nFor now we can use the chain name to distinguish between versions, but won't work once we introduce breaking changes in our public testnet via runtime upgrade.\r\n\r\nWe would like to pass something like this to API contractor and have it detect and use correct types based on spec version\r\n\r\n```\r\nexport const typeChain: Record<string, OverrideVersionedType[]> = {\r\n  acala: [\r\n    {\r\n      minmax: [undefined, 403],\r\n      types: {\r\n        Weight: 'u32'\r\n      }\r\n    }\r\n  ]\r\n};\r\n```"},
{"text": "With the [3.7 release](https://guides.spreecommerce.org/releasenotes/spree370.html) we've included the brand new [Storefront API v2](https://spree-guides.now.sh/api/v2/storefront) which is a modern lightweight REST API based on [JSON Api](https://jsonapi.org/) spec. We will continue this work in the form of the Platform API which is designed to:\r\n\r\n- [ ] completely replace the old legacy REST API v1\r\n- [ ] expose all of the features of the Spree platform\r\n- [ ] power our new Admin Panel experience (Spree 5.0)\r\n- [ ] documented in [OpenAPI](https://github.com/OAI/OpenAPI-Specification) format\r\n- [ ] tested with requests specs"},
{"text": "This has been dragging locally-WIP for too long, get it in."},
{"text": "### Goal:\r\n\r\nUpdate `signAsync` to make the second argument optional, and default to `{}`\r\n\r\n### Background:\r\n\r\nCurrently this code will crash:\r\n\r\n`const signed = await api.system.remark().signAsync(pair, options);` when `options` is `undefined`, it gives this error\r\n\r\n```\r\n(node:78914) UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'nonce' of undefined\r\n    at Submittable.value (/Users/xiliangchen/projects/playground/nodemodules/@polkadot/api/submittable/createClass.js:59:42)\r\n    at Submittable.value (/Users/xiliangchen/projects/playground/nodemodules/@polkadot/api/submittable/createClass.js:129:99)\r\n    at /Users/xiliangchen/projects/playground/nodemodules/@polkadot/api/submittable/createClass.js:251:110\r\n    at /Users/xiliangchen/projects/playground/nodemodules/@polkadot/api/promise/Api.js:78:14\r\n    at Submittable.signAsync (/Users/xiliangchen/projects/playground/nodemodules/@polkadot/api/submittable/createClass.js:251:170)\r\n    at /Users/xiliangchen/projects/playground/index.ts:28:33\r\n    at step (/Users/xiliangchen/projects/playground/index.ts:33:23)\r\n    at Object.next (/Users/xiliangchen/projects/playground/index.ts:14:53)\r\n    at /Users/xiliangchen/projects/playground/index.ts:8:71\r\n    at new Promise (<anonymous>)\r\n```\r\n\r\n### Actions:\r\n\r\nMake second argument optional\r\n\r\nhttps://github.com/polkadot-js/api/blob/e77050b9f59a0b772d1d0b8c5c1da488a7c36810/packages/api/src/submittable/createClass.ts#L72\r\n\r\nUpdate interface\r\n\r\nhttps://github.com/polkadot-js/api/blob/e77050b9f59a0b772d1d0b8c5c1da488a7c36810/packages/api/src/submittable/types.ts#L55\r\n\r\nProvide appropriate unit tests.\r\n\r\n----\r\n\r\n@jacogr please tag this if you are happy about this.\r\n\r\nTags:\r\n`bounty-awaiting-approval`\r\n`bounty-XS`"},
{"text": "As commented here https://github.com/polkadot-js/api/issues/1937#issuecomment-592561157\r\n\r\nExplicitly we want to have the following functionality, https://github.com/polkadot-js/api/blob/master/packages/types/src/known/overrides.ts#L8-L34"},
{"text": "As discussed in riot chat, this function would work like `.at(blockHash)` but return results in a range of blocks"},
{"text": "Right now the only way of using queryMulti is providing a callback that gets called whenever the stored values change. It would be useful if there was also a callback-less version that simply returned the values wrapped in a promise (as with other query methods)"},{"text": "As soon as paritytech/substrate#4895 is merged"},
{"text": "API should also include this dep as to not \"lose functionality\", i.e. user scripts. Form @polkadot/types we can then remove the @polkadot/api dependency (that is there for chain retrieval in the scripts)"},
{"text": "To be deployed to Kusama and Polkadot"},
{"text": "I'm using an `Option<MyType>` in a custom extrinsic signed payload.\r\nThe expected behavior is unclear for the `None` case. \r\nThe rust side expects a `0` to be encoded for the signature to verify.\r\nThis implementation encodes nothing for `None` when `isBare = true`\r\n\r\n`Option` is an implementation detail so it seems that polkadot-js/api is taking the correct approach here but the change will be much harder to implement on the substrate side.\r\n\r\nhttps://github.com/polkadot-js/api/blob/56dc116d92ed3de5994e3dc7c767940617e088e9/packages/types/src/codec/Option.ts#L133-L149"},
{"text": "The current rpc mocks do something, but don't do it very well and it exceptionally messy to say the least.\r\n\r\nWant to steal some stuff from @jnaviask for this, he actually laid the base for something that actually works properly..."},
{"text": "I'm trying to migrate my signer lib to use the new signer interface, then i noticed the dependency requirement changed. Before, my signer lib didn't have to depend on `@polkadot/api` or `@polkadot/types`, extrinsic is passed in and i only need to get the key pair what it's asked for.\r\nBut now, I need SignaturePayload from `@polkadot/types`, which means future change from `@polkadot` might break my signer lib.\r\n\r\nso, can we make SignerPayload support toU8a(), which return the same encoded u8a we passed to IKeyringPair.sign(). (cennznet/api/packages/types/src/primitive/Extrinsic/util.ts)\r\n\r\nthen i can remove the dependency of `@polkadot/types` from my lib."},
{"text": "i.e. `api.type(<Name>, ...args)` to map to as-is to `createType`. Mention `createType` as removed in the future"},
{"text": "... when available there, i.e. via https://github.com/paritytech/substrate/issues/2921\r\n\r\n- A number of parameters, previously in storage, have already moved, e.g. https://github.com/paritytech/substrate/blob/master/node/runtime/src/lib.rs#L171\r\n- More are on the way/in-progress, e.g. all fees\r\n\r\nOnce available on metadata, add to the api (like we have `genesisHash`, `runtimeVersion`) so it is available to all that want to query it. Additionally, update at least the derive interfaces to pull known/interesting values from here, if available."},{"text": "As implemented in https://github.com/paritytech/substrate/pull/2880"},
{"text": "Extending Struct makes it very difficult to re-use and extend, i.e. as in https://github.com/polkadot-js/api/pull/1015/files#diff-3b3e7230b87e430da514800b9ae4b684R44 (that should be an extend, not a complete re-definition)\r\n\r\nWith the new typing infrastructure introduced by @amaurymartiny this should, hopefully, be possible."},
{"text": "This needs to go after https://github.com/polkadot-js/api/pull/1007\r\n\r\nBasically, currently we only allow people to operate directly on the raw contract calls, i.e. https://github.com/polkadot-js/api/blob/master/packages/api/test/e2e/promise-contract.spec.ts#L92\r\n\r\nIn the above, we would really just like something along the following lines -\r\n\r\n```js\r\nimport { Contract } from '@polkadot/api-contract';\r\n\r\nawait new Contract(<api instance>, <abi>, <addr>)\r\n  .flip(<value to go along>, <max gas>, <...params>)\r\n  .signAndSend(<account>, (submittableResult) => {...});\r\n```\r\n\r\nThat is the end-result, earlier in the chain, we want to be able to deploy or attach code -\r\n\r\n```js\r\nimport { Blueprint } from '@polkadot/api-contract';\r\n\r\n// here it becomes a bit fuzzy... we need to \r\n//  - make the deploy call\r\n//  - as a result, we want a Contract instance (previous code block)\r\n//  - basically, it would take the address and abi to construct one\r\nawait new Blueprint(<api instance>, <abi>, <hash>)\r\n  .deploy(<value>, <gas>, <...params>)\r\n  .signAndSend(<account>, (submittableResult, contract) => {...});\r\n\r\n// even better\r\nconst contract = await new Blueprint(...).deploy(<account>, ...);\r\n```\r\n\r\nDeployment of code (putCode) needs something similar -\r\n\r\n```js\r\nimport { Code } from '@polkadot/api-contract';\r\n\r\n// same as above with a bit of fuzziness\r\n//  - as a result, we would like to retrieve a Blueprint instance\r\n//  - basically it would take the retrieved hash and abi, to construct one\r\nawait new Code(<api instance>, <abi>, <wasm bytes>)\r\n  .deploy(<endowment>, <max gas>)\r\n  .signAndSend(<account>, (submittableResult, blueprint) => {...});\r\n\r\n// even better\r\nconst blueprint = await new Code(...).deploy(<account>, ...);\r\n```\r\n\r\nSo as a crazy code returns blueprint, returns contract, it ends up -\r\n\r\n```js\r\nconst blueprint = await new Code(...).deploy(<account>, ...);\r\nconst contract = await blueprint.deploy(<account>, ...);\r\n\r\nawait contract\r\n  .flip(<value to go along>, <max gas>, <...params>)\r\n  .signAndSend(<account>, (submittableResult) => {...});\r\n```\r\n\r\n... or something like that\r\n\r\nAll-in-all, needs some playing to see what works and what doesn't, how to integrate and how to return values. Basically we want to move away from people having to manually inspect events, we can just handle all that transparently and have explicit interfaces.\r\n\r\nAdditionally, need to see how we tie this to all the type of api interfaces we are supporting, current RxJs & Promise (Similar approach as in Submittables?)"},
{"text": "The ApiBase currently provides an `on` function for listening to connection-related events, but there's no way to unregister these event handlers. It would be nice to have an `off` function or similar that allows us to remove them. Thanks!"},
{"text": "Currently every instance of the Api object will create and maintain a web socket connection.\r\nIt never disconnects itself and will attempt to reconnect if connection dropped for any reason.\r\n\r\nWe have an issue that developers are not aware of this behavior and construct a new instance of Api on every method call.\r\nBecause the Api object can never automatically GC'ed by JS due to the fact the ws connection is open, this results connection leaks, memory leaks, and unstable application.\r\n\r\nI don't know what is the best way to solve this issue other than code review.\r\nI wonder if have polkadot.js enforce shared Api object will help. i.e. have a shared pool of all created api instances, return existing one if parameters (ws url) matches."},
{"text": "Currently every new instance of Api will query metadata information and then it will become usable.\r\nThis adds a significantly initialization time which is not ideal. Image someone wants to develop a serverless lambda with polkadot.js.\r\nIt will be good if the constructor of Api takes a metadata json object and if it presents, use it instead of fetch from server.\r\nMay optionally still fetch from server and print a warning if mismatch found.\r\n"},
{"text": "Related to https://github.com/polkadot-js/api/issues/429\r\n\r\nExample -\r\n\r\n```json\r\n{\r\n    \"name\": \"Incrementer\",\r\n    \"deploy\": {\r\n        \"args\": [\r\n            {\r\n                \"name\": \"initvalue\",\r\n                \"type\": \"u32\"\r\n            }\r\n        ]\r\n    },\r\n    \"messages\": [\r\n        {\r\n            \"name\": \"inc\",\r\n            \"selector\": 257544423,\r\n            \"mutates\": true,\r\n            \"args\": [\r\n                {\r\n                    \"name\": \"by\",\r\n                    \"type\": \"u32\"\r\n                }\r\n            ],\r\n            \"returntype\": null\r\n        },\r\n        {\r\n            \"name\": \"get\",\r\n            \"selector\": 4266279973,\r\n            \"mutates\": false,\r\n            \"args\": [],\r\n            \"returntype\": \"u32\"\r\n        },\r\n        {\r\n            \"name\": \"compare\",\r\n            \"selector\": 363906316,\r\n            \"mutates\": false,\r\n            \"args\": [\r\n                {\r\n                    \"name\": \"x\",\r\n                    \"type\": \"u32\"\r\n                }\r\n            ],\r\n            \"returntype\": \"bool\"\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nDispatch on the contract side is pretty simple:\r\n\r\n- For deploy just take all the parameters (arguments) in the order they are listed in the JSON and decode them using SCALE.\r\n- For call the first 4 bytes are encoding the message selector and the rest is again just the SCALE encoded arguments for the selected message.\r\n\r\ncc @RobbePop"},
{"text": "(Please correctly me if I am mistaken)\r\n\r\nSubstrate drops the ws connection if it is idle for a minute. It will be good if there is a way to have polkadot.js sending some ping message in a configurable amount of interval (with a reasonable default) to ensure the connection is alive."},
{"text": "Currently it seems like the \"unknown Metadata version\" is swallowed and then use of the API spits out wrong errors. \r\n\r\nI believe it it the fallback to v0 that just parses garbage. Can remove v0 parsing support as a start. "},
{"text": "Currently the type registry is a shared global variable. polkadot.js will fail with unclear error message when multiple versions `@polkadot/types` are installed because each version of the lib have its own type registry.\r\n\r\nUntil the type registry is been refactored to not using shared global variable, it should detects if multiple versions are installed and print a big warning message.\r\n"},
{"text": "(Split from https://github.com/polkadot-js/apps/issues/800)\r\n\r\nThe `statestorage` subs can support multiple keys per subscription. Currently we are only doing a single subscription for each query even when we know from the use side that we need to do multiples in a row. Doing multiples in one-go can reduce and optimise the load/number of subs greatly on both sides.\r\n\r\nEven if at first we only implement this via the same type, maybe even `api.query.balances.freeBalance.multi(Alice, Bob, Dave, ...)` it will drop a lot of the use and can be used to make thing more optimal, for instances in the case of `api.derive.*`\r\n\r\nFrom @amaurymartiny \r\n\r\nThe `.multi` sounds good, it's clear on what it does (principle of least surprise), so I'm okay with this addition.\r\n\r\nBut as you said, it's only on one storage key, so I won't help in most `derive.*` where the storage keys are different, e.g. I can't do a multi-subscription to:\r\n\r\n```js\r\napi.query.balances.freeBalance(Alice)\r\napi.query.system.accountNonce(Alice)\r\n```\r\n\r\nboth in one subscription. I'd see something like:\r\n\r\n```js\r\napi.query.multi(\r\n  api.query.balances.freeBalance(Alice),\r\n  api.query.system.accountNonce(Alice)\r\n).then/subscribe()\r\n```\r\n\r\nThe freeBalance example above would be slightly more verbose:\r\n\r\n```js\r\napi.query.multi(\r\n  api.query.balances.freeBalance(Alice),\r\n  api.query.balances.freeBalance(Bob),\r\n  api.query.balances.freeBalance(Dave),\r\n)\r\n```\r\n\r\nso we could still add the `query.balances.freeBalance.multi` syntax as a shorthand. But I think the `query.multi()` one is more important."},
{"text": "Several changes required so that we can use TypeRegistry to replace the default Extrinsic.\r\n1. priority changes: TypeRegistry should have higher priority than default implementation.\r\n2. A Extrinsic interface.\r\n2. SubmittableExtrinsic: change to have a extrinsic member instead of extends directly."},
{"text": "Initial discussion happened on https://github.com/polkadot-js/apps/pull/684#discussionr255298425\r\n\r\n- Make API more explorable by default and without tools, i.e. if we add something like this for `tx`, `query`, `derive`\r\n```js\r\napi.tx.listAll() // shows all section.methods with description\r\n```\r\n\r\n- dev-only exposure, probably around here - https://github.com/polkadot-js/apps/blob/master/packages/ui-api/src/Api.tsx#L92\r\n\r\n> So expose once initialised and we are on dev chain (we should be able to use this https://github.com/polkadot-js/apps/blob/master/packages/ui-api/src/util/isTestChain.ts - looking at it now, now 100% on the `dev|loc` match, explicit via `local` and `development` is probably better. I know why it is short, dev chains originally retuned `dev` instead of `development`)"},
{"text": "cc @ianhe8x \r\n\r\nStarted playing a bit with the signer, I think it could make sense to make it a bit richer to show the send part to submit the actual status through as well. This way both the dapp and the signer app can actually track what is happening and display status as/when required.\r\n\r\nSo what I'm basically suggesting -\r\n\r\n```js\r\nexport interface Signer {\r\n  sign (e: SubmittableExtrinsic, a: string, o: SignatureOptions): Promise<number>;\r\n  update?: (id: number, status: Hash | SubmittableResult) => void \r\n}\r\n```\r\n\r\nFirst off, the sign returns an internal id that identifies the operation. An optional `update` on the signer interface then can receive status updates from the sending operation -\r\n\r\n- `id: number` - the id that was returned via `sign`\r\n- `status: Hash | SubmittableResult` - returns `Hash` when the `signAndSend` has no callback, otherwise the `SubmittableResult`\r\n\r\nBasically the status updates gets provided to both the signer as well as the underlying call that is made. So we just check `api.signer && api.signer.update` and then provide the status as required to the signer in addition to the actual caller.\r\n\r\nStumbled across this in `apps` when trying to get the JS console to have the updates, but have the signing done by the apps signer. And both the apps UI as well as the actual JS call needs the ability to have the status.\r\n\r\n(As an complete off-topic here, would love to integrate external signers as/when they are available in apps, so if you have something, would love to start working with you on that to integrate over there)"},
{"text": "Need to be more explicit in the url strings to pass through in the samples. Ie. \r\n\r\n\"Somewhere.com:9944\" will not work, however \"ws://somewhere.com:9944\" will (also with secure)\r\n\r\nAlso just check errors thrown on these, no silent failures. (We just need to be slightly more explicit to convey the info, it is there, but not always 100% obvious and/or somewhat hidden)"},
{"text": "This prevents to node.js program exit cleanly because there is no disconnect / close method so `process.exit` have to be used.\r\n\r\n`WSProviderInterface` should have a `disconnect` method to close ws connection\r\n\r\nIn additional to that, I think following changes can be helpful (not very confident so feel free to ignore those suggestions)\r\n\r\nMove `connect` method to `ProviderInterface` and have `disconnect` on there as well\r\n\r\nAdd `open` and `close` to `ApiBase` method which does connect & setup and disconnect & teardown.\r\n\r\n----\r\n\r\nCurrent we are using `provider.websocket.close()` which logs \r\n\r\n```\r\nAPI-WS: disconnected from ws://127.0.0.1:9944::1000: Normal connection closure\r\n```"},
{"text": "As a dapp, it doesn't always have access to key pair. Key pairs could be managed in a browser extension or hardware wallet. So I propose to introduce signer to api and let it do the signing. This will allow our browser extension to work with the api by exposing a signer into browser's global.\r\n\r\n```\r\nexport interface Signer {\r\n  sign(extrinsic: Extrinsic, opt: {from: string} & SignatureOptions): Promise<void>;\r\n}\r\n```\r\n\r\nsignAndSend is the place to use the signer. I prefer to changing the signature to below which looks more neat\r\n```\r\n  signAndSend (options: SignAndSendOptions)\r\n  signAndSend (options: SignAndSendOptions, statusCb: StatusCb)\r\n  signAndSend (options: SignAndSendOptions, statusCb?: StatusCb)\r\n\r\ninterface SignAndSendOptions extends Partial<SignatureOptions> {\r\n  from: string | KeyringPair\r\n}\r\n```\r\nthough to keep backward compatible, it could also be like:\r\n```\r\nsignAndSend (account: KeyringPair | string, options?: Partial<SignatureOptionsPartial> | StatusCb, statusCb?: StatusCb)\r\n```\r\n\r\nI would like to hear from you if it's the right direction before submitting my PR.\r\n"},
{"text": "Basically just a wrapper (2 lines) around the registry exposing the same. The rationale: no additional imports needed, and it is currently quite core. (Is exposed via the constructor, but injection can happen at any point)"},
{"text": "I'm suggesting that instead of `sign(pair, nonce, era)` we pass through `sign({ pair, nonce, era })`. (Need to look into this exactly)\r\n\r\nCurrently we don't allow the nice `signAndSend` shortcut to set the eras at all. So it is driven from that perspective. So `signAndSend(pair)` can still work as-is. `signAndSend({ pair, ... })` can actually pass in `nonce` (optional, can be detected), `era` (optional) and `blockHash` (optional, generis or detected) then as well.\r\n\r\nI would almost suggest that the `signAndSend` default era be set - not sure how many blocks to allow. So by default we actually do mortal instead of immortal."},
{"text": "Here we determine if something is an observable and act appropriately -\r\n\r\nhttps://github.com/polkadot-js/api/blob/master/packages/rpc-rx/src/index.ts#L104\r\n\r\nWe should change that signature so it returns the following interface -\r\n\r\n```js\r\ninterface RpcObervable {\r\n  (...params: Array<any>): Observable<any>;\r\n  isSubscription: boolean\r\n}\r\n```\r\n\r\nThis will be used in the api base `OnCall`, e.g. https://github.com/polkadot-js/api/blob/master/packages/api/src/Base.ts#L52 instead of the manually specified flag.\r\n\r\nOriginally came up as part of #598 (and subsequent quick-fix in #601) "},
{"text": "- [x] Add a chapter for ApiRx examples like https://polkadot.js.org/api/examples/promise/\r\n- [x] Go through examples in https://polkadot.js.org/api/api/classes/rxindex.apirx.html and update them to work with current API, polkadot and substrate versions"},
{"text": "Started in https://github.com/polkadot-js/api/pull/539, complete the loop"},
{"text": "Same as sign + send with some signature changes -\r\n\r\n- Takes in `(accountKeypair, statusCallback)`\r\n- Determines nonce by default (makes call to `system.accountNonce(accountKeypair.address()`)\r\n- Uses genesisHash in encoding\r\n\r\nGenerally caters for the basic use-case - for finer control the sign/send pair can be used individually. This just combines the 2 functions and only requires the keypair to operate."},
{"text": "Based on this https://github.com/polkadot-js/api/pull/527/files#diff-4ac32a78649ca5bdd8e0ba38b7006a1eR1 for api/promise -\r\n\r\n- Update in-code samples\r\n- Update actual docs/examples samples"},
{"text": "https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-2.html#bigint\r\n\r\nThis is a big refactor."},
{"text": "Currently api.query will perform queries (one-shot) leaving out the optional head hash. It is useful to do historic queries as well where it is added. \r\n\r\nSuggesting adding \u2018at\u2019 that will take the hash params. So basically current we can only do\r\n\r\n```\r\napi.query.balances.freeBalanceOf(Alice)\r\n```\r\n\r\nwhich performs the query for the latest block. An extension to this could be\r\n\r\n```\r\napi.query.balances.freeBalanceOf.at(hash, Alice) \r\n```\r\n\r\nwhich will perform at a specific block. The reason for the `at` is that it would be simple to determine which route (i.e. which underlying RPC call) to take. It is a bit tougher and messier to try and adjust the calls and determine intent by just using the current endpoints, i.e. it is a one-time query, a subscription query or a one-time-at query. At the same time we still have control of which params goes where. Additionally can be added without breaking old use cases. \r\n\r\n(Suggestions and comments obviously welcome on approach)"},
{"text": "Right now [I'm adding a LightApi](https://github.com/polkadot-js/light-ui/tree/master/packages/light-api) on top of the ApiRx object. But thinking about it, it could go below.\r\n\r\n**Proposal: Add a LightApi layer below Api**\r\n\r\nIdeally, LightApi is something that is totally invisible to the user, i.e. we keep the exact same final usage: `const api = new Api(new Provider(...))`.\r\n\r\nHowever, add some \"light logic\", i.e. make the minimum viable number of jsonrpc calls to the node, assuming the node doesn't hold state. For example, if the user wants to subscribe on a account's balance, then make a subscription/polling on `newHead`, check the events on new head, and only make a subsequent jsonrpc call `getStorage` if the events show that the relevant part of the state has changed.\r\n\r\nSo, on load, Api will check:\r\n- the provider\r\n- the roles of the node, more specifically if it's a light node\r\n\r\nand perform:\r\n\r\n||Full node|Light Node|\r\n|-|-|-|\r\n|**WsProvider**|Use the current impl.|Use light logic|\r\n|**HttpProvider**|Use light logic|Use light logic|\r\n\r\nIn this case, Providers stay \"dumb\", i.e. this invalidates #435 (dumb polling is enough for HttpProvider). The global idea is to not put the logic in #435 in rpc-provider, but in a layer on top of rpc-provider, but below api.\r\nAnd it's totally transparent to all other layers, e.g. #427 doesn't care if the underlying Api is using full logic or light logic."},
{"text": "Hi, thanks for providing such a great tool for us. May I ask, do you have a plan to let the SDK can generate abi after deploying smart contract?"},
{"text": "Rename the actual RPC wrappers -\r\n\r\n- api -> rpc-core\r\n- api-rx -> rpc-rx\r\n- api-provider -> rpc-provider\r\n\r\nThen for the actual high-level API -\r\n\r\n- api-observable -> api\r\n\r\nIn this `api-codec` seems like the odd man out, I would make the published package `@polkadot/codec`\r\n\r\n@amaurymartiny Thoughts?"},
{"text": "Convert to ES6 classes in the same fashion as https://github.com/polkadot-js/client/pull/132\r\n\r\nhttps://github.com/polkadot-js/api/blob/master/packages/api-provider/src/http/index.ts\r\nhttps://github.com/polkadot-js/api/blob/master/packages/api-provider/src/ws/index.ts\r\n\r\nNot the only 2 places in the API layer, but a start. (Bonus convert all the closures with self)"},
{"text": "https://github.com/paritytech/ink/pull/296\r\n\r\nThis opens the door to get rid of the custom parser and just define the objects and parse directly into them."},
{"text": "The ink! ABI JSON encoding changed with these two issues being fixed:\r\n\r\nhttps://github.com/paritytech/ink/issues/199\r\nKeys in the layout section of a contract metadata were previously encoded as an array of 32 bytes. They are now encoded as a hex string of the form `\"0xABC\u2026\"` (the `0x` prefix is in there, despite the original issue description).\r\n\r\nhttps://github.com/paritytech/ink/issues/200\r\nFunction/constructor selectors in the JSON representation of the contract metadata were previously encoded as `u32`. To prevent issues with misinterpretation because of endian-ness we encode them as four hex-encoded bytes in an array now (`\"selector\":[\"0x07\",\"0x5B\",\"0xCD\",\"0x15\"]`).\r\n\r\nThe old ABI stays the same in both cases."},
{"text": "It'd be great to have support of `at` for derive queries:\r\ne.g `api.derive.democracy.referendumInfo.at(BlockHash, RefIndex)`\r\n\r\nThe reason in this particular case is that each client needs to implement its own logic (e.g for backward compatibility or `referendumInfoOf` that recently changed), although the compatibility layer has already been implemented in derive."},
{"text": "We need a length - progress here https://github.com/polkadot-js/api/blob/master/packages/api-derive/src/staking/info.ts#L54"},
{"text": "We do cache on the RPC layer. However the derives are quite hairy and pull from a number of sources - this means that even when a derive is used, it is re-created. \r\n\r\nThis certainly has an impact on use. For instance with all inner derive functions in a memo, derives (warmed up) are close to instantaneous. Without, as per the status quo, they are not. (A good example would be staking.overview usage in the apps UI)\r\n\r\nInner memo has other issues when no memo cleanup is available. "},
{"text": "Code TODO -\r\nhttps://github.com/polkadot-js/api/pull/1317/files#diff-42453350b90abd3faf65d39e023fdc4eR41\r\n\r\nRemoval -\r\nhttps://github.com/polkadot-js/api/pull/1317/files#diff-e4ae210f1f695bace5a518b3cc0b0762L1154"},
{"text": "Elections now uses chunking for approvals and voters (see: https://github.com/paritytech/substrate/blob/master/srml/elections/src/lib.rs#L243). It would be useful to have a set of derives that let the caller \"just get everything\" without worrying about the chunking. There is already an internal function that does this (https://github.com/paritytech/substrate/blob/master/srml/elections/src/lib.rs#L1047) but no way to access it via RPC.\r\n\r\nThe derive would also be useful if it converted ApprovalFlag to booleans, as seen here: https://github.com/paritytech/substrate/blob/master/srml/elections/src/lib.rs#L1030\r\n\r\nThanks!"},
{"text": "https://github.com/paritytech/substrate/pull/2905\r\n\r\nAdditionally derive fees needs to cater for both 1.x and 2.x with feature detection"},
{"text": "Since we removed CodecResult/SubscriptionResult, `api.derive.*` does not return Codec anymore, so we don't need StructAny and VectorAny. Vanilla objects and arrays will do the trick\r\n\r\n"},
{"text": "While working on the api-derive tests I noticed that the test setup in API could urgently need some attention. \r\nNow that we're more people working on it who don't necessarily know the whole code by heart (including external contributors) we should put more effort in making the tests more bulletproof and the setup more intuitive.\r\n\r\n**Suggested tasks:**\r\n- [x] Reorder and better categorize existing tests (https://github.com/polkadot-js/api/pull/1132)\r\n- [x] Have a central file where all local tests are imported to be able to toggle all tests that require a local dev node (`.skip`) (https://github.com/polkadot-js/api/pull/1117)\r\n- [x] Move all e2e tests to `./api` (see https://github.com/polkadot-js/api/issues/908#issuecomment-503513811) (https://github.com/polkadot-js/api/pull/1132)\r\n- [x] Have a setup that runs all tests with **ApiPromise** and **ApiRx** for both **Polkadot** (wss://poc3-rpc.polkadot.io/) and the latest **Substrate** master on a local machine (Added docker-compose file with all supported versions https://github.com/polkadot-js/api/pull/1100)\r\n- [ ] Replace console.logs in tests with actual [Jest methods](https://jestjs.io/docs/en/expect.html)\r\n- [ ] Try to bring up test coverage\r\n- [ ] Fix broken / outdated tests\r\n- [ ] Add some documentation for external contributors\r\n\r\n**Already existing tickets:**\r\n- [x] #329 Convert all tests to TypeScript \r\n- [ ] #868 Add tests for api-derive methods\r\n"},
{"text": "I believe we should drop the actual nominator balances stuff completely, they are available now via the staking interfaces and doesn't need to be calculated. (Effectively this is basically almost ignore what is there in terms of balance retrieval and redo)\r\n\r\nOnce properly retrieved (including locked, available, free, reserved), can pull this into the UI and drop any recent calculations there.\r\n\r\ncc @Tbaut  - it could help if you drop some of the pointers from your travels in the last couple of days\r\ncc @kwingram25 - as discussed"},
{"text": "None of the methods have any tests at the moment. they should gradually be added. "},
{"text": "`HeaderExtended`  is a type only used by the `api-derive\u00b4 module so it shouldn't be part of the  typeRegistry"},
{"text": "I can't find a way to add custom derives without forking this repo. It will be good if this is extensible.\r\n\r\nThe real issue I am having is that we have implemented a generic asset module using `StorageDoubleMap` which there are no metadata support (https://github.com/paritytech/substrate/issues/1715), so I want to implements the query manually. I can do it via rpc calls but can't find a good way to bind this with UI and everything seems very tacky to me."},
{"text": "https://github.com/polkadot-js/api/blob/master/packages/api-derive/src/democracy/referendumInfos.ts#L13\r\n\r\nThe current work-around is along the lines of https://github.com/polkadot-js/apps/blob/master/packages/app-democracy/src/Referendums.tsx#L45-L52\r\n\r\nBut it doesn't quite make sense to have to do this everywhere in multiple apps when derive should have nicely-packaged information. This index is required to (a) get any votes, (b) make a vote, so very relevant to acting on the information presented.\r\n\r\ncc @jnaviask"},
{"text": "- Once https://github.com/paritytech/substrate/pull/1404 is available\r\n- move the account <-> id mappings into api.derive.indices\r\n- fallback to balances (for now) when indices not available (do check against endpoints for calls, detect which version to use)"},
{"text": "Is there a way to convert DataFrames containing nullable types to DataFrames containing the equivalent non-nullable types?\r\n\r\nFor example:\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> a = pd.DataFrame()\r\n>>> a['a'] = pd.Series([1, None, 3], dtype=\"Int32\")\r\n>>> a['b'] = pd.Series([True, False, None], dtype=\"boolean\")\r\n>>> a\r\n      a      b\r\n0     1   True\r\n1  <NA>  False\r\n2     3   <NA>\r\n>>> a.dtypes\r\na      Int32\r\nb    boolean\r\ndtype: object\r\n```\r\n\r\nIs there a way to convert this to a DataFrame with `int32` and `bool` dtypes, assuming the user can specify `navalues` for those? Ideally, if there are no nulls in the input, then the user wouldn't even have to specify `navalue`.\r\n\r\n### Context:\r\n\r\ncuDF is [moving in the direction](https://github.com/rapidsai/cudf/issues/5754) of returning nullable Pandas objects when the `.topandas()` method is called on cuDF objects. Our hope is that this is a step in the right direction of the community adopting arrow-like nullable types. In the near term, however, this will break user workflows (e.g., https://github.com/rapidsai/cudf/issues/5928) and it would be fantastic if we could point users to a convenient way to recover \"classical\" Pandas objects from what we return.\r\n\r\ncc: @brandon-b-miller @kkraus14 "},
{"text": "xref  #6249, #18262, #35301 \r\n\r\nFollowing up on #35301. This is an outstanding piece of `DataFrame.xs` functionality that's currently not available in `.loc`. Since `xs` is slated for deprecation I'd like implement this in `.loc`\r\n\r\n#### Example\r\nLet's define a multiindexed dataframe (example from `xs` docs)\r\n```\r\n   ...:  import pandas as pd\r\n   ...:  \r\n   ...: d = {  \r\n   ...:     'numlegs': [4, 4, 2, 2],  \r\n   ...:     'numwings': [0, 0, 2, 2],  \r\n   ...:     'class': ['mammal', 'mammal', 'mammal', 'bird'],  \r\n   ...:     'animal': ['cat', 'dog', 'bat', 'penguin'],  \r\n   ...:     'locomotion': ['walks', 'walks', 'flies', 'walks'] \r\n   ...:     }  \r\n   ...:  \r\n   ...: df = pd.DataFrame(data=d) \r\n   ...: df.setindex(['class', 'animal', 'locomotion'], inplace=True)                                                 \r\n\r\nIn [2]: df                                                                                                            \r\nOut[2]: \r\n                           numlegs  numwings\r\nclass  animal  locomotion                     \r\nmammal cat     walks              4          0\r\n       dog     walks              4          0\r\n       bat     flies              2          2\r\nbird   penguin walks              2          2\r\n```\r\n\r\n`xs` and `.loc` both let us take a cross-section through a `MultiIndex`\r\n\r\n```\r\nIn [3]: import pandas.testing as tm \r\n    ...: \r\n    ...: resxs = df.xs(('mammal', slice(None))) \r\n    ...: resloc  = df.loc[('mammal', slice(None))] \r\n    ...: tm.assertframeequal(resxs, resloc)                                                                       \r\n```\r\nbut with `xs` we can choose to keep the index columns through which we're slicing using the `droplevel` argument:\r\n```\r\nIn [4]: df.xs(('mammal', slice(None), 'flies'), droplevel=False)                                                    \r\nOut[4]: \r\n                          numlegs  numwings\r\nclass  animal locomotion                     \r\nmammal bat    flies              2          2\r\n```\r\n\r\n#### new API\r\nLooking for ideas on what's appropriate here. Based off of suggestions in #6249 would something like this work?\r\n```\r\nIn [ ]: df.loc(droplevels=False)[('mammal', slice(None), 'flies')]      \r\nOut[4]: \r\n                          numlegs  numwings\r\nclass  animal locomotion                     \r\nmammal bat    flies              2          2\r\n                 \r\n```"},
{"text": "**Is your feature request related to a problem?**\r\nWhen doing feature selection for models, or with handling missing data, in most cases you want to dynamically set a threshold to drop column which exceed this. A percentage is a more general metric to use to see how many data is missing data per row or per column. Right now we have the `thresh` argument, which accepts an int, but in most cases this is not desirable. For example when running the code in production, the shape of the data changes over time and thus a percentage threshold would make more sense to me. Or when you don't know the data well enough, what does \"a number\" mean in this case. Percentage is standarized.\r\n\r\nBesides that, the thresh right now is not flexible to go over column or index axis. For data cleaning you might want to drop rows which exceed the threshold, but for feature selection, you want to treat each column separately.\r\n\r\n**Describe the solution you'd like**\r\nHave a percentage threshold for both column and index axis.\r\n\r\n**API breaking implications**\r\nNot that I can think of, but not 100% sure.\r\n\r\n**Describe alternatives you've considered**\r\nWriting own custom functions.\r\n\r\n**Additional context**\r\n\r\nLocally I created a version where the `perc` argument is created (see linked draft PR):\r\n\r\n**example 1**:\r\n```python\r\n>>> df = pd.DataFrame({'col': [\"A\", \"A\", \"B\", \"B\"],\r\n...                    'A': [80, np.nan, np.nan, np.nan],\r\n...                    'B': [80, np.nan, 76, 67]})\r\n>>> df\r\n  col     A     B\r\n0   A  80.0  80.0\r\n1   A   NaN   NaN\r\n2   B   NaN  76.0\r\n3   B   NaN  67.0\r\n\r\n>>> df.dropna(perc=0.5)\r\n  col     A     B\r\n0   A  80.0  80.0\r\n2   B   NaN  76.0\r\n3   B   NaN  67.0\r\n\r\n>>> df.dropna(perc=0.5, axis=1)\r\n  col     B\r\n0   A  80.0\r\n1   A   NaN\r\n2   B  76.0\r\n3   B  67.0\r\n```\r\n---\r\n**example 2**:\r\n```python\r\n>>> df = pd.DataFrame(np.random.randint(1, 10, (10,4)), columns=list('ABCD'))\r\n>>> df.loc[3, 'B':] = np.nan\r\n>>> df.loc[8, :'B'] = np.nan\r\n>>> df.loc[1:6, 'C'] = np.nan\r\n>>> df\r\n     A    B    C    D\r\n0  7.0  2.0  3.0  7.0\r\n1  3.0  2.0  NaN  9.0\r\n2  2.0  2.0  NaN  2.0\r\n3  7.0  NaN  NaN  NaN \r\n4  1.0  9.0  NaN  4.0\r\n5  9.0  9.0  NaN  1.0\r\n6  2.0  2.0  NaN  6.0\r\n7  9.0  3.0  5.0  6.0\r\n8  NaN  NaN  9.0  5.0\r\n9  7.0  7.0  3.0  1.0\r\n```\r\nAs we can see, index 3 (`axis=0`) has 75% missing values and column C (`axis=1`) has 60% missing values. With the `percentage` argument we can specify what the threshold is, but also consider row wise or column wise:\r\n\r\nExample consider **per row**:\r\n```python\r\n>>> df.dropna(perc=.4, axis=0)\r\n     A    B    C    D\r\n0  7.0  2.0  3.0  7.0\r\n1  3.0  2.0  NaN  9.0\r\n2  2.0  2.0  NaN  2.0\r\n4  1.0  9.0  NaN  4.0\r\n5  9.0  9.0  NaN  1.0\r\n6  2.0  2.0  NaN  6.0\r\n7  9.0  3.0  5.0  6.0\r\n9  7.0  7.0  3.0  1.0\r\n```\r\nAbove we can see row 3 and 8 got dropped because these had ` > 40%` missing values.\r\n```python\r\n>>> df.dropna(perc=.4, axis=1)\r\n     A    B    D\r\n0  7.0  2.0  7.0\r\n1  3.0  2.0  9.0\r\n2  2.0  2.0  2.0\r\n3  7.0  NaN  NaN\r\n4  1.0  9.0  4.0\r\n5  9.0  9.0  1.0\r\n6  2.0  2.0  6.0\r\n7  9.0  3.0  6.0\r\n8  NaN  NaN  5.0\r\n9  7.0  7.0  1.0\r\n```\r\nSame command but with `axis=1`, so we can consider percentage threshold **per column**, and we see that column C got dropped because it had 60% missing values\r\n\r\n\r\n\r\n"},
{"text": "xref https://github.com/pandas-dev/pandas/pull/34998#issuecomment-658318898.\r\n\r\nCurrently (on master and in my PR at https://github.com/pandas-dev/pandas/pull/34998), `.groupby(...).apply` has some value-dependent behavior. Specifically\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df1 = pd.DataFrame({\"A\": [2, 1, 2], \"B\": [1, 2, 3]})\r\n   ...: df2 = pd.DataFrame({\"A\": [2, 1, 2], \"B\": [1, 2, 1]})  # duplicates in group \"2\"\r\n\r\nIn [3]: df1.groupby(\"A\", groupkeys=False).apply(lambda x: x.dropduplicates())\r\nOut[3]:\r\n   A  B\r\n0  2  1\r\n1  1  2\r\n2  2  3\r\n\r\nIn [4]: df2.groupby(\"A\", groupkeys=False).apply(lambda x: x.dropduplicates())\r\nOut[4]:\r\n   A  B\r\n1  1  2\r\n0  2  1\r\n```\r\n\r\nInternally, groupby constructs a list of DataFrames, one per group, that are the results of the UDF applied to each group. Those are concatenated together, and are at this point in \"group\" order. If we detect that the `.apply` was actually a transform, we reindex the concatenated result back to the original index.\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/b6222ec976a71b4ba0c643411606e2376e744c4d/pandas/core/groupby/groupby.py#L1114-L1128\r\n\r\nOut[3] has been viewed as a transform and so was reindexed. Whether or not the UDF was a transform depends on the values, and we generally discourage this type of values-dependent behavior.\r\n\r\nTo solve this, we have a few options\r\n\r\n1. Implement a \"table-wise\" transform. This solves the usecase where people are using `.apply` rather than transform just because it operates on dataframes rather than columns. We could do this through `.groupby(..., axis=None).transform()` or through `.groupby(...).transformtable()` / `transformframe()`. This doesn't help with the `dropduplicates` example, which is more of a filter (that sometimes doesn't filter anything).\r\n2. Implement a \"table-wise\" filter. Currently `.groupby().filter()` expects the UDF to return a scalar, and filters *groups* based on that. It could be expanded to also allow the UDF to return an array. In this case it would filter *rows* where the returned value evaluates to True. This would solve the `dropduplicates` use case, but not all use cases.\r\n3. Regardless of whether 1 or 2 are implemented, add a `reindexoutput` keyword to groupby to control this very narrow case. This would only be relevant when `groupkeys=False` and we've detected an apply. It gives users control over whether or not the result is reindexed.\r\n\r\n```python\r\n>>> df1.groupby(\"A\", groupkeys=False, reindexoutput=True).apply(lambda x: x.dropduplicates())\r\n   A  B\r\n0  2  1\r\n1  1  2\r\n2  2  3\r\n\r\n>>> df1.groupby(\"A\", groupkeys=False, reindexoutput=False).apply(lambda x: x.dropduplicates())\r\n   A  B\r\n1  1  2\r\n0  2  1\r\n2  2  3\r\n```\r\n\r\nIt has no effect in any other case, including `groupkeys=False`. By default, it can be `None` to preserve the values-dependent behavior on master. Though we can explore deprecating it if there's any desire."},
{"text": "In most cases, there is a relationship between `index.getloc` and `index.eq`.  `index.getloc(key)` roughly matches `(index == key).nonzero()`\r\n\r\n`DatetimeIndex.getloc` has special treatment for strings that we could extend to DatetimeIndex (and DatetimeArray) `eq`.\r\n\r\n```\r\n>>> dti = pd.daterange(\"2016-01-01 20:00\", periods=10, freq=\"H\")\r\n>>> s = \"2016-01-01\r\n>>> dti.getloc(s)\r\nslice(0, 4, None)\r\n>>> dti == s\r\narray([False, False, False, False, False, False, False, False, False, False])\r\n```\r\n\r\nUnder this proposal, the last comparison here would give `array([True, True, True, True, False, False, False, False, False, False])`\r\n\r\nThe main benefit of this would be internal consistency.  Possibly some code sharing, but I'm not ready to make any promises on that front."},
{"text": "i.e. when we get `pd.Timestamp.now(\"UTC\")` we should return a Timestamp with `datetime.timezone.utc` rather than `pytz.UTC`.  Similarly when we parse an ISO8601 datetime we should use tzinfo of `timezone(timedelta(seconds=val*60))` instead of `pytz.FixedOffset(val)`\r\n\r\nThis isn't that hard to implement, but doing it breaks a couple dozen tests where we are currently checking for pytz objects.  This would technically be an API change, so putting it up for discussion before implementing it."},
{"text": "xref mailing list [discussion](https://mail.python.org/pipermail/pandas-dev/2020-June/001246.html)\r\n\r\nATM most calls to `DataFrame(foo)` will end up going through either `core.internals.managers.createmanagerfromblocks` or `core.internals.managers.createblockmanagerfromarrays`, both of which call `mgr.consolidateinplace()` before returning a `BlockManager` object.  I think we should consider disabling this consolidation.  (There is also consolidation-by-another-name within formblocks that im still tracking down)\r\n\r\nBehavior-wise, consider:\r\n\r\n```\r\narr = np.random.randn(10)\r\nser = pd.Series(arr)\r\n\r\ndf = pd.DataFrame({\"A\": arr, \"B\": ser})\r\n```\r\n\r\nATM we get a new 2x10 array backing `df`.  The proposed change would mean that we keep 2 separate arrays, and `df[\"B\"]` shares data with `ser`.  I find the proposed behavior more intuitive.\r\n\r\nPerformance-wise, we expect faster construction but slower subsequent operations. [note to self: get some estimates/measurements here].  For long-lived DataFrames where the subsequent slowdowns add up, we can suggest users call `consolidate()` (which i guess we would have to de-privatize)\r\n"},
{"text": "#### Is your feature request related to a problem?\r\n\r\nEvery single time I have exported a dataframe (usually with .tocsv), I have not needed the index. \r\n\r\n#### Describe the solution you'd like\r\n\r\nChange the default from True to False.\r\n\r\n#### API breaking implications\r\n\r\nAll code that has not explicitely set index=True should make it.\r\n\r\n#### Describe alternatives you've considered\r\n\r\nLeave things as they are now.\r\n\r\n\r\nNot using the index is something usual or am I biased by my own experience? \r\n\r\n\r\n"},
{"text": "Currently, the `SparseArray.astype` function will always convert the specified target dtype to a sparse dtype, if it is not one. For example, this gives:\r\n\r\n```\r\nIn [64]: arr = pd.arrays.SparseArray([1, 0, 0, 2])  \r\n\r\nIn [65]: arr   \r\nOut[65]: \r\n[1, 0, 0, 2]\r\nFill: 0\r\nIntIndex\r\nIndices: array([0, 3], dtype=int32)\r\n\r\nIn [66]: arr.astype(float)  \r\nOut[66]: \r\n[1.0, 0.0, 0.0, 2.0]\r\nFill: 0.0\r\nIntIndex\r\nIndices: array([0, 3], dtype=int32)\r\n```\r\n\r\nThis ensures that a simple `astype` doesn't densify the sparse array (and you don't need to do `astype(pd.SparseDtype(float, fillvalue))`). \r\nAnd note this also gives this behaviour to `Series.astype(..)`\r\n\r\nBut, this also gives the inconsistency that `arr.astype(targetdtype).dtype != targetdtype`, so you can rely on the fact that you get back an array of the actual dtype that you specified. \r\nSee eg the workaround I need to add for this in https://github.com/pandas-dev/pandas/pull/34338\r\n"},
{"text": "Would it be possible to create a default pandas global setting to enable numba engine whenever possible?\r\n\r\nIn pandas.DataFrame.apply, .transform, etc.\r\n\r\nThanks!"},
{"text": "#### Is your feature request related to a problem?\r\n\r\nI wish I could use pandas to do\r\n\r\n```\r\ns = pd.Series([1,2,1,3,1,2])\r\ns.groupby()\r\n```\r\nrather than\r\n\r\n```\r\ns.groupby(s)\r\n```\r\n\r\n#### Describe the solution you'd like\r\n\r\nI would like pandas to interpret the series values as `grouper` when `by` or `level` is not provided.\r\n\r\n#### API breaking implications\r\n\r\nI can't think of any reason this will affect the API.\r\n\r\n#### Describe alternatives you've considered\r\n\r\nGrouping using series itself as `grouper`.\r\n\r\n`s.groupby(s)`\r\n"},
{"text": "Followup to https://github.com/pandas-dev/pandas/issues/31925.\r\n\r\nOver there, we removed `ensuretype` because it broke things like\r\n\r\n```python\r\nimport pandas as pd\r\nimport pandas.testing as pdt\r\n\r\nclass MyDataFrame(pd.DataFrame):\r\n    pass\r\n\r\nmdf = MyDataFrame()\r\ndf = pd.DataFrame()\r\n# In DataFrame.reindexlike, the derived class is compared to\r\n# the base class. The following line will throw\r\n# 'AssertionError: <class 'pandas.core.frame.DataFrame'>'\r\nmdf.reindexlike(df)\r\n```\r\n\r\nHowever, I don't think that example is very compelling. It's strange for MyDataFrame.reindex to return anything other than MyDataFrame. The root cause is MyDataFrame.constructor returning `DataFrame`, rather than `MyDataFrame`.\r\n\r\nI think we should somehow get subclasses to have their constructor return something that returns their subclass. We can\r\n\r\n1. Require this, by doing a runtime check on the  `DataFrame.constructor`\r\n2. Change `DataFrame.constructor` to return `type(self)`\r\n\r\n2 is nicer, but it requires that the subclasses `init` method be compatible enough with DataFrame's. It's *probably* safe to assume that, but it might be an API breaking change."},
{"text": "I would like to add functionality to pandas so that data frames can be queried like database tables, similar to the way that they can be in spark-sql. \r\n\r\nI think it should work in a similar fashion.\r\n\r\nA table can be registered using registertemptable(dataframe, tablename).\r\n\r\nThen using ```pandas.query(\"select * from tablename\")``` you can query the data frame or any other ones registered using standard sql syntax.\r\n\r\nI've already implemented the entire thing, but I was told to open an issue for it. \r\n\r\nAlso I'm aware that there is a package called pandassql but this package actually just puts a data frame into a sql lite database, as opposed to querying a data frame directly, and transforming the sql into pandas methods that are then applied to the data frame.\r\n\r\nMotivation:\r\nThe motivation for this enhancement is to make pandas more accessible to a crowd of users that may not be as technical and also to provide ease of transition for legacy code in systems like sas that have SQL already embedded in their programs. I'll supply a context free grammar in my documentation to show exactly what this system can handle, but it can basically handle any traditional SQL select statement, including subqueries, joins, where clauses, group by clauses, any aggregate function already supported by pandas, limit, and order by clauses. It also has support for rank and denserank window functions. It can't do things that sql wouldn't normally do like cross tab and you can't use a user defined function in it although I think that could be a good add-on. \r\n\r\nDatatypes:\r\nThe interface supports all pandas datatypes, so to cast something as an integer the syntax would currently be cast(somenumber as int64) or cast(someint as object). I've played around with the idea of varchar, char, bigint and smallint, but I think those would be misleading as those aren't datatypes that are supported by pandas currently.\r\n\r\nErrors:\r\nCurrently the exceptions that it will throw that come this api are based solely around trying to select from an unregistered table, or from submitting an improperly written sql query, both of which you wouldn't want to silence so there's only one error mode. \r\n\r\nApi Choices:\r\nThe reason I made the registertemptable section of the api top level was to avoid attaching a method to DataFrame although if others think it might be better as a method, I would change it in that manner (DataFrame.registertemptable(tablename)). The reason pandas.query is a top level method is that it's relational in structure. You can select from multiple tables and join them and such and so it wouldn't make sense for it to be on a DataFrame level. The only similarity to the .query DataFrame method though is the name. DataFrame.query is just an alternate way of expressing things like DataFrame[somecondition] whereas my .query encompasses a large amount of the pandas api.\r\n\r\nBuilt In:\r\nI have two reasons that I think this would be better built in. The first is that the target audience for this is less technical pandas users. Part of making this api easier to use is lessening the burden of researching code and learning how python works, so I think that for them to go looking for an external package may be hard to begin with and they would also need to know to look for one. \r\nMy second reason is that, from using what I've built, I've found pandas a lot easier to use just as a developer.\r\nSuppose we have a DataFrame with one column called A, it goes from \r\nThis code:\r\n```\r\ndataframe[name1] = dataframe[a] - 1\r\ndataframe[name2] = dataframe[a] + 1\r\ndataframe = dataframe[dataframe[name1] == dataframe[name2]]\r\ndataframe.drop(columns=['a'], inplace=True)\r\n```\r\n\r\nTo this code:\r\n```pd.query(\"select a - 1 as name1, a + 1 as name2 from sometable where name1 = name2\")```\r\n\r\nAlso although I did implement registertemptable as an api level function, it would serve best as a method on a DataFrame so that's another thing to consider. \r\n\r\nI can't really provide any support for the lark part, other than that it seemed like the best tool for what I was making.\r\n\r\nI apologize for the style and such, I'll be fixing all that before I'm done. I implemented this outside of pandas first, so that's why there are so many style and documentation discrepancies.  \r\n\r\n"},
{"text": "`df.plot(subplots=True)`  will create one subplot per column. Is there a way to group multiple columns on the same subplot (and leave the rest of the column separated)?\r\n\r\nI'd be happy to submit a PR if that's something you'd consider? In terms of API  `subplot` could accept a list of tuples where each tuple indicates which columns should be grouped together."},
{"text": "#### Problem description\r\nBasically these are performance tools in SQL to get analysis in multiple dimensions and they are missing in Pandas out of the box. Some of these can be achieved by a pivot table and melt/stack functions but being tools for analysis these functions should be a must and it also decreases the number of lines of code.\r\n\r\nGroup by Grouping set will help to rewrite the query with multiple groups by clauses combined with union statements into a single query. Cube is shorthand notation of grouping sets if the user chooses all the combinations of the fields listed in the cube clause \r\n\r\n```SQL Code\r\nSELECT\r\n    column1,\r\n    column2,\r\n    aggregatefunction (column3)\r\nFROM\r\n    tablename\r\nGROUP BY\r\n    GROUPING SETS (\r\n        (column1, column2),\r\n        (column1),\r\n        (column2),\r\n        ()\r\n);\r\n\r\nSelect   column1,\r\n            column2,\r\n            column3,\r\n            column4,\r\n            aggregatefunction (column5)\r\nfrom table\r\ngroup by column1, column2, cube (column3,column4)```\r\n\r\nCurrent way\r\n```pseudo code\r\n  a= <pandas dataframe>\r\n  a1 = a.groupby([column1]).sum(column5)\r\n  a2  = a.groupby([column1,column2]).sum(column5)\r\n   ...\r\n  an = a.groupby([column1,...,columnn]).sum(column5)\r\n result= union(a1,a2,......an)\r\n```\r\nExpected way \r\n```pseudo code\r\n  a= <pandas dataframe>\r\n  \r\n  gropbycube1 = a.gropby([column1,column2]).cube([column3,.....,columnn]).sum(column5)\r\n   gropbycube2 = a.gropby.cube([column1,column2,.....,columnn]).sum(column5)\r\n\r\n   gropbysets1 = a.gropby.sets( {column1,column2} ,{column1,column2,column3} ,{}).sum(column5)\r\n   gropbysets2 = a.gropby([column1,column2).sets({column1,column2,column3} ,{} ).sum(column5)\r\n\r\n   gropbyrollup1 = a.gropby.rollup({column1,column2,column3}).sum(column5)\r\n   gropbyrollup2 = a.gropby([column1,column2).rollup({column3} ).sum(column5)\r\n```\r\n\r\n"},
{"text": "For a use case in pyarrow, I need to get the underlying values of a Series: an ExtensionArray if it is back with one, or otherwise the numpy array.\r\n\r\nIs there public API to get this? We have `Series.array`, but this always returns an ExtensionArray, and we have `Series.values`, but this return a numpy array for eg periods (for historical reasons).\r\n\r\nFor Index, we have the private `Index.values` described in the docstring as the \"best array representation\". And I think `Series.values` is somewhat similar.\r\n\r\nBut the question is: do we want a public way to get to this? I am personally not sure we should, as there are still dubious cases (like datetime64/timedelta64, for this one, `Index.values` and `Series.values` is actually different ...). \r\n\r\nBut if we don't add it, do we have a recommended way for external projects to do this? \r\n(basically it is something like the `extractarray(..., extractnumpy=True)` ?) \r\n\r\ncc @TomAugspurger @jbrockmendel @jreback \r\n\r\n"},
{"text": "Working on #27138 I've found that ``MultiIndex`` keeps nan-likes in the levels, but encode them all to -1:\r\n\r\n```python\r\n>>> levels, codes = [[nan, None, pd.NaT, 128, 2]], [[0, -1, 1, 2, 3, 4]]\r\n>>> mi = pd.MultiIndex(levels, codes)\r\n>>> mi.codes[0]\r\n[-1, -1, -1, -1, 3, 4]\r\n>>> mi.levels[0]\r\nIndex([nan, None, NaT, 128, 2], dtype='object')\r\n```\r\n\r\nAll the MultiIndex nan-likes are encoded to -1, so it's not possible to decode them to their constituent values. So it's not possible to get more than one nan-like values out of the MultiIndex, so in this case ``None`` and ``NaT`` disappears when converting:\r\n\r\n```python\r\n>>> mi.toframe()[0].array\r\n<PandasArray>\r\n[nan, nan, nan, nan, 128, 2]\r\nLength: 6, dtype: object\r\n```\r\n\r\nI think if nan-likes are all encoded to -1, it'd be more consistent to not have them in the levels, similarly to how ``Categorical`` does it already.\r\n\r\n```python\r\n>>> c = pd.Categorical(levels[0])\r\n>>> c.codes\r\narray([-1, -1, -1,  1,  0], dtype=int8)\r\n>>> c.categories\r\n>>> Int64Index([2, 128], dtype='int64')\r\n```\r\n\r\nIs there acceptance to change the MultiIndex API so we get nan-likes out of the labels? That would give them an API more similar to ``Categorical``.\r\n\r\n@pandas-dev/pandas-core."},
{"text": "It seems pandas `percentrank` works like the `cumedist` of SQL databases.\r\n\r\nAs `ibis-framework` tries to use the same pandas API as much as possible ... `ibis-framework` `percentrank` is equal to `pandas`.\r\n\r\nAs in SQL databases there are these 2 operations, I need a way to implement the SQL `percentrank`\r\n\r\nI don't know very well which path I should take. Is there an initial thoughts here: https://github.com/ibis-project/ibis/issues/1975\r\n\r\nI wonder if there was any discussion about this topic before.\r\n\r\nAny comment, recommendation or guidance would be very appreciated.\r\n"},
{"text": "We currently have a public `Index.tonativetypes()` method (https://dev.pandas.io/docs/reference/api/pandas.Index.tonativetypes.html) that somehow formats the values of the index.\r\n\r\nExample:\r\n```\r\nIn [3]: pd.daterange(\"2012\", periods=3).tonativetypes() \r\nOut[3]: array(['2012-01-01', '2012-01-02', '2012-01-03'], dtype=object)\r\n```\r\n\r\nI don't think this needs to be in the public API?"},
{"text": "Part of the discussion on missing value handling in https://github.com/pandas-dev/pandas/issues/28095, detailed proposal at https://hackmd.io/@jorisvandenbossche/Sk0wMeAmB.\r\n\r\n*if* we go for a new NA value, we also need to decide the behaviour of this value in comparison operations. And consequently, we also need to decide on the behaviour of boolean values with missing data in logical operations and indexing operations.  \r\nSo let's use this issue for that part of the discussion.\r\n\r\nSome aspects of this:\r\n\r\n- Behaviour in **comparison operations**: currently np.nan compares unequal (`value == np.nan -> False`, `values > np.nan -> False`, but we can also propagate missing values (`value == NA -> NA`, ...)\r\n-  Behaviour in **logical operations**: currently we always return False for `|` or `&` with missing data. But we could also use a \"three-valued logic\" like [Julia](https://docs.julialang.org/en/v1/manual/missing/index.html#Logical-operators-1) and SQL (this has, eg, `NA | True = True` or `NA & True = NA`).\r\n- Behaviour in **indexing**: currently you cannot do boolean indexing with a boolean series with missing values (which is object dtype right now). Do we want to change this? For example, interpret it as False (not select it) \r\n  (TODO: should check how other languages do this)\r\n\r\nJulia has a nice documentation page explain how they support [missing values](https://docs.julialang.org/en/v1/manual/missing/index.html), the above ideas largely match with that.\r\n\r\nBesides those behavioural API discussions, we also need to decide on how to approach this technically (boolean ExtensionArray with boolean numpy array + mask for missing values?) Shall we discuss that here as well, or keep that separate?\r\n\r\ncc @pandas-dev/pandas-core "},
{"text": "I would like to propose that any pandas API that allows specification of a column name also works when specifying an index level name.  Today, this works in some places, but not all.  Here is a list of places where things work, and where things don't work and could be improved.  (The list is most likely incomplete, so additions are welcome).  References to existing issues are given when they already exist (and if I knew about them):\r\n\r\nHoping to include this in the roadmap #27478\r\n\r\nThings that work:\r\n- `DataFrame.query()` allows queries to use column names and index level names\r\n- `DataFrame.merge()` and `pd.merge()` allow both column names and index level names to be specified as key fields to use in the merge (but see below)\r\n- `DataFrame.groupby()` allows both column names and index level names to be mixed in the groupby list\r\n\r\nThings that could be improved:\r\n- Allow `.loc()` and `.getitem()` to specify index level names wherever it allows column names.\r\n- Modify `DataFrame.rename()` to allow renaming of index levels with a dict argument. (#20421)\r\n- Allow index names to be specified like columns in `pd.Grouper()` (#19542)\r\n- When merging on a subset of `MultiIndex` levels, preserve the levels not included in the merge.  (#13371 is somewhat related)\r\n- Have `.itertuples()` return a named tuple that includes index names (#27407) \r\n- Allow `.assign` to refer to columns that correspond to level names (although the `.loc` and `.getitem()` suggestion above might handle this\r\n\r\n"},
{"text": "https://github.com/pandas-dev/pandas/pull/27461#discussionr305168936\r\n\r\n> i think we need. way for EA to hook into this for an EA scalar\r\n> eg an IPaddress from cyberpandas could register a scalar i think\r\n\r\nBefore we move on this, I think we need to clarify in which situations we care about `lib.isscalar(x)` vs the simpler `np.ndim(x) == 0`"},
{"text": "In #26414 we splitted the pandas plotting module into a general plotting framework able to call different backends and the current matplotlib backends. The idea is that other backends can be implemented in a simpler way, and be used with a common API by pandas users.\r\n\r\nThe API defined by the current matplotlib backend includes the objects listed next, but this API can probably be simplified. Here is the list with questions/proposals:\r\n\r\nNon-controversial methods to keep in the API (They provide the `Series.plot(kind='line')`... functionality):\r\n- LinePlot\r\n- BarPlot\r\n- BarhPlot\r\n- HistPlot\r\n- BoxPlot\r\n- KdePlot\r\n- AreaPlot\r\n- PiePlot\r\n- ScatterPlot\r\n- HexBinPlot\r\n\r\nPlotting functions provided in pandas (e.g. `pandas.plotting.andrewscurves(df)`)\r\n- andrewscurves\r\n- autocorrelationplot\r\n- bootstrapplot\r\n- lagplot\r\n- parallelcoordinates\r\n- radviz\r\n- scattermatrix\r\n- table\r\n\r\nShould those be part of the API and other backends should also implement them? Would it make sense to convert to the format `.plot` (e.g. `DataFrame.plot(kind='autocorrelation')`...)? Does it make sense to keep out of the API, or move to a third-party module?\r\n\r\nRedundant methods that can possibly be removed:\r\n- histseries\r\n- histframe\r\n- boxplot\r\n- boxplotframe\r\n- boxplotframegroupby\r\n\r\nIn the case of `boxplot`, we currently have several ways of generating a plot (calling mainly the same code):\r\n1. `DataFrame.plot.boxplot()`\r\n2. `DataFrame.plot(kind='box')`\r\n3. `DataFrame.boxplot()`\r\n4. `pandas.plotting.boxplot(df)`\r\n\r\nPersonally, I'd deprecate number 4, and for number 3, deprecate or at least not require a separate `boxplotframe` method in the backend, but try to reuse `BoxPlot` (for number 3 comments, same applies to `hist`).\r\n\r\nFor `boxplotframegroupby`, didn't check in detail, but not sure if `BoxPlot` could be reused for this?\r\n\r\nFunctions to register converters:\r\n- register\r\n- deregister\r\n\r\nDo those make sense for other backends? \r\n\r\nDeprecated in pandas 0.23, to be removed:\r\n- tsplot\r\n\r\nTo see what each of these functions do in practise, it may be useful this notebook by @liirusuk: https://github.com/python-sprints/pandasplottinglibrary/blob/master/AllPlottingExamples.ipynb\r\n\r\nCC: @pandas-dev/pandas-core @tacaswell, @jakevdp, @philippjfr, @PatrikHlobil"},
{"text": "Hi, first of all thanks for the amazing library everyone!\r\n\r\nThis is a minor issue I'm sure, but I find it impossible to remember the name of the `.pctchange()` function.\r\n\r\nPartly this is because of confusion with the existing .diff() function which is easy to remember - I always try things like:\r\n- pctdiff()\r\n- pctdiff()\r\n- percentdiff()\r\n\r\nNormally after a couple of goes I admit defeat and Google it yet again :)\r\n\r\nSo this is a request for an additional function with the name `.pctdiff()` (or whatever spelling is preferred) that replicates/calls `.pctchange()`, in order to standardise the API.\r\n\r\nThanks!"},
{"text": "cc @mrocklin for dask.dataframe visibility\r\n\r\nI'm one of the developers of https://github.com/rapidsai/cudf and we're working on adding GPU-accelerated file readers / writers to our library. It seems most of the standard formats are covered quite nicely in the Pandas API, but ORC isn't. Before we went off defining our own API I wanted to open a discussion for defining what that API would look like so we can be consistent with the Pandas and Pandas-like community.\r\n\r\nAt the top level, I imagine it would look almost identical to Parquet in something like the following:\r\n```\r\ndef readorc(path, engine='auto', columns=None, **kwargs):\r\n    \"\"\"\r\n    Load an orc object from the file path, returning a DataFrame.\r\n\r\n    Parameters\r\n    ----------\r\n    path : string\r\n        File path\r\n    columns : list, default=None\r\n        If not None, only these columns will be read from the file.\r\n    engine : {'auto', 'pyarrow'}, default 'auto'\r\n        Orc library to use. If 'auto', then the option\r\n        ``io.orc.engine`` is used. The default ``io.orc.engine``\r\n        behavior is to use 'pyarrow'.\r\n    kwargs are passed to the engine\r\n\r\n    Returns\r\n    -------\r\n    DataFrame\r\n    \"\"\"\r\n    ...\r\n\r\n\r\ndef toorc(self, fname, engine='auto', compression='snappy', index=None,\r\n           partitioncols=None, **kwargs):\r\n    \"\"\"\r\n    Write a DataFrame to the binary orc format.\r\n\r\n    This function writes the dataframe as a `orc file\r\n    <https://orc.apache.org/>`. You can choose different orc\r\n    backends, and have the option of compression. See\r\n    :ref:`the user guide <io.orc>` for more details.\r\n\r\n    Parameters\r\n    ----------\r\n    fname : str\r\n        File path or Root Directory path. Will be used as Root Directory\r\n        path while writing a partitioned dataset.\r\n    engine : {'auto', 'pyarrow'}, default 'auto'\r\n        Orc library to use. If 'auto', then the option\r\n        ``io.orc.engine`` is used. The default ``io.orc.engine``\r\n        behavior is to use 'pyarrow'.\r\n    compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\r\n        Name of the compression to use. Use ``None`` for no compression.\r\n    index : bool, default None\r\n        If ``True``, include the dataframe's index(es) in the file output.\r\n        If ``False``, they will not be written to the file. If ``None``,\r\n        the behavior depends on the chosen engine.\r\n    partitioncols : list, optional, default None\r\n        Column names by which to partition the dataset\r\n        Columns are partitioned in the order they are given\r\n    **kwargs\r\n        Additional arguments passed to the orc library. See\r\n        :ref:`pandas io <io.orc>` for more details.\r\n    \"\"\"\r\n    ...\r\n```\r\n    "},
{"text": "While adding the keyword and functionality (beyond the cython backend) is only gonna happen past v.0.24, I think the EA interface should be adapted to allow for the possibility of the `returninverse`-kwarg.\r\n\r\nThe default implementation could just recommend to start with\r\n``` \r\nif returninverse:\r\n    raise NotImplementedError('this array type does not yet support `returninverse=True`')\r\n```\r\n\r\nOf course, this could be added along with the other changes to `.unique`, but I was wondering if this should already be prepared before v.0.24 so that EA authors (who will probably expect the EA interface to be more stable with v.0.24) can already prepare for it.\r\n\r\n@jreback @TomAugspurger "},
{"text": "We need to better describe the exact semantics of `ndarrayvalues`: what is it expected to return and how it is used.\r\n\r\nCurrenlty it is defined on the ExtensionArray, but mentioned it is not part of the \"official\" interface:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/712fa945c878eaed18f79d4cf99ed91e464d51b1/pandas/core/arrays/base.py#L687-L697\r\n\r\nOne Series/Index, the property will either give you what `EA.ndarrayvalues` gives, or the underlying ndarray:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/712fa945c878eaed18f79d4cf99ed91e464d51b1/pandas/core/base.py#L768-L780\r\n\r\n---\r\n\r\nWhat it currently is for the EAs:\r\n\r\n* Categorical: integer codes\r\n* IntegerArray: the integer `data`, so but losing any information about missing values\r\n* PeriodArray: the integer ordinals\r\n* IntervalIndex: object array of Interval objects\r\n\r\n---\r\n\r\nFor what it is currently used (this needs to be better looked at, copying now from https://github.com/pandas-dev/pandas/issues/19954#issuecomment-436374598, quoting Tom here):\r\n\r\n- Index.itemsize (deprecated)\r\n- Index.strides (deprecated)\r\n- Index.engine\r\n- Index set ops\r\n- Index.insert\r\n- DatetimeIndex.unique\r\n- MultiIndex.equals\r\n- pytables.convertindex (shared across integer and period)\r\n\r\nThere are a few other uses (mostly datetime / timedelta / period) that could maybe uses asi8 instead. I'm not familiar enough with indexing to know whether that can operate on something other than ndarrays. In theory, EAs can implement the buffer protocol, which would get the data to cython. But I don't know what ops would be required when we're down there.\r\n\r\n"},
{"text": "These occur naturally with `.groupby(extensionarray)` and `Seres[extensionarray].valuecounts`. We should define a public API so that we don't have to convert to object in these cases.\r\n\r\nYou'd likely end up with an `Index` with a non-object dtype.\r\n\r\n---\r\n\r\n*more radically* this kind of removes the *need* for all our Index subclasses, aside from MultiIndex. But we can think about that separately from the interface.\r\n\r\nI don't think this is blocking for 1.0, but it may not be too much effort."},
{"text": "In lots of places, pandas does something like `if np.any(arr.isna())`, which is wasteful as we have to create an ndarray of booleans just to check whether there are any `True` values.\r\n\r\nSome arrays, like Arrow, know ahead of time whether there are any NAs in the array. Would it make sense to expose an API for an array saying whether they have any missing values?\r\n\r\nWith indexes, we work around this by caching a `hasnans` value. That wouldn't work for mutable arrays."},
{"text": "*This ticket is an outgrowth of a discussion in pull request #22587*\r\n\r\nBy [my rough count](https://docs.google.com/spreadsheets/d/1hiQ5Pexs5SuCT2pfbN8OH6ld6ZyvWqTEaH2W-xkEJE/edit?usp=sharing), the [`readcsv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.readcsv.html) method has nearly 50 keyword arguments.\r\n\r\nOf those, 32 arguments are made up or two or more words. Twenty of those multi-word arguments use an underscore to mark the space between words, like  `skipblanklines` and `parsedates`. Twelve do not, like `chunksize` and `lineterminator`. \r\n\r\nIt is my opinion this is a small flaw in pandas' API, and that the library would benefit by standardizing how spaces are handled. It would make pandas more legible and consistent, and therefore easier for users of all experience levels.\r\n\r\nI have taught pandas to dozens of newbies across the country and I can testify from experience that small variations in the naming style of commonly used methods introduces unnecessary frustration, and \r\ncan even reduce user confidence in the quality of the overall product.\r\n\r\nAs a frequent user of pandas, I can also attest that the inconsistencies require me, someone who uses the library daily, to routinely consult the documentation to ensure I use the proper kwarg naming style.\r\n\r\nI am sympathetic to the desire to maintain backwards compatibility, which I believe could be managed with deprecation warnings that, if included, could be temporary, and ultimately removed in a future version, much in the way `sortvalues` was introduced.\r\n\r\nSince the underscore method of handling word breaks is more common and more legible, I propose it be adopted. All existing multi-word arguments without an underscore would need to be modified. You can find an experimental patch of the `skiprows` kwargs, and considerable support from other users for pursuing this type of change, in #22587.\r\n\r\nIf that pull request is ultimately merged, and the maintainers agree with the larger goal I've tried to articulate here, I would be pleased to lead an effort to expand whatever design pattern is agreed upon to other keyword arguments across the library."},
{"text": "Based on the SciPy sprint discussions, and the discussions on related issues, seems like `fromrecords` should be the pandas way to create a `DataFrame` from row based data.\r\n\r\nThe current signature is next:\r\n- ```def fromrecords(cls, data, index=None, exclude=None, columns=None, coercefloat=False, nrows=None):```\r\n\r\nAnd it currently supports input `data` as:\r\n- dict\r\n- numpy.ndarray\r\n- DataFrame\r\n- Iterable of list\r\n- Iterable of tuple\r\n- Iterable of dict\r\n\r\nWhat I propose is to make `fromrecords` only work when `data` is an iterable of array-like (list, tuple, np.array...) or an iterable of dict. And deprecate the other cases (dict and DataFrame). After searching on GitHub repos and blogs, couldn't find cases where it's used with dict or DataFrame, and IMO the DataFrame constructor is a better way for those cases. This would make the code simpler.\r\n\r\nThen, I'd add a new parameter `dtypes` expecting a list or a dict with the dtypes of the new DataFrame. With this, and for the case when `data` is a generator, and `nrows` and `dtypes` are specified, we wouldn't need to exhaust the generator and load it to Python structures. Meaning that we'd just need to allocate the DataFrame memory, and there wouldn't be any intermediate memory requirements.\r\n\r\nRelated issues: #5902, #2305, #2193, #4464, #1794, #13818.\r\n\r\n@wesm, @jreback, @jorisvandenbossche any comments? Are you ok with this approach and the proposed changes to the API?\r\n\r\n\r\n\r\n\r\n\r\n"},
{"text": "We should make SparseArray a proper ExtensionArray.\r\n\r\nIt seems like this will be somewhat difficult to do *properly* when SparseArray subclasses ndarray. Basic things like `np.asarray(sparsearray)` don't match the required ExtensionArray API (https://github.com/pandas-dev/pandas/issues/14167). Fixing this, especially when we subclass ndarray, is going to be difficult. I can't override the behavior of `np.asarray(sparsearray)` in Python.\r\n\r\nSo, some questions\r\n\r\n1. Do people *rely* on SparseArray being an ndarray subclass?\r\n2. Do we want to make a clean break, or introduce deprecations for things that will need changing (but with no clear upgrade path)?\r\n\r\nMy current preference is to just break things, but I don't use sparse. SparseArray would compose an ndarray of dense values and a `SparseIndex`, but it would no longer subclass ndarray.\r\n\r\nCCing some people who seem to use pandas' sparse: @hexgnu @kernc @Licht-T "},
{"text": "Proposal: expose `CategoricalDtype` in the top-level `pandas` namespace.\r\n\r\nAs of pandas 0.23.0, `.astype(..., categories=...)` raises a `FutureWarning` but `CategoricalDtype`, the recommended alternative, is still buried in `pandas.api.types`. This makes its use considerably more verbose than the old interface:\r\n\r\n```\r\n# previous use (prior to 0.23.0)\r\ndf['col'] = df['col'].astype('category', categories=mycategories)\r\n\r\n# current use (0.23.0)\r\nfrom pandas.api.types import CategoricalDtype\r\ncat = CategoricalDtype(mycategories)\r\ndf['col'] = df['col'].astype(cat)\r\n\r\n# proposed (0.24+)\r\ndf['col'] = df['col'].astype(pd.CategoricalDtype(mycategories))\r\n```\r\n"},
{"text": "#### Problem description\r\n\r\nI'd like to suggest a modification to [df.pop(item)](https://github.com/pandas-dev/pandas/blob/a00154dcfe5057cb3fd86653172e74b6893e337d/pandas/core/generic.py#L632-L680). Currently, `pop(item)` deletes the column from the dataframe it's being called on and returns that column as a series. It doesn't accept multiple items.\r\n\r\nIt might be a nice convenience to:\r\n\r\n1. pop multiple columns at once (ex: `pop(['A', 'B'])`\r\n2. specifying an `axis` parameter (default: axis=1) to allow popping rows and columns (ex: `pop(1, axis=0)`)\r\n3. pop slices (ex: `pop([1:3], axis=1)`)\r\n\r\nThought I'd throw it out there to the pandas gods and see if it is interesting. If it's not the best API design decision for `pop`, I completely understand.\r\n\r\n#### Common use-case\r\n\r\n1. you have one or multiple problem rows you want to delete from a dataframe but still keep for later evaluation. You'd just pop the rows and they'd be deleted from your existing dataframe and saved to a new variable.\r\n2. many times people seem to need to pop the last row, or second row. It is easy to pop the last row using `.iloc[:-1]` but popping the second row in one swoop isn't as easy I think. It could be if you just pop it out of there using pop.\r\n3. sometimes people loop through a dataframe. not recommended I understand, but in such a scenario, you could pop a row based on a condition while looping perhaps in a complex manner.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n```python\r\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]},\r\n                  columns=['A', 'B', 'C'])\r\n\r\ndef pop(df, values, axis=1):\r\n    if axis == 0:\r\n        if isinstance(values, (list, tuple)):\r\n            poppedrows = df.loc[values]\r\n            df.drop(values, axis=0, inplace=True)\r\n            return poppedrows\r\n        elif isinstance(values, (int)):\r\n            poppedrow = df.loc[values].toframe().T\r\n            df.drop(values, axis=0, inplace=True)\r\n            return poppedrow\r\n        else:\r\n            print('values parameter needs to be a list, tuple or int.')\r\n    elif axis == 1:\r\n        # current df.pop(values) logic here\r\n        return df.pop(values)\r\n```\r\n\r\n#### Example Usage\r\n```python\r\n# example df\r\n>>> df\r\n   A  B  C\r\n0  1  4  7\r\n1  2  5  8\r\n2  3  6  9\r\n\r\n# pop multiple indices, delete from df inplace, return popped rows\r\n# the df param wouldn't exist in the pop method; it'd be self\r\n# df param just shown here to illustrate the idea\r\n>>>pop(df, [0, 2], axis=0)\r\n   A  B  C\r\n0  1  4  7\r\n2  3  6  9\r\n\r\n# pop one index value, delete from df, return row as a dataframe (not series)\r\n>>> pop(df, 1, axis=0)\r\n   A  B  C\r\n1  2  5  8\r\n\r\n\r\n```\r\n\r\n#### Demand for such a feature\r\n[How to pop rows from a dataframe?](https://stackoverflow.com/questions/42285806/how-to-pop-rows-from-a-dataframe)"},
{"text": "This issue is created based on the discussion from #15931 following the deprecation of relabeling dicts in `groupby.agg`. A lot of what is summarized below was already discussed in the previous discussion. I would recommend in particular https://github.com/pandas-dev/pandas/pull/15931#issuecomment-336139085 where the problems are also clearly stated.\r\n\r\nThe motivation behind the deprecation of #15931 was mostly related to bringing a consistent interface for `agg()` between Series and Dataframe (see also #14668 for context).\r\n\r\nThe relabeling functionality with a nested dict has been described by some as being too complex and/or inconsistent and thus deprecated.\r\n\r\nHowever, this comes at a price: the impossibility to aggregate and rename at the same time leads to very annoying issues and some backward incompatibility where no sensible workaround is available:\r\n- [annoying] no more control over the names of the resulting columns\r\n- [annoying] you need to find a way to rename the MultiIndex after performing the aggregation, requiring to keep track of the order of columns at two places in the code.... not practical at all and sometimes downright impossible (cases below).\r\n-  **[breaking]** cannot apply more than one callable with the same internal name on the same input column. This results in two sub-cases:\r\n    -  **[breaking]** you can't apply anymore two or more lambda aggregators on the same column\r\n    -  **[breaking]** you can't apply anymore two or more aggregators from partial functions unless you alter their hidden `name` attribute\r\n\r\n## Example\r\n\r\n(please note, this is a crafted example for the purpose of demonstrating the problem in as short a code as possible, but all of the demonstrated issues here did bite me in real life since the change, and in situations not as simple as here)\r\n\r\n### Input Dataframe\r\n\r\n```python\r\nmydf = pd.DataFrame(\r\n    {\r\n        'cat': ['A', 'A', 'A', 'B', 'B', 'C'],\r\n        'energy': [1.8, 1.95, 2.04, 1.25, 1.6, 1.01],\r\n        'distance': [1.2, 1.5, 1.74, 0.82, 1.01, 0.6]\r\n    },\r\n    index=range(6)\r\n)\r\n```\r\n\r\n```\r\n  cat  distance  energy\r\n0   A      1.20    1.80\r\n1   A      1.50    1.95\r\n2   A      1.74    2.04\r\n3   B      0.82    1.25\r\n4   B      1.01    1.60\r\n5   C      0.60    1.01\r\n```\r\n\r\n### Before:\r\n\r\neasy to write and read, and works as expected\r\n\r\n```python\r\nimport numpy as np\r\nimport statsmodels.robust as smrb\r\nfrom functools import partial\r\n \r\n# median absolute deviation as a partial function\r\n# in order to demonstrate the issue with partial functions as aggregators\r\nmadc1 = partial(smrb.mad, c=1)\r\n\r\n# renaming and specifying the aggregators at the same time\r\n# note that I want to choose the resulting column names myself\r\n# for example \"totalxxxx\" instead of just \"sum\"\r\nmydfagg = mydf.groupby('cat').agg({\r\n    'energy': {\r\n        'totalenergy': 'sum',\r\n        'energyp98': lambda x: np.percentile(x, 98),  # lambda\r\n        'energyp17': lambda x: np.percentile(x, 17),  # lambda\r\n    },\r\n    'distance': {\r\n        'totaldistance': 'sum',\r\n        'averagedistance': 'mean',\r\n        'distancemad': smrb.mad,   # original function\r\n        'distancemadc1': madc1,  # partial function wrapping the original function\r\n    },\r\n})\r\n```\r\n\r\nresults in\r\n\r\n```\r\n          energy                             distance\r\n    totalenergy energyp98 energyp17 totaldistance averagedistance distancemad distancemadc1\r\ncat\r\nA           5.79     2.0364     1.8510           4.44            1.480     0.355825           0.240\r\nB           2.85     1.5930     1.3095           1.83            0.915     0.140847           0.095\r\nC           1.01     1.0100     1.0100           0.60            0.600     0.000000           0.000\r\n```\r\n\r\nand all is left is:\r\n\r\n```python\r\n# get rid of the first MultiIndex level in a pretty straightforward way\r\nmydfagg.columns = mydfagg.columns.droplevel(level=0)\r\n```\r\n\r\nHappy dance praising pandas  !\r\n\r\n### After\r\n\r\n```python\r\nimport numpy as np\r\nimport statsmodels.robust as smrb\r\nfrom functools import partial\r\n\r\n# median absolute deviation as a partial function\r\n# in order to demonstrate the issue with partial functions as aggregators\r\nmadc1 = partial(smrb.mad, c=1)\r\n\r\n# no way of choosing the destination's column names...\r\nmydfagg = mydf.groupby('cat').agg({\r\n    'energy': [\r\n    \t'sum',\r\n    \tlambda x: np.percentile(x, 98), # lambda\r\n    \tlambda x: np.percentile(x, 17), # lambda\r\n    ],\r\n    'distance': [\r\n    \t'sum',\r\n    \t'mean',\r\n    \tsmrb.mad, # original function\r\n    \tmadc1,   # partial function wrapping the original function\r\n    ],\r\n})\r\n```\r\n\r\nThe above breaks because the lambda functions will all result in columns named `<lambda>` which results in\r\n\r\n    SpecificationError: Function names must be unique, found multiple named <lambda>\r\n\r\n**Backward incompatible regression: one cannot apply two different lambdas to the same original column anymore.**\r\n\r\nIf one removes the `lambda x: np.percentile(x, 98)` from above, we get the same issue with the partial function which inherits the function name from the original function:\r\n\r\n    SpecificationError: Function names must be unique, found multiple named mad\r\n\r\nFinally, after overwriting the `name` attribute of the partial (for example with `madc1.name = 'madc1'`) we get:\r\n\r\n```\r\n    energy          distance\r\n       sum <lambda>      sum   mean       mad madc1\r\ncat\r\nA     5.79   1.8510     4.44  1.480  0.355825  0.240\r\nB     2.85   1.3095     1.83  0.915  0.140847  0.095\r\nC     1.01   1.0100     0.60  0.600  0.000000  0.000\r\n```\r\n\r\nwith still \r\n\r\n- one column missing (98th percentile)\r\n- the handling of the MultiIndex columns \r\n- and the renaming of the columns\r\n\r\nto deal with in separate step.\r\n\r\nThere is no control possible for the column names after aggregation, the best we can get in an automated way is some combination of original column name and the aggregate function's name like this:\r\n\r\n```python\r\nmydfagg.columns = [''.join(col) for col in mydfagg.columns]\r\n```\r\n\r\nwhich results in:\r\n\r\n```\r\n     energysum  energy<lambda>  distancesum  distancemean  distancemad distancemadc1\r\ncat\r\nA          5.79           1.8510          4.44          1.480      0.355825           0.240\r\nB          2.85           1.3095          1.83          0.915      0.140847           0.095\r\nC          1.01           1.0100          0.60          0.600      0.000000           0.000\r\n```\r\n\r\nand if you really need to have different names, you can do it like this:\r\n\r\n```python\r\nmydfagg.rename({\r\n    \"energysum\": \"totalenergy\",\r\n    \"energy<lambda>\": \"energyp17\",\r\n    \"distancesum\": \"totaldistance\",\r\n    \"distancemean\": \"averagedistance\"\r\n    }, inplace=True)\r\n```\r\n\r\nbut that means that you need to be careful to keep the renaming code (which must now be located at another place in the code) in sync with the code where the aggregation is defined...\r\n\r\nSad pandas user \ud83d\ude22  (which still loves pandas of course)\r\n\r\n---\r\n\r\nI am all in for consistency, and at the same time I deeply regret the deprecation of the aggregate and rename functionality. I hope the examples above make the pain points clear.\r\n\r\n---\r\n\r\n### Possible solutions\r\n\r\n- Un-deprecate the dict-of-dict relabeling functionality\r\n- Provide another API to be able to do it (but why should there be two methods for the same main purpose, namely aggregation?)\r\n- ??? (open to suggestions)\r\n\r\n---\r\n\r\nOptional read:\r\n\r\nWith respect to the aforementioned discussion in the pull request which has been going on already for a few months, I only recently realized one of the reasons why I am so bothered by this deprecation: \"aggregate and rename\"  is a natural thing to do with GROUP BY aggregations in SQL since in SQL you usually provide the destination column name directly next to the aggregation expression, e.g. `SELECT col1, avg(col2) AS col2mean, stddev(col2) AS col2var FROM mytable GROUP BY col1`.\r\n\r\nI'm **not** saying that Pandas should necessarily provide the same functionalities as SQL of course. But the examples provided above demonstrate why the dict-of-dict API was in my opinion a clean and simple solution to many use-cases.\r\n\r\n(* I don't personally agree that the dict-of-dict approach is complex.)"},
{"text": "These are only to provide some inside into a sparse dtype. These should in theory be encoded in the dtypes (in pandas2), but in reality these are just cluttering the API and actually an implementation detail."},
{"text": "xref https://github.com/pandas-dev/pandas/pull/17826\r\n\r\n`DatetimeIndex.toseries` has a `keeptz` keyword with a default of False (= dropping the timezone information). This stems from a time we could not store datetime tz data inside a series (with `keeptz=True`, you would get an object dtyped Series of Timestamps). Nowadays this just gives a series with `datetime64[ns, tz]` dtype, so it makes sense to make this the default and deprecate the keyword.\r\n\r\nSince it is an API change, ideally we first raise a warning that the behaviour will be changed (the default will change from False to True). You could then suppress this warning by passing `keeptz=True`. \r\nHowever, that means that we cannot directly deprecate the keyword itself, as we have to keep it for suppressing the warning / getting the future behaviour.\r\n\r\n"},
{"text": "There are a bunch of issues outstanding that relate to `Period` ops and comparison.  AFAICT making a decision about the comparison issue will snowball into resolving (some of) the ops issues.\r\n\r\n#5202 ENH: Period ops NaT & timedelta ops\r\n#10798 Date / Datetime in Period Index\r\n#6779 Adding Period and Offset not implemented\r\n#13077 ENH/API: Decide what to return Period - Period subtraction\r\n#17112 MultiIndex - Comparison with Mixed Frequencies (and other FUBAR)\r\n\r\nRight now two Period objects are comparable iff they equal `freq` attributes.  AFAICT this is to avoid guessing in cases where the \"correct\" answer is ambiguous.  But there are some other cases with an obviously correct answer.  Namely, if the `per1.endtime < per2.starttime`, then it should be the case that `per1 < per2` unambiguously.  This intuition also extends to `datetime` and `Timestamp` objects that do not lie between `per.starttime` and `per.endtime`. \r\n\r\nFor cases with overlap there are a couple of reasonable approaches.  My preferred approach is lexicographic: first compare `per1.starttime` with `per2.starttime`.  If they are equal, then compare the `endtime`s.  Then we treat `datetime` and `Timestamp` objects as analogous to zero-duration periods.\r\n\r\nThoughts?"},
{"text": "### Melt Enhancement\r\n\r\n**Summary**: This is a proposal with a pull request to enhance `melt` to simultaneously melt multiple groups of columns and to add functionality from `widetolong` along with better MultiIndexing capabilities. See [this notebook]( https://nbviewer.jupyter.org/github/tdpetrou/Machine-Learning-Books-With-Python/blob/master/melt%20enhancemenet.ipynb) for more examples.\r\n \r\n- Melts different groups of columns by passing a list of lists into `valuevars`. Each group gets melted into its own column. This feature replaces the need for lreshape.\r\n- When melting different groups of columns, groups do not have to be the same length. The shorter groups are filled with missing values.\r\n- Adds parameters `stubnames`(boolean), `prefix` and `sep` from function `widetolong`. It keeps the suffixes in separate columns and does not align them in the same way.\r\n- Can select any number of MultiIndex levels and greatly increase MultiIndex functionality\r\n- Works with repeated column names, which normally show up when selecting a subset of MultiIndex levels\r\n- Performance is ~30-40% faster than original `melt`, slightly slower than `lreshape` and much faster than `widetolong`\r\n\r\n\r\n\r\n```python\r\n>>> df = pd.DataFrame({'City': ['Houston', 'Austin', 'Hoover'],\r\n                   'State': ['Texas', 'Texas', 'Alabama'],\r\n                   'Name':['Aria', 'Penelope', 'Niko'],\r\n                   'Mango':[4, 10, 90],\r\n                   'Orange': [10, 8, 14], \r\n                   'Watermelon':[40, 99, 43],\r\n                   'Gin':[16, 200, 34],\r\n                   'Vodka':[20, 33, 18]},\r\n                 columns=['City', 'State', 'Name', 'Mango', 'Orange', 'Watermelon', 'Gin', 'Vodka'])\r\n\r\n      City    State      Name  Mango  Orange  Watermelon  Gin  Vodka\r\n0  Houston    Texas      Aria      4      10          40   16     20\r\n1   Austin    Texas  Penelope     10       8          99  200     33\r\n2   Hoover  Alabama      Niko     90      14          43   34     18\r\n```\r\nUse a list of lists in `valuevars` to melt the fruit and drinks\r\n```\r\n>>> df.melt(idvars=['City', 'State'], valuevars=[['Mango', 'Orange', 'Watermelon'], ['Gin', 'Vodka']], \r\n                    varname=['Fruit', 'Drink'], valuename=['Pounds', 'Ounces'])\r\n\r\n      City    State       Fruit  Pounds  Drink  Ounces\r\n0  Houston    Texas       Mango       4    Gin    16.0\r\n1   Austin    Texas       Mango      10    Gin   200.0\r\n2   Hoover  Alabama       Mango      90    Gin    34.0\r\n3  Houston    Texas      Orange      10  Vodka    20.0\r\n4   Austin    Texas      Orange       8  Vodka    33.0\r\n5   Hoover  Alabama      Orange      14  Vodka    18.0\r\n6  Houston    Texas  Watermelon      40    nan     NaN\r\n7   Austin    Texas  Watermelon      99    nan     NaN\r\n8   Hoover  Alabama  Watermelon      43    nan     NaN\r\n```\r\n`widetolong` functionality. Added parameters `stubnames`(boolean), `sep` and `suffix`.\r\n```\r\n>>> df1 = pd.DataFrame({'group': ['a', 'b', 'c'],\r\n                   'exp1':[4, 10, -9],\r\n                   'exp2': [10, 8, 14], \r\n                   'res1':[8, 5, 4],\r\n                   'res3':[11, 0, 7]}, columns=['group', 'exp1', 'exp2', 'res1', 'res3'])\r\n\r\n  group  exp1  exp2  res1  res3\r\n0     a      4     10      8     11\r\n1     b     10      8      5      0\r\n2     c     -9     14      4      7\r\n\r\n>>> df1.melt(idvars='group', valuevars=['exp','res'], stubnames=True, sep='')\r\n\r\n  group  variableexp  exp  variableres  res\r\n0     a             1    4             1    8\r\n1     b             1   10             1    5\r\n2     c             1   -9             1    4\r\n3     a             2   10             3   11\r\n4     b             2    8             3    0\r\n5     c             2   14             3    7\r\n```\r\nAlso adds support for all kinds of multiindexing\r\n```\r\n>>> df2 = df.copy()\r\n>>> df2.columns = pd.MultiIndex.fromarrays([list('aabbcccd'), list('ffffgggg'), df.columns], \r\n                                       names=[None, None, 'some vars'])\r\n\r\n                 a                  b            c                     d\r\n                 f                  f            g                     g\r\nsome vars     City    State      Name Mango Orange Watermelon  Gin Vodka\r\n0          Houston    Texas      Aria     4     10         40   16    20\r\n1           Austin    Texas  Penelope    10      8         99  200    33\r\n2           Hoover  Alabama      Niko    90     14         43   34    18\r\n\r\n>>> df2.melt(idvars=[('a', 'f', 'State')], \r\n           valuevars=[[('b', 'f', 'Name'), ('c', 'g', 'Watermelon')],\r\n                       [('b','f','Mango'), ('c','g', 'Orange'), ('d', 'g', 'Vodka')]],\r\n           varname=[['myvar1', 'myvar2', 'myvar3'],\r\n                     ['nextmyvar1', 'nextmyvar2', 'nextmyvar3']],\r\n           valuename=['some values', 'morevalues'])\r\n\r\n (a, f, State) myvar1 myvar2      myvar3 some values nextmyvar1 nextmyvar2  \\\r\n0         Texas      b      f        Name        Aria           b           f   \r\n1         Texas      b      f        Name    Penelope           b           f   \r\n2       Alabama      b      f        Name        Niko           b           f   \r\n3         Texas      c      g  Watermelon          40           c           g   \r\n4         Texas      c      g  Watermelon          99           c           g   \r\n5       Alabama      c      g  Watermelon          43           c           g   \r\n6         Texas    nan    nan         nan         NaN           d           g   \r\n7         Texas    nan    nan         nan         NaN           d           g   \r\n8       Alabama    nan    nan         nan         NaN           d           g   \r\n\r\n  nextmyvar3  morevalues  \r\n0       Mango            4  \r\n1       Mango           10  \r\n2       Mango           90  \r\n3      Orange           10  \r\n4      Orange            8  \r\n5      Orange           14  \r\n6       Vodka           20  \r\n7       Vodka           33  \r\n8       Vodka           18 \r\n```\r\n\r\n#### Problem description\r\n\r\nCurrently, there is poor support for simultaneous melting of multiple groups of columns. `lreshape` is old and undocumented. `widetolong` api does not match `melt` and it's slow.\r\n"},
{"text": "Following up on some discussion from #17554. \r\n\r\nThe main point of discussion was regarding merging the functionality of `cdaterange` into `bdaterange`.  This would involve adding the two additional keywords supported by `cdaterange` to `bdaterange` (`weekmask` and `holidays`), which would only be used (if provided) when a custom frequency is passed to `bdaterange`.   \r\n\r\nThis wouldn't be terribly invasive, as `cdaterange` is only top-level on master (not 0.20.3), and there is no documentation mentioning it.  Performing the merge would help keep the top-level API clean, and reduce the number of `daterange` related functions. On the user end, the change would amount to `cdaterange(...)` -> `bdaterange(..., freq='C')`.\r\n\r\nAn alternative would be to merge both `bdaterange` and `cdaterange` into `daterange`.  The only difference between `bdaterange` and `daterange` is the default value of `freq`, so from a technical standpoint it wouldn't add complexity to implement compared to just merging `cdaterange` -> `bdaterange`.\r\n\r\nThis would be a more invasive change though, as `bdaterange` currently is currently top-level, and there is some documentation mentioning it, so there'd be some additional work in terms of deprecation/doc modification.  It would provide the advantage of having only one `daterange` related function.  On the user end, the change would amount to `cdaterange(...)` -> `daterange(..., freq='C')`, and `bdaterange(...)` -> `daterange(..., freq='B')`.\r\n\r\nThe current plan is to do the `cdaterange` -> `bdaterange` merge, but we're interested in community input.\r\n\r\ncc @jorisvandenbossche \r\n"},
{"text": "For my own purposes, I am considering writing classes to represent PMFs and CDFs using Pandas Series as the implementation, and an API similar to what I did in the thinkstats2 library (but made more stylistically consistent with Pandas).\r\n\r\nHas there been any discussion of adding something like this to Pandas?  If I develop it, would you be interested in seeing a proposal to include it?  (I ask now because it might influence some design decisions if I am targeting inclusion in Pandas).\r\n\r\n\r\n"},
{"text": "xref #4916, #17312\r\n\r\nexample of where it is needed\r\n```python\r\nitems = [\r\n ('a', [1,2,3]),\r\n ('b', ['j', 'k', 'l'])\r\n]\r\n\r\ndf = pd.DataFrame.fromitems(items)\r\ndf\r\nOut[3]: \r\n   a  b\r\n0  1  j\r\n1  2  k\r\n2  3  l\r\n```\r\n\r\nWe possibly could deprecate this to shrink the construction api - I find `DataFrame(dict(items))` easier to think about / remember (item is a pretty generic term), only downside I see is it discards order on <3.6\r\n"},
{"text": "Two proposals:\r\n\r\n## Consolidate all inference to the `Index` constructor\r\n\r\n- Retain `Index(...)` inferring the best container for the data passed\r\n- Remove `MultiIndex(data)` returning an `Index` when data is a list of length-1 tuples (xref https://github.com/pandas-dev/pandas/pull/17236)\r\n\r\n## Passing `dtype=object` disables inference\r\n\r\n`Index(..., dtype=object)` disable all inference. So `Index([1, 2], dtype=object)` will give you an `Index` instead of `Int64Index`, and `Index([(1, 'a'), (2, 'b')], dtype=object)` an `Index` instead of `MultiIndex`, etc.\r\n\r\n(original post follows)\r\n\r\n---\r\n\r\n\r\nOr how much magic should we have in the Index constructors? Currently we infer the index type from the data, which is often convenient, but sometime difficult to reason able behavior. e.g. `hashtuples` currently doesn't work if your tuples all happen to be length 1, since it uses a MultiIndex internally.\r\n\r\nDo we want to make our `Index` constructors more predictable? For reference, here are some examples:\r\n\r\n```python\r\n>>> import pandas as pd\r\n# 1.) Index -> MultiIndex\r\n>>> pd.Index([(1, 2), (3, 4)])\r\nMultiIndex(levels=[[1, 3], [2, 4]],\r\n           labels=[[0, 1], [0, 1]])\r\n\r\n>>> pd.Index([(1, 2), (3, 4)], tupleizecols=False)\r\nIndex([(1, 2), (3, 4)], dtype='object')\r\n\r\n# 2.) Index -> Int64Index\r\n>>> pd.Index([1, 2, 3, 4, 5])\r\nInt64Index([1, 2, 3, 4, 5], dtype='int64')\r\n\r\n# 3.) Index -> RangeIndex\r\n>>> pd.Index(range(1, 5))\r\nRangeIndex(start=1, stop=5, step=1)\r\n\r\n# 4.) Index -> DatetimeIndex\r\n>>> pd.Index([pd.Timestamp('2017'), pd.Timestamp('2018')])\r\nDatetimeIndex(['2017-01-01', '2018-01-01'], dtype='datetime64[ns]', freq=None)\r\n\r\n# 5.) Index -> IntervalIndex\r\n>>> pd.Index([pd.Interval(3, 4), pd.Interval(4, 5)])\r\nIntervalIndex([(3, 4], (4, 5]]\r\n              closed='right',\r\n              dtype='interval[int64]')\r\n\r\n# 5.) MultiIndex -> Index\r\n>>> pd.MultiIndex.fromtuples([(1,), (2,), (3,)])\r\nInt64Index([1, 2, 3], dtype='int64')\r\n```\r\n\r\nOf these, I think the first (`Index -> MultiIndex` if you have tuples) and the last (`MultiIndex -> Index` if you're tuples are all length 1) are undesirable. The `Index -> MultiIndex` one has the `tupleizecols` keyword to control this behavior. In https://github.com/pandas-dev/pandas/pull/17236 I add an analogous keyword to the MI constructor. The rest are probably fine, but I don't have any real reason for saying that `[1, 2, 3]` magically returning an Int64Index is ok, but `[(1, 2), (3, 4)]` returning a `MI` isn't (maybe the difference between a MI and Index is larger than the difference between an Int64Index and Index?). I believe that in either the `RangeIndex` or `IntervalIndex` someone (@shoyer?) had objections to overloading the `Index` constructor to return the specialized type.\r\n\r\nSo, what should we do about these? Leave them as is? Deprecate the type inference? My vote is for merging #17236 and leaving everything else as is. To me, it's not worth breaking API over.\r\n\r\ncc @jreback, @jorisvandenbossche, @shoyer "},
{"text": "I am opening this issue because I want (to try) to pursue this in GeoPandas to add a custom `GeometryBlock` (work together with Matthew Rocklin in https://github.com/geopandas/geopandas/pull/467, ultra short motivation: we want to store integers (pointers to C objects) in a column but box it to shapely python objects when the user interacts with the column (repr, accessing element, ..))\r\n\r\nI am of course free to try this :-), but I wanted to raise this because it has some consequences. With the \"allow external libraries\" in the issue title, I mean the following:\r\n\r\n- agree that this is 'OK' which means that we try to not break the Block API (to a certain extent of course, or try to change it backwards compatible)\r\n- accept some changes to pandas to make this possible where needed (as long as they are only some internal clean-ups)\r\n\r\nI don't think we plan many internal refactorings for pandas 0.x / 1.x, so on that regard the Block API should/could remain rather stable (of course for 2.0 this is a whole other issue).\r\n\r\nSo this issue can serve as general discussion for this (or if people have input or feedback) and as a reference for when changes in pandas are made for this.\r\n\r\ncc @pandas-dev/pandas-core "},
{"text": "I propose a more generic type checking function for pandas, based on subclass checking of dtype.types.\r\n\r\nQuite often (most of the time?) when checking dtypes, you simply want to check if a dtype's type is a subclass of some base numpy/pandas type, but today you can't take such a subclass approach.\r\n\r\nMost type checking is possible today using functions in ``pd.api.types`` using various functions, but I propose a single, more generic ``isdtypesubclass`` function (name mirrors ``isdtypeequal``), that checks if a ``Series.dtype.type``, a ``dtype.type`` a numpy/pandas type etc.  is a subclass of another.\r\n\r\nI find this approach very pythonic and practically useful. The downside is that it will have overlaps with other type checking functions, but as this function allows making very specific type subclass checks, I feel it would add value to pandas. Should I proceed with this (write tests, docs etc) or do you feel that this function wouldn't add value?\r\n\r\nSee code example below.\r\n\r\n```python\r\n    import numpy as np\r\n    import pandas as pd\r\n\r\n    def isdtypesubclass(source, target):\r\n        \"\"\"\r\n        Check if a dtype.type is a subclass of another dtype.type.\r\n        \r\n        For any given args, the relevant dtype.type will be found,\r\n        so you can give this function different numpy/pandas objects to compare,\r\n        e.g. compare a Series to a numpy type. Strings will be coerced into the relevant types\r\n        per the usual rules of pandas/numpy.\r\n\r\n        Parameters\r\n        ----------\r\n        source : The first object to compare\r\n        target : The second object to compare\r\n\r\n        Returns\r\n        ----------\r\n        boolean : Whether or not ``source`` dtype.type is a subclass of ``target`` dtype.type.\r\n\r\n        Examples\r\n        --------\r\n        >>> import pandas as pd, numpy as np\r\n        >>> isdtypesubclass(pd.Series([1, 2, 3]), np.integer)\r\n        True\r\n        >>> isdtypesubclass(np.dtype('float32'), np.number)\r\n        True\r\n        >>> isdtypesubclass('int64', np.integer)\r\n        True\r\n        >>> isdtypesubclass('int64', 'int32')\r\n        False\r\n        >>> isdtypesubclass(int, float)\r\n        False\r\n        >>> isdtypesubclass(object, \"category\")\r\n        False\r\n        >>> isdtypesubclass(pd.Categorical([]), \"category\")\r\n        True\r\n        >>> isdtypesubclass(np.datetime64, \"datetime64\")\r\n        True\r\n        >>> isdtypesubclass(pd.daterange(start='2017-06-23', periods=5), 'datetime64[ns]')\r\n        True\r\n        \"\"\"\r\n\r\n        def gettype(arg):\r\n            \"\"\"Get the dtype.type of ``arg``.\r\n            \"\"\"\r\n            if isinstance(arg, pd.DataFrame):\r\n                raise ValueError(\"Arg is a dataframe.\"\r\n                                 \" Check each column individually\")\r\n            elif isinstance(arg, (pd.Series, pd.Index,\r\n                                  pd.Categorical, pd.Interval,\r\n                                  pd.DatetimeIndex)):\r\n                return arg.dtype.type\r\n            elif isinstance(arg, np.dtype):\r\n                return arg.type\r\n            elif np.issubclass(arg, np.generic):\r\n                return arg\r\n            dt = pd.core.dtypes.dtypes\r\n            dtypes = (dt.CategoricalDtype, dt.DatetimeTZDtype,\r\n                      dt.IntervalDtype, dt.PeriodDtype)\r\n            for dtype in dtypes:\r\n                if arg in (dtype, dtype.name, dtype.str, dtype.type):\r\n                    return dtype.type\r\n            return np.dtype(arg).type\r\n\r\n        return issubclass(gettype(source), gettype(target))\r\n```\r\n"},
{"text": "PR #15998 removed the `pandas.types` module which AFAICT wasn't deprecated.\r\n\r\nThis breaks 3rd party libraries such as `dask` without giving them any warning or time to adjust to the new api.\r\n\r\nIMHO a shim should be put in place with a deprecation warning to give 3rd party libraries a chance to adjust to the new api"},
{"text": "Currently, the public `pandas.api.types` modules holds the following functions:\r\n\r\n```\r\nIn [11]: [f for f in dir(pd.api.types) if not f.startswith('')]\r\nOut[11]: \r\n['inferdtype',\r\n 'isanyintdtype',\r\n 'isbool',\r\n 'isbooldtype',\r\n 'iscategorical',\r\n 'iscategoricaldtype',\r\n 'iscomplex',\r\n 'iscomplexdtype',\r\n 'isdatetime64anydtype',\r\n 'isdatetime64dtype',\r\n 'isdatetime64nsdtype',\r\n 'isdatetime64tzdtype',\r\n 'isdatetimetz',\r\n 'isdictlike',\r\n 'isdtypeequal',\r\n 'isextensiontype',\r\n 'isfilelike',\r\n 'isfloat',\r\n 'isfloatdtype',\r\n 'isfloatingdtype',\r\n 'ishashable',\r\n 'isint64dtype',\r\n 'isinteger',\r\n 'isintegerdtype',\r\n 'isinterval',\r\n 'isintervaldtype',\r\n 'isiterator',\r\n 'islistlike',\r\n 'isnamedtuple',\r\n 'isnumber',\r\n 'isnumericdtype',\r\n 'isobjectdtype',\r\n 'isperiod',\r\n 'isperioddtype',\r\n 'isre',\r\n 'isrecompilable',\r\n 'isscalar',\r\n 'issequence',\r\n 'issignedintegerdtype',\r\n 'issparse',\r\n 'isstringdtype',\r\n 'istimedelta64dtype',\r\n 'istimedelta64nsdtype',\r\n 'isunsignedintegerdtype',\r\n 'pandasdtype',\r\n 'unioncategoricals']\r\n```\r\n\r\nTwo questions I would like to discuss a bit:\r\n\r\n- **Do we need to expose all of them?**\r\n  Putting the functions here, means keeping them stable. So we could also limit it to the more pandas specific ones. For example, `isre`, `isdictlike`, `isiterator`, etc are general utility functions not actually related to pandas specifics. Of course it can be handy for other projects that they can use them instead of implementing it themselves, but it increases the number of functions we have to keep stable.\r\n\r\n\r\n~~- **Do we need to expose more of the pandas extension types API?** (xref #16099)~~\r\n\r\n  From comment of @wesm (https://github.com/pandas-dev/pandas/pull/15541#issuecomment-286008496):\r\n\r\n  > I've been running into these internal API / external API issues lately (e.g. needing to use `DatetimeTZDtype` in an external library), so it might be worth documenting the pandas < 0.19 way to get some of these APIs\r\n\r\n  It is how the dtypes will be implemented in pandas 1.x, so shouldn't we just expose this officially and put it in `types`? `pyarrow` uses the datetime tz one, and in https://github.com/pandas-dev/pandas/pull/16015 we will probably also like to add `CategoricalDtype` \r\n  Not sure if the others (interval, period) are also needed.\r\n  "},
{"text": "this may seem counter intutive, but this would allow us to store nulls as well (efficiently, rather than as ``object`` dtype), or casting to floats.\r\n\r\nI am sure if this would really be possible w/o some API breaks, so will have a look."},
{"text": "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [2]: s = pd.Series(range(8), index=pd.MultiIndex.fromproduct([[1,2], [3,4], [3,4]],\r\n                                                                 names=['a', 'b', 'c']))\r\n\r\nIn [3]: s.loc[s.index] # Works as expected\r\nOut[3]: \r\na  b  c\r\n1  3  3    0\r\n      4    1\r\n   4  3    2\r\n      4    3\r\n2  3  3    4\r\n      4    5\r\n   4  3    6\r\n      4    7\r\ndtype: int64\r\n\r\nIn [4]: s.loc[s.iloc[2:-1].index] # Works as expected\r\nOut[4]: \r\na  b  c\r\n1  4  3    2\r\n      4    3\r\n2  3  3    4\r\n      4    5\r\n   4  3    6\r\ndtype: int64\r\n\r\nIn [5]: s.loc[s.index.droplevel('c')] # Just reindexes... weird\r\nOut[5]: \r\n1  3   NaN\r\n   3   NaN\r\n   4   NaN\r\n   4   NaN\r\n2  3   NaN\r\n   3   NaN\r\n   4   NaN\r\n   4   NaN\r\ndtype: float64\r\n\r\nIn [6]: s.loc[s.index.droplevel(['b', 'c']), :] # Works (flat index)\r\nOut[6]: \r\na  b  c\r\n1  3  3    0\r\n      4    1\r\n   4  3    2\r\n      4    3\r\n2  3  3    4\r\n      4    5\r\n   4  3    6\r\n      4    7\r\ndtype: int64\r\n\r\nIn [7]: s.loc[s.index.droplevel(['b', 'c'])] #... but fails if I use the shortened notation!\r\n[...]\r\nTypeError: unhashable type: 'Int64Index'\r\n\r\nIn [8]: s.loc[s.swaplevel('b', 'c')] # Works\r\nOut[8]: \r\na  b  c\r\n1  3  3    0\r\n      4    1\r\n   4  3    2\r\n      4    3\r\n2  3  3    4\r\n      4    5\r\n   4  3    6\r\n      4    7\r\ndtype: int64\r\n\r\nIn [9]: s.loc[s.index.swaplevel('b', 'c')]  # Different result! (reindexes)\r\nOut[9]: \r\na  c  b\r\n1  3  3    0\r\n   4  3    2\r\n   3  4    1\r\n   4  4    3\r\n2  3  3    4\r\n   4  3    6\r\n   3  4    5\r\n   4  4    7\r\ndtype: int64\r\n\r\nIn [10]: s.loc[pd.MultiIndex.fromproduct([[1,2], [3], [4]],\r\n                                          names=['a', 'c', 'b'])] # Does not respect column names!\r\nOut[10]: \r\na  c  b\r\n1  3  4    1\r\n2  3  4    5\r\ndtype: int64\r\n\r\n\r\n```\r\n#### Problem description\r\n\r\nThis clearly needs a unified approach (and I can try).\r\n\r\n#### Expected Output\r\n\r\nI guess most expected outputs above are obvious, except for ``In [10]:`` (and maybe ``In [5]:``, which however is already discussed [elsewhere](15452)). That is: it is not obvious whether level names in the indexer should be matched to level names in the indexed, when both are set ([see this comment](https://github.com/pandas-dev/pandas/pull/15425#issuecomment-280500783)). It would probably be more ``pandas``-ish if they were.\r\n\r\nIn other terms, while there is no doubt that\r\n\r\n``` python\r\nOut[10]: \r\na  c  b\r\n1  3  4    1\r\n2  3  4    5\r\ndtype: int64\r\n```\r\nis wrong, we must decide whether we want\r\n``` python\r\nOut[10]: \r\na  b  c\r\n1  3  4    1\r\n2  3  4    5\r\ndtype: int64\r\n```\r\nor\r\n``` python\r\nOut[10]: \r\na  b  c\r\n1  4  3    2\r\n2  4  3    6\r\ndtype: int64\r\n```\r\n\r\n#### Output of ``pd.showversions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.7.0-1-amd64\r\nmachine: x8664\r\nprocessor: \r\nbyteorder: little\r\nLCALL: None\r\nLANG: itIT.utf8\r\nLOCALE: itIT.UTF-8\r\n\r\npandas: 0.19.0+478.g12f2c6a\r\npytest: 3.0.6\r\npip: 8.1.2\r\nsetuptools: 28.0.0\r\nCython: 0.23.4\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.4.8\r\npatsy: 0.3.0-dev\r\ndateutil: 2.5.3\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nfeather: None\r\nmatplotlib: 2.0.0rc2\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: 0.999\r\nhttplib2: 0.9.1\r\napiclient: 1.5.2\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandasdatareader: 0.2.1\r\n\r\n</details>\r\n"},
{"text": "### Summary (*)\r\nQuoteRepository and API quote requests do not return items or extension attributes for quotes that are not active. More specifically, QuoteRepository and API quote requests return all quote information **except** items and extension attributes, for quotes that are not active. The data should be returned whether the quote is active or not.\r\n\r\nVerified on Magento 2.1.15; all versions 2.1.0 - 2.3.0 affected.\r\n\r\n### Examples (*)\r\nSteps to reproduce: \r\n\r\n1. Find or create an inactive quote: `select * from quote where isactive=0 limit 1;`\r\n2. Query the API to fetch the inactive quote: `GET https://example.com/rest/V1/carts/2924`\r\n3. See that the results indicate an item but contain no actual items.\r\n```\r\n{\r\n    \"id\": 2924,\r\n    \"createdat\": \"2018-10-17 01:13:23\",\r\n    \"updatedat\": \"2018-12-17 16:34:24\",\r\n    \"isactive\": false,\r\n    \"isvirtual\": false,\r\n    \"itemscount\": 1,\r\n    \"itemsqty\": 1,\r\n    \"customer\": {\r\n...\r\n```\r\n4. Change the quote to active and repeat the API request. Note that it now contains the item(s):\r\n```\r\n{\r\n    \"id\": 2924,\r\n    \"createdat\": \"2018-10-17 01:13:23\",\r\n    \"updatedat\": \"2018-12-17 16:34:24\",\r\n    \"isactive\": true,\r\n    \"isvirtual\": false,\r\n    \"items\": [\r\n        {\r\n            \"itemid\": 4749,\r\n            \"sku\": \"ADDONS-USER-LICENSE-SUB\",\r\n            \"qty\": 1,\r\n            \"name\": \"Additional User License\",\r\n            \"price\": 500,\r\n            \"producttype\": \"simple\",\r\n            \"quoteid\": \"2924\"\r\n        }\r\n    ],\r\n    \"itemscount\": 1,\r\n    \"itemsqty\": 1,\r\n    \"customer\": {\r\n...\r\n```\r\n\r\n### Proposed solution\r\nThe issue is caused by this check in [\\Magento\\Quote\\Model\\QuoteRepository\\LoadHandler->load()](https://github.com/magento/magento2/blob/2.3/app/code/Magento/Quote/Model/QuoteRepository/LoadHandler.php#L42):\r\n```\r\n    public function load(CartInterface $quote)\r\n    {\r\n        if (!$quote->getIsActive()) {\r\n            return $quote;\r\n        }\r\n```\r\n\r\nI suggest removing this if() statement entirely. The quote in question is already loaded and behaves normally otherwise, whether it is active or not. This active check and return *only* prevent the quote items and extension attributes from being assigned properly and returned with QuoteRepository and API requests. This results in unexpected behavior for inactive quotes compared to active ones."},
{"text": "<!---\r\nPlease review our guidelines before adding a new issue: https://github.com/magento/magento2/wiki/Issue-reporting-guidelines\r\nFields marked with (*) are required. Please don't remove the template.\r\n-->\r\nI try to assign a customer to a quote using the REST API. \r\nThis is a guest-endpoint so it should not require authentication. However, when I try to use it, the API tells me I am not allowed to do so\r\n\r\n### Preconditions (*)\r\n<!---\r\nProvide the exact Magento version (example: 2.2.5) and any important information on the environment where bug is reproducible.\r\n-->\r\n1.Magento 2.2.7\r\n\r\n### Steps to reproduce (*)\r\n<!---\r\nImportant: Provide a set of clear steps to reproduce this bug. We can not provide support without clear instructions on how to reproduce.\r\n-->\r\n1.Setup a quote as a guest and get it's masked ID (in this example it's Ai6z71bxdK3igp04qLBujM7RErkrhem6\r\n2.Create a customer and get it's ID (in this example it's 10)\r\n3.Create the API call. For example:\r\n\r\ncurl -X PUT -H \"Content-Type: application/json\" \\\r\n    -d '{\"customerId\":10}' \\\r\n    \"http://www.domain.com/rest/V1/guest-carts/Ai6z71bxdK3igp04qLBujM7RErkrhem6/\"\r\n\r\n### Expected result (*)\r\n<!--- Tell us what do you expect to happen. -->\r\n1.A logic response from the API and the customer saved to the quote (as documented)\r\n\r\n### Actual result (*)\r\n<!--- Tell us what happened instead. Include error messages and issues. -->\r\n1. 401 response with the following message:\r\n\r\n{\r\n  \"message\": \"Consumer is not authorized to access %resources\",\r\n  \"parameters\": {\r\n    \"resources\": \"self\"\r\n  }\r\n\r\n"},
{"text": "#### Problem description\r\nThis is an enhancement request to allow handling and saving comment strings with DataFrame text file IO. Two related stackoverflow questions about such feature: \r\nhttp://stackoverflow.com/questions/39724298/pandas-extract-comment-lines\r\nhttp://stackoverflow.com/questions/29233496/write-comments-in-csv-file-with-pandas\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n```python\r\ndf = pd.readcsv(\"mydata.csv\", comment=\"#\")\r\ndf.comment\r\n\"this is a comment from mydata.csv file\"\r\ndf.tocsv(\"output.csv\", comment=\"#\") # saves the `comment` string by pasting \"#\" before its each line and putting it before the table. \r\n```\r\n\r\n#### Output of ``pd.showversions()``\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x8664\r\nprocessor: i386\r\nbyteorder: little\r\nLCALL: None\r\nLANG: enUS.UTF-8\r\nLOCALE: enUS.UTF-8\r\n\r\npandas: 0.19.0\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 29.0.1\r\nCython: 0.24\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.0.0\r\nsphinx: 1.4.5\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: None\r\nhttplib2: 0.9.2\r\napiclient: 1.5.1\r\nsqlalchemy: 1.0.14\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandasdatareader: None\r\n</details>\r\n"},
{"text": "Currently `readcsv` has some ways to deal with \"bad lines\" (bad in the sense of too many or too few fields compared to the determined number of columns):\r\n\r\n- by default, it will error for too many fields, and fill with NaNs for too few fields\r\n- with `errorbadlines=false` rows with too many fields will be dropped instead of raising an error (and in that case, `warnbadlines` controls to get a warning or not) \r\n- with `usecols` you can select certain columns, and in this way deal with rows with too many fields.\r\n\r\nSome possibilities are missing in this scheme:\r\n\r\n- \"process\" bad lines with too many fields, i.e. drop the excessive fields instead of either raising an error or dropping the full row (discussed in #9549)\r\n- getting a warning or error with too few fields instead of automatically filling with NaNs (asked for in #9729), or dropping those rows\r\n\r\nApart from that, https://github.com/pandas-dev/pandas/issues/5686 makes the request to be able to specify a custom function to process a bad line, to have even more control.\r\n\r\nIn https://github.com/pandas-dev/pandas/issues/9549#issuecomment-76498787 (and surrounding comments) there was some discussion about how to integrate this, and some idea from there from @jreback and @selasley:\r\n\r\nProvide more fine grained control in a new keyword (and deprecate `errorbadlines`):\r\n\r\n```\r\nbadlines='error'|'warn'|'skip'|'process'\r\n```\r\n\r\nor leave out `'warn'` and keep `warnbadlines` to be able to combine a warning with both 'skip' and 'process'.\r\n\r\nWe should further think about whether we can integrate this with the case of too few fields and not only too many.\r\n\r\nI think it would be nice to have some better control here, but we should think a bit about the best API for this.\r\n "},
{"text": "Inspired by the timedelta plotting issue, I thought to look again at our timeseries plotting machinery. We know it is quite complex, and due to that several bugs, inconsistencies or unexpected behaviours exist (eg different results depending on order of plotting several serieses, wrong results when combining different types of time series, among others https://github.com/pandas-dev/pandas/issues/9053, https://github.com/pandas-dev/pandas/issues/6608, https://github.com/pandas-dev/pandas/issues/14322, ..). \r\nThere has been some discussion related to this on the tsplot refactor PR of @sinhrks https://github.com/pandas-dev/pandas/pull/7670 (not merged).\r\n\r\nOne of the reasons of the complexities is the distinction between 'irregular' and 'regular' time series (see eg https://github.com/pandas-dev/pandas/pull/7670#issuecomment-149235874):\r\n\r\n- 'regular' time series plotting is based on Periods, and is used for timeseries with a freq or inferredfreq (and also for periods)\r\n- 'irregular' time series plotting is based on the default matplotlib's handling of dates, i.e. converting to 'numerical values' (floats representing time in days since 0001-01-01, http://matplotlib.org/api/datesapi.html). You can always get this also for regular timeseries by passing `xcompat=True`.\r\n\r\nSo part of the problems and confusions comes from the differences between both (eg different label formatting) and from combining those two. Leading to the question:\r\n\r\n### Do we need both types of timeseries plotting?\r\n\r\nThe question is what the reason is that we convert DatetimeIndex to periods for plotting. The reasons I can think of:\r\n\r\n- **Performance**. Currently, the regular plotting is faster (so for a regular series `ts.plot()`is faster as `ts.plot(xcompat=True)`). However, I think this could be solved as most of the time is spent in converting the datetimes to floats (which should be vectorizable).\r\n- **Nicer tick label locations and formatting**. This is a clear plus, our (convoluted) ticklocators and formatters give much nicer results as the default matplotlib (IMO)\r\n\r\nOthers reasons that I am missing?\r\n\r\nBut, there are also clear drawbacks. Apart from the things mentioned above, you sometimes get clearly wrong behaviour: see eg the plot in https://github.com/pandas-dev/pandas/pull/7670#issuecomment-57410361. In this case, the dates somewhere within a month, are snapped to the month edges when first a regular series is plotted with monthy frequency. \r\nAnother example of 'wrong' plotting is a yearly series (bug with freq 'A-dec', so end of year) plotted in the beginning of a year. See http://nbviewer.jupyter.org/gist/jorisvandenbossche/c0c68dce2fa02f1dfc4a8c343ec88cb6. But of course, in many cases, this behaviour is can also be the desired behaviour.\r\n\r\nBut do we need both? Would we want, if possible, to unify into one approach?\r\n\r\n### Can we unify both approaches?\r\n\r\nCan we just use the matplotlib floats for timeseries plotting? Or always use the period-based machinery?\r\n\r\n* **Using matplotlib's float-based plotting**\r\n  * Do we want this? It will give slightly different behaviour for certain 'regular' cases.\r\n  * Assuming we can implement a similar tick locator/formatter comparable to period-based one. But, this may be impossible and the reason we have the current situation?\r\n  * But we could keep the PeriodConverter for purely plotting actual Periods\r\n  * Problem: float64 representing days can only give a precision of \\~5\u00b5s, not up to 1ns (note: the period-based plotting can also not handle ns, but can handle 1\u00b5s precision).\r\n* **Using period-based plotting for all timeseries**\r\n  * Do we want this? (deviates more from matplotlib -> larger difference in plotting dates with and without importing pandas)\r\n  * What prevents us from converting an irregular timeseries to Periods? I would think we can find some common freq in almost all cases? (just a high-precision freq if needed)\r\n* **Or create a new converter based on datetime64[ns] (so int64)**?\r\n  * Instead of using matplotlibs floats, and instead of varying freq Periods (at least for DatetimeIndex)\r\n  * Again, assuming we can have nice tick label locator/formatting for this\r\n\r\ncc @pandas-dev/pandas-core (especially @TomAugspurger and @sinhrks, I think you haven been most involved in plotting code recently, or @wesm for historical viewpoint)\r\nI know it's a long issue, but if you could give it a read and give your thoughts on this, very welcome!\r\n"},
{"text": "#### Problem description\r\n\r\nThe `quantile` method is currently not supported on `resample`. While `resample(...).agg(lambda x: x.quantile(..))` works fine, it would be nice to add the `resample(..).quantile(..)` shortcut.\r\n\r\n#### Code Sample\r\n\r\n```\r\nIn [19]: ts = pd.Series(range(20), index=pd.daterange('2016-01-01', periods=20))\r\n\r\nIn [21]: ts.resample('W').agg(lambda x: x.quantile(0.75))\r\nOut[21]: \r\n2016-01-03     1.5\r\n2016-01-10     7.5\r\n2016-01-17    14.5\r\n2016-01-24    18.5\r\nFreq: W-SUN, dtype: float64\r\n```\r\n\r\nSo it would be nice the following would give the same result:\r\n```\r\nIn [22]: ts.resample('W').quantile(0.75)\r\n/home/joris/miniconda3/envs/dev/bin/ipython:1: FutureWarning: \r\n.resample() is now a deferred operation\r\nYou called quantile(...) on this deferred object which materialized it into a series\r\nby implicitly taking the mean.  Use .resample(...).mean() instead\r\n  #!/home/joris/miniconda3/envs/dev/bin/python\r\nOut[22]: 14.25\r\n```\r\n\r\nThe fact that this currently implicitly takes the mean before calculating the quantile (`ts.resample('W').mean().quantile(0.75)`) would make this change slightly API breaking.\r\n\r\n\r\nUsing pandas master, 0.19.0+289.g1bf94c8\r\n"},
{"text": "There are at least three things that many of the IO methods must deal with: reading from URL, reading/writing to a compressed format, and different text encodings. It would be great if all io functions where these factors were relevant could use the same code (consolidated codebase) and expose the same options (uniform API).\r\n\r\nIn https://github.com/pandas-dev/pandas/pull/14576, we consolidated the codebase but more consolidation is possible. In [`io.common.py`](https://github.com/pandas-dev/pandas/blob/dc4b0708f36b971f71890bfdf830d9a5dc019c7b/pandas/io/common.py), there are three functions that must be sequentially called to get a file-like object: `getfilepathorbuffer`, `infercompression`,  and `gethandle`. This should be consolidated into a single function, which can then delegate to sub functions.\r\n\r\nCurrently, pandas supports the [following io methods](http://pandas-docs.github.io/pandas-docs-travis/io.html). First for reading:\r\n\r\n- [ ] readcsv\r\n- [ ] readexcel\r\n- [ ] readhdf\r\n- [ ] readfeather\r\n- [ ] readsql\r\n- [ ] readjson\r\n- [ ] readmsgpack (experimental)\r\n- [ ] readhtml\r\n- [ ] readgbq (experimental)\r\n- [ ] readstata\r\n- [ ] readsas\r\n- [ ] readclipboard\r\n- [ ] readpickle\r\n\r\nAnd then for writing:\r\n\r\n- [ ] tocsv\r\n- [ ] toexcel\r\n- [ ] tohdf\r\n- [ ] tofeather\r\n- [ ] tosql\r\n- [ ] tojson\r\n- [ ] tomsgpack (experimental)\r\n- [ ] tohtml\r\n- [ ] togbq (experimental)\r\n- [ ] tostata\r\n- [ ] toclipboard\r\n- [ ] topickle\r\n\r\nSome of these should definitely use the consilidated/uniform API, such as `readcsv`, `readhtml`, `readpickle`, `readexcel`.\r\n\r\nSome functions perhaps should be kept separate, such as `readfeather` or `readclipboard`."},
{"text": "This is to discuss pushing the `Categorical.categories` and\r\n`Categorical.ordered` information into the extension type `CategoricalDtype`.\r\n\r\n```python\r\npd.CategoricalDtype(categories, ordered=False)\r\n```\r\n\r\nNote that there is no `values` argument. This is a type constructor, that\r\nisn't attached to any specific `Categorical` instance.\r\n\r\n## Why?\r\n\r\nSeveral times now (`readcsv(..., dtype=...)`, `.astype(...)`, `Series([], dtype=...)`)\r\nwe have places where we accept `dtype='category'` which takes the values\r\nin the method (the series, or column from the CSV, etc.)\r\nand hands it off to the *value* constructor, with no control over the\r\n`categories` and `ordered` arguments.\r\n\r\n```python\r\nCategorical(values, categories=None, ordered=False)\r\n```\r\n\r\nThe proposal here would add the `categories` and `ordered`\r\nattributes / arguments to `CategoricalDtype` and provide a common API\r\nfor specifying non-default parameters for the `Categorical` constructor\r\nin methods like `readcsv`, `astype`, etc.\r\n\r\n\r\n```python\r\nt = pd.CategoricalDtype(['low', 'med', 'high'], ordered=True)\r\npd.readcsv('foo.csv', dtype={'A': int, 'B': t)\r\npd.Series(['high', 'low', 'high'], dtype=t)\r\n\r\ns = pd.Series(['high', 'low', 'high'])\r\ns.astype(t)\r\n```\r\n\r\nWe would continue to accept `dtype='category'`.\r\n\r\nThis becomes even more import when doing operations on larger than memory datasets with something like `dask` or even (`readcsv(..., chunksize=N)`). Right now you don't have an easy way to specify the `categories` or `ordered` for columns (assuming you know them ahead of time).\r\n\r\n## Issues\r\n\r\n1. `CategoricalDtype` currently isn't part of the public API. Which methods\r\non it do we make public?\r\n2. Equality semantics: For backwards compat, I think all instances\r\nof `CategoricalDtype` should compare equal with all others. You can use\r\nidentity to check if two types are the same\r\n\r\n```python\r\nt1 = pd.CategoricalDtype(['a', 'b'], ordered=True)\r\nt2 = pd.CategoricalDtype(['a', 'b'], ordered=False)\r\n\r\nt1 == t2  # True\r\nt1 is t2  # False\r\nt1 is t1  # True\r\n```\r\n\r\n3. Should the `categories` argument be required? Currently `dtype='category'`\r\nsays 1.) infer the categories based on the *values*, and 2.) it's unordered.\r\nWould `CategoricalDtype(None, ordered=False)` be allowed?\r\n4. Strictness? If I say\r\n\r\n```python\r\npd.Series(['a', 'b', 'c'], dtype=pd.CategoricalDtype(['a', 'b']))\r\n```\r\n\r\nWhat happens? I would probably expect a `TypeError` or `ValueError` as `c`\r\nisn't \"supposed\" to be there. Or do we replace `'c'` with `NA`? Should\r\n`strict` be another parameter to `CategoricalDtype` (I don't think so).\r\n\r\nI'm willing to work on this over the next couple weeks.\r\n\r\nxref https://github.com/pandas-dev/pandas/issues/14676 (astype)\r\nxref https://github.com/pandas-dev/pandas/issues/14503 (readcsv)"},
{"text": "- [x] `.groupby` #5677\r\n- [ ] `.sortvalues` #14353, though this is directly in combat with `.sortindex` and non-explict\r\n- [x] `.merge` (this issue)\r\n#### Overview\r\n\r\n@jorisvandenbossche \r\nAs a part of the [Pandas 1.0](https://docs.google.com/document/d/151ct8jcZWwh7XStptjbLsda6h2b3C0IuiHhfZnUA58/edit#) goal to \"Make the index/column distinction less painful (#5677, #8162)\" I propose that the `df.merge` method support merging DataFrames on a combination of columns and index levels.\r\n\r\nThis could be accomplished in the API by allowing the `on`, `lefton`, and `righton` keywords to accept a combination of column names and index level names.  Any index levels that are joined on would be preserved as index levels in the resulting merged DataFrame, while all other index levels would be removed.\r\n\r\nThis proposal is in the spirit of #5677 for `df.groupby`  and #14353 for `df.sortvalues`.\r\n"},
{"text": "#### Background\n\nDuring the review of @jreback's PR last year to cleanup the sorting API (#10726) there was some discussion of how the DataFrame API could eventually support sorting by a combination of columns and index levels.  I'm interested in working on implementing this soon and would like to continue the discussion of where this should fit into the DataFrame sorting API.\n\nIn https://github.com/pydata/pandas/pull/10726#issuecomment-128066486 @jorisvandenbossche made the following suggestion\n\n> If we want to add this enhancement to simultaneously specify to sort on index levels and columns (the 5d option of above), then the question is: where do we add this functionality and how? In sorted, sortindex or both? I would then lean towards saying: only add it in sorted, where the by keyword can also denote a index level name.\n\nThis approach makes good sense to me. Each object passed to the `by` keyword of `sortvalues` (referred to as `sorted` in the quote above) could refer to either a column or an index level.  For backwards compatibility, column references would take precedence. And my assumption is that we would want to preserve the index when sorting by a combination of columns and index levels this way.\n\nThis proposal is the sorting analog of the groupby proposal in #5677 (which I will be working on soon)\n"},
{"text": "Triggered by #13785, I looked at the top level namespace, and I was wondering if the following functions are needed:\r\n- [ ] `pd.Expr`\r\n- [ ] `pd.pnow()`: returns the current date as a Period with specified freq (https://github.com/pydata/pandas/issues/12641)\r\n- [ ] `pd.info()`\r\n- [ ] `pd.groupby()`\r\n- [ ] `match`: \"Compute locations of tomatch into values\" (imported from `core/api.py` from `\\core\\algorithms.py` together with `factorize`, `unique` and `valuecounts`, but I don't think `match` is used anywhere in the docs)\r\n- [ ] `pd.Term`\r\n\r\nQuite a lot are already deprecated: `TimeSeries`, `SparseTimeSeries`, all `rolling` and `expanding` methods\r\n\r\nI also learned the existence of `pd.lreshape` and `pd.plotparams` .. :-) (but the latter is mentioned in the docs, `lreshape` not).\r\n"},
{"text": "xref #13147\n\nideally add doc-strings and add to API.rst? for the introspectors\nthese are pretty intuitve (e.g. `isinteger`), but have some nuances that\ndoc-strings (and an example) would be useful.\n"},
{"text": "In xref #13147\n\nestablished a bit of a public api in `pandas.api`; ATM this only contains the type introspection routines.\n## 1st question\n\nSome disagreement on whether we should not do this, and rather just expose `pandas.types` directly.\n\nI think `pandas.api` is actually very important because:\n\n1) limits scope of what we choose to expose in the future; its not simply 'everything' that isnt nailed down (the case now). This does change pandas to make everything by default private, EXCEPT what is explicitly public.\n\n2) a single entry point for other packages to use the pandas public API that is known / stable / maintained (will with c-API as well)\n\n3) provide consistent naming externally (and allow us to fix / hide internally as needed)\n\n4) namespaced. I only import what I need as a user / other package, rather than everything\n## ~~2nd question~~\n\n as discussed [here](https://github.com/pydata/pandas/pull/13147#discussionr70594911), should these deprecated API functions should be `DeprecationWrarning` rather than `FutureWarning`?\n-> done in https://github.com/pydata/pandas/pull/13990\n\nIdeally we should resolve this before the end of 0.19.0 (or remove it).\n"},
{"text": "xref #8640 #12699 #13361 #13410 \n\nThere's been discussion of a few overlapping uses of `Categorical`:\n1. as 'true' categorical data with a known set of values\n2. as 'lazy' categorical data which adds new categories as needed\n3. as an interned string data type, with no particular categorical interpretation\n\nOption 1 is currently well-supported. Option 2 can be achieved explicitly by `unioncategorical`, see #13361 #13410, but will not happen automatically e.g. when setting values or concatenating, see #12699. Option 3 is similar to 2, and has been discussed as a new `String` type, see #8640, but may have different semantics to 2.\n\nWhile I completely agree that option 1 should be the default, I'd like to see more support for option 2 if possible; there are cases where I really do want to work with categorical data, but I don't yet know what the categories are, e.g. when concatenating a table together from several different source files.\n\nOne option would be to mimic Matlab's 'protected' flag for categorical data. By default, a `Categorical` would be created protected, and throw errors when performing actions which implicitly change the category set. However, the user could choose to declare a `Categorical` as unprotected, in which case these operations would be performed as intended.\n\nWhile this behaviour could be achieved with the proposed `String` type, it's unclear whether that type would share the `Categorical` API, or be efficiently convertible to `Categorical`. Having the behaviour as part of `Categorical` would allow the user to build up a `Categorical` iteratively, from multiple sources, and then quickly mark it protected once the category set is known.\n"},
{"text": "Hey everyone,\n\nI was working on a project that involves working with data acquired via the world bank pandas api (i.e. `pandas.io.wb`). This library has a function called `pandas.io.wb.search(str)` which is very useful. In data transformation terms, it's basically does this- given a string 's', filter the dataframe by retaining only the tuples where a string match occurs with the given string 's'.\n\nThis operation occurs in my workflow on a regular basis, and I was wondering if anyone else feels the need to have this function natively in pandas. Would be a huge time saver in my opinion!\n\nWas thinking of a function that looks like this:\n`DataFrame.search(searchstr, isignorecase=True, ismatchfullword=False, **kwargs)`\n\nWould love to hear from the community! :)\n"},
{"text": "Many DataFrame methods ([now including `getitem`](https://github.com/pydata/pandas/issues/11485)) accept callables that take the DataFrame as input, e..g, `df[lambda x: x.sepallength > 3]`.\n\nHowever, this is annoyingly verbose. I recently suggested (https://github.com/pydata/pandas/issues/13040) enabling argument-free lambdas like `df[lambda: sepallength > 3]`, but this isn't a viable solution (too much magic!) because it's impossible to implement with Python's standard scoping rules.\n\n[pandas-ply](https://github.com/coursera/pandas-ply) and [dplython](https://github.com/dodger487/dplython) provide an alternative approach, based on a magic `X` operator, e.g.,\n\n``` python\n(flights\n  .groupby(['year', 'month', 'day'])\n  .plyselect(\n    arr = X.arrdelay.mean(),\n    dep = X.depdelay.mean())\n  .plywhere(X.arr > 30, X.dep > 30))\n```\n\npandas-ply also introduces (injects onto pandas.DataFrame) two new dataframe methods `plyselect` and `plywhere` that accept these symbolic expression build from `X`. dplython takes a different approach, introducing it's own dplyr like API for chaining expressions instead of using method chaining. The pandas-ply approach is much closer to what makes sense for pandas proper, given that we already support method chaining.\n\nI think we should consider introducing an object like `X` into pandas proper and supporting its use on all pandas methods that accept callables that take the DataFrame as input. \n\nI don't think we need to port `plyselect` and `plywhere`, because support for expressions in `DataFrame.assign` and indexing is a good substitute.\n\nSo my proposed syntax (after `from pandas import X`) looks like the following:\n\n``` python\n(flights\n .groupby(['year', 'month', 'day'])\n .assign(\n     arr = X.arrdelay.mean(),\n     dep = X.depdelay.mean())\n [(X.arr > 30) & (X.dep > 30)])\n```\n\nIndexing is a little uglier than using the `plywhere` method, but otherwise this is a nice improvement. \n\nBest of all, we don't need do any special tricks to introduce new scopes -- we simply define `X.getattr` to looking attributes as columns in the DataFrame context. I expect we could even reuse the expression engines from pandas-ply or dplython directly, perhaps with a few modifications.\n\nIn my mind, this would mostly obviate the need for pandas-ply, though the alternate API provided by dpython would still be independently useful. In an ideal world, our `X` implementation in pandas would be something that could be reused by dplython.\n\ncc @joshuahhh @dodger487\n"},
{"text": "With [a little bit of magic](https://github.com/shoyer/pandas-magic/blob/master/pandasmagic/init.py#L13-L33), we could make the following syntax work:\n\n``` python\n(df[lambda: sepallength > 3]\n .groupby(lambda: pd.cut(sepalwidth, 5))\n .apply(lambda: petalwidth.mean()))\n```\n\nSyntax like `df[lambda: sepallength > 3]` is an alternative to more verbose alternatives like the recently added `df[lambda x: x.sepallength > 3]` (https://github.com/pydata/pandas/issues/11485). Here we use `lambda` essentially in place of a macro that would allow for delayed evaluation (which of course are not supported by Python syntax).\n\nMy proposal is to add support for such \"thunks\" to every pandas method that accepts a callable function that currently must take a single argument work on DataFrames.\n\nUnder the covers, this works by (1) copying the `globals()` dictionary at evaluation time and (2) injecting the current DataFrame into it. We would further ensure that this only works on lambda functions, by checking `f.funcname == '<lambda>'`.\n\nThe main gotcha is that [it isn't possible to dynamically override ~~local~~ non-global variables](http://stackoverflow.com/questions/8028708/dynamically-set-local-variable-in-python) without [some true dark magic](https://gist.github.com/njsmith/2347382#file-inject-py-L40). This means that code like the following is going to behave contrary to expectations:\n\n``` python\ndef xplusone(df):\n    x = 0\n    # uses x = 0 instead of df.x\n    return df.pipe(lambda: x + 1)\n\ndf = pd.DataFrame({'x': np.arange(100)})\nresult = xplusone(df)  # all 1s, not range(1, 101)\n```\n\nIs this so bad? Shadowing variables in an outer scope is already poor design, but this is a pretty serious departure from expectations.\n\nThe other danger is that this could mask bugs, e.g., if a user mistakenly types `df.pipe(lambda: x)` instead of `df.pipe(lambda x: x)`. This is an unavoidable danger of spelling two APIs with similar syntax.\n\nOn the plus side, this proposal is safer than @njsmith's \"true dark magic\" context manager (see above) for injecting DataFrame columns, because there's no possibility of variable assignment inside a `lambda`.\n\nWould this be a good idea for pandas?\n"},
{"text": "from `pandas.tseries.api`\n\nif someone can take a look and see when this was added?\n\nI don't think this is useful at all. Let's just deprecate and remove this function entirely. Could always add a `Period.now()`.\n"},
{"text": "There are known ways of droping inf and nan (.replace(np.inf, np.nan).dropna()) but still, the API could be expanded:\n\n```\n.dropinf(inplace=False) \n\n.dropna(dropinf = False)\n```\n\nAre both self explanatory wishes and it is no stretch to see how they are usefull. \nNote for instance that today's API does not provide a one liner to drop inf in place while keeping nans.\n\nBest\n\nPS: not sure how to submit wishes.\n"},
{"text": "I'm working on a `fromdummies` right now, and I'm trying to achieve (roughly)\n\n``` python\ndf = pd.fromdummies(pd.getdummies(df))\n```\n\nOne problem is that `getdummies` will append all the dummy columns to the end of the DataFrame, after the numeric columns.\n\n``` python\nIn [15]: df = pd.DataFrame({\"A\": [1, 2], \"B\": ['a', 'b'], 'C': [1, 2]})\nIn [18]: pd.getdummies(df)\nOut[18]:\n   A  C  Ba  Bb\n0  1  1    1    0\n1  2  2    0    1\n```\n\nSo when we go to invert with `fromdummies` we don't know when column goes where. If instead we had\n\n``` python\n   A  Ba  Bb  C\n0  1    1    0  1\n1  2    0    1  2\n```\n\nwe'd know what the original ordering was by position.\nLooking through the docs, I don't see anywhere where we say that the ordering must be this way, but it'll still be an API change and I'm not sure whether it's worth it.\n"},
{"text": "similar to #11603 \n\nthis would transform:\n\n`s.resample('D',how='max')`\n\nto\n\n`s.resample('D').max()`\n\nThis would be a breaking API change, as the default is `how='mean'`, meaning, that `s.resample('D')` returns the `mean` of the resampled data. However it would be visible at the very least and not simply change working code.\n\nThis would bring `.resample` (which is just a groupby type operation under the hood anyhow) into the API syntax for `.groupby` and `.rolling` et. al.\n\nFurthermore this would allow geitem / aggregate type operations with minimal effort\ne.g.\n\n`s.resample('D').agg(['min','max'])`\n"},
{"text": "Derived from #10507. Consider better API for testing functions.\n### 1. How to specify comparison tolerance\n\nCurrent assert functions have `checklessprecise` and `checkexact` flags to specify how to compare values:\n- `checkexact=True`: Use `assertequal`, comparison using `==`.\n- `checkexact=False` and `checklessprecise=False`: Use `assertalmostequal`, comparing 5 digits after decimal points (default)\n- `checkexact=False` and `checklessprecise=True`: Use `assertalmostequal`, comparing 3 digits after decimal points\n\nThere can be single kw to specify above behaviors?\n### 2. Make integrated assert  function\n\nI think it's nice to have test functions which supports all `pandas` objects for users who is starting contribution / using pandas as their dependencies. Changing `assertEquals` and `assertAlmostEquals` to internally use current `assertindex/series/frameequal` is one idea.\n### 3. Remove `assertalmostequal`\n\nRemove `assertalmostequal` and use class-based validation method. Also add input type validation to `numpyassertarrayequal`.\n\nxref #9895.\n"},
{"text": "In my discussion with Jonathan and others and at the SciPy sprints, we agreed that it would be really nice to expose some minimal tools for manipulating and view the internal pandas blocks system. For example, it should be possible to:\n1. manually consolidate blocks\n2. view a representation of the internal blocking of a dataframe (via matplotlib?)\n\nIt's not so much that we want to create and use blocks directly, but that we want to make it easier to understand the internal data model and make performance with more predictable.\n\nAt the same time, we would like to disable automatic consolidation of blocks in the DataFrame constructor and when inserting new columns. Consolidation is certainly a useful feature, but it is currently not always possible to even predict when it will happen.\n\nMost users never notice or care about consolidation. Power users (concerned about memory or performance) are at least as likely to find it frustrating as helpful, so we should make this something that they can trigger explicitly (as part of the blocks API). This would make it possible to create dataframes while guaranteeing that none of the data is copied (#9216).\n\ncc @jonathanrocher @sinhrks @jreback @cpcloud @TomAugspurger @ARF1 @quicknir\n"},
{"text": "At this moment, `pd.io.sql.getschema` is not documented (not in the API docs, and not in the io docs). But, it is a potential useful function, so I think it would be good to be more explicit about its status (by mentioning it in the docs).\n\nHowever, there are some quirks about the function:\n- The signature: `pd.io.sql.getschema(frame, name, flavor='sqlite', keys=None, con=Non\n  e, dtype=None)` -> flavor keyword in the third place, while we want to deprecate it (and this means you cannot do `getschema(df, 'name', engine)`, but always have to do `getschema(df, 'name', con=engine)`.  \n  Ideally this should have the same arguments (order) as `tosql` (`pd.io.sql.tosql(frame, name, con, flavor='sqlite', schema=None, ifexists='fail', index=True, indexlabel=None, chunksize=None, dtype=None)`) (only chunksize is not relevant)\n- It should have all options to modify the resulting sql table as `tosql`\n- Maybe also the option to return the sqlalchemy `Table` instead of the string itself?\n\nThat we maybe should first solve before making it more explicitely public?\n\nTriggered by http://stackoverflow.com/questions/29749356/python-pandas-export-structure-only-no-rows-of-a-dataframe-to-sql/\n"},
{"text": "A lot of other projects that use pandas will (like to) use pandas testing functionality like `assertframeequal` in their test suite. Although the pandas testing functions are available in the namespace (#6188), they are not really 'officially' labeled as public API that other projects can use (and rely upon). \nNumpy has a similar submodule  `numpy.testing` (http://docs.scipy.org/doc/numpy/reference/routines.testing.html)\n\nSome things we could do:\n- make a selection of the functions in `util.testing` that we want to label as public\n- add this list somewhere to the docs\n- write docstrings for these public ones (the other could use that as well of course ..)\n- add some tests for the public API\n- I would also import them into a `pandas.testing` module, so it is this one we can publicly advertise (and users are less tempted to use other non-public functions in the `pandas.util.testing` namespace)\n"},
{"text": "xref #9667 Now we also have string methods available on the index, the possible options for `returntype` ('series', 'frame') are a bit confusing I think: it will be confusing for users to get a Index back even if he/she supplies `returntype='series'` on a Index.str.split, or to get a series back with `Series.str.split(.., returntype='index')`? \n\nPossible ways to make this a better API:\n\n1) An `expand` keyword (or another name), that indicates for `False`: give same dimension back (so for series/index keep it a series/index), and for `True`: expand series to dataframe. \n  This would then be a duplicate for `returntype` of course. But the `returntype` was only introduced in 0.15.1 (and for `partition` it is still in a PR), so if we want to change this: better now than later. Or has this ship sailed?\n\n2) easier solution of @jreback: `returntype='same'|'expand'` to satisfy this need? (and can be easily back-compat) -> so no need to change the name of the keyword, only the arguments.\n"},
{"text": "Looking at the API docs, neither the function `pd.unique` nor the order of the unique values from `unique` is mentioned. I would like to:\n1. Add `pd.unique` to API > General functions > Data manipulations\n2. Note that `Series.unique()` and `unique()` return values in order of appearance\n3. Add unit tests to verify the \"order of appearance\" nature `unique` (untested directly AFAICT, though I'm sure it's relied on implicitly)\n\nAny objections? If not, I'll put together a PR.\n\nThis lack of documentation has caused some hesitation for relying on this functionality in Seaborn: https://github.com/mwaskom/seaborn/issues/361\n"},
{"text": "Building off of #9229, it's generally much better to create new dataframes rather than to modify existing ones. It's both more chainable and less prone to the bugs/errors associated with mutable state.\n\nSo it seems to me that a saner API design would be to make assignment operations with eval/query return a new frame rather than modify the existing one, i.e., `df.eval('x = y ** 2')` should be equivalent to `df.assign(x = lambda df: df.y ** 2)` (as currently defined in #9239).\n\nIn my experience, something like `df.query('x = 0')` is more likely intended to be equivalent to `df.query('x == 0')` rather than `df['x'] = 0`, which has the unfortunate effect of also modifying the original data. In fact, I have made this exact mistake before -- and apparently others have as well (see #8664).\n\nThoughts? The `query`/`eval` syntax is marked as experimental, so I think we have the freedom to change this.\n\nCC @TomAugspurger @cpcloud @jreback \n"},
{"text": "The current documentation for the limit parameter looks like this:\n\n```\nlimit : int, default None.\n    Maximum number of consecutive NaNs to fill.\n```\n\nSeveral features were not immediately obvious to me and could probably be resolved by slightly better docstring or API design:\n1. What happens where the max number of consecutive NaNs is reached? pandas simply stops replacing values, e.g., if there is a gap of 3 values and limit=2, pandas replaces the first 2 values. The alternative would be to entirely ignore gaps of more than 2 values. I'm not saying this would be a better alternative, but we should clarify this.\n2. The limit refers to forward filling only, even though interpolation is not inherently directional. It would be nice if there was some way to trigger a limit for back filling at the same time, e.g., with `forwardlimit` and `backwardlimit` arguments:\n   \n   ```\n   >>> pd.Series([1, np.nan, np.nan, np.nan, 5]).interpolate(forwardlimit=1, backwardlimit=1)\n   0     1\n   1     2\n   2   NaN\n   3     4\n   4     5\n   dtype: float64\n   ```\n\nThoughts?\n"},
{"text": "After hours of tearing my hair, I've come to the conclusion that it is impossible to create a mixed dtype DataFrame without copying all of its data in. That is, no matter what you do, if you want to create a mixed dtype DataFrame, you will inevitably create a temporary version of the data (e.g. using np.empty), and the various DataFrame will constructors will always make copies of this temporary. This issue has already been brought up, a year ago: https://github.com/pydata/pandas/issues/5902. \n\nThis is especially terrible for interoperability with other programming languages. If you plan to populate the data in the DataFrame from e.g. a call to C, the easiest way to do it by far is to create the DataFrame in python, get pointers to the underlying data, which are np.arrays, and pass these np.arrays along so that they can be populated. In this situation, you simply don't care what data the DataFrame starts off with, the goal is just to allocate the memory so you know what you're copying to.\n\nThis is also just generally frustrating because it implies that in principle (depending potentially on the specific situation, and the implementation specifics, etc) it is hard to guarantee that you will not end up using twice the memory you really should.\n\nThis has an extremely simple solution that is already grounded in the quantitative python stack: have a method analagous to numpy's empty. This allocates the space, but does not actually waste any time writing or copying anything. Since empty is already taken, I would propose calling the method fromempty. It would accept an index (mandatory, most common use case would be to pass np.arange(N)), columns (mandatory, typically a list of strings), types (list of acceptable types for columns, same length as columns). The list of types should include support for all numpy numeric types (ints, floats), as well as special Pandas columns such as DatetimeIndex and Categorical. \n\nAs an added bonus, since the implementation is in a completely separate method, it will not interfere with the existing API at all.\n"},
{"text": "With the new `Timedelta` type (see https://github.com/pydata/pandas/pull/8184), we introduced behavior that breaks compatibility with the superclass `timedelta`: `td.seconds` refers to seconds since the last minute, rather than seconds since the last day (as in the superclass).\n\nI think breaking compatibility with the superclass here was a mistake. User code clearly does rely on the old behavior, e.g., as manifested in bugs for exporting datetime columns to Excel: #9139. Moreover, it's likely to remain an issue causing silent bugs, because external libraries are going to continue to use `isinstance` checks to verify that something is a `datetime`/`timedelta` object. We chose to subclass `timedelta` here, so we really are stuck with full API compatibility.\n\nSo I think we should make this change in 0.16. It's a breaking change, but 0.15 was already a breaking change in this respect, and in any case I think we should give ourselves some allowance for API changes for new types.\n\nCC @jorisvandenbossche @jreback \n"},
{"text": "n.b. I'm looking at how to store a `DataFrame` in BSON format (see #4329) in MongoDB. This is my attempt to gather all the relevant information before adding yet another *SON serializer. Please don't treat this as a change request - I'm just trying to get my head around all the relevant bits. Maybe skip to the summary and start from there ...\n## Refactoring *SON to use common schema\n\nThere are a number of *SON formats which are similar: e.g. JSON, Msgpack (in pandas), BSON (not in pandas) recently announced [JBSON](http://www.phoronix.com/scan.php?page=newsitem&px=MTY0MTU). They have some common components:\n1. Conversion of python object graph into Document(s) (think dict) made up only of JSON/Msgpack/BSON primitives. You can think of this in terms of a schema, as discussed in #3525, #3297, #2485. In an ideal world, this schema would be shared by all formats.\n2. Serialization into a binary format (Msgpack/BSON only) or text (JSON). This is generally done by an external library separately for each format.\n\nCurrently, I don't think any of the code between JSON and Msgpack is shared. The schema used by Msgpack is entirely different from JSON. I'm guessing this is by design; JSON has been kept as human-readable as possible, whereas Msgpack is more focused on a serialization format. To illustrate this, two examples. First a simple Dataframe:\n\n```\n                          bid     offer\n2014-12-15 10:07:00  0.562804  0.398798\n2014-12-15 10:07:01  0.399399  0.896809\n2014-12-15 10:07:02  0.747191  0.098048\n```\n\nproduces the following document before serialization in msgpack:\n\n``` python\n{'axes': [{'data': ['bid', 'offer'],\n   'dtype': 17,\n   'klass': 'Index',\n   'name': None,\n   'typ': 'index'},\n  {'data': '...',\n   'dtype': 21,\n   'freq': 'S',\n   'klass': 'DatetimeIndex',\n   'name': None,\n   'typ': 'datetimeindex',\n   'tz': None}],\n 'blocks': [{'compress': 'blosc',\n   'dtype': 12,\n   'items': {'data': ['bid', 'offer'],\n    'dtype': 17,\n    'klass': 'Index',\n    'name': None,\n    'typ': 'index'},\n   'klass': 'FloatBlock',\n   'shape': (2, 3),\n   'values': '...'}],\n 'klass': 'DataFrame',\n 'typ': 'blockmanager'}\n```\n\nI produced this by calling `encode` recursively in `packing.py`, replicating what `pack` does. There have been plenty of discussions regarding storage of metadata in #3525, #3297. Now the result of calling `tojson()`:\n\n``` python\n'{\"bid\":{\"1418638020000\":0.5628044127,\"1418638021000\":0.3993987818,\"1418638022000\":0.7471914537},\"offer\":{\"1418638020000\":0.398797779,\"1418638021000\":0.8968090851,\"1418638022000\":0.0980482752}}'\n```\n\nBoth appear to use blocking (see #9130 ). Also, as described in #3525, dates are stored as integers (in strings) for performance reasons.\n\nThe second example, which has a multi-index:\n\n```\n                       bid                                ask                \\\n                      95.0   95.5   96.0   96.5   97.0   97.5   98.0   98.5   \n2014-12-15 10:07:00  25030  16800  42580  58560  75110  10400  89240   3990   \n2014-12-15 10:07:01  42620  57860  80010  81500  98880  99610  65770  83930   \n2014-12-15 10:07:02  94170  98040  74840  41690  48960  76510  88530  48770   \n2014-12-15 10:07:03  65090  23700  16390  45700    500  29290  32370  68350  \n```\n\nThis kind of works for `tomsgpack()` but not for `tojson()` (throws an exception):\n\n```\n{'axes': [{'data': [('bid', 95.0),\n    ('bid', 95.5),\n    ('bid', 96.0),\n    ...\n    ('ask', 99.5)],\n   'dtype': 17,\n   'klass': 'MultiIndex',\n   'names': FrozenList([None, None]),\n   'typ': 'multiindex'},\n  {'data': '...',\n   'dtype': 21,\n   'freq': 'S',\n   'klass': 'DatetimeIndex',\n   'name': None,\n   'typ': 'datetimeindex',\n   'tz': None}],\n 'blocks': [{'compress': 'blosc',\n   'dtype': 12,\n   'items': {'data': [('bid', 95.0),\n     ('bid', 95.5),\n     ('bid', 96.0), \n     ...\n     ('ask', 99.5)],\n    'dtype': 17,\n    'klass': 'MultiIndex',\n    'names': FrozenList([None, None]),\n    'typ': 'multiindex'},\n   'klass': 'FloatBlock',\n   'shape': (10, 4),\n   'values': '...'}],\n 'klass': 'DataFrame',\n 'typ': 'blockmanager'}\n```\n\nIt would be nice to expose the API of the 'intermediate' representation (i.e. at the end of step 1) for the advanced user, in order to store the dataframe in multiple 'Documents'. If storing to a file, this doesn't make any sense - you want to have a single document with many embedded documents; This is the JSON paradigm. But if you're storing into a database, you would want flexibility in order to store each nested doc/dict in a different collection/table. There are reasons for this e.g. a technical reason is the MongoDB maximum record/document size; however, it would generally be driven by a use-case. \n\nWhat would this intermediate document look like? A dictionary, with only python (and numpy - see below) primitives? What are the primitives that are different for *SON formats (e.g. dates)?\n## Serialization of vectorized data\n\nA separate issue, that is somewhat at odds with the above, is the storage of vectorized data. i.e. the underlying numpy array in `tomsgpack()` is encoded into a string and stored in binary format.  While this makes sense for performance reasons (see https://github.com/pydata/pandas/pull/3525#issuecomment-17442698), it's giving up one of the advantages of using msgpack (or BSON going forward), in my opinion: portability. If you go to http://msgpack.org, you'll see they have APIs for every conceivable language (likewise for MongoDB). Wouldn't it be nice if you could use one of these APIs and load up your pandas data in e.g. Java, Haskell or C? At present, this is possible with the exception of the 'values' fields, which you would have to decode by hand. Sure you could argue to use JSON for portability, but then you're trading off performance.\n\nThis is at odds with adding compression - which we still want. A compromise could be to use the native *SON list type when `compress=None`, when we assume that speed isn't important, and the current solution (encode as string) when compression is active, and speed is important (and compressed data is not handled by the abovementioned APIs)?\nNote also, as described in links in https://github.com/pydata/pandas/issues/4329#issuecomment-27095723, BSON/MongoDB appears to have some support for vectorised lists that avoids having to convert to a list and then serialize each element separately.\n## Summary \n\nIn adding yet another *SON serializer, it would not make sense to have a third codebase that handles the issues discussed above in a new way:\n- Would it make sense to combine existing msgpack/JSON 'packing' code, to have a standard intermediate 'schema', shared by all *SON? cons: breaks backward compatibility, makes JSON document much more complex, pros: support more complex pandas structures (DF, panel, sparse df etc.).\n- Could code for storing vector data be shared? Could we allow native storage of vector data (e.g. supported by BSON), for example, when not using the `compress` argument in `tomsgpack`, to allow portability? Ideally also using compress? could we somehow replicate what monary is doing for BSON in the msgpack serializer, i.e. altogether avoiding conversion to lists and memory copies?\n\nAgain, I want to stress I'm just trying to create some discussion here rather than opening a specific change req... Sorry if raising an issue wasn't the correct procedure.\n"},
{"text": "As the next step of separation-of-concerns plan (#6744) I'd like to\npropose adding a method (or several, actually) to `Index` class that\nwould encapsulate the details of `foo.loc[l1,l2,...]` lookup.\n### Implementation Idea\n\nRoughly, the idea is to make `loc`'s getitem as simple as\n\n``` python\ndef getitem(self, indexer):\n    axes = self.obj.axes\n    return self.obj.iloc[axes[0].lookuplabelsnd(indexer, axes[1:], typ='loc')]\n```\n\nNot quite, but hopefully you get the point. The default `lookuplabelsnd` implementation would then look something like this:\n\n``` python\ndef lookup(self, indexer, otheraxes, typ=None):\n    if not isinstance(indexer, tuple):\n        return self.lookuplabels(indexer, typ=typ)\n    else:\n        # ndim mismatch error handling is omitted intentionally\n        return (self.lookuplabels(indexer[0]),) + \\\n               tuple(ax.lookuplabels(ix, typ=typ)\n                     for ax, ix in zip(otheraxes, indexer))\n\n```\n\nThe result should be an object that could be fed to an underlying\nBlockManager to perform the requested operation.  To support adding\nnew rows with \"setitem\", it is only needed to agree that `lookuplabelsnd` will\nnever return negative indices unless they reference newly appended\nitems along that axis.\n\nThis would allow to hide Index-subclass-specific lookup peculiarities\nin their respective overrides of `lookuplabelsnd` and `lookuplabels` (proposals for\nbetter names are welcome), e.g.:\n- looking up str in DatetimeIndex/PeriodIndex\n- looking up int in FloatIndex\n- looking up per-level slices in MultiIndex\n### Benefits\n- no more confusing errors due to `try .. catch` block carpet-catching a\n  logic error, because corner cases will be handled precisely where\n  they are needed and nowhere else\n- no more relying on isinstance checks and exceptions to seek for\n  alternative lookup scenarios, meaning more performance\n- the API will provide a contract that is simple to grasp, test, benchmark and,\n  eventually, cythonize (as a side effect of this point I'd like to try putting\n  up a wiki page with indexing API reference)\n"},
{"text": "As pointed out by @dsm054, there are multiple lurking split/partition API requests. Here are the issues and a short summary of what they would do (there are some duplicates here, I've checked off those issues/PRs that have been closed in favor of a related issue):\n- [x] #414: (i think) original issue for these ideas going back 3 years\n- [x] #936: windowing with time-length windows like `pd.rollingmean(ts, window='30min')` and possibly even arbitrary windows using another column\n- [x] #3066: `split` method on pandas objects, playing around with ideas\n- [x] #3101: a closed PR by @y-p to use the args of lambda to group a frame into views of a sliding window\n- [x] #3685: resampling using the first `n` samples of a bin.\n- [ ] #4059: `np.arraysplit` style API where you can split a pandas object into a list of `k` groups of possibly unequal size (could be a thin wrapper around `np.arraysplit`, or more integrated into the pandas DSL). IMO, this issue provides the best starting point for an API. [SO usage](http://stackoverflow.com/questions/24461906/how-can-i-divide-up-a-pandas-dataframe/24462529#24462529)\n- [x] #5494: an API for to allow pandas' `groupby` to have `itertools.groupby` semantics (i.e., preserve the order of duplicated group keys), i.e., `'aabbaa'` would yield groups `['aa', 'bb', 'aa']` rather than `['aaaa', 'bb']`. There'd have to be some changes to the use of `dict` in the groupby backend as noted by @y-p here https://github.com/pydata/pandas/issues/4059#issuecomment-29061036.\n- [ ] #6675: Ability to select ranged groups via another column, like \"select all rows between the values X and Y from column C\", e.g., an \"events\" column where you have a start and end markers and you want to get the data in between the markers. There are a couple of ways you can do this, but it would be nice to have an API for this. This is very similar to #936.\n\nThe [`toolz`](https://github.com/pytoolz/toolz) library has a `partitionby` function that provides a nice way to do some of the splitting on sequences and might provide us with some insight on how to approach the API.\n\ncc @jreback @jorisvandenbossche @hayd @danielballan\n"},
{"text": "Pandas specifies an Index API in the docs: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-internal-details\n\nHowever, it's not currently possible (as far as I can tell) to create an object that can be used as a pandas Index unless it's an actual `pandas.Index` subclass.\n\nFrom the perspective of someone who would like to be able to create custom Index-like objects, I would prefer not to need to subclass `pandas.Index` so the implementation details of my index can be decoupled from pandas. I might still use a pandas.Index internally for speed, but in my experience composition is generally easier to reason about than inheritance.\n"},
{"text": "``` python\ndef factorize(values, sort=False, order=None, nasentinel=-1):\n    \"\"\"\n    Encode input values as an enumerated type or categorical variable\n\n    Parameters\n    ----------\n    values : ndarray (1-d)\n        Sequence\n    sort : boolean, default False\n        Sort by values\n    order :\n    nasentinel: int, default -1\n        Value to mark \"not found\"\n\n    Returns\n    -------\n    labels : the indexer to the original array\n    uniques : the unique values\n\n    note: an array of Periods will ignore sort as it returns an always sorted PeriodIndex\n    \"\"\"\n```\n\nBut it isn't used anywhere. Technically this is an API change, if people aren't using kwargs. So I'll add it to the deprecation list.\n"},
{"text": "I often find myself writing code that looks like this:\n\n``` python\nstartgroup = df[df.strcol == 'start'].index.values + 1\nendgroup = df[df.strcol == 'end'].index.values - 1\nfor s, e in zip(startgroup, endgroup):\n    somefunction(df[s:e])\n```\n\nto get frame subsets between repeated start and end indicators from a particular column.\n\nPossible API:\n\n```\ndf.grange(markercol='strcol', start='start', end='end')\n```\n\nwhich would return an iterator over the the subsets.\n\nI'm not sure if this is general enough to include in library code. I'll happily close the issue if it's not. There also might be a way to do this already.\n"},
{"text": "@dsm054 posted ridiculous fast version for this, perhaps we can wrap it in an API with filter?\n\nhttp://stackoverflow.com/questions/22208562/replace-rarely-occurring-values-in-a-pandas-dataframe#comment3371768222208838\n\ncc @danielballan\n"},
{"text": "Currently, with `query` and `eval` you can use local variables a la the `@` symbol. It's a bit confusing since you're not allowed to have a local variable and a column name with the same name, but it will try to pull the local if possible.\n\nCurrent API:\n\nFails with a `NameError`:\n\n``` python\na = 1\ndf  = DataFrame({'a': randn(10), 'b': randn(10)})\ndf.query('a > b')\n```\n\nBut this works:\n\n``` python\ndf.query('@a > b')\n```\n\nAnd so does this, which is confusing:\n\n``` python\na = 1\ndf = DataFrame({'b': randn(10), 'c': randn(10)})\ndf.query('a < b < c')\n```\n\nAs suggested by @y-p and @jreback, the following API is less confusing IMO. \n\nFrom now on, all local variables will need an explicit reference and if there is a column name and a local with the same name then the column will be used. Thus you can always be sure that you're referring to a column, or it doesn't exist, in which case you'll get an error. And if you use `@` then you can be sure that you're referring to local, and likewise get an error if it doesn't exist. As a bonus ( :wolf: in :sheep: 's clothing), this allows you to use both a local and a column name with the same name.\n\nExamples:\n\n``` python\na = 1\ndf = DataFrame({'a': randn(10), 'b': randn(10)})\n\n# uses the column 'a'\ndf.query('a > b')\n\n# uses the local\ndf.query('@a > b')\n\n# fails because I didn't reference the local and there's no 'c' column\nc = 1\ndf.query('a > c')\n\n# local and a column name\ndf.query('b < @a < a')\n```\n"},
{"text": "I'd like to propose `readnetcdf` ([netCDF (Network Common Data Format)](http://www.unidata.ucar.edu/software/netcdf/)) a new Pandas I/O api feature with a similar top level interface as the other reader functions.  This format is widely used in the scientific community.   Furthermore, netCDF4 is implemented on top of the HDF5 library, making this a natural extension to functionality already in the api.  \n\nMost likely this would sit on top of the exiting [Python/numpy interface to netCDF](https://code.google.com/p/netcdf4-python/), and because each variables metadata is stored in the file header, no complicated parsing would necessary. Multidimensional variables could be handled in a similar manner as hdf.\n\nThis may have been brought up in the past but my search here and on the google didn't bring anything up.\n"},
{"text": "I've been contributing a little bit to GeoPandas / working on some of my own custom code using pandas that I want to share with others and I realized that I wasn't sure what is and is not part of the public API for pandas.\n\nHere's what I have in my head:\n\nDefinitely public:\n- anything in pandas toplevel namespace (except for modules imported into that namespace)\n- non-`` classmethods on NDFrame\n- DateOffsets (in that they have a set interface), but not necessarily direct use of getoffset and friends\n\nSomewhat public:\n- Subset of Index methods:\n  - Definitely `union`, `intersection`, `difference`, `levels`, `labels`, `names` (and the set methods for them)\n  - Somewhat: `getindexer`, `getindexernonunique`, `groupby`, `getloc`, `slicelocs`, `equals`, `identical`, `values` property\n  - Not public: `is`, `isunique`, lexsort properties on MI\n- `getoffset` and `DateOffset` subclasses\n\nNice to make public right now:\n- `compat` module (useful to provide this for modules that depend on pandas...at least in terms of guaranteeing that whatever's in the namespace now will go through a deprecation period).\n- test utils (otherwise have to reinvent the wheel)\n- `cachereadonly` decorator\n\nInternal methods/properties that subclasses may rely on/manipulate:\n- `finalize`\n- `metadata` property\n- `constructor`, `constructorsliced`\n- `internalnames` (maybe - weird behavior with `setattr` if you don't do this)\n- `resetcache`, `updateinplace`, possibly `maybeupdatecacher`\n\nThings we could consider making public in the future:\n- many of the `is*` methods in core/common.\n\nAm I missing anything here? Does this make sense?\n"},
{"text": "tl;dr - add true support for Categoricals in NDFrame. \n\nThere was an issue on the mailing list about using cut and sorting the results that brought this to mind. The issue is both that (I believe) a categorical loses its representation when you put it in a DataFrame and so the output of cut has to just be strings. I propose the following:\n1. Add a `CategoricalBlock` (or `FactorBlock`) internally that can handle categoricals like those produced from cut that could share most of MI's internals, as a 2D int ndarray with an associated list of indexes for each column (again, nearly the same as MI except most ops would be working on just one 'level' and underlying could/would be 2D rather than list of Int64Index). Probably also would mean abstracting common operations to a separate mixin class.\n2. Change `Categorical` to be a Series subclass with a SingleBlockManager that's a CategoricalBlock. This would not change its API, but it would gain Series methods.\n3. Add a `tocategorical` method to Series (bonus points if we change convertobjects to detect if there are < SomeMax number of labels and convert object dtypes to categoricals).\n4. Add a registration method to makeblock so it iterates over a set of functions that either return a klass or None before falling back to ObjectBlock (so abstract existing else clause into a function and make the list of functions semi-public).\n\nI'm going to work on this and I don't think it will be that difficult to implement, but it would make pandas more useful for representing level sets and other normalized data.\n"},
{"text": "related is #2094\nrelated is #6847 (fixes kind and some arg ordering)\nrelated is #7121 (make `sortlevel` a part of `sortindex` by adding level arg)\n\nthe sorting API is currently inconsistent and confusing. here is what exists:\n\nSeries:\n- `sort`: calls `Series.order`, in-place, defaults `quicksort`\n- `order`: do the sort on values, return a new object, defaults `mergesort`\n- `sortindex`: sort by labels, returns new object\n\nFrame:\n- `sort`: calls `sortindex`\n- `sortindex`: sorts by the index with no args, otherwise a nested sort of the passed columns\n\nThe semantics are different between `Series` and `DataFrame`. In `Series`, `sort` mean in-place, `order` returns a new object. `sort/order` sort on the values, while `sortindex` sorts on the index. For a `DataFrame`, `sort` and `sortindex` are the same and sort on a column/list of columns; `inplace` is a keyword.\n\nProposed signature of combined methods. We need to break a `Series` API here. because `sort` is an in-place method which is quite inconsistent with everything else.\n\n```\ndef sort(self, by=None, axis=0, level=None, ascending=True, inplace=False,\n                   kind='quicksort', nalast=True):\n```\n\nThis is what I think we should do:\n- make `Series.sort/order` be the same.\n- by can take a column/list of columns (as it can now), or an index name / `index` to provide index sorting (which means sort by the specifiied axis)\n- default is `inplace=False` (which is the same as now, except for `Series.sort`).\n- `Series.sortindex` does `s.sort('index')`\n- `DataFrame.sortindex` does `df.sort('index')`\n- eventually deprecate `Series.order`\n- add `DataFrame.sortcolumns` to perform axis=1 sorting\n\nThis does switch the argument to the current `sortindex`, (e.g. axis is currently first), but I think then allows more natural syntax\n- `df.sort()` or `df.sortindex()` or `df.sortindex('index')` sort on the index labels\n- `df.sort(['A','B'],axis=1)` sort on these columns (allow 'index' here as well to sort on the index too)\n- `df.sortcolumns()` or `df.sort('columns')` sort on the column labels\n- `df.sortcolumns()` defaults `axis=1`, so `df.sortcolumns(['A','B'])` is equiv of - - `df.sort(['A','B'],axis=1)`\n- `s.sort()` sort on the values\n- `s.sort('index')` or `s.sortindex()` sort on the series index\n"},
{"text": "Changing to not be an ndarray subclass should make a number of things simpler (especially the API unification discussed in #3268). Plus, it will clarify the required interface for an Index object. It avoids all the workarounds to make Index immutable (instead, will define `array` and `arraywrap` and choose which other functions should be allowed).\n- [  ] Move Index underlying data to `data` to mirror other objects. Keep metadata on object itself. (MI should work with only minor tweaks, given that it was always a fake ndarray)\n- [  ] Reimplement fastpath. (figure out how to do this inference again).\n- [  ] Move MI underlying data to `data` (potentially turn levels into 2d int64 ndarray and keep labels as list of lists or as ordered-dict-like. not sure whether row-major or column-major makes more sense. I'm guessing row-major since that would speed key lookups) \n\n---\n\nHow much backwards compatibility do we need to support? (pickle compat should be simple). E.g. do we want to do something like this:\n\n``` python\ndef getattr(self, attr): \n    res = getattr(self.data, attr) \n    warnings.warn(\"Accessing %s on %s is deprecated. Convert to an ndarray first.\" %  \n                 (attr, self.class.name))\n    return res\n```\n"},
{"text": "post #4324\n- [ ] Fillna column wise, #4514\n- [x] interpolation, consolidate Series/DataFrame interpolation to internals.Block.interpolatte, #4434, cc @TomAugspurger\n- [ ] update, #3025 \n- [ ] shift, #4865, #4994\n- [ ] sort, #5190\n- [ ] dropna API for Series, see #5234, #5250\n- [ ] in place dropna , #1960 \n"},
{"text": "renamed `carray` package: https://github.com/Blosc/bcolz\n\nSoliciting any comments on this proposal to create a columnar access table in `HDFStore`.\n\nThis is actually very straightforward to do.\n\nneed a new kw argument to describe the type of format for\nstorage: ``format='s|t|c'` (also allows expansion in the future to other formats)\n- `s` is the `Storer` format (e.g. `store['df'] = value`), implied currently\n  with `put``\n- `t` is the `Table` format (e.g. `store.append('df',value)`, created\n  with `table=True` when using `put` or using `append`\n- `c` is a `CTable` format (new), which is a column oriented table\n\nso will essentially deprecate `append=,table=` keywords (or just translate them)\nto a `format=` kw.\n\n```\ndf.tohdf('test.h5','df',format='c')\n```\n\nWill have a master node which holds the structure.\nWill store a format with a single column from a `DataFrame` in a sub-node of the\nmaster.\n\nadvantages:\n- index(s) are kept in their own columns (this is true with `Table` now)\n- allows easy delete/add of columns (somewhat tricky in the `Table` format)\n- allows appends (interesting twist is that have to keep the indices in sync)\n- selection is straightforward as everything is indexed the same\n- selecting a small number of columns relative to the total should be faster than an equivalent `Table`\n- API will be the same as current. This is essentially an extension of the `appendasmultiple / selectasmultiple` multi-table accssors.\n- can be included/coexist alongside existing `Table/Storer`s\n\ndisadvantages:\n- selecting lots of columns will be somewhat slower that an equivalent `Table`\n- requires syncing of all the indices (the coordinates of all rows)\n- delete operations will be somewhat slower than an equivalent `Table`\n\nThere are actually 2 different formats that could be used here, I propose just the single-file for now. However, The sub-nodes could be spread out in a directory and stored as separate files. This allows concurrent access with some concurrent reads allowed (this is pretty tricky, so hold off on this for now).\n\nThis `CTable` format will use the existing `PyTables` infrastructure under the hood; it is possible to use the `ctable` module however http://carray.pytables.org/docs/manual/ (this is basically what BLAZE uses under the hood for its storage backend)\n"},
{"text": "A new module `core.align` would hold a couple of generic alignment functions, one of which will be `alignmultiple` or possibly `align2` (taking 2 pandas objects) and then `align` would take a sequence of pandas objects to align.\n\nNeed to think about the API a bit more.\n\nPsuedocode-ish:\n\n``` python\ndf = DataFrame(...)\ndf2 = DataFrame(...) # possibly different index and columns than df\ns = Series(...)\nresult = pd.align([df, df2, s], how='outer') # compute the union of the indexes\n\n# smoke test 1\nresult.columns == df.columns.join(df2.columns, how='outer').join(s.index, how='outer')\nresult.index == df.index.join(df2.index, how='outer')\n\nresult2 = pd.align([df, df2, s], how='inner') # intersection\n\n# smoke test 2\nresult2.columns == df.columns.join(df2.columns).join(s.index)\nresult2.index == df.index.join(df2.index)\n```\n\ninput with `Index` types would return the union/intersection of the objects. not clear what would happen if aligning an `Index` and another non-`Index` pandas object\n"},
{"text": "This has been suggested in the pydata mailing list. seems like a simple fix and would be a nice API improvement\n"},
{"text": "The numpy way\n\n```\nIn [183]: x\nOut[183]: \n   one two\n0    1   a\n2    2   c\n3    3   d\n4    4   d\n5    5   d\n\nIn [184]: np.unique(x['two'].values,returninverse=True)\nOut[184]: (array(['a', 'c', 'd'], dtype=object), array([0, 1, 2, 2, 2]))\n```\n\nThe pandas way - maybe provide a better API to this\nmaybe: `uniques, indexer = Index(x['two']).getuniques()` ??\n\n```\nIn [186]: uniques = x['two'].unique()\n\nIn [187]: uniques\nOut[187]: array(['a', 'c', 'd'], dtype=object)\n\nIn [188]: Index(uniques).getindexernonunique(x['two'])\nOut[188]: (Int64Index([0, 1, 2, 2, 2], dtype=int64), array([], dtype=int64))\n```\n"},
{"text": "Have you ever written code that looks like this:\r\n\r\n```\r\nif isinstance(d.index, MultiIndex):\r\n    results = []\r\n    for l in d.index.levels:\r\n       for x in baz(l):\r\n          results.append(foo)\r\nelif  isinstance(d.index, Index):\r\n    for x in d.index:\r\n       foo\r\n```\r\n\r\nI've had to special case the handling of index vs. multindex several times in the past.\r\nConceptually, I should be able to treat index as a private case of MultIndex\r\nwith nlevels =1, and supporting that in the API would make things nicer.\r\n\r\n---\r\n\r\n**Edit by @cpcloud:**\r\nTasks :\r\n### API Unification\r\n\r\nMethod unification is relatively simple:\r\n- [ ]  `Index.fromtuples` and `Index.fromarrays` are just `MultiIndex.fromtuples` and `MultiIndex.fromarrays` moved to classmethods of `Index`.\r\n- [ ] `droplevel`, ` just raises on Index (right? what would it mean?): #21115\r\n- [ ] `hasduplicates` is straightforward\r\n- [ ] `truncate` should be equivalent to slicing\r\n- [ ] `reorderlevels` raises if not level=0 or name of index\r\n- [ ] `equallevels` - straightforward\r\n- [ ] `levshape` - (len(ind),)\r\n- [ ] `sortorder` - None\r\n- [ ] `getloclevel` - I think meaningless with tuple, raises whatever if not 0 or index name\r\n- [ ] `islexsorted` - doesn't need to change\r\n- [ ] `islexosrtedtuple` - doesn't need to change\r\n- [ ] `ismonotonic*`\r\n- [ ] `lexsortdepth` - doesn't need to be changed at all\r\n- [ ] `searchsorted`\r\n- [ ] `repeat`\r\n- [ ] `levels` and `labels` property for Index - question on whether it should be sorted.\r\n- [ ] change to `rename` behavior: `Index` will accept either string or single-element list; MI continues to handle only list\r\n"},
{"text": "Currently the API documentation is only available as a set of webpages that can be [here](https://www.monicahq.com/api). A user who would like to build a client of the API needs to browse 27 different pages and manually collect the information needed to build the client. Publishing Swagger document makes it easier for developers to integrate Monica into their apps and the other way around. It will also make it easy to generate the API client for the mobile app once it's decided to be rebuilt.\r\n\r\nP.S.: I tried a bit with [DarkaOnLine/L5-Swagger](https://github.com/DarkaOnLine/L5-Swagger) to see if I can do it for the project, but it doesn't seem to be a one-step thing. It's giving me errors. I'm not a PHP dev, so most of the errors are Greek to me and could take me longer to figure out."},
{"text": "Hi!\r\n\r\nI've been excited about importing contact to Monica [since 2017](https://github.com/monicahq/monica/issues/658).\r\n\r\nMy concern is that bugs with the API keep popping up that make importing not work, and with a monthly release cycle, I'm very hopeful that each fix will be \"the one\" instead of unlocking a bug farther down the line.\r\n\r\nAs an example, [I found this bug in November](https://github.com/monicahq/monica/issues/2119\r\n), it was fixed in December and released in late December. That allowed some progress on importing contacts with tags, which [ran into this bug](https://github.com/monicahq/monica/issues/1951) that was fixed in January and is pending release.\r\n\r\nI know you're a small, amazingly devoted dev team, and we're not at all entitled to speed. However, since this is an important workflow (not just to me, but to at least @degan6 and [Lukas](https://github.com/monicahq/monica/issues/932)), it might be great to have a test verifying that the full API import flow works as part of your test coverage.\r\n\r\nThanks for considering, and for all your hard work!"},
{"text": "**Is your feature request related to a problem? Please describe.**\r\n\r\nit is really hard to get behind the api. when coding in a type-save language it is hard to convert a received json into a usable object.\r\n\r\nfor example: https://app.quicktype.io?share=kwYLvUf0kZlJklI9hfmQ\r\n\r\nit gives me only half the truth of what is optional/nullable\r\n\r\n**Describe the solution you'd like**\r\nIt would be great to have the json schemas available from the api documentation\r\n"},
{"text": "We should create const in ApiController to list error codes\r\n```\r\n                    return $this->setHTTPStatusCode(400)\r\n                              ->setErrorCode(39)\r\n                              ->respondWithError(config('api.errorcodes.39'));\r\n```\r\n"},
{"text": "It would be great if we could add a way to select a contact field and search for a match in that field with any contact.\r\n\r\nI want to use the API as a contact look when someone calls my voip phone but i need a way to check if their phone number is in Monica through the API."},
{"text": "It would be awesome to add a way to get users of a tag through the API."},
{"text": "I was able to import 2,000+ contacts from my old system using the API, so am very happy with the API overall so far. The big stumbling block for me is a lack of an easy helper method to add a Tag to a Contact, even if I have the Tag.id already. Though I might be incorrect, right now it looks like I'd have to GET the Contact, add the exact right JSON for the Tag(s), and PUT it back.\r\n\r\nThanks for considering!"},
{"text": "This issue serves the purpose of tracking the progress of the API development. I haven't found anything on this yet other than it needs to be done for mobile apps, so I figured we could track or even discuss its implementation here. I'm eagerly looking forward to contributing to the iOS version of the mobile app, so this is an issue I will be watching very closely."},
{"text": "Today, the API does not allow us to get the resources of a given note something like `/note/:id/resources` could be helpful. \r\n\r\nThe hidden reason is to use the `store` of the JS framework to keep the resource for the note and avoid to trigger a request for each resource :)\r\n"},
{"text": "### Request\r\n\r\nDescribe REST API using [OpenAPI Specification](https://github.com/OAI/OpenAPI-Specification).\r\n\r\n### Why?\r\n\r\n- One source of truth for REST API\r\n- API documentation is generated from the specification. \r\n- Code can be generated to help interface with or implement the API.\r\n\r\n### How?\r\n\r\nUtilize the following [tools](https://swagger.io/tools/):\r\n\r\n- [swagger-inspector](https://swagger.io/tools/swagger-inspector/) - Gives you a good starting point from the existing implementation.\r\n- [swaggerhub](https://swagger.io/tools/swaggerhub/) - suite of tools focused on editing specification, \r\n visualizing results, and generating code.\r\n\r\n"},
{"text": "Hi,\r\n\r\nToday the API works great with existing endpoints, but search endpoint is missing I think.\r\nmaybe that will be the moment to think about searching keyword to facilitate this point ?\r\nfor example : [evernote query search](https://help.evernote.com/hc/en-us/articles/208313828-How-to-use-Evernote-s-advanced-search-syntax)\r\n"},
{"text": "Quota spec binding is used to bind mixer quota resource name to a proxy. It's done through an intermediary called `IstioService` (see https://github.com/istio/api/blob/14a33dbe9d2d387b06dbf2da68bb22581c0f20f4/mixer/v1/config/client/quota.proto#L126). We should re-consider how quotas are selected at the server-side sidecar in light of the deprecation of IstioService and v1alpha1 APIs.\r\n\r\ncc @geeknoid @ayj @louiscryan "},
{"text": "We should make use of the built in [Eloquent Resources](https://laravel.com/docs/5.7/eloquent-resources) over our custom `AbstractApiController`. Eloquent resources give us more flexibility of what is returned in the results + meta and require far less code to maintain.\r\n\r\nAs an added side effect, we should be able to switch with little effort because both solutions wrap the results in `meta` and `data`, so the output should remain the same."},
{"text": "I just had an issue where my server got a huge number of hits to the `/api/v1/status` endpoint, which basically killed my PHP-FPM server (it hit the maximum number of workers, preventing people from connecting to any other sites in the same pool).\r\n\r\nDoes Cachet cache the response to this API call? It would be good to cache its response (even if only for a short period of time, like 30 seconds), so that a large number of hits don't cause any perf issues. If a lot of hits come in at the same time, they could all receive the cached response rather than having to recompute it + hit the DB."},
{"text": "Before we can work on https://github.com/CachetHQ/Cachet/issues/1375 we need to ensure that the API is both adequately and accurately tested. We should **always** be able to rely on the API to work as expected, returning the desired results.\r\n\r\nWe should be testing for:\r\n\r\n- Events firing\r\n- Data being created, deleted and updated\r\n- Result format to be correct\r\n- Expected cases of errors\r\n\r\n## Tests to check\r\n\r\n- [x] `ComponentGroupTest`\r\n- [x] `ComponentTest`\r\n- [x] `IncidentTest`\r\n- [ ] `IncidentUpdateTest`\r\n- [ ] `MetricPointTest`\r\n- [ ] `MetricTest`\r\n- [ ] `ScheduleTest`\r\n- [ ] `SubscriberTest`"},
{"text": "I've searched existing issues, and couldn't find an answer.\r\nIs it possible to do API sorting on status for component first, then component groups?\r\n\r\nCondition:\r\nAssume you have one component in a group that is currently having major outage. All other components in other groups is OK. When I query the API, I wanted the one that is having problem is shown first.\r\n\r\nCode used:\r\n `http://localhost:8000/api/v1/components/groups?sort=status&order=desc` \r\n\r\nPlease let me know whether this can't be done.\r\n\r\nThanks!"},
{"text": "Hello,\r\n\r\nIf we have a [Stable API](https://airflow.readthedocs.io/en/latest/stable-rest-api-ref.html) ready, we can start working on tools that use it.  We can start with the Terraform provider. Terraform. is a tool for building, changing, and versioning infrastructure as a code.  A terraform provider is a way to integrate other services. Integrations already exist for a wide variety of services and software, including [Kubernetes](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs), [Google Cloud](https://www.terraform.io/docs/providers/google/index.html), [Keycloak](https://github.com/mrparkers/terraform-provider-keycloak).\r\n\r\n@houqp started working on it and prepared [small POC](https://github.com/houqp/terraform-provider-airflow).  Currently, this allows you to manage Airflow variables.\r\n\r\nI discussed this topic with good citizen @ad-m and he said we can use [terraform-provider-openapi](https://github.com/dikhan/terraform-provider-openapi) to generate a terraform provider based on the OpenAPI specification. He pointed out that this is how the company for which he works developed the [terraform-provider-hyperone](https://github.com/hyperonecom/terraform-provider-hyperone). This is also an implementation idea that we can explore.\r\n\r\nI talked to @houqp , He pointed out that implementations may be affected by the range of resources we want available. Not all resources in the API makes sense as resources in Terraform, eg TaskInstnace.\r\n\r\nWhat do you think about the Terraform provider for Airflow? What use cases should we support? What are your ideas for implementing this concept?\r\n\r\nBest regards,\r\nKamil Bregu\u0142a\r\n\r\nCC: @jaketf @potiuk"},
{"text": "## Rationale\r\nAirflow currently has 185 permissions (where 1 permission is a combo of Class + Action). The design of the permissions is inconsistent, making them difficult to modify or organize into roles. This is especially problematic for the stable API. Because the UI permissions are named after the UI view and methods, and are intended to control access to the UI view rather than the underlying resource, they don't map cleanly to API methods. \r\n\r\nWe need to define a set of permissions that control access to underlying resources, independent from the interface used to access those resources.\r\n\r\n### Aside about names\r\nFlask AppBuilder names permissions in a confusing manner. It uses the term \"permission\" to refer to both an action (for example, \"canedit\") and an action + resource pair (\"canedit on the TaskInstanceModelView\"). This document refers to a `permission` as a unique combination of `action` + `domain`, where `domain` is the entity to which access is granted. For old permission design, the domain is the UI view class. For new permission design, the domain refers to the resource model.\r\n\r\n**Description**\r\n\r\nThere are multiple types of permissions. We should consider each in turn.\r\n\r\n* **Variants of create/read/edit/delete for Airflow modelViews** - (ex. `domain: RoleModelView, permission: canshow`) - Consolidate into `cancreate`, `canread`, `canedit`, and `candelete` for the `Role` resource.\r\n\r\n* **Menu access for Airflow modelViews** - (ex. `domain: Task Instances, permission: menuaccess`) - We should do away with menu-specific permissions. If a user has read access for the resource, they should have menu access in the UI.\r\n\r\n* **Muldelete for Airflow modelViews** - (ex. `domain: PoolModelView, permission: muldelete`) - I propose combining `muldelete` and `delete` into a single `delete` action. If you can delete one, you can delete multiple.\r\n\r\n* **List for Airflow modelViews** - (ex. `domain: PoolModelView, permission: canlist`) - Same as `muldelete`. I propose combining `list` and `read` into a single `read` permission.\r\n\r\n* **Default FAB permissions** - (ex. `domain: ResetPasswordView, permission: canthisformpost`) - I propose leaving these in place. We technically could change these to match the new pattern by subclassing the default FAB views and settings custom permission mappings. If we choose to do that, it should be part of a separate issue.\r\n\r\n* **Edit/read for DAGs** - (ex. `domain: examplebranchdopoperatorv3, permission: candagread`) - Change to `canedit` and `canread` for the same `examplebranchdopoperatorv3` domain.\r\n\r\n* **Airflow view permissions** - The following is a list of examples, mapping existing permissions to proposed new ones. Where possible, this involves mapping the view permission to `model.canread`, or one of the other CRUD actions.\r\n\r\n> * Airflow.canredirecttoexternallog => TaskInstance.canread\r\n> * Airflow.candelete => Dag.candelete\r\n> * Airflow.cantask => Task.canread\r\n> * Airflow.cantrigger => Dag.cantrigger\r\n> * Airflow.candagdetails => Dag.canread\r\n> * Airflow.canclear => TaskInstance.candelete\r\n> * Airflow.canrefreshall => Dag.canread\r\n> * Airflow.canextralinks => Dag.canedit\r\n> * Airflow.canindex => Seems unnecessary?\r\n> * Airflow.canrefresh => Dag.canread\r\n> * Airflow.canxcom => XCom.canread\r\n> * Airflow.canrendered => TaskInstance.canread\r\n> * Airflow.canblocked => Dag.canread (does this modify anything?)\r\n\r\n**Additional considerations**\r\nThere are two motivations.\r\n1. Make view permissions fit for API endpoints.\r\n2. Simplify permissions to make them more usable.\r\n\r\nUltimately, there are two key questions for this issue:\r\n1. Are these permission updates the right changes to make.\r\n2. If they're the right ones, is now the right time to make them?\r\n\r\n**Related Issues**\r\n\r\n#8112 \r\n"},
{"text": "**Description**\r\n\r\nWe need additional auth backends for the new API to allow access to users who aren't use an auth proxy service.\r\n\r\n**Use case / motivation**\r\n\r\nThe new API does not have support for common authentication methods, such as JWT or basic authentication. It exclusively supports use of an authentication proxy service. Small teams using Airflow are unlikely to have auth proxies, meaning the API is difficult for them to use. By adding pre-built auth backends for common auth schemes, we increase the number of users who can easily build on top of the API\r\n\r\n**Related Issues**\r\n\r\nhttps://github.com/apache/airflow/issues/8111\r\nhttps://github.com/apache/airflow/issues/8112\r\n"},
{"text": "Hello,\r\n\r\nWe use [Speccy](https://github.com/wework/speccy) for linting API, but we should start using [Spectral](https://meta.stoplight.io/docs/spectral/README.md).\r\n\r\n> What is the difference between Spectral and Speccy\r\n> Speccy was a great inspiration for Spectral, but was designed to work only with OpenAPI v3. Spectral can apply rules to any JSON/YAML object (including OpenAPI v2/v3 and AsyncAPI).\r\n> \r\n> Speccy has been abandoned, but Spectral is steaming ahead, adding loads of functionality like custom functions, exceptions, and AsyncAPI support.\r\n\r\nBest regards,\r\nKamil\r\nCC: @houqp "},
{"text": "Hello,\r\n\r\nWe have a reference API based on [redoc](https://github.com/Redocly/redoc).\r\nhttps://airflow.readthedocs.io/en/latest/stable-rest-api/redoc.html\r\nIt has possibility to add code samples of using the API in different languages thanks to the x-codesamples extension. This will facilitate the use of this API and will also provide ready documentation for these clients. Less duplicate documentation.\r\n![image](https://github.com/Redocly/redoc/raw/master/docs/images/code-samples-demo.gif)\r\n\r\nWe can think about generating code samples automatically using ready-made tools (If possible)\r\nhttps://github.com/ErikWittern/openapi-snippet\r\nhttps://github.com/richardkabiling/openapi-snippet-cli/blob/master/src/index.ts\r\nhttps://github.com/cdwv/oas3-api-snippet-enricher\r\n\r\nBest regards,\r\nKamil Bregu\u0142a\r\n"},
{"text": "**Description**\r\n\r\nI notice that we have API specifications that require manual maintenance over time. In order to minimize the risk that it will become outdated, I propose to use its elements during integration tests ( https://github.com/apache/airflow/tree/master/tests/apiconnexion/endpoints ).\r\n\r\nFrom my own experience I know that simple verification whether the scheme (selected by hard-coded name) contained in the specification contains all the fields placed in the sample answers allows to avoid many oversights. More attention may be required if the relevant schema will be searched based on the path rather than based on the schema name. Then, however, it should not be difficult to check if the status code of the answer is provided by the specifications also.\r\n\r\nIt requires attention to ensure that this type of test verifies both that the required fields are included in the response and that the response does not contain any fields not included in the specification.\r\n\r\nThe implementation of basic tests of this type of tests should not be complex, as the current tests contain a response structure in memory, so you should find the appropriate scheme and validate this response, just as HTTP response status code is now verified: \r\n\r\nhttps://github.com/apache/airflow/blob/5eb2808/tests/apiconnexion/endpoints/testtaskendpoint.py#L131-L133\r\n\r\nExample implementation in JavaScript:\r\n\r\nhttps://github.com/hyperonecom/h1-cli/blob/5895f7414d3ebf8ceed912ef5443ebc01dd9eb69/lib/tests.js#L55-L65\r\nhttps://github.com/hyperonecom/h1-cli/blob/5895f7414d3ebf8ceed912ef5443ebc01dd9eb69/lib/tests.js#L188-L191\r\n\r\nOpenAPI uses the JSONSchema standard for the schema definition, for which there are numerous validators even for old-fashion Python ( https://json-schema.org/implementations.html).\r\n\r\n**Use case / motivation**\r\n\r\nReduce maintenance burden of OpenAPI specification\r\n\r\n**Related Issues**\r\n\r\n#8107"},
{"text": "I am not sure whether to put this as a bug or feature request (depends on how you see this).\r\nAs this might be completely independent from versions, os, k8s, ... I will not add this detail here.\r\n\r\nShort: When trying to trigger a DAG via the REST API interface more than once per second the Airflow returns a HTTP 500 error code. \r\n\r\nLong: When using Airflow REST API I can send any amount of POST calls, but when sending more calls per second returning an HTTP 500 might be very misleading. If it is expected to not send more requests as 1 per second, it should return an HTTP 400 and have a good error message. If it is intended to be able to send more than 1 POST call per second, I would expect this to be a bug and hopefully be fixed. AFAIK Airflow inside its implementation uses a timestamp with precision of seconds to create a DAG run. But this might not be fitting to having received multiple requests in the same second, since then the uniqueness requirement might be failing."},
{"text": "Hello,\r\n\r\nWe have a fantastic REST API.  However, something can always be improved. The current problem reported by users are not very friendly error messages for end-users  To address this issue, we should follow [RFC-7807](https://tools.ietf.org/html/rfc7807) more closely. \r\n\r\nOur error responses do not include the URI in the type field. We can fill this field with the link to the specification in Web UI:\r\nhttps://github.com/apache/airflow/pull/9504\r\nhttps://github.com/apache/airflow/pull/9144 \r\n\r\n>    This specification does this by identifying a specific type of problem (e.g., \"out of credit\") with a URI [RFC3986]; HTTP APIs can do this by nominating new URIs under their control, or by reusing existing ones.\r\n\r\nTitle and details are not standardized.  The API was developed by many contributors and there was freedom in the content of these fields. The content of these fields was not reviewed. \r\nIt is worth noting that the `title` field should do not contain any arguments. A common mistake is to put identifiers in the title e.g. `Dag run with ID = {} is not found`. It should not change from occurrence to occurrence of the problem.  The `details` field may contain more detailed information.\r\n\r\nBest regards,\r\nKamil\r\n\r\n  "},
{"text": "**Description**\r\n\r\nWe should prepare documentation for the REST API:\r\n\r\n- [Guide about API authentication (including custom)](https://github.com/apache/airflow/issues/8123)\r\n- [Guide about authorization and permission](https://github.com/apache/airflow/issues/8122)\r\n- [Migration guide from the experimental API to the REST API](https://github.com/apache/airflow/issues/8121)\r\n- [Guide \"How to use REST API\"](https://github.com/apache/airflow/issues/8120)\r\n- [REST API Reference](https://github.com/apache/airflow/issues/8119)\r\n\r\nMore information about the REST API is available:\r\n[AIP-32 - Airflow REST API - High-level info](https://github.com/apache/airflow/issues/8107)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A\r\n"},
{"text": "**Description**\r\n\r\nWe need endpoints that allow you to retrieve an element based on the object key - `lookup`.\r\n\r\n- GET /variables/lookup\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need endpoints that allow you to read [extra links](https://airflow.readthedocs.io/en/latest/howto/defineextralink.html).\r\n\r\nGET /dags/{dagid}/taskInstances/{taskid}/{executiondate}/links\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need to have endpoints that allow us to read information about workflow - DAG, Task. Endpoints should properly support serialization.\r\n\r\n- GET /dags/{dagid}/details\r\n- GET /dags/{dagid}/tasks\r\n- GET /dags/{dagid}/tasks/{taskid}\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nHello, \r\n\r\nWe need an endpoint that allows you to read the DAG File source code. \r\nWhen ``storedagcode`` = True, then the code should be read from the database.\r\nWhen ``storedagcode`` = False, then the code should be read from the file.\r\n\r\n- `GET /dagSources/{filetoken}`\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nHello,\r\n\r\nWe need an endpoint that allows us to **read** the current Airflow configuration.\r\n\r\n- GET /config\r\n\r\nMore details about API Endpoints:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\nHello,\r\n\r\nWe need to prepare an endpoint that will read the logs for the task. Logs are already available in Web UI, so abstractions should be created and used in both places. The code responsible for reading the log in the Web UI is in the file [`airflow/www/views.py`](https://github.com/apache/airflow/blob/master/airflow/www/views.py) (method `getlogswithmetadata`)\r\nThe token is metadata data serialized to JSON and encoded with base64. This prevents user from building clients based on a specific metadata structure. We cannot guarantee their permanent structure because it use external plugins.\r\n\r\n- GET /dags/{dagid}/taskInstances/{taskid}/{executiondate}/logs/{tasktrynumber}\r\n\r\nMore details about API Endpoints: \r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **XCOM** . We need the following endpoints:\r\n\r\n- GET /dags/{dagid}/taskInstances/{taskid}/{executiondate}/xcomValues\r\n- GET /dags/{dagid}/taskInstances/{taskid}/{executiondate}/xcomValues/{key}\r\n\r\nFor now, we focus only on read-only operations, but the others will also be implemented as the next step.\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},
{"text": "**Description**\r\n\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **Variable** . We need the following endpoints:\r\n\r\n- GET /variables\r\n- GET /variables/{variablekey}\r\n\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},
{"text": "**Description**\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **Task instance** . We need the following endpoints:\r\n\r\n- GET /dags/{dagid}/dagRuns/{dagrunid}/taskInstances\r\n- GET /dags/{dagid}/dagRuns/{dagrunid}/taskInstances/{taskid}\r\n\r\nFor now, we focus only on read-only operations, but others will also be implemented as the next step.\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},
{"text": "**Description**\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **Pools** . We need the following endpoints:\r\n\r\n- GET /pools\r\n- GET /pools/{poolname}\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},
{"text": "**Description**\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **Import error** . We need the following endpoints:\r\n\r\n- GET /importErrors\r\n- GET /importErrors/{importerrorid}\r\n\r\nFor now, we focus only on read-only operations, but others will also be implemented as the next step.\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},
{"text": "Hello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **DAG Runs** . We need the following endpoints:\r\n\r\n- GET /dags/{dagid}/dagRuns\r\n- GET /dags/{dagid}/dagRuns/{dagrunid}\r\n\r\nFor now, we focus only on read-only operations, but others will also be implemented as the next step.\r\n\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLOVE,\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},
{"text": "**Description**\r\n\r\nHello \r\n\r\nWe need to create several endpoints that perform basic read-only operations on **DAG Model** . We need the following endpoints:\r\n\r\n- GET /dags\r\n- GET /dags/{dagid}\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLove,\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},
{"text": "**Description**\r\n\r\nHello \r\n\r\nWe need to create several endpoints that perform read-only operations on **Connection** . We need the following endpoints:\r\n\r\n- GET /connections\r\n- GET /connections/{connectionid}\r\n\r\nFor now, we focus only on read-only operations, but others will also be implemented as the next step.\r\n\r\nDetailed information is available in the issue:\r\nhttps://github.com/apache/airflow/issues/8118\r\n\r\nLots of love,\r\n\r\n**Use case / motivation**\r\nN/A\r\n\r\n**Related Issues**\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need a view that makes it easier to set identical permissions for Web UI and API.\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe should write a guide that describes how to add new ones or use the current authentication method.\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe should prepare a guide that will describe how to manage permissions for the API. If there is a guide for Web UI, we can extend it. If it does not exist, then we must write from scratch.\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe should prepare a guide that will facilitate the migration from the experimental API to the API for Airflow 2.0.\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need one that will facilitate the use of the REST API\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need documentation to be available in a user-friendly form.  The YAML file does not belong to this format. We can use swagger UI or something similar\r\n\r\nMore information about the docs for REST API is available:\r\n[Docs for REST API](https://github.com/apache/airflow/issues/8143)\r\n\r\n**Use case / motivation**\r\n\r\nUsers expect the specification to be in an accessible form.\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe should create endpoints that allow us to perform operations in Airflow\r\n\r\nWe need the following read-only endpoints - Database:\r\n\r\n- [API Endpoints - Read-only - Connection ](https://github.com/apache/airflow/issues/8127)\r\n- [API Endpoints - Read-only - DAG Model ](https://github.com/apache/airflow/issues/8128)\r\n- [API Endpoints - Read-only - DAG Runs ](https://github.com/apache/airflow/issues/8129)\r\n- [API Endpoints - Read-only - Import errors ](https://github.com/apache/airflow/issues/8130)\r\n- [API Endpoints - Read-only - Pools ](https://github.com/apache/airflow/issues/8131)\r\n- [API Endpoints - Read-only - Task Instance ](https://github.com/apache/airflow/issues/8132)\r\n- [API Endpoints - Read-only - Variable ](https://github.com/apache/airflow/issues/8133)\r\n- [API Endpoints - Read-only - XCOM ](https://github.com/apache/airflow/issues/8134)\r\n\r\nWe need to implement other endpoints: \r\n\r\n- [API Endpoint - Logs ](https://github.com/apache/airflow/issues/8135)\r\n- [API Endpoint - Config ](https://github.com/apache/airflow/issues/8136)\r\n- [API Endpoint - Dag source ](https://github.com/apache/airflow/issues/8137)\r\n- [API Endpoint - Dags structure/Task ](https://github.com/apache/airflow/issues/8138)\r\n- [API Endpoint - Links ](https://github.com/apache/airflow/issues/8140)\r\n- [API Endpoint - Health - Spec and impelementataion](https://github.com/apache/airflow/issues/8144)\r\n\r\n\r\n\r\nMore information about the REST API is available:\r\n[AIP-32 - Airflow REST API - High-level info](https://github.com/apache/airflow/issues/8107)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nLinks should be added to the objects to make it easier to use. I recommend https://flask-marshmallow.readthedocs.io/en/latest/\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need to build views that share common behavior. It would be nice if the code was not repeated. We need views that will only perform CRUD operations (Create, Update, List, Update, [GET])\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need a common mechanism for deserialization and serialization of Python objects.\r\n\r\nPR with this change should define all schemas with Python objects (DAG. BaseOperator)  in accordance with OpenAPI specification\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThis will make writing new endpoints much easier. We will also have one place defining the structure, which will facilitate backward compatibility.\r\n\r\n**Related Issues**\r\n\r\nThis task is related to\r\nhttps://github.com/apache/airflow/issues/8110\r\nhttps://github.com/apache/airflow/issues/8114\r\nhttps://github.com/apache/airflow/issues/8115\r\nIt was divided so that we could do the work with smaller pieces."},
{"text": "**Description**\r\n\r\nWe need a common mechanism for deserialization and serialization of common objects. I recommend marshmallow\r\nhttps://marshmallow.readthedocs.io/en/latest/\r\nPR with this change should define all schemas without database objects and Python objects (DAG. BaseOperator)  in accordance with OpenAPI specification\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThis will make writing new endpoints much easier. We will also have one place defining the structure, which will facilitate backward compatibility.\r\n\r\n**Related Issues**\r\n\r\nThis task is related to\r\nhttps://github.com/apache/airflow/issues/8110\r\nhttps://github.com/apache/airflow/issues/8114\r\nhttps://github.com/apache/airflow/issues/8115\r\nIt was divided so that we could do the work with smaller pieces."},
{"text": "**Description**\r\n\r\nWe need a simple mechanism to authorize operations performed by the API.  It should be compatible with the Flask App Builder used by Airflow.\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThe purpose of the authorization is access control, which confirms whether a given entity is authorized to use the requested resource.\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe should prepare an authentication mechanism that allows easy extension and adding a new authentication method.\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThe purpose of authentication is to achieve a certain level of confidence that the entity is in fact the one it claims to be. Extending authentication methods allows the security requirements of various organizations to be met.\r\n\r\nPR does not need to implement any common authentication mechanism. All they have to do is accept the user ID in the request and log in.\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need a common mechanism for deserialization and serialization of SQLAlchemy objects. I recommend marshmallow-sqlalchemy\r\nhttps://marshmallow-sqlalchemy.readthedocs.io/en/latest/\r\nPR with this change should define all schemas for database objects in accordance with OpenAPI specification\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nThis will make writing new endpoints much easier. We will also have one place defining the structure, which will facilitate backward compatibility.\r\n\r\n**Related Issues**\r\n\r\nThis task is related to\r\nhttps://github.com/apache/airflow/issues/8110\r\nhttps://github.com/apache/airflow/issues/8114\r\nhttps://github.com/apache/airflow/issues/8115\r\nIt was divided so that we could do the work with smaller pieces."},
{"text": "**Description**\r\n\r\nWe must provide basic integration of Airflow with connexion. This should contain a foundation for other works. For now, it can return empty objects on any request. Each endpoint should be publicly available.\r\n\r\nMore information about High-level information about REST API:\r\nhttps://github.com/apache/airflow/issues/8107\r\n\r\n**Use case / motivation**\r\n\r\nCreating a foundation for other work.\r\n\r\n**Related Issues**\r\n\r\nN/A"},
{"text": "**Description**\r\n\r\nWe need to develop an OpenAPI spec that contains at least the following elements:\r\n\r\n- endpoints\r\n- scheme\r\n- response bodies\r\n\r\nSubsequent PR may extend the Open API spec of subsequent elements, e.g. HATEOAS, but HATEOAS must be defined after the endpoints, so it seems logical to split this task.\r\n\r\nPR is available: https://github.com/apache/airflow/issues/8107\r\n\r\nMore information about the REST API is available:\r\n[AIP-32 - Airflow REST API - High-level info](https://github.com/apache/airflow/issues/8107)\r\n\r\n**Use case / motivation**\r\n\r\nN/A\r\n\r\n**Related Issues**\r\n\r\nhttps://github.com/apache/airflow/issues/8107"},
{"text": "We currently have one [experimental API|https://airflow.readthedocs.io/en/latest/rest-api-ref.html], but despite its existence for 2 years, it has not reached a stable level. \r\n\r\nThe Polidea and Google teams together with the community want to make another attempt based on our and community experience. Airflow deserves new stable solutions.\r\n\r\nDetails are available in the AIP:\r\n\r\nThis is the first ticket regarding AIP-32 on Github.  I will try to update it regularly to inform you about the high-level status of this task.\r\n\r\nWe have merged spec. You could preview it using the following links:\r\nSwaagger UI:\r\nhttps://editor.swagger.io/?url=https://raw.githubusercontent.com/apache/airflow/master/airflow/apiconnexion/openapi/v1.yaml\r\nRedoc:\r\nhttps://redocly.github.io/redoc/?url=https://raw.githubusercontent.com/apache/airflow/master/airflow/apiconnexion/openapi/v1.yaml\r\n\r\nThis task consists of the following areas:\r\n\r\n- [API Endpoints](https://github.com/apache/airflow/issues/8118)\r\n- [Docs for REST API](https://github.com/apache/airflow/issues/8118)\r\n\r\nIn addition, we have the following tasks:\r\n- [Basic OpenAPI spec](https://github.com/apache/airflow/issues/8108)\r\n- [Basic integration Airflow and connexion](https://github.com/apache/airflow/issues/8109)\r\n- [(de)serialization for Python objects](https://github.com/apache/airflow/issues/8115)\r\n- [(de)serialization common objects](https://github.com/apache/airflow/issues/8114)\r\n- [(de)serialziation for SQLAlchemy objects](https://github.com/apache/airflow/issues/8110)\r\n- [HATEOS for API](https://github.com/apache/airflow/issues/8117)\r\n- [CRUD Framework for API](https://github.com/apache/airflow/issues/8116)\r\n- [Authorization and Permissions](https://github.com/apache/airflow/issues/8112)\r\n- [Authentication in API](https://github.com/apache/airflow/issues/8111)\r\n- [Custom WEB UI screen to control permissions](https://github.com/apache/airflow/issues/8124)\r\n- [API security tests](https://github.com/apache/airflow/issues/8113)\r\n\r\n# Resources:\r\n\r\nAIP-32 on Wiki:\r\nhttps://cwiki.apache.org/confluence/display/AIRFLOW/AIP-32%3A+Airflow+REST+API\r\nDiscussion about AIP on the mailing list:\r\nhttps://lists.apache.org/thread.html/rbcee49452f1e2714fbdb91f12cfcb115d24681d01ccb6a3845a7e699%40%3Cdev.airflow.apache.org%3E\r\nVoting: on the mailing list \r\nhttps://lists.apache.org/thread.html/rcc379dc7067397e1a631c19b9f194f2f572a2ea7a338648788911c91%40%3Cdev.airflow.apache.org%3E\r\n\r\n# Contribution\r\n\r\nWe invite everyone to contribute.  We have #sig-api to decisions and to coordinate our work.\r\nRegistration link: https://apache-airflow-slack.herokuapp.com/\r\nAll changes are labelled [\"area:API\"](https://github.com/apache/airflow/labels/area%3AAPI) on Github.\r\nInformation about current tasks are available on Github Project: https://github.com/apache/airflow/projects/1\r\n\r\n\r\n"},
{"text": "<!--\r\n\r\nWelcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.\r\nDon't worry if they're not all applicable; just try to include what you can :-)\r\n\r\nIf you need to include code snippets or logs, please put them in fenced code\r\nblocks.  If they're super-long, please use the details tag like\r\n<details><summary>super-long log</summary> lots of stuff </details>\r\n\r\nPlease delete these comment blocks before submitting the issue.\r\n\r\n-->\r\n\r\n**Description**\r\n\r\nFrom: https://issues.apache.org/jira/projects/AIRFLOW/issues/AIRFLOW-621?filter=allopenissues&orderby=affectedVersion+ASC%2C+priority+DESC%2C+updated+DESC\r\n\r\n\"I'm looking for a mechanism to fetch a scheduled DAG instance's expected start time, in Oozie world it is populated as nominal time.\r\n\r\nFor my specific use case, I have a DAG that is scheduled to run every 15 mins and one of the task needs to use the DAG instance start time to query database and fetch all the rows that have been inserted/updated in last 15 mins. I need a mechanism to fetch the expected start time of the DAG instance and pass it to the task since the task actual start time might be after the expected start time.\"\r\n\r\n\r\n**Use case / motivation**\r\n\r\na mechanism to fetch a scheduled DAG instance's expected start time\r\n\r\n**Related Issues**\r\n\r\n<!-- Is there currently another issue associated with this? -->\r\n"},
{"text": "Currently, the creation of a user via the REST API returns either \"success\" or \"error\". Adding the Zulip ID of the new user in the response would be an improvement that would facilitate integrations."},
{"text": "In `doupdateembeddeddata`, we generate a variant `updatemessage` event, which has not been properly maintained and doesn't actually match the normal format.  I fixed one issue in 00fd9afad5147beb373caf3410ddc34b0c68a83f; but we should figure out how to make this event actually match the format of a normal `updatemessage` event, and then update the `GET /events` documentation with clear advice on handling it.  \r\n\r\nI'd like us to move that event to be more clearly match the format of normal `updatemessage` events, with perhaps some clear marker for these variant events.\r\n\r\n@amanagr @showell I'd be interested in your thoughts on this."},
{"text": "User should be able to refer exact argument via a #reference link like https://chat.zulip.org/api/register-queue#arguments even for all the arguments to an endpoint. \r\n\r\nFor example, a link like https://chat.zulip.org/api/register-queue#clientcapabilities  should go directly to `clientcapabilities` argument within https://chat.zulip.org/api/register-queue#argument."},
{"text": "Tim Abbott: In our API endpoint for editing a stream's properties: https://chat.zulip.org/api/update-subscription-properties\r\n\r\nWe appear to have an unusual return value scheme, where we return basically what the client sent rather than an empty HTTP response.  While there's potentially some mild utility for communicating what changes were confirmed by the server, it feels to me like this is probably just a bug; none of our other POST/PATCH endpoints for doing a write have this pattern of returning what you just sent (which was a pattern used in the early days of Zulip, before we had the real-time events system).\r\n\r\nThe fix is to just have this endpoint `return jsonsuccess()` rather than gathering a response.  I believe no known clients actually look at the response from this endpoint, so we can just make the change (bumping `APIFEATURELEVEL` appropriately).\r\n\r\nSee https://chat.zulip.org/#narrow/stream/3-backend/topic/update-subscription-properties/near/900984 for discussion.\r\n\r\nTagging as a priority since this sort of API cleanup is valuable to do sooner rather than later."},
{"text": "Now that we're doing things that from a user perspective are bulk-deletions of messages, we need to update the interface for how Zulip's real-time events API notifies clients of message deletions to support bulk updates.  We'll use this in a few places:\r\n* The existing `dodeletemessages`, even in cases where all messages are in the same stream/topic, ends up doing individual events for each message in a giant loop.\r\n* This is a blocker for https://github.com/zulip/zulip/pull/15277.\r\n\r\nWhat's super tricky about this is that the `deletemessage` event format wasn't designed with bulk messages in mind.  So we need to detect which clients support the newer format, using our handy `clientcapabilities` feature.  Here's the plan:\r\n* Add a new supported `bulkmessagedeletion` client capability to `eventsregisterbackend` and the API documentation, in a first commit.\r\n* pass that property through `doeventsregister`.  We may want to do a preparatory refactoring commit to pass the whole `clientcapabilities` structure through to `doeventsregister` as future-proofing, (replacing the `notificationsettingsnull` parameter).  We'll ultimate need that to end up in `requesteventqueue` and finally being stored on the `ClientDescriptor` object generated created in our events system.\r\n* Then, we migrate the format for `deletemessage` events to use a `messageids` list rather than a `messageid` singleton integer.  For backwards-compatibility, clients without the feature would have a list of events generated for them.\r\n\r\n```\r\n    elif event['type'] == \"deletemessage\" and len(users) > 0 and isinstance(users[0], dict):        \r\n        # dodeletemessages used to send events with users in dict format {\"id\": <int>}             \r\n        # This block is here for compatibility with events in that format still in the queue         \r\n        # at the time of upgrade.                                                                    \r\n        # TODO: Remove this block in release >= 2.3.                                                 \r\n        userids = [user['id'] for user in cast(Iterable[Mapping[str, int]], users)]                 \r\n-        processevent(event, userids)                                                               \r\n+        processdeletionevent(event, userids)                                                               \r\n\r\n\r\ndef processdeletionevent(event: Mapping[str, Any], users: Iterable[int]) -> None:                           \r\n    for userprofileid in users:                                                                    \r\n        for client in getclientdescriptorsforuser(userprofileid):                              \r\n            if not client.acceptsevent(event):\r\n                continue\r\n            if client.hascapability(\"bulkmessagedeletion\"):\r\n                client.addevent(event)                                                              \r\n                continue\r\n            for messageid in event['messageids']:\r\n                compatibilityevent = event.copy()\r\n                compatibilityevent['messageid'] = messageid\r\n                del compatibilityevent['messageids']\r\n                client.addevent(event)\r\n```\r\n\r\nWe'll want to push hard on releasing updated apps that support the new `clientcapability`, to minimize how often the backwards-compatibility code ever runs.\r\n\r\n(And we might want to default all non-ZulipMobile app clients to assume the new capability to avoid accidental use of the old format in the future)."},
{"text": "Unless I'm mistaken, the isoldstream property on Stream objects in our API is an unnecessary; it's a boolean that is equivalent to `streamweeklytraffic != null`.\r\n\r\nNoticed while reviewing #15178.\r\n\r\nLike all API cleanups, this is a priority issue, but should be pretty accessible.  We only use `isoldstream` is a couple places in the frontend codebase, and I think they could be replaced by just computing it in `streamdata.updatecalculatedfields`."},
{"text": "Zulip's API endpoint for fetching a realm's configured custom emoji, has a legacy structure for the `author` object that contains a userid/email/name for the uploader.  Clients that display this type of settings information should have data on users by ID, so we should migrate this to an integer `authorid` field for a cleaner API.\r\n\r\nI think this just requires changes to `getrealmemojidicts` plus to tests, assuming that current mobile clients won't be sad if we change this detail.\r\n\r\nTagging as a priority since this sort of API cleanup is best done sooner rather than later."},
{"text": "We want to add a \"feature level\", which is a monotonically-increasing integer which the server advertises in the API. The purpose is to provide a way for (non-webapp) clients, like the mobile and terminal apps, to tell whether the server it's talking to is new enough to support a given API feature -- in particular a way that\r\n* is finer-grained than release numbers, so that for features developed after e.g. 2.1.0 we can use them immediately on servers deployed from master (like chat.zulip.org and zulipchat.com) without waiting the months until a 2.2 release;\r\n* is reliable, unlike e.g. looking at the number of commits since a release;\r\n* doesn't lead to a growing bag of named feature flags which the server has to go on sending forever.\r\n\r\nDiscussion in chat starting [here](https://chat.zulip.org/#narrow/stream/243-mobile-team/topic/Server.20version.20number/near/814964).\r\n\r\nSpecifically:\r\n* This would appear in the `serversettings` and `register` responses, just like `zulipversion` does. It might look like (name open for discussion):\r\n```\r\n  \"zulipfeaturelevel\": 1,\r\n```\r\n\r\n* The value will live in `version.py` -- e.g.\r\n```python\r\nFEATURELEVEL = 1\r\n```\r\n\r\n* The [protocol will be](https://chat.zulip.org/#narrow/stream/243-mobile-team/topic/Server.20version.20number/near/815008):\r\n  * When we add a feature to the API, such that clients might want to know it's available before trying to use it, we bump the feature level.\r\n  * It's OK if we forget, or don't notice a given change qualifies for it before merging the change -- we can just go back and bump it later.\r\n    * The consequence is just that clients looking for the feature will only find it on servers with versions after the feature-level bump. Effectively, from a client's perspective, it just means the new feature wasn't actually complete until the feature level was bumped.\r\n\r\n* A client can then look for the feature level, in the same way the mobile app looks for `zulipversion` since zulip/zulip-mobile#3839.\r\n  * A missing feature level should be treated as zero, corresponding to all versions before we introduce feature levels. For distinguishing different pre-feature-level versions, we'll resort to comparing version numbers. (This is pretty OK, because the finer-grained distinctions mainly matter for servers that follow master ahead of the latest release.)\r\n  * Feature level >= 1 means the server has all API features introduced before feature levels, [notably `streampostpolicy`](https://chat.zulip.org/#narrow/stream/243-mobile-team/topic/Add.20functionality.20to.20create.20announcement.20streams/near/855688).\r\n  * Future clients will look for feature levels >= other thresholds, for future features.\r\n\r\n* What about stable releases?\r\n  * Stable releases will never bump the feature level -- all 2.1.x releases will have a missing/zero feature level, all 2.2.x releases will have the feature level of 2.2.0, etc.\r\n  * Usually we [don't backport new features to stable releases](https://chat.zulip.org/#narrow/stream/243-mobile-team/topic/Server.20version.20number/near/814979) at all, so this is perfect.\r\n  * Occasionally we might. If so, we can introduce a named feature flag which the stable release with the backport can send. Then the client's condition for \"is this API feature supported\" might look like \"anything with feature level >= $threshold, *or* with feature flag `foo`\". Importantly, newer releases from master don't send the flag -- the feature level subsumes all older flags -- so flags don't accumulate.\r\n"},
{"text": "Currently the get user presence endpoint (https://zulipchat.com/api/get-presence) is the only endpoint on zulip which takes an email address. This is pretty inconsistent with the rest of the API, which only takes userids. Get user presence should be updated to only take the user id.\r\n\r\nRelates to #14302 , which would allow looking up a user via email address (to get the user ID)"},
{"text": "This is a new issue replacing #10044, which has become cluttered with time.  Zulip has done a lot of big investments over the last few years in making our API documentation use the OpenAPI format and have automated tests, which makes it possible for it to be maintained reasonably.  This issue documents the important work that needs to happen for our API documentation to be complete for common use cases.  This bundle of work could probably end up being 1-2 GSoC projects this summer.\r\n\r\n* [ ] Improve the documentation for the system to cover the automated testing system and how it works (see https://github.com/zulip/zulip/issues/12571).\r\n* [x] Change every endpoint to use the `description: |` syntax (with a paragraph afterwards), which is much nicer for writing paragraph text and allows use of markdown.\r\n* [x] Change the way the heading sections at the top of the `templates/zerver/api/` pages are written to use the OpenAPI endpoint description field instead (currently, that field in `zulip.yaml` is completely ignored and usually has junk in it).  So we'd want to migrate the text and then use the same sort of machinery we use to render arguments to render the top-of-page descriptions.  \r\n* [x] Deduplicate various common constructs, like `eventtypes`, to live in `components` and be shared between the documentation for multiple endpoints.  This will make it a lot easier to improve the quality of the descriptions of our parameters.\r\n* [ ] Many minor updates to the docs for specific endpoints are recorded [in the issue label](https://github.com/zulip/zulip/issues?q=is%3Aopen+is%3Aissue+label%3A%22area%3A+documentation+%28api+and+integrations%29%22); a lot of these could be good warm-up projects.\r\n* [x] Move any useful content from `zulip-2.0.yaml` to `zulip.yaml` and delete it.\r\n* [x] Make `zerver/openapi/zulip.yaml` work with the [Swagger online editor](https://editor.swagger.io/), which largely requires migrating any ways we've extended the OpenAPI syntax for our own purpose to something that's more standard.  Probably the first step is to upgrade to OpenAPI 3, which I think explicitly allows adding random extra keys.\r\n* [x] Figure out how to migrate the remaining bit of content in the legacy `templates/zerver/api/fixtures.json` to live in `zerver/openapi/zulip.yaml` so that we can stop maintaining two duplicate formats for fixtures.  \r\n* [x] And if we can, similarly for `templates/zerver/api/arguments.json`.\r\n* [x] Create a system similar to `generateapiargumentstable` for displaying nicer details about the formats of the various keys in the responses, ideally reusing rather than duplicating as much code as possible.  Ideally, this would do nesting correctly (where we'd visually display sub-keys indented 30px).\r\n* ~~[ ] Integrate the `/events` introspection logic in https://github.com/zulip/zulip/pull/13499 https://github.com/zulip/zulip/pull/13204~~\r\n* [ ] Extending our documentation to cover important endpoints.  A prioritized list is documented extensively in `pendingendpoints` in `testopenapi.py`; it seems best to maintain the details there.\r\n* [x] Add tags matching the organization in the `/api` sidebar (e.g. `users` for the user-related endpoints).  https://swagger.io/docs/specification/grouping-operations-with-tags/\r\n* [x] Add `operationId` values for every endpoint.  Ideally, these would match the name we'd want for a function for the API, e.g. `getmessages`.  Probably we can use the names from `python-zulip-api` for all the endpoints have have a name there.  This would fix https://github.com/zulip/zulip/issues/14629\r\n* [x] Make sure all of our example responses are currently schemas from the current REST API and validate that with automated tooling.\r\n* [x] Mark \"Deprecated Parameters\" appropriately in our documentation, and adjust our display logic to display them appropriately (Sorted to end, and with a \"Deprecated\" marker on them, similar to the Optional marker).  \r\n* [ ] Look at doing fancier styling for how our responses are rendered in the API docs\r\n* [ ] If supported, do the same deprecated handling for our response objects.\r\n\r\nDon't claim this issue via zulipbot; as it's a multi-part item; folks can coordinate with comments on what they're working on."},
{"text": "Currently, all of our API requests go through decorators that call `processclient`.  This does 3 things:\r\n* Parse the User-Agent and determine whether the client (claims to be) an official Zulip client or not\r\n* Call `getclient` and store `request.client` with a `Client` object, which is needed for\r\n* Call `processuseractivity` to trigger UserActivity logging.  For `getevents`, #13917 has some ideas for further work we'll want to do there that one should be mindful of.\r\n\r\nThere are a few problems:\r\n* `writelogline` only can specify which client was used in requests that fail authentication or rate limiting, since `processclient` is never called.  This could be fixed by calling `getclientname` in the except clause, but that's duplicating work.\r\n* `writelogline` doesn't have access to the client version, which would be a nice touch to include in `writelogline` where we display the client (e.g. `ZulipMobile/2.1.3`), since that's not stored in Client objects.  It just does `?` inside an `except` block instead.  \r\n\r\nWhat I'd like to do instead is to split `processclient`:\r\n* `processuseragent`: Does `getclientname`, attaches it as `request.clientname` and if it's an official client, store the version in `request.clientversion` (otherwise set that to None).  Called early, potentially it could be in the LogRequests `processrequest` middleware since it'd be really cheap and there'd be a lot of duplication involved in calling it everywhere via decorators.\r\n* `processclient`: Calls `getclient` and `updateuseractivity`, using `request.clientname` as an input.  We'd still store `request.client`.\r\n\r\nWe may want to do the version piece as commits at the end as it may involve some refactoring of the User-Agent code to plumb the data through properly.\r\n\r\nAs a sidenote, `processuseractivity` should probably transmit the client by ID, not name, since we don't need the name in the queue worker.  We'll definitely need backwards-compatibility code for old events here; @mateuszmandera maybe you should take this bundle of work since you're planning to work on some related Tornado-adjacent things anyway?"},
{"text": "We have a useful API for fetching historical data on topics, `/users/me/{streamid}/topics`.  This API is intended to be fast (it's used whenever one clicks \"more topics\"), and on chat.zulip.org, it is (50ms-200ms runtime).  But on zulipchat.com, where we have history from a great many organizations, it can end up quiet slow, e.g. this:\r\n\r\n```\r\nzulip=> EXPLAIN ANALYZE                 SELECT\r\n                    \"zervermessage\".\"subject\" as topic,\r\n                    max(\"zervermessage\".id) as maxmessageid\r\n                FROM \"zervermessage\"\r\n                WHERE (\r\n                    \"zervermessage\".\"recipientid\" = 103098\r\n                )\r\n                GROUP BY (\r\n                    \"zervermessage\".\"subject\"\r\n                )\r\n                ORDER BY max(\"zervermessage\".id) DESC\r\n;\r\n                                                                      QUERY PLAN                                                                       \r\n-------------------------------------------------------------------------------------------------------------------------------------------------------\r\n Sort  (cost=344541.59..344541.63 rows=14 width=14) (actual time=1327.835..1329.169 rows=2250 loops=1)\r\n   Sort Key: (max(id)) DESC\r\n   Sort Method: quicksort  Memory: 225kB\r\n   ->  HashAggregate  (cost=344541.18..344541.32 rows=14 width=14) (actual time=1324.231..1325.936 rows=2250 loops=1)\r\n         Group Key: subject\r\n         ->  Bitmap Heap Scan on zervermessage  (cost=4195.50..343615.42 rows=185152 width=14) (actual time=62.340..1152.213 rows=157280 loops=1)\r\n               Recheck Cond: (recipientid = 103098)\r\n               Heap Blocks: exact=121738\r\n               ->  Bitmap Index Scan on recipientidgroups  (cost=0.00..4149.21 rows=185152 width=0) (actual time=32.107..32.107 rows=159735 loops=1)\r\n                     Index Cond: (recipientid = 103098)\r\n Planning time: 0.134 ms\r\n Execution time: 1331.549 ms\r\n```\r\n\r\nI think what's happening here is that postgres is using a query plan that, for a stream with 160K messages of history, is doing a \"Bitmap Heap Scan\" across a large portion of the `zervermessage` table.  The structural problem, I think, is that while we have an index on `subject/topic` and an index on `zerverrecipient`, we don't have a joint index on the two, so there's no way for the database to look through the range of `subject/topic` values within a given `zerverrecipient` value (aka stream).\r\n\r\nSo my guess is the fix for this will be to create a new index, `zerverrecipienttopic`, that makes it possible for the database to do this sort of query efficiently.\r\n"},
{"text": "as we are using the zulip api a lot, we realized that creating a new stream via the post subscription api is very slow.\r\nits always more then 1 second, especially if the user already have a lot of streams. \r\n\r\nseeing that the get subscriptions  endpoint have a parameter to ignore subscribers, would this also be an option for the post endpoint ? "},
{"text": "This is an element of the broader `subject` -> `topic` migration (see #1192) that should be straightforward to change, because I believe the mobile apps don't access `subjectlinks` yet, so there's no compatibility work required.  (What the data is used for in the webapp is the little in-topic-field links we show when there is a link or linkifier matching the topic line of the message).\r\n\r\n@gnprice to confirm I'm reading the mobile codebase correctly that it's indeed not accessed.\r\n\r\nNoticed in #13587; tagging as a priority since this sort of API migration gets more complex when delayed.  We should be sure to look again at updating the docs as discussed in #13587 once this is complete."},
{"text": "Zulip should have features to set API endpoints, which would work as custom authentication endpoints for services. \r\nLike for auth, \r\nif the custom API endpoint is https://IP:PORT/login, params as userid, password -> \r\nThen authentication backend should have ability for making call to the same and verifying if the user is logged in, and can also fetch required information. "},
{"text": "Currently, Zulip's \"declare bankruptcy\" UI will remove all unread messages.  We likely want to provide a less aggressive version that preserves messages the user is likely to want to read given that they are coming back to Zulip:\r\n\r\n* Private messages\r\n* Mentions (just the `mentioned` flag, not `wildcardmentioned`)\r\n* The most recent `BANKRUPTCYKEEPRECENTCOUNT=100` messages in the user's history.  \r\n\r\nWe have a `markallasread function`/API, which probably should do what it says (mark everything as read); I'm thinking we add a few API options, e.g. keepmentions, keepprivate, and keeprecent, and have the bankruptcy JavaScript code pass all 3 as True to produce the desired user-facing behavior.\r\n\r\nImplementation-wise, in addition to changing the backend, we'll need to change the frontend as well; this function currently just marks everything as read:\r\n```\r\nexports.declarebankruptcy = function () {                                                           \r\n    exports.unreadpmcounter.clear();                                                               \r\n    exports.unreadtopiccounter.clear();                                                            \r\n    exports.unreadmentionscounter.clear();                                                         \r\n    unreadmessages.clear();                                                                         \r\n};                                                                                                   \r\n```\r\n\r\nThere's two good options for how to make that happen:\r\n* Probably the right option is to have the bankruptcy UI send your browser to a special /bankruptcy page that is effectively a loading indicator for the bankruptcy process, that when finished reloads them back to the Zulip webapp.  We might need to build a UI like this for soft-deactivated users anyway, which may save some work.\r\n* Or just have a highly visible loading indicator.\r\n* If we want to avoid that, we'll need to do something complicated involving determining which unread messages to keep on the frontend.\r\n\r\n(Split out of #6512)."},
{"text": "#### Background for anyone new to this issue/system:\r\n- Issue https://github.com/zulip/zulip/issues/12521\r\n- Pull Request https://github.com/zulip/zulip/pull/12681\r\n- Pull Request https://github.com/zulip/zulip/pull/12751 [where type checking was initially added]\r\n- Pull Request https://github.com/zulip/zulip/pull/12934 [system extended to lists]\r\n\r\n#### Issue:\r\nThe OpenAPIArgumentsTest, as far as type checking is concerned, currently compares the type hints in the python code with the type listed in the `zulip.yaml` file (static code analysis). But this isn't enough. We need to also check the type against the \"REQ converter\" element (see the REQ class in `zerver/lib/requests.py` and almost any view function in`zerver/views`) and resolve the TODO in OpenAPIArgumentsTest about converter elements:\r\n~~~\r\n# TODO: The below inference logic in cases where\r\n# there's a converter function declared is incorrect.\r\n# Theoretically, we could restructure the converter\r\n# function model so that we can check what type it\r\n# excepts to be passed to make validation here\r\n# possible.\r\n~~~"},
{"text": "Now that the OpenAPI test suite has been sufficiently developed. We should begin work on eliminating all of the endpoints in the `buggydocumentationendpoints` list under `OpenAPIArgumentsTest`. The ideal way to approach this is by adding 1 commit per endpoint resolved and then 1 commit at the end to remove the variable `buggydocumentationendpoints` and code for it from the test itself.\r\nRelated: https://github.com/zulip/zulip/issues/12521"},
{"text": "We removed all use of the legacy API endpoint for adding/removing reactions a long time ago (addreactionlegacy, removereactionlegacy); I verified that no clients use this endpoint using server logs.  We should remove the implementation.\r\n\r\nThe main effort here will be migrating existing unit tests that use this endpoint to use the newer reactions endpoint.\r\n"},
{"text": "This is really important because we've had a series of incidents of the `curl` examples in our docs being wrong.\r\n\r\nHere's a proposal for how to do this without losing any functionality (e.g. the ability to hand-decide which values to use in the examples):\r\n\r\n* We plan to use the `example` values we already have in the OpenAPI parameter data structures for this.  If we needed to, we could add support for reading a new `curlexample` value, adjacent to `example`, to the OpenAPI data in `zulip.yaml`, that has the example value to be used in the curl example for that parameter; but I don't think that should be required; it should be fine to have the `curl` example use the same example strings we use in the code.  \r\n* We add a function constructcurlexample(authemail, authapikey), with code this:\r\n```\r\ncurlfirstline = [\"curl\"] + curlmethodarguments(httpmethod)\r\nif authenticationrequired:\r\n    curlauthline = [\"-u\", \"%s:%s\" % (authemail, authapikey)]\r\nfor param, examplevalue in openapiexampleparams:\r\n    curlarguments += formatcurlargument(paramname, examplevalue)\r\nreturn curlfirstline, curlauthline, curlarguments\r\n```\r\n* We add another function, e.g. `rendercurlexample(endpoint)` that calls `constructcurlexample(\"BOTEMAILADDRESS\", ...)` and renders the nicely line-wrapped curl examples into the markdown file.\r\n* And we extend `tools/test-api` to have a loop that actually calls the curl example from the `constructcurlexample` (using `subprocess`) against the test server and checks it gets a 200 response.\r\n* We then convert the endpoints to use this new `rendercurlexample` system; we'll probably want to do this as one commit per endpoint because we may be fixing things in those individual endpoints, and the endpoints should be independent of each other (so it'll be easy to merge the easy ones fast and then revisit the rest).\r\n"},
{"text": "Something that's incredibly valuable in maintaining API documentation is making sure that we keep it up to date as the API changes.  Our API documentation isn't complete yet (see https://github.com/zulip/zulip/issues/10044), but in 1de4b94fbe04e57c5959001ec8e881f36718cb15 I added an automated test that does a comparison between our API documentation and the actual arguments declared in our view functions.  There's a bunch of TODOs remaining to make this system really nice and have it add more coverage of possible bugs (I've marked the ones with an explicit TODO comment in the test, below):\r\n\r\n* [x] Not all arguments are intended to be documented (e.g. there's backwards compatibility, etc.).  We should add an option to `REQ` with a name like `intentionallyundocumented=True` that causes it to not add the argument to the `argumentsmap` data structure. (and thus not to considered for whether we've documented all arguments that should be documented in our test).  This should let us remove a bunch of things from `BUGGYDOCUMENTATIONENDPOINTS` that aren't actually buggy, they just have arguments we don't want to document (e.g. because they're old legacy things).\r\n* [ ] Remove other things from `BUGGYDOCUMENTATIONENDPOINTS` that are there just because we added an argument and didn't update the documentation when we did so.  E.g. https://github.com/zulip/zulip/issues/11136.\r\n* [x] TODO: Figure out a way to handle this matching between the OpenAPI format for parameters and the `urls.py` format: `/messages/{messageid} <-> r'^messages/(?P<messageid>[0-9]+)$'`.  We might want to handle this by having a loop over all OpenAPI endpoints in addition to our existing loop over all `urls.py` endpoints, because doing so would allow us to also verify that every endpoint in the OpenAPI codebase is being successfully matched to the corresponding actual code.\r\n* [x] TODO: Add support for using this syntax supported by `restdispatch`: `{'POST': ('zerver.views.report.reporterror', {'intentionallyundocumented'})}),` to mark in the `urls.py` which endpoints we expect to never have public API documentation (e.g. because they're used only internally by the webapp codebase).\r\n* [x] TODO: This is lower priority than the main issue, but it'd be really cool to cross-validate the types declared in OpenAPI against the types declared in `REQ` (via the `validator` parameter).  We may not be able to do this always, but 95% of the time it's `checkint` or `checkstr` or `checkdict`, and we can certainly map those to the expected type.\r\n\r\nOur \"writing views\" developer tutorial is good reading prior to working on this, as are https://zulip.readthedocs.io/en/latest/documentation/api.html and https://zulip.readthedocs.io/en/latest/documentation/openapi.html."},
{"text": "Currently, only the /register API returns deactivated streams; the `GET /streams` endpoint only returns active streams.  While this is what one wants in most cases, there are use cases where one's explicitly looking for deactivated streams.\r\n\r\nWe need to add an argument, e.g. `includealldeactivated`, which when passed `dogetstreams`, causes it to return deactivated streams as well.\r\n\r\nSee https://chat.zulip.org/#narrow/stream/9-issues/topic/api/near/740596 for the original report."},
{"text": "At the moment there doesn't seem to be an endpoint for getting individual users through the API. This is kinda important when wanted to edit, deactivate or reactivate individuals on a server.\r\n\r\n[related topic at chat.zulip.org](https://chat.zulip.org/#narrow/stream/170-ZulipAPI.2ENET/topic/endpoint.20questions)\r\n[and here](https://chat.zulip.org/#narrow/stream/127-integrations/topic/api.20endpoint.20for.20single.20user/near/739431)"},
{"text": "This issue is appropriate for your only if you have experience researching Zulip code and writing basic docs. \r\n\r\nWe have docs for updating message flags:\r\n\r\nhttps://chat.zulip.org/api/update-message-flags\r\n\r\nThis lists all the flags that a user should update, but it doesn't explain what the flags mean.  Some flags are sort of self-explanatory to folks who use Zulip a lot--starred, read, mentioned, etc.--while others are a bit more obscure--summarizeinstream, forceexpand, etc.\r\n\r\nThe doc piece here would be pretty simple--just a table of flag and description.  Descriptions can be as short as \"true if user has starred message in an app.\"\r\n\r\nThe tricky part here is that for the more obscure flags, you'll want to research them to find out their precise meaning and what we expect API users to do with them."},
{"text": "## Description\r\n As a developer I want to be able to reuse existing files a bot has uploaded in order to save resources on the installation. \r\nI wrote a bot that provides data in the form of images to the user and at the moment I have to re-upload the same data if a user asks for it, because the bot cannot access the list of uploaded files in order to check, if this data was already provided.\r\n\r\n## Possible Solutions\r\nAdd `https://ZULIPSERVER/json/attachments` to the list of api calls, e.g. allow me to access the same data by calling `GET https://ZULIPSERVER/api/v1/attachements`.\r\nFor my purpose it would be enough to have the list of attachements, but it would make sense to also support the deletion of such, the functionality for everything like this already exists in the UI and for me it would be ok if it works the same way."},
{"text": "Currently, the `subscribe` APIs tend to access streams by name, which is great for quick API tools, but we should also support interacting with a stream by ID, so that one can be specific in a way that's resilient to streams being renamed.\r\n\r\nI believe fixing this on the backend requires tweaking `listtostreams`, and then we should follow up with frontend work to send stream IDs rather than names in the main `subscribe` code paths.  \r\n\r\n(We also had an issue where due to HipChat import violating the \"unique stream names\" invariant, and this combined with how `listtostreams` works currently to subscribe users to both the new and old streams with a given name.\r\nSee https://chat.zulip.org/#narrow/stream/127-integrations/subject/hipchat/near/656572 for context.)"},
{"text": "On `/register` when the `realmuser` event type is included, we return the data for certain users in `crossrealmbots`.\r\n\r\nThis is fine and dandy, but they're missing certain information. Most notably, `avatarurl`:\r\n```\r\n$ curl -s https://TESTORG.zulipchat.com/api/v1/register -u [...] \\\r\n  -d 'eventtypes=[\"realmuser\"]' | jq .crossrealmbots\r\n[\r\n[...]\r\n  {\r\n    \"fullname\": \"Welcome Bot\",\r\n    \"isadmin\": false,\r\n    \"isbot\": true,\r\n    \"email\": \"welcome-bot@zulip.com\",\r\n    \"userid\": 100007\r\n  },\r\n[...]\r\n]\r\n```\r\n\r\nIn fact, these bots do have avatars set, at least on my test org on zulipchat.com which I used above (on chat.zulip.org, they seem to just have gravatars):\r\n![image](https://user-images.githubusercontent.com/28173/43509328-3f4784a8-9527-11e8-8f1c-72395c9c82c9.png)\r\n\r\nThis actually fools the mobile app into not showing these bots' avatars in certain places, though we do in the message list:\r\n![image](https://user-images.githubusercontent.com/28173/43509403-71b634ca-9527-11e8-9e0f-2ab1fdaafb15.png)\r\n\r\nWe should just start including the avatarurl.\r\n\r\n(More broadly, perhaps the cross-realm bots should just be folded into the `realmusers` field of the response; I'm not sure any client benefits from getting them separately. In the mobile app, the separation has been a source of confusion in the code and small bugs; @roberthoenig 's https://github.com/zulip/zulip-mobile/pull/2830 is cleaning some of that up, and will probably end up just folding the `crossrealmbots` data into the same place we store what we get from `realmusers`. See also #5414.)\r\n"},
{"text": "This is a large multi-part issue.  The use case for this is to support custom fields like \"employee ID\" that (A) one might want to sync from another database like LDAP and (B) one might want to not allow users to change manually.\r\n\r\nThe stack of work for this would be:\r\n* [x] Add support for admins editing other users' profile fields via the API (currently not supported).  While we're at it, I imagine we'd add UI in the admin user-editing feature to modify these as well.\r\n* [ ] Document that API, maybe providing nice example code for doing this from LDAP or another data store, perhaps supporting using either the Zulip API or a `manage.py` command (the latter for LDAP is #10976, where we already have a pretty direct integration).\r\n* [ ] Add a flag for custom profile fields, allowing you to configure a field as not-user-editable."},
{"text": "Zulip has a pretty well-designed API, with a lot of good properties, but it's documentation is incomplete. \r\n The overall goal is to fully document the Zulip API on https://zulipchat.com/api, so that it's easy both for Zulip app developers (mobile, terminal, etc.) and people building custom integrations to understand how to use Zulip's API.  We have documented a few dozen endpoints, but there are many missing.  https://zulip.readthedocs.io/en/latest/documentation/api.html is our documentation on how that documentation works (thought it is somewhat out of date, in that we are migrating to using the OpenAPI format file `zerver/openapi/zulip.yaml` for some of the things like argument definitions that used to have a separate `fixtures` file).  Our general philosophy is to build a system that can be extensively unit-tested, so that we can ensure the documentation stays up-to-date as we expand and adjust the Zulip API .  Some good tasks in this area include:\r\n* [ ] Document a few endpoints.  Read 69da22d998c6f5f56931c8449f441b3fc114d59f as a recently added endpoint to refer to.  I've noted a few that seem like good starter items here.\r\n   * [ ] Document messages/${messageId}/reactions, the endpoint for adding emoji reactions\r\n   * [x] Work through the \"user groups\" endpoints mentioned below\r\n* [ ] Contribute to https://github.com/zulip/zulip/issues/12521, which has a bunch of coding tasks\r\n* [ ] Once one has done a few, update `docs/documentation/api.md` to correct the parts that are out of date.  (This is https://github.com/zulip/zulip/issues/12571)\r\n\r\nI'm very happy to provide help and support to folks working on this area.\r\n\r\n------------------------------------------------\r\n\r\nA good priority for doing this is to make sure we document all the endpoints that we need to use in the mobile (and terminal) apps, just because that's API documentation that will be readily and frequently consumed.  \r\n\r\nHere's the current list of endpoints the mobile apps interact with that don't have API docs, organized somewhat by priority.\r\n\r\nBelow are things that are not documented, and should be:\r\n* [ ] /messages/${messageId}/reactions\r\n* [x] /typing\r\n* [ ] /users \r\n\r\nAccessing uploaded files:\r\n* [ ] useruploads\r\n\r\nUser settings for notifications:\r\n* [x] settings/notifications (https://github.com/zulip/zulip/pull/10342)\r\n\r\nPresence:\r\n* [x] users/me/presence (core presence endpoint)\r\n\r\nStarring and unread counts endpoints:\r\n* [x] markallasread\r\n* [x] markstreamasread\r\n* [x] marktopicasread\r\n* [x] messages/flags\r\n\r\n------------------------------\r\n\r\nI believe the app doesn't support editing these organization settings.  Further, it should be just getting these data from `/register` with the right `eventtypes` value, so it's probably a mobile bug if we're accessing these:\r\n* [x] realm/emoji\r\n* [x] realm/filters\r\n(Unless I'm wrong and the app supports managing user groups somewhere secret?)\r\n* [x] usergroups/create\r\n* [x] usergroups/${id}\r\n* [ ] usergroups/${id}/members\r\n* [x] users/me/usergroups\r\n\r\nSame story with these stream settings/deactivation (though I could imagine changing these):\r\n* [x] streams/${id} -- does the app actually support administering streams this way? \r\nUser stream settings (color, notification settings, muted topics; does the app support administering these?): \r\n* [ ] users/me/subscriptions/mutedtopics\r\n* [x] users/me/subscriptions/properties\r\n\r\nAnd these user-level settings:\r\n* [ ] users/me/alertwords\r\n\r\n\r\n-------------------------------------------\r\n\r\nThis endpoint is mostly just for mobile/desktop to get organization icon, but has some use beyond mobile/desktop:\r\n* [x] serversettings\r\n\r\nThese are used only by mobile, so potentially lower priority for main API docs:\r\n* [x] devfetchapikey\r\n* [ ] devlistusers\r\n* [ ] fetchapikey\r\n* [ ] users/me/androidgcmregid\r\n* [ ] users/me/apnsdevicetoken\r\n\r\nAlready documented, I think (unless it's a different method):\r\n* [x] events\r\n* [x] register (Ignoring documentation of the full data format, which is its own issue)\r\n* [x] messages\r\n* [x] messages/${id}\r\n* [x] messages/${messageId}\r\n* [x] users/me/${streamId}/topics\r\n* [x] users/me/subscriptions\r\n* [x] streams\r\n\r\n--------------------------\r\n\r\n"},
{"text": "The \"Usage\" section of the recently merged new /api site should be expanded to be 1 article per common things users want to do.\r\n\r\nYou can see how to interact with the API correctly in our Python bindings in https://github.com/zulip/python-zulip-api/, specifically under `zulip/zulip/examples/`.  And what you need to generate `curl` examples is in `zproject/urls.py` and the relevant view files; see http://zulip.readthedocs.io/en/latest/writing-views.html for notes on how those work.\r\n\r\nI think the goal should be to add sections for a few really popular things next, maybe using:\r\n* [ ] Split out `calloneachevent` from the existing \"send message\" docs\r\n* [ ] Subscribe (`subscribe`)\r\n* [ ] Unsubscribe (`unsubscribe`)\r\n* [ ] List users in a Zulip organization (`list-members`)\r\n* [ ] List your subscriptions (`list-subscriptions`)\r\n* [ ] Fetch historical messages (`recent-messages`)\r\n\r\nWhile we work on this, I imagine we'll find we need to add features to the framework.  And possibly reorganze things a bit.\r\n\r\nAlso, I think we'll want to add something linked from `docs/README.md` on how to add documentation for an endpoint.  We can use the examples for the \"zulip-send\" category, which probably should be eventually renamed to something more broad.\r\n\r\nI should also mention the resource we have of Swagger data in `static/swagger/zulip.yaml`.\r\n\r\nAnyone working on this should start with bite-size pieces: Migrate one or two and then get feedback, since I think there will be a lot of little decisions to make on standards."},
{"text": "This is related to #6837 but should be a separate issue.\r\n\r\nWhat we need is a new API endpoint, that on 'GET' returns a list that is:\r\n* chronological\r\n* includes all unique PMs and Huddles for the last N months (would 3 months be a good value? we don't need it configurable but it can be)\r\n* includes the last message in each PM and huddle (for preview purposes)\r\n* includes the unread message count (can be 0 as this is not a list of unreads)\r\n"},
{"text": "Currently, our 500 errors for all URLs is sorta a 500 webpage.  For API endpoints, we should try to return something in JSON format (though still with status code 500).  I think this should be possible in most cases by doing something in our middleware.  "},
{"text": "Right now, if the server 500s on a JSON request from a client in a way that doesn't return valid JSON, we get this sort of JS exception sent back to the server.  \r\n\r\n```\r\nMessage:\r\nUnexpected token < in JSON at position 0\r\n\r\nStacktrace:\r\nSyntaxError: Unexpected token < in JSON at position 0\r\n    at JSON.parse (<anonymous>)\r\n\r\n    at Object.a.xhrerrormessage (static/min/app.f993cb5274ab.js:1331:372)\r\n       = static/js/channel.js line 110 column 27\r\n\r\n    at error (static/min/app.f993cb5274ab.js:2062:742)\r\n       = static/js/reactions.js line 25 column 28\r\n```\r\n\r\nI think we should fix this in 2 ways;\r\n* Make it ~impossible for an invalid-JSON response to come back from `channel.js` (e.g. have a set of 500 error pages for API URLs that are in JSON format), and mostly not change this code.  This probably involves changes to both our `nginx` configuration (for \"unexpected errors\" where Django isn't even running).  And then there's a probably simpler piece of changing `zerver/middleware.py` to handle unexpected exceptions in JSON style API routes at the Django level.\r\n* Make `channel.js` catch this exception and provide a more clear error message for what's happening (perhaps including the invalid JSON it received) in the report, via `blueslip.error` or something.\r\n\r\n\r\n"},
{"text": "Some users may have as many as 1M unread messages but still be caught up, because these other unread messages are in muted or even unsubscribed streams/topics.\r\n\r\nWe'll want to probably eliminate the situation of unread messages on muted streams, but for the case of muting, as well as users who just are really far behind, we should limit the `unreadmsgs` API's number of messages to look at to something sane like 25K.  Anyone with more unread messages than that probably is in a weird state, and also probably doesn't care about the really old ones.\r\n\r\nSee https://chat.zulip.org/#narrow/stream/backend/subject/unread.20data/near/259338 for some details on a specific case of this issue."},
{"text": "We are about to have a partial index for unread user messages, and it would be nice to have the expensive bankruptcy codepath use it.\r\n\r\nThis would be a simple, low risk change if there were a dedicated code path for marking messages as read, but right now the backend code (as well as the API) is conflated with starring and mentioning messages.\r\n\r\nThe bankruptcy API is low level and probably over-generalized.  The endpoint is called `/messages/flags`, and it takes a special parameter called `all` that only sensibly applies to unread messages.  You would never want a user to star all of their messages or mark them all as mentioned.  The API also allows you to set flags that are truly internal, like `ismemessage`.\r\n\r\nThe goal of this issue is not to fix every possible flaw in the `/messages/flags` API, but I think it does make sense to break out a special API for marking messages as read, and then only that API would take arguments like \"stream\" and \"all,\" and then we could also grow new features like leaving the last N hours of messages still unread.\r\n\r\nOnce we have a dedicated API for bankruptcy, it should be a fairly simple matter to take advantage of the partial indexes.  In order to use our partial index on unread messages, we do something like this:\r\n\r\n~~~\r\n    usermsgs = UserMessage.objects.filter(\r\n        userprofile=userprofile\r\n    ).extra(\r\n        where=[UserMessage.whereunread()]\r\n    )"},
{"text": "We have a few places where we're using a `PUT` HTTP method where the endpoint isn't actually idempotent.  This causes exceptions when things are automatically retransmitted by the browser incorrectly.   The main ones are:\r\n\r\n```\r\n    url(r'^realm/icon$', restdispatch,                                                              \r\n        {'PUT': 'zerver.views.realmicon.uploadicon',                                               \r\n```\r\n\r\n```\r\n    url(r'^realm/emoji/(?P<emojiname>.*)$', restdispatch,                                          \r\n        {'PUT': 'zerver.views.realmemoji.uploademoji',                                             \r\n```\r\n\r\n```\r\n    url(r'^users/me/avatar$', restdispatch,                                                         \r\n        {'PUT': 'zerver.views.usersettings.setavatarbackend',                                     \r\n```\r\n\r\n```\r\n    url(r'^users/me/alertwords$', restdispatch,                                                    \r\n        {'GET': 'zerver.views.alertwords.listalertwords',                                         \r\n         'POST': 'zerver.views.alertwords.setalertwords',                                         \r\n         'PUT': 'zerver.views.alertwords.addalertwords',                                          \r\n```\r\n\r\n3/4 of these are because we're uploading a file and we should just change to `POST`; the alertwords one is more interesting (but also hasn't been a source of problems yet).  Ideally we'd change them all. \r\n\r\nThe uploads part of this, at least, we should fix soon, since it's easy to do and I don't believe those APIs are in use in any apps (and we'd like to fix this before that changes)."},
{"text": "### Preconditions\r\nmagento 2.4-develop\r\n\r\nThe devdocs clearly state that authenticated admin users can access the rest api.\r\n\r\nhttp://devdocs.magento.com/guides/v2.0/get-started/authentication/gs-authentication.html\r\n\r\n> Resources for which administrators or integrators are authorized. For example, if administrators are authorized for the MagentoCustomer::group resource, they can make a GET /V1/customerGroups/:id call.\r\n\r\nBut this does not work. \r\n\r\n\r\n### Steps to reproduce\r\n1.  Install magento 2.1.5 from composer archive\r\n2. Log into you new admin account\r\n3. Open domain.com/rest/V1/customers/1 in the same browser in a new tab.\r\n\r\n### Expected result\r\nA response saying that this customer does not exist.\r\n\r\n### Actual result\r\nA response saying me I have no acccess rights.\r\n\r\n`<response>\r\n   <message>Consumer is not authorized to access %resources</message>\r\n    <parameters>\r\n      <resources>MagentoCustomer::customer</resources>\r\n    </parameters>\r\n</response>`\r\n"},
{"text": "<!---\r\nPlease review our guidelines before adding a new issue: https://github.com/magento/magento2/wiki/Issue-reporting-guidelines\r\nFields marked with (*) are required. Please don't remove the template.\r\n-->\r\n\r\n### Summary (*)\r\n<!--- Describe the issue you are experiencing. Include general information, error messages, environments, and so on. -->\r\nCurrently i'm using  **POST** **/V1/carts/mine/items** rest api endpoint to add products to carts.\r\nSo for grouped product if use the payload as mentioned in devdoc then this picks the default quantity which is used in adminhtml to add those products to cart.\r\n\r\nSo if i want to add custom quantity for each product to add to cart, i am unable to use \"productoption\" in payload for grouped product to specify individual quantity as i do for other product types.\r\n\r\n### Examples (*)\r\n<!--- Provide code examples or a patch with a test (recommended) to clearly indicate the problem. -->\r\nThis is the payload mentioned on swagger for above api to add products to cart.\r\n<code>\r\n{\r\n  \"cartItem\": {\r\n    \"itemid\": 0,\r\n    \"sku\": \"string\",\r\n    \"qty\": 0,\r\n    \"name\": \"string\",\r\n    \"price\": 0,\r\n    \"producttype\": \"string\",\r\n    \"quoteid\": \"string\",\r\n    \"productoption\": {\r\n      \"extensionattributes\": {\r\n        \"customoptions\": [\r\n          {\r\n            \"optionid\": \"string\",\r\n            \"optionvalue\": \"string\",\r\n            \"extensionattributes\": {\r\n              \"fileinfo\": {\r\n                \"base64encodeddata\": \"string\",\r\n                \"type\": \"string\",\r\n                \"name\": \"string\"\r\n              }\r\n            }\r\n          }\r\n        ],\r\n        \"bundleoptions\": [\r\n          {\r\n            \"optionid\": 0,\r\n            \"optionqty\": 0,\r\n            \"optionselections\": [\r\n              0\r\n            ],\r\n            \"extensionattributes\": {}\r\n          }\r\n        ],\r\n        \"configurableitemoptions\": [\r\n          {\r\n            \"optionid\": \"string\",\r\n            \"optionvalue\": 0,\r\n            \"extensionattributes\": {}\r\n          }\r\n        ],\r\n        \"downloadableoption\": {\r\n          \"downloadablelinks\": [\r\n            0\r\n          ]\r\n        },\r\n        \"giftcarditemoption\": {\r\n          \"giftcardamount\": \"string\",\r\n          \"customgiftcardamount\": 0,\r\n          \"giftcardsendername\": \"string\",\r\n          \"giftcardrecipientname\": \"string\",\r\n          \"giftcardsenderemail\": \"string\",\r\n          \"giftcardrecipientemail\": \"string\",\r\n          \"giftcardmessage\": \"string\",\r\n          \"extensionattributes\": {}\r\n        }\r\n      }\r\n    },\r\n    \"extensionattributes\": {\r\n      \"negotiablequoteitem\": {\r\n        \"itemid\": 0,\r\n        \"originalprice\": 0,\r\n        \"originaltaxamount\": 0,\r\n        \"originaldiscountamount\": 0,\r\n        \"extensionattributes\": {}\r\n      }\r\n    }\r\n  }\r\n}\r\n</code>\r\n\r\nHow am i supposed to add grouped product with individual quantity specified for the products in group product.\r\n### Proposed solution\r\n<!--- Suggest your potential solutions for this issue. -->\r\nThere should be another key in \"productoption\" for grouped products to specify quantity.\r\nWell if i use **PUT** **/V1/carts/mine/items** endpoint to update the quantity for products using item id after adding them, then i get the desired result.\r\nSo this approach is difficult to follow on mobile end and is also very slow.\r\n\r\n"},
{"text": "Usage of this method customerCustomerRepositoryV1SaveRequest results in error if you miss some required parameters in the address section (addresses/item), eg lastname, firstname.\n\nBut the customer account is created and get his own Id! \n\nThis should not happen, the complete message should be rejected and not 1 part should be processed.\n"},
{"text": "Hi guys,\n\nmore details here\n[https://forums.asp.net/t/2085117.aspx?SOAP+Envelop+change+namespace](https://forums.asp.net/t/2085117.aspx?SOAP+Envelop+change+namespace)\n\nI'm trying to write a wrapper for magento 2,\n\nat the moment im encountering an issue with the soap request & response,\n\ni have setup 2 different sites for testing purposes (magento2 & magento2data)\n\nafter i generated the service reference when pointing to 1 of the sites, i could successfully login and get a token\n\nbut when i change the endpoint in my app.config file, i run into an error\n\nMessage = \"Server returned an invalid SOAP Fault.  Please see InnerException for more details.\"\nwhich the message is\n\"Unbound prefix used in qualified name 'rpc:ProcedureNotPresent'.\"\n\ni narrowed it down to the soap envelope namespace which contains the endpoint,\n\n```\n<?xml version=\"1.0\"?>\n<s:Envelope xmlns:s=\"http://www.w3.org/2003/05/soap-envelope\">\n<s:Body xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\">\n<integrationAdminTokenServiceV1CreateAdminAccessTokenRequest xmlns=\"http://mylocal:8085/magento2/soap/default?services=integrationAdminTokenServiceV1%2CcustomerCustomerRepositoryV1\">\n<username xmlns=\"\">some-username</username>\n<password xmlns=\"\">some-password</password>\n</integrationAdminTokenServiceV1CreateAdminAccessTokenRequest>\n</s:Body>\n</s:Envelope>\n```\n\n```\n<?xml version=\"1.0\"?>\n<s:Envelope xmlns:s=\"http://www.w3.org/2003/05/soap-envelope\">\n<s:Body xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\">\n<integrationAdminTokenServiceV1CreateAdminAccessTokenRequest xmlns=\"http://mylocal:8085/magento2data/soap/default?services=integrationAdminTokenServiceV1%2CcustomerCustomerRepositoryV1\">\n<username xmlns=\"\">some-username</username>\n<password xmlns=\"\">some-password</password>\n</integrationAdminTokenServiceV1CreateAdminAccessTokenRequest>\n</s:Body>\n</s:Envelope>\n```\n\nis there a way we make magento accept what ever the xmlns=\"http://mylocal:8085/magento2data/soap/default?services=integrationAdminTokenServiceV1%2CcustomerCustomerRepositoryV1\" in the request object\n\nfurther to my quest in finding a resolution, i managed to post to a difference endpoint by adjusting the xmlns:url in the request soap\n\nbut on the response soap it also contains the xmlns='url to which ever end point i posted the data to'\nproblem is when the code tries to deserialise in the service reference, it crashes\n\nthis is because the endpoint i used to add the service reference in my development project differs from the endpoint i am calling\n\nperfect scenario of why this is important would be\n1. build a backend windows service to pull data from multiple (differenct url) magento 2 sites\n2. when development we add the service reference using a dev url\n3. when deploying the soap endpoint url is changed dynamically according to a clients website\n4. call the clients website url and pull data down (customer, orders, invoices...etc...)\n\nso the question is could we remove the namespace generation in the reference.cs file when the magento 2 classes are generated when adding the wsdl as a service reference\n\neg... remove \n\n```\n[System.Xml.Serialization.XmlTypeAttribute(Namespace=@\"http://mylocal:8085/magento2data/soap/default?services=integrationAdminTokenServiceV1%2CcustomerCustomerRepositoryV1.....\n[System.ServiceModel.MessageBodyMemberAttribute(Namespace=@\"http://mylocal:8085/magento2data/soap/default?services=integrationAdminTokenServiceV1%2CcustomerCustomerRepositoryV1....\n[System.ServiceModel.ServiceContractAttribute(Namespace=@\"http://mylocal:8085/magento2data/soap/default?services=integrationAdminTokenServiceV1%2CcustomerCustomerRepositoryV1.....\n```\n"},
{"text": "## Steps to reproduce\n1. Create configurable product\n2. Call REST API to get products, for example `GET /rest/V1/products?searchCriteria[pagesize]=3`\n## Expected result\n1. Returned array should contain your added product with extensionattributes key inside\n## Actual result\n1. The extensionattributes is not present\n\nIn ProductRepository there is a line `$this->extensionAttributesJoinProcessor->process($collection);` which calls method responsible for extensionattributes, but despite that they are not included into product. If you request product details by calling `GET /rest/V1/products/:sku` the extensionattributes array is present.\n\nIt really makes no sense to call API once to get products, then call API to get configurable product details and then, finally, call API to get values and labels for configurable options. Is there easier way?\n"},
{"text": "## Steps to reproduce\n1. Install Magento from `develop` branch.\n2. In Magento Admin: System > Extensions > Integrations click Add New Integration.\n3. Enter an integration name, email address, callback url and identity link url.\n4. Click API tab on the left and choose Resource Access All.\n   Click Save.\n5. The new integration displays in the list of integrations. Click the \"Authorize\" link for the new integration.\n6. Verify API authorizations and click the Authorize button.\n## Expected result\n1. Popup should appear containing the contents of the Identity Link URL.\n## Actual result\n1. Nothing happens unless you disable the popup blocker.\n2. There is not even a notification that the popup was blocked.\n\nNeed to improve the flow so the popup avoids the popup blocker.  As long as the popup is a direct response to a user action it will not trigger the popup blocker.\n\nAnother option is to use an iFrame.\n"},
{"text": "In the webservice catalogProductRepositoryV1SaveRequest I miss a section for websites, right now it is not possible to assign a product to multiples websites. Can you bring this back again? I would expect a section called `<websites><item>1</item><item>2</item></websites>`\n"},
{"text": "## Steps to reproduce\n1. Install Magento Community Edition (v 2.0.5) and configure multiple stores.\n2. Create a customer on a specific store (using API or web UI)\n3. Get an authorization token using API \"rest/V1/integration/customer/token\" with previous created customer\n4. Create a cart using \"V1/carts/mine\" using authorization token from previous call\n## Expected result\n1.  Cart is created with customer address and customer's store id\n## Actual result\n1. Cart is created with default store id (from default website)\n\nI'm new to Magento 2 and API but it looks like the API are not working as espected.\n\nBecause the cart a with default storeid will report wrong prices and wrong stock.\n\nMoreover, working with guest cart i have the same issue, in this case we are working with an anonymous customer and during cart creation could be right to take a default store.\nDo i have a way to change the storeid in cart calling an API method ?\n\nWe are delivering a multi store e.commerce site integrated with a c# application and we have no way to place order on the right store.\n\nThanks\n"},
{"text": "## Steps to reproduce\n1. Add items to the cart thru the rest API and inspect the result of the xhr\n## Expected result\n1. Response should be the cart. Same as when requesting '/rest/V1/guest-carts/29c8ee939e3a6146518e7340dfef75eb/totals'\n## Actual result\n1. The response is the product that we just added. eg:\n   `{\"itemid\":52,\"sku\":\"MH01-XS-Black\",\"qty\":1,\"name\":\"Chaz Kangeroo Hoodie-XS-Black\",\"price\":52,\"producttype\":\"simple\",\"quoteid\":\"569\"}`\n## \n\nIf we don\u00b4t get the totals as response we need to make twice as many requests..\n- One for adding item to the cart and then\n- One to get the cart totals.\n\nIt would be way smoother to get the cart totals as the response when adding to the cart.\n"},
{"text": "## Steps to reproduce\n1. create mine cart\n2. add product to carts\n3. select billing address for mine cart (POST /V1/carts/mine/billing-address)\n## Expected result\n1. API call for select shipping address the same as Billing Address. There is few api calls which are not working as we expected\n\nGET /V1/carts/mine/shipping-methods\nresponse: \"message\": \"Shipping address not set.\"\n\nI know that there is POST /V1/carts/mine/shipping-information but there I need to send shipping address and shipping method...\n\nSo question: Is it possible to select shipping address to mine cart?\n\nThx\n"},
{"text": "## Steps to reproduce\n1. Install Magento 2.1.0\n2. Add Simple product\n3. Add Configurable product which based on Simple\n4. Add another Simple to Configurable by WebAPI\n\nor just check these code lines:\n\nhttps://github.com/magento/magento2/blob/develop/app/code/Magento/ConfigurableProduct/etc/webapi.xml#L29\nhttps://github.com/magento/magento2/blob/develop/app/code/Magento/ConfigurableProduct/Model/Product/Type/Configurable.php#L636\n## Expected result\n1. Simple product will be associated to configurable\n## Actual result\n1. Nope\n2. Developer silently weeps\n"},
{"text": "<!--- Provide a general summary of the issue in the Title above -->\n\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\n### Preconditions\n\n<!--- Provide a more detailed information of environment you use -->\n\n<!--- Magento version, tag, HEAD, etc., PHP & MySQL version, etc.. -->\n1. Magento 2.1.0\n### Steps to reproduce\n\n<!--- Provide a set of unambiguous steps to reproduce this bug include code, if relevant  -->\n1. Add a product attribute with multiple select input type and define several options (values).\n2. Choose a sku set some values for the attribute.\n3. GET the sku with /V1/products/{sku}\n4. Wrap the returned JSON body with { \"product\": `<JSON GET result>` }.\n5. PUT the sku with /V1/products/{sku}\n6. Edit the JSON body and set the attribute value to `[]` (empty array).\n7. PUT the sku again.\n### Expected result\n\n<!--- Tell us what should happen -->\n1. The unchanged results of a GET should PUT without errors.\n2. A multi select attribute with a single option with id \"123\" selected should be sent as [\"123\"].\n3. Multiple options selected should look like [\"123\",\"456\"].\n4. Putting a an empty array should clear all the selected options for the attribute.\n### Actual result\n\n<!--- Tell us what happens instead -->\n1. Putting an unchanged GET returns errors on the multiselect attribute: `\n   {\n   \"message\": \"Error occurred during \\\"customattributes\\\" processing. Attribute \\\"occasion\\\" has invalid value. Invalid type for value: \\\"string\\\". Expected Type: \\\"string[]\\\".\"\n   }`\n2. If the attribute has a single option, it is sent as \"123\", not [\"123\"].\n3. If the attribute has multiple options, it is sent as \"123,456\", not [\"123\",\"456\"].\n4. Putting an empty array does not remove whatever options are selected.\n\n<!--- (This may be platform independent comment) -->\n"},
{"text": "<!--- Provide a general summary of the issue in the Title above -->\n\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\n### Preconditions\n\n<!--- Provide a more detailed information of environment you use -->\n\n<!--- Magento version, tag, HEAD, etc., PHP & MySQL version, etc.. -->\n1. Magento 2.1.0 CE & EE\n2. MySQL 5.6\n### Steps to reproduce\n\n<!--- Provide a set of unambiguous steps to reproduce this bug include code, if relevant  -->\n1. Open WSDL via browser or SoapUI tool: `http://{magento-2-base-url}/soap/default?wsdl&services=customerAccountManagementV1`\n2. You will only see few methods: `customerAccountManagementV1CreateAccount()`, `customerAccountManagementV1InitiatePasswordReset()`, `customerAccountManagementV1IsEmailAvailable()` & `customerAccountManagementV1ValidateResetPasswordLinkToken()`\n3. If you check the code: `$client = new SoapClient('http://{magento-2-base-url}/soap/default?wsdl&services=customerAccountManagementV1', $soapOptionsWithBearerTokens); vardump($client->getFunctions());`\n4. You can see much more available methods. But still `authenticate()` method which is mentioned in service contract (Magento\\Customer\\Api\\AccountManagementInterface) is missing.\n### Expected result\n\n<!--- Tell us what should happen -->\n1. I would want to see the `autheticate()` method in the list so that we can authenticate the customer via web service.\n\nMay be I am missing something but badly needed the feature to authenticate the customer via Web Service (SOAP).\n### Questions:\n- Why methods are limited for anonymous users? If this is the case how protected methods can be tested with tools like SoapUI ?\n- How customer is authenticated with SOAP web services? Any other workaround? Even we are not getting passwordhash in any response.\n\nWaiting for the feedback.\n"},
{"text": "1. Magento 2.1.2\n### Steps to reproduce\n1. Create new cart with basic integration token\n2. Add product to cart\n3. Customer login (for example login before checkout)\n4. Than you cant continue with calls (carts/{cartId}..) because Consumer is not authorized to access\n5. You need to assign cartId to customer so you can use call PUT carts/{cartId}\n\n{\n  \"customerId\": 124252,\n  \"storeId\":1\n}\n### Expected result\n1. This should working always. So cart should be assigned to customerId\n### Actual result\n1. When customer have created cart it is not working\n   ERROR: Cannot assign customer to the given cart. Customer already has active cart.\n   There is no possibility to change this and finish order as logged customer...\n\n**In API is missing call to clear cart something like DELETE carts/mine which is bug.**\n\nThanks\n"},
{"text": "### Preconditions\n\nThis issue exists in current HEAD in the develop branch (0c6227d2c0b68f55bc1d7fb0e878a3c597b50653). The bug is very apparent from looking at the code. The environment is irrelevant to the issue.\n### Steps to reproduce\n\nSave a non-existent product more than once simultaneously (e.g. via the REST API).\n### Expected result\n\nFor all but the \"winning\" save operation (i.e. the one which reaches the database first), either an exception should be raised or updates should be made to the product which was created by the \"winning\" save operation.\n### Actual result\n\nEither:\nMultiple products are created with the same SKU.\nOr:\nMultiple products are created with different SKUs (a number is appended to the end of the SKU to make it unique). THIS IS REALLY BAD! If the application detects a duplicate SKU when creating a product then this should be an error! It is critical that products are created with the actual SKU a business uses, and not some other meaningless SKU.\n\nThe cause of this issue is very clear when looking at the code [1] [2]. SKU uniqueness is enforced within the application code but not by a constraint in the database. This is totally not robust enough for an application of Magento's importance. Please add database constraints to enforce SKU uniqueness and remove this damaging `generateUniqueSku()` method [1].\n\n[1] [Uniqueness is enforced in application code](https://github.com/magento/magento2/blob/0c6227d2c0b68f55bc1d7fb0e878a3c597b50653/app/code/Magento/Catalog/Model/Product/Attribute/Backend/Sku.php#L99)\n[2] [There is no uniqueness constraint in the database schema for SKU](https://github.com/magento/magento2/blob/0c6227d2c0b68f55bc1d7fb0e878a3c597b50653/app/code/Magento/Catalog/Setup/InstallSchema.php#L93)\n"},
{"text": "### Preconditions\r\nMagento version: 2.1.2\r\nPHP 7\r\n\r\n### Steps to reproduce\r\n1. Create or update a product via API\r\n2. Specify a multiselect value with the same ID multiple times in an array\r\n3. Try to reindex Magento\r\n\r\n### Expected result\r\n1. Magento should just reindex because the duplicate values should also be filtered by Magento.\r\n\r\n### Actual result\r\n1. SQLSTATE[23000]: Integrity constraint violation: 1062 Duplicate entry '8419-139-3-2476' for key 'PRIMARY', query was: INSERT INTO `catalogproductindexeavidx` (`entityid`,`attributeid`,`storeid`,`value`) VALUES (?, ?, ?, ?)..... etc.\r\n\r\n### Comment\r\nI do understand we should also check this before adding the product to the API, and from now on we do. But ofcource it could always happen and I think it would be best if Magento would check if because otherwise the whole index will be broken untill the duplicate values are removed from the database.\r\n"},
{"text": "<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\r\n\r\n<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\r\n\r\n### Preconditions\r\n1. Magento 2 CE or EE (all versions)\r\n\r\n### Steps to reproduce\r\n<!--- Provide a set of unambiguous steps to reproduce this bug include code, if relevant  -->\r\n1. View Magento-generated Swagger Specification\r\n\r\n### Expected result\r\n<!--- Tell us what should happen -->\r\n1. Search criteria described in working format\r\n\r\n### Actual result\r\n<!--- Tell us what happens instead -->\r\n1. HTTP 400 response from Magento when request sent in described format. \r\n\r\n---\r\n\r\nThe Magento [dev docs](http://devdocs.magento.com/guides/v2.0/howdoi/webapi/search-criteria.html) correctly describe REST API search criteria field groups and filters usage as needing an index per-field group / filter:\r\n\r\n```\r\nsearchCriteria[filtergroups][<index>][filters][<index>][field=<fieldname>]\r\nsearchCriteria[filtergroups][<index>][filters][<index>][value=<searchvalue>]\r\nsearchCriteria[filtergroups][<index>][filters][<index>][conditiontype=<operator>]\r\n```\r\n\r\nWhen viewing the [generated Swagger documentation](http://devdocs.magento.com/swagger/), search criteria is described as follows (without a numerical index):\r\n\r\n```\r\nsearchCriteria[filterGroups][][filters][][field]\r\n```\r\n\r\nSending a request using `searchCriteria` in the format described by schema.json (without the numerical index) produces an HTTP 400 response. It also does not allow a user to define more than one `filterGroup` or `filter`, and breaks [swagger-codegen](https://github.com/swagger-api/swagger-codegen) clients.\r\n\r\nThe current `searchCriteria` implementation doesn't appear to be compatible with the [Open API Specification](http://swagger.io/specification/) in the current format. \r\n"},
{"text": "When you add a tracking code using the REST API, Magento doesn't throw an error if you try to add a tracking code for a non-existing order. Take the following example:\r\n\r\n```json\r\n{\r\n\t\"entity\": {\r\n\t\t\"orderid\": 20,\r\n\t\t\"parentid\": 12,\r\n\t\t\"tracknumber\": \"string\",\r\n    \t\"title\": \"string\",\r\n    \t\"carriercode\": \"custom\"\r\n\t}\r\n}\r\n```\r\n\r\nIn the above example, there is no order with ID '20' in my database. However, I have an order with id '19' that has a shipment with ID '12'. So posting the above to the REST API will not throw an error. Instead it will return something similar to this:\r\n\r\n```json\r\n{\r\n  \"orderid\": 20,\r\n  \"createdat\": \"2016-12-12 13:37:57\",\r\n  \"entityid\": 12,\r\n  \"parentid\": 12,\r\n  \"updatedat\": \"2016-12-12 13:37:57\",\r\n  \"weight\": null,\r\n  \"qty\": null,\r\n  \"description\": null,\r\n  \"tracknumber\": \"string\",\r\n  \"title\": \"string\",\r\n  \"carriercode\": \"custom\"\r\n}\r\n```\r\n\r\nI came across this 'feature' when a 3rd party was trying to add tracking codes to the Magento 2 API, but they accidentally used the increment ID instead of the order ID\r\n\r\n### Preconditions\r\n\r\n1. Magento 2.1.2\r\n\r\n### Steps to reproduce\r\n\r\n1. Create some orders where the increment ID is not equal to the order ID (not necessarily, but it illustrates the problem).\r\n2. Give on of the orders a shipment so you have a valid shipment ID voor the parentid in the request.\r\n3. Create an API-request with a non-existing order-ID, but the existing shipment ID.\r\n\r\n### Expected result\r\n\r\n1. I would expect the API to return an error message like 'order not found' or something.\r\n\r\n### Actual result\r\n\r\n1. It creates the tracking code on to a shipment that does not match to any order (yet).\r\n\r\nThe big problem with this is as soon as the non-existing order gets created. In that case you have a tracking number that matches with an order and a shipment, but those shipment and order do not match with each other.\r\n\r\nThis was the case in my scenario, where the shipment tracking code would show up in the 'Comments History' panel, but not on any of the actual shipments that were linked to the order. At the same time, the order that should have the tracking code didn't have the comment, but it had the tracking code in the shipment. But this was more because at that moment the increment ID and entity ID of the shipments were the same (inc id #000000013 was entity id 13).\r\n\r\nAs soon as increment ID's and entity ID's of orders / and or shipments are not 'the same', using the increment ID in the API would give unexpected results.\r\n\r\nI think an extra check on the API could prevent a lot of errors:\r\n\r\n- Check if orderid exists.\r\n- Check if parentid is actually a shipment of orderid.\r\n"},
{"text": "<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\r\n\r\n### Preconditions\r\n<!--- Provide a more detailed information of environment you use -->\r\n<!--- Magento version, tag, HEAD, etc., PHP & MySQL version, etc.. -->\r\n1. Install Magento 2.1.3 with sample data.\r\n2. Set up Rest integration.\r\n\r\n### Steps to reproduce\r\n<!--- Provide a set of unambiguous steps to reproduce this bug include code, if relevant  -->\r\n1. Try to delete an attribute option, using a REST request to delete /V1/products/attributes/{attributeCode}/options/{optionId} using method DELETE. An optionid is asked when, according to documentation of the attribute options, attribute options uses the following schema:\r\n```\r\n[\r\n  {\r\n    \"label\": \"string\",\r\n    \"value\": \"string\",\r\n    \"sortorder\": 0,\r\n    \"isdefault\": true,\r\n    \"storelabels\": [\r\n      {\r\n        \"storeid\": 0,\r\n        \"label\": \"string\"\r\n      }]\r\n  }\r\n]\r\n```\r\nand has no optionid as field.\r\n\r\n### Expected result\r\n<!--- Tell us what should happen -->\r\n1. Attribute options should contain an optionid to be identified.\r\n\r\n### Actual result\r\n<!--- Tell us what happens instead -->\r\n1. Attribute options are identified using the value field as an id. The value can't be set via creation because it acts just as an id.\r\n\r\n### Notes\r\nI understand this is no big of a deal, but it did make me lose time when I tried to modify and delete attribute options programmatically and so I feel like I should write this. It should be explained in the documentation that the option id is the value and that the value is not a real value but rather, just an automatic id to identify options. I think that Value is an ambiguous name for an automatic identifier.\r\n"},
{"text": "Reverting Graphql CORS from 2.4.1 as it poses a security concern\r\n\r\nI think the Admin Panel control is a business consideration, you guys like behavior, that's fine.\r\nBut my other comment on implementation is that the current implementation is wrong. (edited) \r\n\r\nThis implementation has some bugs:\r\nWe also check whether or not the domain is allowed, otherwise you'll have headers attached when you shouldn't./\r\nAdditionally some headers should be only be on OPTIONS some on the subsequent GraphQL request\r\n\r\nThese bugs can lead to **security** concerns so it's best to just revert and fix them in 2.4.2 \r\n\r\nhttps://github.com/magento/magento2/issues/28561\r\nhttps://github.com/magento/magento2/issues/26425\r\n"},
{"text": "### Preconditions\r\n1. Tested in Magento version: 2.1.4\r\n\r\n### Steps to reproduce\r\n1. Fresh install\r\n2. Make following REST request: \r\n\r\n> POST /rest/default/V1/products/attributes/manufacturer/options\r\n\r\nwith following Body:\r\n```\r\n{\r\n  \"option\": {\r\n    \"label\": \"Manufacturer 1\"\r\n  }\r\n}\r\n```\r\n\r\n### Expected result\r\nOne of following:\r\n1. The response Body should contain the ID of newly created option\r\n2. The response Header should contain the ID of newly created option\r\n\r\nThe reason is that external systems will usually remember the ID so that it can be later referenced (for example when the external system needs to delete that option later).\r\n\r\n### Actual result\r\n1. The response body contains just following string:\r\n`true`\r\n"},
{"text": "Should be able to search for a specific category using the name, parent, and other criteria. Otherwise, to sync products to Magento 2 and lookup the proper category IDs, we need to retrieve the full category tree which is costly if you have a large category tree."},
{"text": "### Preconditions\r\n\r\nMagento 2.1.2 CE\r\nMultiple websites configured\r\n\r\n### Steps to reproduce(working):\r\n- go in admin area\r\n- clear url-key field in product\r\n- keep checked \"Create Permanent Redirect for old URL\"\r\n- changes product name\r\n- save\r\n\r\n### Steps to reproduce(broken):\r\n- call myownmagento.test/index.php/rest/all/V1/products with correct authentication token with json data and urlkeycreateredirect or saverewriteshistory to 1\r\nexample with sample data:\r\n\r\n> {\"product\":{\"sku\":\"WSH10\",\"name\":\"Ana Running Short changed\",\"attributesetid\":9,\"price\":0,\"status\":1,\"visibility\":4,\"typeid\":\"configurable\",\"weight\":\"0.1\",\"extensionattributes\":{\"configurableproductlinks\":[2016,2017,2018,2019,2020,2021],\"stockitem\":{\"useconfigenableqtyinc\":false,\"managestock\":true,\"isinstock\":true}},\"customattributes\":[{\"attributecode\":\"taxclassid\",\"value\":2},{\"attributecode\":\"requiredoptions\",\"value\":1},{\"attributecode\":\"hasoptions\",\"value\":1},{\"attributecode\":\"categoryids\",\"value\":[\"254\"]},{\"attributecode\":\"description\",\"value\":\"Test\"},{\"attributecode\":\"urlkey\",\"value\":\"\"},{\"attributecode\":\"urlkeycreateredirect\",\"value\":1}]}}\r\n\r\n### Expected result\r\n1. old product url is redirected to new one \r\n\r\n### Actual result\r\n1. old url go in 404 error\r\n\r\n### Some additionals notes:\r\n\r\nI noticed that in admin area saving product and import there is a urlkeycreateredirect check by code to force the url redirection that missing completely in API feature"},
{"text": "Newsletter module has no Service Contracts and therefore Web API capabilities. Little amount of information is available via API for Customer module but it does not cover most of the use cases.\r\n\r\n**To Do**\r\n* Design Service Contracts for the Newsletter module\r\n* Implement all of the designed services\r\n* Cover with tests, specially api-functional\r\n* Possibly refactor some of the internal logic to use newly created services."},
{"text": "When adding a new product via REST API (POST/PUT) and the product contains media image with base64 encoding the performance increases by 300% (from .60 sec to over 2 sec depending on base64 image size.\r\n\r\n### Preconditions\r\n1. Magento CE 2.1.6\r\n\r\n### Steps to reproduce\r\n1. REST API products POST (add)\r\n\r\n### Expected result\r\n1. Normal or close to normal speeds\r\n\r\n### Actual result\r\n1. Above 2 sec response times. Below 1 sec if media is not attached\r\n\r\nAny other way of achieving the import without the massive slow-down? Calling the media URL to add the image yields the same slow-down.\r\n"},
{"text": "In the Swagger UI the services with session-based authentication are not shown even if I'm logged in.\r\n\r\nThis bug has been introduced in Magento CE 2.1.7, more precisely by APPSEC-1679 (https://magento.com/security/patches/magento-2014-and-217-security-update). This security fix disables the session-based authentication on any request without the header `X-Requested-With: XMLHttpRequest`. Swagger UI to load the services list makes a request to `http://your.domain/rest/default/schema?services=all` without that header, thus receiving the list of services with anonymous authentication.\r\n\r\n### Preconditions\r\n1. Magento CE 2.1.7\r\n2. Browser Chrome\r\n\r\n### Steps to reproduce\r\n1. Open the homepage of your Magento instance in a browser\r\n2. Log in as a customer\r\n3. Open the Swagger UI (`http://your.domain/swagger`)\r\n\r\n### Expected result\r\n4. I see all the services with anonymous AND session-based authentication\r\n\r\n### Actual result\r\n4. I don't see the services with session-based authentication\r\n\r\n### Technical details\r\nThe class providing the check on the `X-Requested-With` header is `Magento\\Webapi\\Model\\Plugin\\Authorization\\CustomerSessionUserContext`. In my opinion, this class should not restrict the access if the current request is for the API schema."},
{"text": "<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\r\n\r\nRe-opening #7511 per comment https://github.com/magento/magento2/issues/7511#issuecomment-336468740 as this affects 2.0 - 2.2 and hasn't been resolved.\r\n\r\n<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\r\n\r\n<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\r\n\r\n### Preconditions\r\n1. Magento 2 CE or EE (all versions)\r\n\r\n### Steps to reproduce\r\n<!--- Provide a set of unambiguous steps to reproduce this bug include code, if relevant  -->\r\n1. View Magento-generated Swagger Specification\r\n\r\n### Expected result\r\n<!--- Tell us what should happen -->\r\n1. Search criteria described in format compatible with [Open API / Swagger specification](https://swagger.io/specification/).\r\n\r\n### Actual result\r\n<!--- Tell us what happens instead -->\r\n1. Format not described in a Swagger-compatible manner. \r\n2. Making searchCriteria-related requests using [swagger-api/swagger-codegen](https://github.com/swagger-api/swagger-codegen) clients produces an HTTP 400 response from Magento when request sent in described format. \r\n\r\n---\r\n\r\nThe Magento [dev docs](http://devdocs.magento.com/guides/v2.0/howdoi/webapi/search-criteria.html) correctly describe REST API search criteria field groups and filters usage as needing an index per-field group / filter:\r\n\r\n```\r\nsearchCriteria[filtergroups][<index>][filters][<index>][field=<fieldname>]\r\nsearchCriteria[filtergroups][<index>][filters][<index>][value=<searchvalue>]\r\nsearchCriteria[filtergroups][<index>][filters][<index>][conditiontype=<operator>]\r\n```\r\n\r\nWhen viewing the [generated Swagger documentation](http://devdocs.magento.com/swagger/), search criteria is described as follows (without a numerical index):\r\n\r\n```\r\nsearchCriteria[filterGroups][][filters][][field]\r\n```\r\n\r\nSending a request using `searchCriteria` in the format described by schema.json (without the numerical index) produces an HTTP 400 response. It also does not allow a user to define more than one `filterGroup` or `filter`, and breaks [swagger-codegen](https://github.com/swagger-api/swagger-codegen) clients.\r\n\r\nThe `searchCriteria` implementation doesn't appear to be compatible with the [Open API Specification](http://swagger.io/specification/) in the current format. \r\n\r\n--- \r\n\r\n### Possible solutions\r\n\r\n1. Provide Swagger code-gen templates to workaround the spec \r\n2. Implement a searchCriteria builder endpoint to accept a searchCritera `POST` body and reply with the query string. \r\n3. Change searchCriteria implementation \r\n"},
{"text": "### Summary\r\nOur Magento administrators were sometimes getting a blank page when opening a reviews grid page (Marketing -> Reviews) in Magento admin panel. We are using standard Magento product reviews functionality.\r\n\r\n### Steps to reproduce\r\n\r\n1. Submit a product review with an invalid UTF-8 escape sequence, e.g. `\\xe9` in details. Make sure your string is not escaped and sent as is. You can craft and send a raw `POST` cURL request.\r\n2. As an administrator, open the reviews grid page, filter it if neccessary such, so the submitted review is in the list of grid items.\r\n\r\n### Expected result\r\nReviews grid is visible.\r\n\r\n### Actual result\r\nSeeing a blank page.\r\n\r\n### Preconditions\r\n1. Magento version: `2.2.3`;\r\n2. PHP version: `7.0.31`;\r\n3. `MAGEMODE`: `production`;\r\n4. `php.ini` settings:\r\n- `errorreporting`: `22527`;\r\n- `displayerrors`: `Off`;\r\n- `logerrors`: `On`.\r\n\r\n### Debug info\r\nUnder the hood, it was caused by Magento error handler throwing an error when catching the `iconvstrlen(): Detected an illegal character in input string` PHP notice.\r\n\r\nAn example record in `system.log`:\r\n\r\n```\r\n[2018-08-14 23:43:45] main.CRITICAL: Notice: iconvstrlen(): Detected an illegal character in input string in /var/www/project/vendor/magento/framework/Stdlib/StringUtils.php on line 152 [] []\r\n```\r\n\r\nWe traced the error to this core method: `Magento\\Framework\\Stdlib\\StringUtils::strlen()`. It is called on the reviews grid page to determine a review detail string length. It uses PHP native `iconvstrlen` function, which may raise a PHP notice if a string contains invalid characters. \r\n\r\nIt's easy to reproduce `iconvstrlen` raising PHP notice scenario locally by using an example from http://php.net/manual/en/function.iconv-strlen.php:\r\n\r\n```\r\n$ php -a\r\nInteractive shell\r\n\r\nphp > $str = \"Itrntin\\xe9liztin\";\r\nphp > print \"iconvstrlen: \".iconvstrlen($str,'UTF-8').\"\\n\";\r\nPHP Notice:  iconvstrlen(): Detected an illegal character in input string in php shell code on line 1\r\n\r\nNotice: iconvstrlen(): Detected an illegal character in input string in php shell code on line 1\r\niconvstrlen: \r\n```\r\n\r\nIn our case, the product review detail string arrived from user input. It was escaped but not sanitized so we got the notice. Even though we do not display PHP errors or notices (only log them) and `MAGEMODE` is set to `production`, Magento error handler wasn't able to handle this PHP notice properly so we ended up seeing a blank page.\r\n\r\n### Fix\r\nThe reviews with broken characters were identified as spam so we deleted them. As a quick solution, to prevent this from happening going forward, we created a plugin around `Magento\\Framework\\Stdlib\\StringUtils::strlen()` and replace `iconvstrlen` with `mbstrlen`, which calculates the length of the input string correctly without raising a PHP notice, even if a string contains an invalid escape sequence.\r\n\r\nWe have't done anything with filtering or sanitization of input product reviews data yet but it would be nice if review records in the database ended up being clean UTF-8 strings."},
{"text": "<!---\r\nPlease review our guidelines before adding a new issue: https://github.com/magento/magento2/wiki/Issue-reporting-guidelines\r\nFields marked with (*) are required. Please don't remove the template.\r\n-->\r\n\r\n### Preconditions (*)\r\n<!---\r\nProvide the exact Magento version (example: 2.2.5) and any important information on the environment where bug is reproducible.\r\n-->\r\n1. Magento 2.3.0 / 2.3.1\r\n2. At least 1 product / customer\r\n\r\n### Steps to reproduce (*)\r\n<!---\r\nImportant: Provide a set of clear steps to reproduce this bug. We can not provide support without clear instructions on how to reproduce.\r\n-->\r\n1. Create an order with rest api\r\n2. Set custom price in cartitem\r\n\r\n### Expected result (*)\r\n<!--- Tell us what do you expect to happen. -->\r\n1. The given price is used for this product\r\n\r\n### Actual result (*)\r\n<!--- Tell us what happened instead. Include error messages and issues. -->\r\n1. price is ignored / overridden with the original price\r\n\r\nI used the same logic in Magento 1, there the custom price is used in the order. In Magento 2 i tried to debug the problem. I saw in a price setter function when the price is set, first the custom price is used, then the function called again with the store price.\r\n\r\nI used following code to set the custom price\r\nAPI: /rest/V1/carts/mine/items\r\n\r\n```\r\n$productData = [\r\n    'cartitem' => [\r\n        'quoteid' => $quoteid,\r\n        'sku' => 'J-Park',\r\n        'qty' => 1,\r\n        'price' => '9.0',\r\n        \"productOption\" => [\r\n            \"extensionAttributes\" => [\r\n                \"customOptions\" => [\r\n                    [\r\n                        \"optionId\" => $optionid,\r\n                        \"optionValue\" => \"My Text\"\r\n                    ]\r\n                ]\r\n            ]\r\n        ]\r\n    ]\r\n];\r\n```"},
{"text": "### Precondition (*)\r\nMagento 2.3.x\r\n\r\n### Summary (*)\r\nI want to return JSON in camelCase in my custom webapi.xml and interface that I have created.\r\nThere is no way to configure in webapi.xml the format that you wish to use.\r\n\r\nBy default M2 will return data like so: servertransactionid but my third party JavaScript is expecting serverTransactionId for example.\r\n\r\nSecondly, It is also rather annoying to have to create an interface to do this instead of just being able to use jsonencode the data. Using jsonencode is not the solution as you end up with \\ in the format.\r\n\r\nSee below:\r\n\r\n### Examples (*)\r\n```\r\ninterface Check3dsVersionInterface\r\n{\r\n    /**\r\n     * @return string\r\n     */\r\n    public function getServerTransactionId();\r\n\r\n    /**\r\n     * @param string $serverTransactionId\r\n     * @return $this\r\n     */\r\n    public function setServerTransactionId($serverTransactionId);\r\n```\r\n\r\nwebapi.xml:\r\n```\r\n<route url=\"/V1/xx-payments/:cartId/check-3ds-version\" method=\"POST\">\r\n        <service class=\"xx\\xx\\Api\\xxServiceInterface\" method=\"check3dsVersion\"/>\r\n        <resources>\r\n            <resource ref=\"anonymous\" />\r\n        </resources>\r\n    </route>\r\n```\r\n\r\n\r\n### Proposed solution\r\nIn webapi.xml, it would be useful to specify the format of the json, for example to return as camelCase or as it is currently which is underscores\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - UI\r\n\r\n##### SUMMARY\r\nWe would like to have the ability to add schedules where we can setup the INVENTORY and the CREDENTIALS we want that scheduled JOB to run against.\r\n\r\nRight now, the only way of doing what I need with a generic JOB is to copy it and set the values for that specific JOB and then I can add a schedule.\r\n\r\n##### ENVIRONMENT\r\n\r\n* AWX version: 1.0.0.550\r\n* AWX install method:  docker on linux\r\n* Ansible version:  2,4.0\r\n* Operating System: FC26\r\n* Web Browser: Firefox/Chrome\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\nI want to run the same JOB over several different clients, and I want to add different schedules for each one.\r\nI'd like the ability to chose an INVENTORY and CREDENTIALS for that specific scheduled JOB.  Also, if the job has surveys, I'd also like to be able to chose those values.  Right now that's feasible via the environment, but it'll be nice to pick it from the survey elements directly, if possible.\r\n\r\nI could be running the same JOB against 500 clients on different schedules and each client has a different CREDENTIAL to connect. \r\n\r\nThe silly way of doing it is probably via the REST API, by duplicating the JOBS with the wanted variables, including INVENTORY and CREDENTIAL, and then adding the specific schedule to it, but I think that duplication should be avoided in this case.\r\n\r\n##### EXPECTED RESULTS\r\n\r\n##### ACTUAL RESULTS\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nAn example:\r\n\r\n-  Click 'add users to team'.\r\n- Select all users\r\n- Hit submit.\r\n- Users are added to team\r\n- Click 'add users to team' again\r\n- All users still available for selection\r\n\r\nThis is global paradigm across the app - we don't filter \"already applied\" items out of available choices. (Other examples: inventory host/group association.)\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nFiled as UI feature. May need API query help.\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nAnsible 2.0 supports the verifycommit option on checkout:\r\n\r\n```\r\nif yes, when cloning or checking out a version verify the signature of a GPG signed commit.\r\nThis requires git version>=2.1.0 to be installed.\r\nThe commit MUST be signed and the public key MUST be trusted in the GPG trustdb.\r\n```\r\n\r\nThis would require:\r\n- [ ] parameter on the project\r\n- [ ] figuring out how to use multiple keys\r\n- [ ] a gpg trustdb exposed to the checkout\r\n\r\nDoing the last of these in a way that's not gross may be complicated.\r\n\r\n##### ADDITIONAL INFORMATION\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nWe currently are fixed on:\r\n- celery 3.1.17\r\n- kombu 3.0.35\r\n\r\nCurrent upstream is:\r\n- celery 4.1.0\r\n- kombu 4.1.0\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nGiven that we're not even on the latest 3.1.x due to issues seen when upgrading, there is a high risk of complication here.\r\n"},
{"text": "##### ISSUE TYPE\r\n<!--- Pick one below and delete the rest: -->\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n<!-- Pick the area of AWX for this issue, you can have multiple, delete the rest: -->\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n<!-- Briefly describe the problem. -->\r\nCurrently the permissions structure does not allow for job template admins to cancel a scheduled job. This can cause issues if a job or workflow gets stuck.\r\n\r\n##### ENVIRONMENT\r\nAWX version: 3.1.4\r\nAnsible version: 2.4\r\nOperating System: RHEL 7\r\nWeb Browser: Chrome\r\n<!--\r\n* AWX version: X.Y.Z\r\n* AWX install method: openshift, minishift, docker on linux, docker for mac, boot2docker\r\n* Ansible version:  X.Y.Z\r\n* Operating System:\r\n* Web Browser:\r\n-->\r\n\r\n##### STEPS TO REPRODUCE\r\n1. schedule a job or workflow as any user\r\n2. attempt to cancel the job with a non-system admin user\r\n\r\n<!-- For bugs, please show exactly how to reproduce the problem. For new\r\nfeatures, show how the feature would be used.  -->\r\n\r\n##### EXPECTED RESULTS\r\n\r\nExpectation is that admin on job template/workflow should allow for these scheduled jobs to be canceled.\r\n\r\n\r\n<!-- For bug reports, what did you expect to happen when running the steps\r\nabove? -->\r\n\r\n##### ACTUAL RESULTS\r\n\r\nReceive a 403 access denied on the UI/API when trying to cancel a scheduled job. \r\n\r\n<!-- For bug reports, what actually happened? -->\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n<!-- Include any links to sosreport, database dumps, screenshots or other\r\ninformation. -->\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nAnsible in 2.4 supports multiple vault credentials, each with its own ID. AWX needs to support this as well.\r\n\r\nThis breaks the 1-1 vault credential/job template mapping, and therefore requires some work.\r\n\r\nLikely has knock-on effects for #223 \r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nCurrently, our GCP credentials are:\r\n- service account email\r\n- project name\r\n- key\r\n\r\nHowever, current GCP best practice is to use the JSON format of their credentials (which is what GCP gives you by default). This JSON file contains all the above data (and more).\r\n\r\nWe should move to just supporting the JSON format, rather than requiring people to cut and paste values out of the JSON.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nInventory variable name/value for determining whether a host is active from ENV, with fallback to **./awx/settings/defaults.py** with fallback to defautls.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.0.592 / latest\r\n* AWX install method: docker on linux\r\n* Ansible version:  2.4.0\r\n* Operating System: CentOS 7\r\n\r\n##### STEPS TO REPRODUCE\r\nset ENV variable for source based inventory in my case, but should be more genral like in your code\r\n- SCMENABLEDVAR\r\n- SCMENABLEDVALUE\r\n\r\n```\r\n./awx/main/tasks.py:        if getattr(settings, '%sENABLEDVAR' % src.upper(), False):\r\n./awx/main/tasks.py:                        getattr(settings, '%sENABLEDVAR' % src.upper())])\r\n./awx/main/tasks.py:        if getattr(settings, '%sENABLEDVALUE' % src.upper(), False):\r\n./awx/main/tasks.py:                        getattr(settings, '%sENABLEDVALUE' % src.upper())])\r\n```\r\n\r\n##### EXPECTED RESULTS\r\nthe hosts enabled state is changed for hosts, where the **ENABLEDVAR** and **ENABLEDVALUE** matches for given source type.\r\n\r\n##### ACTUAL RESULTS\r\nWhens et ENV variabel for SMC based inventory, nothing happens. The **installer/imagebuild/files/settings.py** needs to be changed and the docker image full rebuild is needed.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nGeneraly, consider to read more variables from ENV, where UI allows to set it."},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nThere are likely cases where multiple secret/temp files are necessary for managed credential types (e.g. [winrm certificate auth](https://github.com/diyan/pywinrm/#valid-transport-options) requires `certpem` and `certkeypem` files).\r\n\r\nThis should be backwards compatible w/ current `template`/`{{tower.filename}}` syntax.\r\n\r\n##### EXPECTED RESULTS\r\n\r\nSomething like:\r\n\r\n```\r\n\"injectors\": {\r\n    \"file\": {\r\n        \"template.cert\": \"{{cert}}\",\r\n        \"template.key\": \"{{key}}\"\r\n    },\r\n    \"env\": {\r\n        \"MYCERT\": \"{{tower.filename.cert}}\",\r\n        \"MYKEY\": \"{{tower.filename.key}}\"\r\n    }\r\n}\r\n```"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nWhen you run a playbook, it ends with a summary of what hosts were changed.\r\n\r\nNot so for inventory imports. For the purpose of allowing tools that integrate with AWX to write idempotent logic, we should produce a simple True/False metric with inventory imports that signifies whether any host/group/variables content was changed, or it if was an effective no-op.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: all\r\n* AWX install method: N/A\r\n* Ansible version:  all\r\n\r\n##### EXPECTED RESULTS\r\n\r\nA \"changed\" flag at the end of an inventory import. Possibly in the API for inventory imports.\r\n\r\n##### ACTUAL RESULTS\r\n\r\nLogs, but these are non-obvious and likely incomplete.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nSomeone using the awx-manage inventory import command might also like to have this info.\r\n\r\nThis could also apply for system jobs.\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nIn my projects I have several playbooks organized by folder. For example:\r\n```\r\n\"\u2500\u2500 playbooks\r\n\u2502\u00a0\u00a0 \"\u2500\u2500 network\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 facts.yml\r\n\u2502\u00a0\u00a0 \"\u2500\u2500 spectrum\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \"\u2500\u2500 common\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \"\u2500\u2500 fixstartss.yml\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 restrict.yml\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \"\u2500\u2500 files\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 fixedStartSS.pl\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \"\u2500\u2500 patch\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \"\u2500\u2500 applypatch.yml\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \"\u2500\u2500 main.yml\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \"\u2500\u2500 recover\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \"\u2500\u2500 filterplugins -> ../../../plugins/filterplugins/\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \"\u2500\u2500 main.yml\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 recover.yml\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wlc\r\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 sysinfo.yml\r\n\u2514\u2500\u2500 plugins\r\n    \u2514\u2500\u2500 filterplugins\r\n        \u2514\u2500\u2500 lsprimary.py\r\n```\r\n\r\nIf I want to reuse the lsprimary filter plugin I need to copy it to every playbook or create a symlink. AWX should load the plugins from the plugins folder in the root of the project, like its done for roles.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.0.561\r\n* AWX install method: docker on linux\r\n* Ansible version:  2.4.0.0\r\n* Operating System: Centos 7.4"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - Installer\r\n\r\n##### SUMMARY\r\nPostgreSQL 10.0 was just released. It features great improvements such as more efficient streaming replication, faster performance, and better user security by using SHA256 instead of MD5 for users' password hashes. Since this is the development environment for Ansible Tower, it would be great to see this version supported and tested out for a future release of Tower (3.3 or 3.4).\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.0-588\r\n* AWX install method: all\r\n\r\n##### STEPS TO REPRODUCE\r\nInstall AWX. It will deploy a PostgreSQL 9.6 container.\r\n\r\n##### EXPECTED RESULTS\r\nInstalling AWX should deploy and use PostgreSQL 10.0.\r\n\r\n##### ACTUAL RESULTS\r\nPostgreSQL 9.6 is installed and used.\r\n\r\n##### ADDITIONAL INFORMATION\r\nRelease highlights: https://www.postgresql.org/about/news/1786/\r\nMajor upgrades notes for PostgreSQL 9 to 10: https://www.postgresql.org/docs/10/static/release-10.html#idm46046834255504\r\nFull release notes: https://www.postgresql.org/docs/10/static/release-10.html"},
{"text": "##### ISSUE TYPE\r\n- Feature Idea\r\n\r\n##### COMPONENT NAME\r\n- API\r\n - UI\r\n\r\n##### SUMMARY\r\nAs an AWX user I should be able to create a smart inventory smart host filter based on host vars associated with a host. The behavior of this feature should be the same as how a user can create a smart host filter based on `ansiblefacts`.\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\nA dictionary called something like `hostvars` should be created that can be searched like this with a smart host filter:\r\n\r\n`hostvars.mycoolhostvar:foo`\r\n\r\n##### ACTUAL RESULTS\r\n\r\nI would expect this to only result in showing hosts that have a host variable `mycoolhostvar` set to `foo`.\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Bug Report\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nWhen you launch a job, the API returns the full serialization of the new job, as if you performed a GET to the job detail view.\r\n\r\nWhen you update a project, it returns a minimal response like `{\"projectupdate\": 42}`.\r\n\r\nArguably, the project update endpoint (and others) should follow suit and return the full serialization of the newly-created object.\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\nPOST to `/api/v2/jobtemplates/N/launch/`\r\n\r\nPOST to `/api/v2/projects/N/update/`\r\n\r\n##### EXPECTED RESULTS\r\n\r\nconsistency\r\n\r\n##### ACTUAL RESULTS\r\n\r\nsomewhat similar"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nOur upstream dependency `os-client-config` [support it](https://docs.openstack.org/os-client-config/latest/user/configuration.html). `tasks.py` should be modified to officially support it, too."},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nWe have an immensely powerful search/filter feature... but it's hard to use.\r\n\r\nExample use case: \"I want to query for all job templates using an AWS credential\".\r\n\r\nAnswer: `jobtemplate.extracredentials.credentialtype.name:\"Amazon Web Services\"`\r\n\r\nThis requires knowing the following about the API structure:\r\n- that you go through the 'jobtemplate' redirect from the unified job template view\r\n- that the credentials are stored in `extracredentials`\r\n- that you now need to look at `credentialtype`\r\n- that it's delineated by `name`, not a `kind:aws` or similar\r\n\r\nIn essence, you need to either know the API like the back of your hand, or have the API browser open in another tab to craft the query.\r\n\r\nWe should do better.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.x\r\n\r\n##### ADDITIONAL INFORMATION\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nAWX backend is not only a REST service provider, but also a client to multiple third-party services, like log aggregators, notification backends, inventory sources and SSO providers. Ideally when AWX communicates with these services (via HTTPS in most of the cases), user should have the freedom to use custom CA certificates for verification, and to toggle off CA cert verification in general.\r\n\r\nThis issue is meant to be a tracker of all issues and pull requests that's related to providing CA cert verification toggle and customization for one or more third-party services, so that we can implement global verification toggle and customization Tower configurations when we have enough individual solutions.\r\n\r\nThis won't be an easy one-off work, as AWX rely on a non-trivial number of services, and individual implementation could be quite scattered around the code base. So any issues or pull requests that could enrich the list below is more than welcomed.\r\n\r\n- [x] Logging handlers.\r\n- [ ] Openstack inventory source: #404 #405.\r\n- [ ] LDAP auth provider: #411.\r\n- [ ] source control: #490 "},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\nIn a job template, I think it would be great to be able to select multiple inventories. Also a feature like being able to target every hosts in every inventories for a job would be fantastic. "},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - Installer\r\n\r\n##### SUMMARY\r\nI cannot use SSL encrypted LDAP authentication because I see no way to provide my CA certificate to the containers or have an option in the installer to disable certificate verification.\r\n\r\nWhen logging in a certificate verification error comes up. \r\n\r\nIt would be awesome to have a `additionalcacertificate` or `skipverifyssl` variable in the insaller inventory.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: d02221702fd680cd38068ec49597a7b627228c04\r\n* AWX install method: docker on linux\r\n* Ansible version:  2.3.2.0\r\n* Operating System: Centos 7 (epel)\r\n* Web Browser: Firefox\r\n\r\n##### STEPS TO REPRODUCE\r\n* Configure ansible-awx to use SSL encrypted ldap auth\r\n* Login\r\n\r\n##### EXPECTED RESULTS\r\nSuccessful login\r\n\r\n##### ACTUAL RESULTS\r\n```\r\n2017-10-12 10:06:12,414 DEBUG    djangoauthldap Initiating TLS\r\n2017-10-12 10:06:12,426 WARNING  djangoauthldap Caught LDAPError while authenticating username: CONNECTERROR({'info': \"TLS error -8172:Peer's certificate issuer has been marked as not trusted by the user.\", 'desc': 'Connect error'},)\r\n```\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea - Documentation\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nPiggybacking on https://github.com/ansible/awx/issues/166, the idea is that things created by a user in a team are created in a 'team' context by default and accessible by that team.\r\n\r\nExample:\r\n- Credentials created by the user are automatically assigned to the team (not the user)\r\n- JTs created by the user are automatically permissioned to the team\r\n- Schedules created by the user are editiable and manageable by the team\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nI think to do this in the most user-obvious and \"makes sense\" way, we'd need to logically map \"the team that gave the user permission to do the thing\" -> \"the team that is assigned to it.\"\r\n\r\nFor example, a user may be in 3 teams A, B, and C, but if team B is the one who can access the job template, team B is the one that can modify the schedules they create for it.\r\n"},
{"text": "##### ISSUE TYPE\r\n - Bug Report\r\n\r\n##### COMPONENT NAME\r\n - UI\r\n\r\n##### SUMMARY\r\n<!-- Briefly describe the problem. -->\r\nTo see the current job status, the execution steps and the real elapsed time, users must reload the webpage manually. AWX should provide these job informations update on the fly to the UI.\r\n\r\n##### ENVIRONMENT\r\n* AWX version:  1.0.1.31\r\n* AWX install method: openshift, minishift, docker on linux, docker for mac, boot2docker\r\n* Ansible version:  2.4.0\r\n* Operating System: VirtualBox 5.2.26 - Centos 7 (AWX using official github installation guide)\r\n* Web Browser: MacOS Sierra --> Google ChromeVersion 61.0.3163.100 (Official Build) (64-bit) and Safari Version 11.0 (12604.1.38.1.7)\r\n\r\n##### STEPS TO REPRODUCE\r\n<!-- For bugs, please show exactly how to reproduce the problem. For new\r\nfeatures, show how the feature would be used.  -->\r\n\r\n1 - Create a job template.\r\n2 - Go to the JOBS view.\r\n3 - Choose it on the list.\r\n4 - Click in the launch/Relaunch(Rocket) button.\r\n5 - The status of the execution will modify to \"PENDING\" status and it will not be updated any more. \r\n6 - Reload the webpage and verify the new job status and the tasks log.\r\n\r\n##### EXPECTED RESULTS\r\n<!-- For bug reports, what did you expect to happen when running the steps\r\nabove? -->\r\nAfter launch/Relaunch the Job, the job status view must load all the new job informations automatically. Users wants to see the job changes and its status on the fly. (Like Jenkins Console screen, for example).\r\n\r\n##### ACTUAL RESULTS\r\n<!-- For bug reports, what actually happened? -->\r\nUsers must reload the JOB webpage manually to be able to see the real job status and task logs.\r\n\r\n##### ADDITIONAL INFORMATION\r\n<!-- Include any links to sosreport, database dumps, screenshots or other\r\ninformation. -->\r\nScreen Recorder of this issue:\r\n\r\nhttps://youtu.be/gB8X2VSf5XE"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nWe have generic clean-up options for old jobs. People ask for multiple enhancements to this:\r\n- only SCM updates\r\n- only inventory updates\r\n- per-job cleanup (on the 'COMPLETED JOBS' page for a particular job)\r\n- per-org cleanup\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nOptions include doing a batch delete (dependent on https://github.com/ansible/awx/issues/19) or a new management job (dependent on writing it)\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nWe support github.com OAuth for connecting to Tower.\r\n\r\nWe should support GitHub Enterprise as well (it's supported by the underlying library).\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nawx/api/serializers.py:\r\n```\r\n    def getexternalaccount(self, obj):\r\n        accounttype = None\r\n        if getattr(settings, 'AUTHLDAPSERVERURI', None) and featureenabled('ldap'):\r\n            try:\r\n                if obj.pk and obj.profile.ldapdn and not obj.hasusablepassword():\r\n                    accounttype = \"ldap\"\r\n            except AttributeError:\r\n                pass\r\n        if (getattr(settings, 'SOCIALAUTHGOOGLEOAUTH2KEY', None) or\r\n                getattr(settings, 'SOCIALAUTHGITHUBKEY', None) or\r\n                getattr(settings, 'SOCIALAUTHGITHUBORGKEY', None) or\r\n                getattr(settings, 'SOCIALAUTHGITHUBTEAMKEY', None) or\r\n                getattr(settings, 'SOCIALAUTHSAMLENABLEDIDPS', None)) and obj.socialauth.all():\r\n            accounttype = \"social\"\r\n        if (getattr(settings, 'RADIUSSERVER', None) or\r\n                getattr(settings, 'TACACSPLUSHOST', None)) and obj.enterpriseauth.all():\r\n            accounttype = \"enterprise\"\r\n        return accounttype\r\n```\r\n\r\nThis field appears to be entirely informational. Any reason to not have it just be the account source ('google-oath', 'saml', etc.)\r\n\r\n##### ADDITIONAL INFORMATION\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nFor all our authentication providers, we have hardcoded icons and tooltip names, i.e., a Google icon and 'Sign in with Google'.\r\n\r\nWe should allow setting a 'name'  for a configuration. This name would be used in the tooltip. For example, setting the 'name' of the configuration to 'frobozz.com google domain'  would change the login tooltip to 'Login with frobozz.com google domain'.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.x\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nThis would just be a field on the login configurartion.\r\n\r\nA custom icon could be an additional enhancement on top of this.\r\n"},
{"text": "##### ISSUE TYPE\r\n<!--- Pick one below and delete the rest: -->\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n<!-- Pick the area of AWX for this issue, you can have multiple, delete the rest: -->\r\n - API\r\n\r\n##### SUMMARY\r\nAdd ability to adjust the timezone for notifications sent from Tower. For example: email notifications are currently hard coded to utilize UTC and cannot be modified.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 3.2+\r\n\r\n##### ADDITIONAL INFORMATION\r\n`$ date -u && cat /var/spool/mail/toweruser`\r\n\r\n```\r\nMon Oct 16 18:51:46 UTC 2017\r\nFrom donotreply@tower.example.com  Mon Oct 16 14:51:43 2017\r\nReturn-Path: <donotreply@tower.example.com>\r\nX-Original-To: toweruser@tower.example.com\r\nDelivered-To: toweruser@tower.example.com\r\nReceived: from tower.example.com (tower.example.com [127.0.0.1])\r\n        by tower.example.com (Postfix) with ESMTP id E1E8C20C978C\r\n        for <toweruser@tower.example.com>; Mon, 16 Oct 2017 14:51:43 -0400 (EDT)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=\"utf-8\"\r\nContent-Transfer-Encoding: 7bit\r\nSubject: Job #25 'Demo Job Template' succeeded:\r\n https://tower.example.com/#/jobs/25\r\nFrom: donotreply@tower.example.com\r\nTo: toweruser@tower.example.com\r\nDate: Mon, 16 Oct 2017 18:51:43 -0000\r\nMessage-ID: <20171016185143.29044.83729@tower.example.com>\r\n\r\nJob #25 had status successful, view details at https://tower.example.com/#/jobs/25\r\n\r\n{\r\n    \"status\": \"successful\", \r\n    \"credential\": \"Demo Credential\", \r\n    \"name\": \"Demo Job Template\", \r\n    \"started\": \"2017-10-16T18:51:36.678918+00:00\", \r\n    \"extravars\": \"{}\", \r\n    \"traceback\": \"\", \r\n    \"friendlyname\": \"Job\", \r\n    \"createdby\": \"admin\", \r\n    \"project\": \"Demo Project\", \r\n    \"url\": \"https://tower.example.com/#/jobs/25\", \r\n    \"finished\": \"2017-10-16T18:51:43.589154+00:00\", \r\n    \"hosts\": {\r\n        \"localhost\": {\r\n            \"skipped\": 0, \r\n            \"ok\": 2, \r\n            \"changed\": 0, \r\n            \"dark\": 0, \r\n            \"failed\": false, \r\n            \"processed\": 1, \r\n            \"failures\": 0\r\n        }\r\n    }, \r\n    \"playbook\": \"helloworld.yml\", \r\n    \"limit\": \"\", \r\n    \"id\": 25, \r\n    \"inventory\": \"Demo Inventory\"\r\n}\r\n```"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nAnsible 2.5 will adjust the playbook stats returned via callback... AWX may need updated for this.\r\nFiling for tracking reasons.\r\n\r\nAFAIK, the UI does not use this event, so it's just on the API side for now.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nhttps://github.com/ansible/ansible/issues/31245"},
{"text": "##### ISSUE TYPE\r\n<!--- Pick one below and delete the rest: -->\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n<!-- Pick the area of AWX for this issue, you can have multiple, delete the rest: -->\r\n - UI\r\n - API\r\n\r\n##### SUMMARY\r\n<!-- Briefly describe the problem. -->\r\n\r\nThis is half Feature Idea, half Bug because this makes ansible galaxy bearly useable in AWX/Tower.\r\n\r\nAs described in http://docs.ansible.com/ansible-tower/3.2.1/html/userguide/projects.html#ansible-galaxy-support , AWX is capable of running ansible-galaxy to load dependencys like Roles over a Ansible mechanism. Unfortunaly AWX/Tower seem to fail providing authentication credentials to galaxy. Thus it is not possible to safely clone from git repositoriy or provide any requirements where authentication is required.\r\n\r\n##### ENVIRONMENT\r\n\r\n- AWX version: 1.0.1.32\r\n- AWX install method: docker install via local build\r\n- Ansible version:  2.4.0.0\r\n- Host System: RHEL 7.4\r\n- Web Browser: Chrome\r\n\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\nThe project must include a roles/requirements.yml file with git requirements which require authentication for cloning.\r\n\r\nMy requirements contain following.\r\n```\r\n# Install a role from a specific git branch\r\n- name: myrole\r\n  src: ansible-awx@mydomain:8081/2911-ansible/roles/myrole.git\r\n  scm: git\r\n  version: master\r\n```\r\n\r\n##### EXPECTED RESULTS\r\n\r\n<!-- For bug reports, what did you expect to happen when running the steps\r\nabove? -->\r\n\r\nThe clone of roles defined in requirements.yml should work.\r\nAt least the credentials for the project should be used for cloning from the same git server via ansible galaxy.\r\nIn the best case, the requirements.yml get detected and parsed after the clone of the project and AWX could somehow implement a method to even supply different credentials to different entrys in the requirements.yml\r\n\r\n##### ACTUAL RESULTS\r\n\r\n<!-- For bug reports, what actually happened? -->\r\n\r\nThe Cloning of a whole project with a requirements.yml is sucessfull however when running a template , ansiblegalaxy gets executed (which was skipped when cloning a whole project) and is cloning the roles without password or empty Passwort possibly because none where provided in the URI\r\n\r\n```\r\nTASK [fetch galaxy roles from requirements.yml] ********************************\r\nThe authenticity of host 'mydomain (10.xx.xx.xx)' can't be established.\r\nECDSA key fingerprint is SHA256:blabla\r\nECDSA key fingerprint is MD5:blabla\r\nAre you sure you want to continue connecting (yes/no)? \r\nansible-awx@mydomain's password: \r\nansible-awx@mydomain's password: \r\nansible-awx@mydomain's password: \r\nfatal: [localhost]: FAILED! => {\"changed\": true, \"cmd\": [\"ansible-galaxy\", \"install\", \"-r\", \"requirements.yml\", \"-p\", \r\n[.....]\r\n```\r\n\r\nSo as workaround one would have to add the credentials to the git SCM what nobody should ever do.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nCould be combined with #106 "},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nIt can take a very long time to run an ad hoc command against a single host if that host exists inside of a larger inventory.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: all\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\nCreate an inventory with a large number of groups & hosts, then run a simple ad hoc command against a single host.\r\n\r\n##### EXPECTED RESULTS\r\n\r\nThe output of `awxrest.py` (soon to be from internal method on Host model) should return only the hosts that the command is being ran against.\r\n\r\n##### ACTUAL RESULTS\r\n\r\nPassing the script contents of the entire inventory to Ansible can take a long time, bottlenecking the command run.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nAfter merging https://github.com/ansible/awx/pull/459 the implementation of this will be much more simple.\r\n\r\n`ansible-inventory` takes an option `--list-hosts`, which is analogous to --limit in playbook commands. Proof:\r\n\r\n```\r\n$ ansible-inventory --list-hosts helloworld -i static/justhelloworld --list\r\n{\r\n    \"meta\": {\r\n        \"hostvars\": {\r\n            \"helloworld\": {}\r\n        }\r\n    }, \r\n    \"all\": {\r\n        \"children\": [\r\n            \"ungrouped\"\r\n        ]\r\n    }, \r\n    \"ungrouped\": {\r\n        \"hosts\": [\r\n            \"helloworld\"\r\n        ]\r\n    }\r\n}\r\n```\r\n\r\n(EDIT: this example was superficial and the functionality doesn't actually exist)\r\n\r\nTo keep the interface standard, that means we should allow use of a queryparam `listhosts`, which will accept a list of hosts, in the same format that --limit does.\r\n\r\nThis should then be used anytime we're running an ad hoc command, because it will not affect the functionality of the command. We can not use this speedup for playbooks, because it will affect variables that can be accessed in the playbook."},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nAdd an organization-level role that would allow a user to execute all templates in the organization\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.0\r\n\r\n##### STEPS TO REPRODUCE\r\nThis would allow assignment of general execute permissions to users without requiring assignment to teams or assignment of permissions at the template level.\r\n"},
{"text": "##### ISSUE TYPE\r\n<!--- Pick one below and delete the rest: -->\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n<!-- Pick the area of AWX for this issue, you can have multiple, delete the rest: -->\r\n - API\r\n\r\n##### SUMMARY\r\n<!-- Briefly describe the problem. -->\r\nWhen using Tower to run the playbook with containing the winpexec module, it would return error, which is working fine in Ansible core\r\n\r\n##### ENVIRONMENT\r\n<!--\r\n* AWX version: 3.0.3\r\n* AWX install method: openshift, minishift, docker on linux, docker for mac, boot2docker\r\n* Ansible version:  2.3.0\r\n* Operating System:\r\n* Web Browser:\r\n-->\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\nThe config in the yml is:\r\n- name: Application Release Execution\r\n  winpsexec:\r\n    chdir: '{{releasefolder}}/'\r\n    command: cmd.exe /c \"{{releasefolder}}/appRelease.bat >> {{releasefolder}}/release.log\"\r\n    executable: '{{psexecpath}}'\r\n    hostnames: localhost\r\n    username: '{{appserverusername}}'\r\n    password: '{{appserverpassword}}'\r\n\r\n##### EXPECTED RESULTS\r\n\r\n<!-- For bug reports, what did you expect to happen when running the steps\r\nabove? -->\r\n\r\n##### ACTUAL RESULTS\r\n\r\nERROR! no action detected in task. This often indicates a misspelled module name, or incorrect module path.\r\n\r\nThe error appears to have been in '/var/lib/awx/projects/1232<projectname>/roles/appserver/tasks/release.yml': line 1, column 3, but may\r\nbe elsewhere in the file depending on the exact syntax problem.\r\n\r\nThe offending line appears to be:\r\n\r\n\r\n- name: Application Release Execution\r\n  ^ here\r\n\r\n\r\nThe error appears to have been in '/var/lib/awx/projects/1232<projectname>/roles/appserver/tasks/release.yml': line 1, column 3, but may\r\nbe elsewhere in the file depending on the exact syntax problem.\r\n\r\nThe offending line appears to be:\r\n\r\n\r\n- name: Application Release Execution\r\n  ^ here\r\n<!-- For bug reports, what actually happened? -->\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n<!-- Include any links to sosreport, database dumps, screenshots or other\r\ninformation. -->\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\nWorkflow job templates do not allow arbitrary extravars entry on launch.\r\n\r\n##### STEPS TO REPRODUCE\r\nEdit a workflow job template.\r\n\r\n##### EXPECTED RESULTS\r\nAbility to prompt for variables on launch.\r\n\r\n##### ACTUAL RESULTS\r\nIt has survey option (which go into extravars), but no ability to prompt for variables on launch like you can with a JT, which allows for arbitrary extravars entry on launch.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nField `askvariablesonlaunch` on JT:\r\n\r\nhttps://github.com/ansible/awx/blob/764356bf477f339dfa933dce7eb825940c3fc2fd/awx/main/models/jobs.py#L248-L251\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nCurrently if a credential type set up an environment variable injector, like `\"ENVNAME\": \"{{someinputvalue}}\"`, but the input value is not provided in credential `inputs`, the environment variable will still be set up as an empty string: `ENVNAME=''`.\r\n\r\nIn reality, however, it is usually preferred the environment variable is *not set in the first place* if the value is not given, rather than providing an empty environment variable.\r\n\r\nIt would be better if we figure out a way of doing that.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: N/A\r\n* AWX install method: docker for mac\r\n* Ansible version: N/A\r\n* Operating System: N/A\r\n* Web Browser: N/A\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nPing @ryanpetrello for opinion.\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nCan the tower-cli have an option to upload inventories?\r\n\r\nInstead of creating the inventory object and adding groups and hosts.\r\n\r\nSimiliar to the functionality that the admin can use tower-manage (tower-manage inventoryimport option).\r\n\r\n##### ENVIRONMENT\r\n* AWX version: AWX 1.0.0.588\r\n* AWX install method:  docker on linux\r\n* Ansible version: Ansible 2.4.0.0\r\n\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\n\r\n##### EXPECTED RESULTS\r\n\r\n\r\n##### ACTUAL RESULTS\r\n\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Documentation\r\n\r\n##### COMPONENT NAME\r\n - UI\r\n\r\n##### SUMMARY\r\nI am attempting to setup ldap integrations to our AD. With the setup there is no feature to allow me to get feedback in regards to errors when querying AD. I am only allowed to save. So far, I have been unsuccessful in configuring this auth mechanism. Do you have additional documentation with steps on setting this up?\r\n\r\n##### ENVIRONMENT\r\n* AWX version: AWX 1.0.1.93\r\n* AWX install method: docker on linux\r\n* Ansible version:  2.4.0.0\r\n* Operating System: CentOS Linux release 7.2.1511 \r\n* Web Browser: Chrome\r\n\r\n##### STEPS TO REPRODUCE\r\nLog into AWX, Settings, Authentication, LDAP\r\n\r\n##### EXPECTED RESULTS\r\n\r\nConnection to Active Directory\r\n\r\n##### ACTUAL RESULTS\r\n\r\n<!-- For bug reports, what actually happened? -->\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n<!-- Include any links to sosreport, database dumps, screenshots or other\r\ninformation. -->\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\nCurrently there is no way (that I can find) to add inventory and limit to an API call (or via the UI) when launching a workflow job template. Regular job templates include this functionality. \r\n\r\n##### ENVIRONMENT\r\n\r\n* AWX version: 1.0.1.50\r\n* AWX install method: docker\r\n* Ansible version:  2.4.0\r\n* Operating System: Linux\r\n* Web Browser: Chrome\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\n\r\n##### EXPECTED RESULTS\r\n\r\n\r\n##### ACTUAL RESULTS\r\n\r\n\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n\r\n##### SUMMARY\r\nAdd ability to limit fields in output for all get list requests (get projects list, get inventory list, get job templates list etc.)\r\n`https://.../?fields=id,description`\r\n\r\n##### EXPECTED RESULTS\r\n\r\nWill be included only `id` and `description` fields in output json.\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nThe UI used to use the `team` field or the `user` field to designate the \"owner\" a long time in the past. It no longer uses this, but the API still caries logic for it. This logic should be on the deprecation schedule.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.1\r\n* AWX install method: docker for mac\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\ndo OPTIONS request for `/api/v2/credentials/`, or post with user or team fields in the body.\r\n\r\nIt's confusing to have fields valid for creation, but not valid for editing.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nInstead of this design, the user who creates the credential should be automatically assigned adminrole, like we do with job templates.\r\n\r\nWe have a serializer dedicated to this specific task.\r\n\r\nhttps://github.com/ansible/awx/blob/a0f1c8fc7c490fa368731690ca8c781b1dce5e2c/awx/api/serializers.py#L2162-L2236\r\n\r\nThat entire thing can be removed when we get rid of these.\r\n"},
{"text": "##### ISSUE TYPE\r\n<!--- Pick one below and delete the rest: -->\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n<!-- Pick the area of AWX for this issue, you can have multiple, delete the rest: -->\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\nAdd an interface which allows users to replace the CACERT bundles used within Tower for custom cacerts. This allows users to add a cacert bundle for requests for scenarios such as winrm interfacing with Windows hosts using custom signed certificates. This is to circumvent modifying the cacert bundle on the backend of the Tower server and losing custom changes during upgrades/re-installations.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 3.2+\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nWhen I update an inventory and look at the jobs list, I can't tell which inventory source I updated\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.1\r\n* AWX install method: docker for mac\r\n\r\n##### STEPS TO REPRODUCE\r\nCreate 1 inventories, and 2 inventory sources inside of it.\r\n\r\nLaunch both inventory sources.\r\n\r\nGo to JOBS in UI.\r\n\r\n##### EXPECTED RESULTS\r\n\r\nI can tell apart updates from the 2 different sources.\r\n\r\n##### ACTUAL RESULTS\r\n\r\nCannot.\r\n\r\nThe API generates the name for the inventory updates.\r\n\r\n###### my example\r\n\r\nIn the API for both updates, you find:\r\n\r\n```json\r\n \"name\": \"Custom Script Example\",\r\n ```\r\n\r\nthe user experience for the inventory sources view:\r\n(you can easily tell the two apart here)\r\n\r\n![screen shot 2017-11-01 at 10 05 40 am](https://user-images.githubusercontent.com/1385596/32278601-8c7e4a84-beec-11e7-93a5-bdda08146a44.png)\r\n\r\n\r\nfor the jobs list:\r\n(you can not tell the two apart at all)\r\n\r\n![screen shot 2017-11-01 at 10 05 53 am](https://user-images.githubusercontent.com/1385596/32278629-9fb03d2e-beec-11e7-81c8-64f07cafbd06.png)\r\n\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nPing @trahman73 \r\n\r\nThe API can solve this by changing its algorithm for how it names the inventory update.\r\n\r\nNote that when the UI navigates to the inventory update details view, it erroneously gives the NAME as the name of the inventory source.\r\n\r\nOne suggestion might be to have the inventory update name be `(<inventory name> + <inventory source name>)`."},
{"text": "##### ISSUE TYPE\r\n\r\n - Feature Idea\r\n\r\n\r\n##### COMPONENT NAME\r\n\r\n - API\r\n - UI\r\n\r\n\r\n##### SUMMARY\r\n<!-- Briefly describe the problem. -->\r\n\r\nWhen looking up if a user is part of a group in LDAP Tower is using the primary UID of the user and compares it with the members listed in the group. But its possible that in the group we don't have the UID of the user, and are using other custom field. We need to be able to specify in the Tower configuration which entry of the user to use in looking up group membership.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.x\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n<!-- Include any links to sosreport, database dumps, screenshots or other\r\ninformation. -->\r\n\r\nCurrently since group search by default is using uid, the group search fails. "},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\nCurrently AWX only supports a single IP/FQDN for the RADIUS and TACACS+ authentication method. I think that it should be support multiple IP/FQDN for the RADIUS and TACACS+.\r\n\r\n##### ENVIRONMEN \r\n* AWX version: 1.0.X\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\n1. Login as a Admin user via WebUI\r\n2. SETTINGS > EDIT CONFIGURATION > AUTHENTICATION > SUB CATEGORY > RADIUS|TACACS+\r\n3. RADIUS SERVER and TACACS+ SERVER parameters do not support multiple IP/FQDN\r\n\r\n##### EXPECTED RESULTS\r\n\r\nRADIUS SERVER and TACACS+ SERVER parameters can be set multiple IP/FQDN like LDAPSERVERURI\r\n\r\n##### ACTUAL RESULTS\r\n\r\nRADIUS SERVER and TACACS+ SERVER parameters do not support multiple IP/FQDN\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nN/A\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nSemantics:\r\n- have a 'file' survey type\r\n- this uses a file upload widget in the survey taker\r\n- the uploaded file is stored as a base64-encoded extra var in the specified variable\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nSet appropriate limits to not DoS the system.\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nWe already pull playbooks from source control - it would be neat if the job template & workflow definitions (including surveys, potentially even roles) could be sourced from there as well.\r\n\r\n(Probably a very large discussion of *how* this would work, and what to support, is required.)\r\n\r\n##### ENVIRONMENT\r\n* AWX version: any\r\n##### ADDITIONAL INFORMATION\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nThe logging service configured could be down, or suffer a transient error.\r\n\r\nHaving a minimal level of 'retry' would avoid message/data loss.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nNot valid for LOGAGGREGATORPROTOCOL=UDP, obvs.\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nOur social and LDAP auth can map users into orgs and teams by assorted attributes. \r\n\r\nThe request is to be able to map them into *roles* (team admin, execute on inventory) via attributes.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n1. The syntax for this must be manageable in the interface. \r\n\r\n2. It could be argued that it might be simpler to define a team with the associated roles, and map into that."},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nA job run links to the various items associated with the job.\r\n\r\nHowever, these are all the current state of those items, which may bear zero relation to the state of those items when the job is launched.\r\n\r\nAs a auditing user looking at the history of job runs, I want to see in a job run the state of the job as it was launched and ran, rather than having to attempt to reconstruct it via activity stream entries if the inventory/credential/template/etc has changed since then.\r\n\r\n(This also affects job relaunch.)\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nThis is almost certainly API-breaking, and might require rethinking of the database."},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nWe have the ability to search and filter based on stored facts with `hostfilter`.\r\n\r\nWe do *not* have the ability to do this for variables. You can do a free-text search, but it's not particularly useful (especially depending on var format).\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.whatever\r\n\r\n##### ADDITIONAL INFORMATION\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nThe list of searchable fields in the UI (in the search 'KEY') is driven off the OPTIONS list for the API, plus `relatedsearch` fields.\r\n\r\nHowever, not all fields returned in OPTIONS are searchable, and some that aren't listed in OPTIONS are.\r\n\r\nAs we can't change the OPTIONS request (as it's used for GET/POST validation by the UI, etc), we should introduce a new concept of 'searchable fields' in the API that the UI can use to populate the search key.\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\nOpen search key.\r\nAttempt to use all items listed.\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n\r\nIt would be better for users if they could create a single Notification Template per 'DESTINATION CHANNEL' and when enabling the notification in a job template, If users are able to override the defaults placed in the Notification Template.\r\nWhen sharing the CHANNEL with multiple teams, it is useful to be able to overwrite LABEL and COLOR for each job the team runs.\r\n\r\n##### ENVIRONMENT\r\n\r\n* AWX version: 1.0.X\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n- If it is able to overide within job template, users do not have to create multiple NOTIFICATION templates for the same HipChat channel.\r\n- For Example UI (Just Idea) : TEMPLATES > %JOBTEMPLATE% > NOTIFICATIONS > SUCCESS | FAILURE | 'NOTIFICATION LABEL' | 'NOTIFICATION COLOR'"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nThe API for inventories/hosts currently has two ways to determine the last time a job was run on a host:\r\n- the `lastjob` summary field\r\n- the `recentjobs` array\r\n\r\nHowever, both of these are cleared when job history is cleaned up. This makes it impossible to find the last time a job was run on the host. Ideally, there would at least be a timestamp that would persist.\r\n\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.any\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nYay computed fields.\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\n\r\nRegular job launch returns job ID.\r\n\r\nProvisioning callback does not. Can't think of a security reason why not to.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.any\r\n\r\n##### ADDITIONAL INFORMATION\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n - UI\r\n\r\n\r\n##### SUMMARY\r\nAllow using network modules (ioscommand, aireoscommand, etc) for ad-hoc commands. \r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.1.223\r\n* AWX install method: docker on linux\r\n* Ansible version:  2.4.1\r\n* Operating System: Centos 7.4\r\n* Web Browser: Chrome 62\r\n\r\n##### STEPS TO REPRODUCE\r\n- Select a group or host in the inventory that contains network devices. \r\n- Click RUN COMMAND\r\n\r\n##### EXPECTED RESULTS\r\nThe user should be able to select the Network Credential and set the connection method to local thus allowing the execution of network modules.\r\n\r\n##### ACTUAL RESULTS\r\nIt is only possible to select Machine Credentials and there is no option to set the connection type as local, making it impossible to successfully run network modules.\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n<!--- Pick one below and delete the rest: -->\r\n - Feature Idea\r\n\r\n\r\n##### COMPONENT NAME\r\n<!-- Pick the area of AWX for this issue, you can have multiple, delete the rest: -->\r\n - API\r\n - UI\r\n\r\n##### SUMMARY\r\n<!-- Briefly describe the problem. -->\r\nCurrently username field is limited to max 30 characters. Its possible to have username >30 chars, especially in case where email id is used as the username. It should be either increased or at least  be configurable to change the max size.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nThe AWX server never lets you launch a job with a credential password marked to ASK.\r\n\r\nThis leads to a user expectation that the server doesn't know the passwords in these cases. That is not true. For job relaunch, the previously-inputted passwords remain stored in `startargs`, they are just not used, and the requirement that the user enters the password is arbitrary.\r\n\r\nFor enhanced security, the server should forget credential passwords the user enters.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.1\r\n* AWX install method: docker for mac\r\n* Ansible version:  N/A\r\n* Operating System: N/A\r\n* Web Browser: N/A\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\n - create a machine credential, set to ask password on launch\r\n - use in a JT\r\n - launch that JT, provide password \"artemis\"\r\n - wait for job to be successful\r\n\r\n##### EXPECTED RESULTS\r\n\r\nUnable to use the server to retrieve the string \"artemis\", even with full admin privileges.\r\n\r\n##### ACTUAL RESULTS\r\n\r\nAdmin can relatively easy obtain it.\r\n\r\n```python\r\nIn [3]: j = Job.objects.orderby('-created').first()\r\n\r\nIn [4]: from awx.main.utils import encryption\r\n\r\nIn [7]: encryption.decryptfield(j, 'startargs')\r\nOut[7]: '{\"sshpassword\": \"artemis\"}'\r\n```\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\n\r\n"},
{"text": "##### ISSUE TYPE\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n - API\r\n\r\n##### SUMMARY\r\nAllow jobs to be named when launching them from a template\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1.0.1.8\r\n* AWX install method: docker on linux\r\n* Ansible version:  2.4.0.0\r\n* Operating System: rhel-7\r\n* Web Browser: chrome\r\n\r\n\r\n##### ADDITIONAL INFORMATION\r\nI used to have the ability with api/v1/ to create a job with a given name based off of a template. This allowed me to make jobs with meaningful names (i.e. that contained hostnames or other information). \r\n\r\nNow that I have moved to using api/v2/ it looks as if you can only launch a job based off of a template by targeting **api/v2/jobtemplates/<id>/launch** which results in a job with the same name of the template. \r\n\r\nIt would be nice to have some way to rename each instance of a job to reflect something a bit more human readable. Either this option exists and I'm doing it wrong, in which case sorry/please point me in the right direction, or can this be considered as a feature. \r\n"},
{"text": "##### ISSUE TYPE\r\n<!--- Pick one below and delete the rest: -->\r\n - Feature Idea\r\n\r\n##### COMPONENT NAME\r\n<!-- Pick the area of AWX for this issue, you can have multiple, delete the rest: -->\r\n - UI\r\n\r\n##### SUMMARY\r\nIt is possible through the API to assign READ roles to users/teams for AWX objects, but READ role does not appear in the drop down when selecting roles within the UI. The only available option for READ through the UI is via an org auditor which garners READ for all organization owned objects.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 1+"},
{"labels":["api",null,null],"text":"#39273 updated Python's circular padding, correcting several errors in the previous implementation. PyTorch has two circular padding implementations, however, one in Python and the other in C++:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/7b547f086fbef74dd2e9c07f4392da3a2d3266b4/torch/nn/functional.py#L3831\r\n\r\nhttps://github.com/pytorch/pytorch/blob/46447045ea450069ab9a9cbbf71e86110013fb0b/torch/csrc/api/include/torch/nn/functional/padding.h#L13\r\n\r\nIt seems like we'd prefer to only have one implementation, although the author of this PR expressed that he was not familiar with C++. \n\ncc @yf225 @glaringlee @albanD @mruberry"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\nI want to be able to determine the version of libtorch in my c++ binary in cases where I don't have access to the python library.\r\n\r\nSomething like:\r\n```\r\n#define TORCH_VERSION_MAJOR=1\r\n#define TORCH_VERSION_MINOR=6\r\n#define TORCH_VERSION_PATCH=0\r\n```\r\n\r\n## Motivation\r\n\r\nThis is useful for \r\n- Turning off torch features depending on the version of torch we are running\r\n- Debugging which version of torch is installed in production where different versions may be running.\r\n\r\n## Pitch\r\n\r\n1) I'm a developer trying to upgrade the API, but a backwards incompatible API change requires me to refactor code before upgrading.  I need to roll this out so I want to only enable the new features if I build my application for the new version of torch.\r\n2) I'm a site reliability engineer and I am rolling out a the server deploy with a new version of libtorch.  There is a bug that is claimed to be fixed in the newer version of torch and I want to verify that the new version was deployed when I validate this issue is resolved.\r\n\r\n## Alternatives\r\n\r\nCurrently I am pulling the version from the strings in libtorch_cpu.so, but this didn't get updated in v1.6.0.  This is an unreliable hack solution that I would like to remove.\r\n```\r\nstrings libtorch_cpu.so | egrep -A1 pytorch_version\r\npytorch_version\r\n1.5.0\r\n```\r\n\r\npytorch has the version number baked into the pip install and in the torch.__version__.  For my purposes I don't have access to either of these as we are not using python in our production environment and the shared libraries are built separate from the ones in the pip wheel.\r\n\r\n## Additional context\r\n\r\nI recently upgraded to libtorch version 1.6.0 to resolve a segfault on destruction of `DeviceThreadHandlePool` which seemed related to: https://github.com/pytorch/pytorch/pull/36416.  After upgrading I am still seeing some segfaults on destruction, but was worried that I might not have deployed the new version of libtorch.   By looking at other strings in that library I believe I am at v1.6.0, but I wasted some time on the red herring of the 1.5.0 version in the strings of libtorch_cpu.so and would like to avoid that in the future.\n\ncc @yf225 @glaringlee"},{"labels":["api",null,null],"text":"There are 9 boolean flags within TensorImpl.cpp which exceeds 8bytes already.\r\nThis made the size of TensorImpl 1 word bigger than before which is not necessary. (introduced in #33033). \r\n\r\nThe purpose of this  issue is to move all the boolean flags into a 8bytes uint as bitfields, so the size of TensorImpl reduced 1 word. \r\nAnd we will also enhence the note of TensorImpl, making it more clear on the process of modifying TensorImpl.\r\n\r\ncc @yf225 @glaringlee @ezyang @bhosmer @smessmer @ljk53"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\nCurrently the lack of such function is really felt. for example, when displaying the results on screen, it comes up a lot that you face something like : \r\n```\r\noffsets.shape: [1, 4, 46, 85]\r\nprobs.shape: [46, 85]\r\noffsets: (1,1,.,.) =\r\n 0.01 *\r\n  0.1006  1.2322\r\n  -2.9587 -2.2280\r\n\r\n(1,2,.,.) =\r\n 0.01 *\r\n  1.3772  1.3971\r\n  -1.2813 -0.8563\r\n\r\n(1,3,.,.) =\r\n 0.01 *\r\n  6.2367  9.2561\r\n   3.5719  5.4744\r\n\r\n(1,4,.,.) =\r\n  0.2901  0.2963\r\n  0.2618  0.2771\r\n[ CPUFloatType{1,4,2,2} ]\r\nprobs: 0.0001 *\r\n 1.4593  1.0351\r\n  6.6782  4.9104\r\n[ CPUFloatType{2,2} ]\r\n```\r\nwhere some tensors dims do not show  their finalized output, instead, it seems, a scaler is shown in the output instead which makes it really hard to read, specially when you are porting a Python module into C++ and you want to compare the intermediate outputs for example. \r\nApart from this, specifying the precision, threshold, edgeitems, etc can be very benificial and make life much easier! \r\n\r\n## Motivation\r\n\r\nDuring porting a Python project into C++ for production, I faced this issue, and it creeps up a lot and makes reading and comparing values between Python and C++ very hard. \r\n\r\n## Pitch\r\n\r\nPlease kindly implement set_printoptions in libtorch as well.\r\n\r\n## Alternatives\r\n\r\nThe only alternative is to alter torch code which is a very bad idea! I know no other way around this! \r\n\r\n\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\nCurrently the only way that seems to be working to check whether a tensor is empty or not, is to check its `has_storage` method. However, it seems, this method always returns false for SparseTensors . Therefore its not considered a proper/canonical way for checking whether a at::Tensor is empty or not.  \r\nProviding a dedicated property or a method for this purpose, thus goes a long way and will be very much appreciated. \r\n\r\n## Motivation\r\n\r\nDealing with situations, where an empy tensor is returned becasue of meeting or not! meeting some creteria and it needs to be determined whether we are dealing with an empty tensor or not!\r\n\r\n## Pitch\r\n\r\nAdd a property for checking the emptiness of a tensor! in libtorch\r\n\r\n## Alternatives\r\n\r\nCurrently `has_storage` is being used for quering whether a tensor is acually empty or not, but its not the case for SparseTensors! \r\nand wont work for them! \r\n\r\n\n\ncc @yf225 @glaringlee"},{"labels":["api",null,null],"text":"Currently `src->deleter` is always called, regardless if it's `nullptr` or not. It would be mildly more hassle-free if it checked `if(src->deleter)` prior to calling, just like fromDLPack accommodates not-specified strides.\r\n\r\n```cpp\r\nauto deleter = [src](void* self) {\r\n    src->deleter(const_cast<DLManagedTensor*>(src));\r\n  };\r\n``` \r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/DLConvertor.cpp#L215\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"## 🚀 Feature\r\nIt would be great for the serialization to be much broader than what it is now in libtorch. at least by providing a way for the users to be able to use existing functionality to serialize their objects. \r\n\r\n## Motivation\r\n\r\nI was trying to serialize a `std::vector<std::tuple<std::string, torch::Tensor>>` in libtorch where I found out its simply impossible. there is no way to extend the `InputArchive ` so I can add the needed logic to load from it. \r\n\r\n## Pitch\r\n\r\nAdd support for `STL` containers at least when it comes to serialization. or allow the users to be able to extend existing functionality to support their own custom types.  \r\n\r\n## Alternatives\r\n\r\nI ultimately ended up using Protobuf for serialization \r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\ntorch.nn.Unflatten has been added into pytorch in #41564, it should be added into C++ Frontend as well.\r\n## Motivation\r\nThis is to add torch.nn.Unflatten support in C++ Frontend, so people who is using pure c++ (libtorch for eg) can use this module as well.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\nIn the PyTorch Python API, it is possible to move a tensor to shared memory via calling the `Tensor.share_memory_()` function. I could not find similar functionality on the  C++ side using `at::Tensor`.\r\n\r\n## Motivation\r\nI have code that reads tensors from network on the C++ side (using multiple threads). These tensors are then pulled into Python land and go through Python multi-processing transforms. I want to avoid copying tensors across process boundaries, so that the multi-processing transforms can be executed efficiently. To do this, I need to move the tensor storage to shared memory. However, for efficiency purposes, I want to move the tensor memory to shared memory on the C++ side, where I have a thread pool for reading tensors from the network. \r\n\r\nAnother potential area where I am planning to use this feature is out-of process execution of PyTorch scripts in C++ land, where again, we can use shared memory to minimize the cost of transferring tensors across processes.\r\n\r\n## Pitch\r\n\r\nWhat I want is a `share_memory()` method on `at::Tensor`, mimicking the one on PyTorch tensors.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @yf225 @glaringlee"},{"labels":["api",null,null],"text":"I found that it is slow to overwrite existing tensor using tensor::save.\r\n\r\n## Code\r\n```\r\n#include <chrono>\r\n#include <filesystem>\r\n#include <torch/torch.h>\r\n\r\n uint64_t now_ms() {\r\n    return static_cast<uint64_t>(\r\n        std::chrono::time_point_cast<std::chrono::milliseconds>(\r\n            std::chrono::steady_clock::now())\r\n            .time_since_epoch()\r\n            .count());\r\n  }\r\n\r\nint main(int argc, char **argv) {\r\n\r\n  auto tensor = torch::randn({1, 200 * 1024});\r\n  auto begin_ms =now_ms();\r\n  for (int i = 0; i < 100; i++) {\r\n    torch::save(tensor, \"tmp_file\");\r\n  }\r\n  auto end_ms = now_ms();\r\n  std::cout << \"insertion used \" << end_ms - begin_ms << \" ms\" << std::endl;\r\n\r\n  begin_ms = now_ms();\r\n  for (int i = 0; i < 100; i++) {\r\n    std::filesystem::remove(\"tmp_file\");\r\n    torch::save(tensor, \"tmp_file\");\r\n  }\r\n  end_ms = now_ms();\r\n  std::cout << \"insertion used \" << end_ms - begin_ms << \" ms\" << std::endl;\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n## result\r\ninsertion used 557 ms\r\ninsertion used 82 ms\r\n\r\n## Strace results\r\n\r\n% time     seconds  usecs/call     calls    errors syscall\r\n------ ----------- ----------- --------- --------- ---------------- \r\n54.03    0.051799         517       100           writev\r\n 25.79    0.024721         103       238        92 openat \r\n16.86    0.016168         110       146           close \r\n\r\n\r\n% time     seconds  usecs/call     calls    errors syscall\r\n------ ----------- ----------- --------- --------- ----------------\r\n 61.46    0.020940         209       100           writev \r\n19.27    0.006565          65       100           unlink    \r\n  6.30    0.002147           9       238        92 openat  \r\n3.76    0.001281           9       137           mmap             \r\n  2.01    0.000684           4       146           close \r\n\r\nThe only significant difference between these two blocks is that writev syscall is twice slower in the first case.\r\n\r\n## Expectation\r\nOverwrite should be as fast as removing the old tensor manually\r\n\r\n## Environment\r\nUbuntu 20.04\r\npytorch version is github master\r\n\r\n\n\ncc @yf225 @glaringlee @VitalyFedyunin @ngimel"},{"labels":["api",null,null],"text":"## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nWhen I try to load a plugin library built with 2 cpp file,\r\n```python\r\ntorch.ops.load_library(\"libtorch_plugins.so\")\r\n```\r\n\r\nI got the following error:\r\n```\r\nOnly a single TORCH_LIBRARY can be used to register the namespace decode; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  Previous registration of TORCH_LIBRARY was registered at /home/cloudhan/workspaces/torch_plugins/src/readjpeg_cpu.cpp:146; latest registration was registered at /home/cloudhan/workspaces/torch_plugins/src/readpng_cpu.cpp:87\r\n```\r\n\r\n## Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nCurrently, the only valid way to build a plugin library is to collect all op registration into one translation unit. \r\nThat is\r\n```cpp\r\n// file foo.cpp\r\n\r\n// define op foo\r\n```\r\n\r\n```cpp\r\n// file bar.cpp\r\n\r\n// define op bar\r\n```\r\n\r\n```cpp\r\n// registration.cpp\r\n\r\n#include \"foo.h\"\r\n#include \"bar.h\"\r\n\r\n// register ops foo and bar \r\n```\r\nIn this way, it is hard to maintain flexible extension op set. Since to add a new op,  you need to directly modify `registration.cpp`\r\nTo remove unneeded ops, you also need to modify `registration.cpp`\r\n\r\nIf distributed registration is supported, all I need is\r\n```cpp\r\n// file foo.cpp\r\n\r\n// define op foo\r\n// register op foo\r\n```\r\n\r\n```cpp\r\n// file bar.cpp\r\n\r\n// define op bar\r\n// register op bar \r\n```\r\n\r\nSimply control which cpp file to be compiled and linked, I can now control the registration easier.\r\n\r\n\n\ncc @yf225 @glaringlee @ezyang @bhosmer @smessmer @ljk53"},{"labels":["api",null],"text":"i am working on translating pytorch code to c++ environment.\r\nat the pytorch code there is support to take and provide tensors of indexes along different dimensions, which change the output tensor shape,\r\nthis feature is not supported at the c++ framework  \r\n\r\nSteps to reproduce the behavior:\r\nat pytorch:\r\n\r\n1. tensor = torch.arange(25*4*96*170).reshape(25,4,96,170)\r\n2. a = torch.arange(25)\r\n3. x=a\r\n4. y=a\r\n5. output = tensor[a, :, y, x]\r\n\r\noutput shape is: [25,4]\r\n\r\nthought maybe to use the torch::index function, but it fails when number of dimensions is larger than 3.\r\n\r\nhow do i produce the same behavior at the c++ framework?\r\n\r\n - PyTorch Version (e.g., 1.0): torch==1.3.0\r\n - OS (e.g., Linux): Linux\r\n - Python version: Python 3.6.8\r\n\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\n\r\nThe ability to turn off scientific notation (standard form) using the C++ API.\r\n\r\n## Motivation\r\n\r\nIt would be useful to turn off the scientific notation used to display tensors. Sometimes I will often get numbers like: 9.9999e-01 which - in the common tongue - is more frequently known as 0.9999 or 1.0. This notation can be really helpful for very small numbers close to zero but in certain situations it is just confusing.\r\n\r\n## Pitch\r\n\r\nIt would be nice to have an option when creating tensors to turn off scientific notation.\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAdd C++ API for [`torch.autograd.functional.jacobian`](https://pytorch.org/docs/stable/_modules/torch/autograd/functional.html#jacobian)\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nI would like to calculate the jacobian of my model output wrt to the input and the feature is available in the python api but was dismayed to find out that it was not in the C++ API.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nInclude `torch::autograd::functional::jacobian` functionality in C++ API\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nN/A\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\r\nJust some links for easy browsing:\r\nhttps://github.com/pytorch/pytorch/tree/master/torch/csrc/autograd\r\nhttps://pytorch.org/cppdocs/api/namespace_torch__autograd.html\r\nhttps://pytorch.org/docs/stable/_modules/torch/autograd/functional.html#jacobian\r\n\r\n\r\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\nAdd cpack support for creating prepackaged builds of libtorch\r\n\r\n## Motivation\r\nDue to https://github.com/pytorch/pytorch/issues/14573 I need to compile libtorch from source and link against system protobuf.  libtorch is installed as part of a CI pipeline and rather than build it from source each time, I would like to be able to point to a prebuilt `.deb` package to install.\r\n\r\n## Pitch\r\nAdd [the few lines](https://gitlab.kitware.com/cmake/community/-/wikis/doc/cpack/Packaging-With-CPack) it would it take to support cpack\r\n\r\n\n\ncc @malfet @yf225 @glaringlee"},{"labels":[null,"api",null],"text":"LibTorch 1.5.0 seemed built on top of GLIBC 2.23 where PyTorch 1.5.0 pip wheel built on lower version. Do we have a plan to support LibTorch on version below 2.23? Or any instruction for user on GLIBC 2.23 and below to use it?\r\n\r\n## To Reproduce\r\n\r\nIf you try to use it on Cent OS 7, you can easily fall into the following issue:\r\n\r\n```\r\ndownload\r\nhttps://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.5.0.zip\r\n```\r\n\r\nError message\r\n```\r\n/lib64/libm.so.6: version `GLIBC_2.23' not found (required by /home/centos/libtorch_cpu.so)\r\n```\r\n\r\n## Expected behavior\r\n\r\nIt should be fine to use the libtorch for lower GLIBC version as claimed here: https://pytorch.org/get-started/locally/#supported-linux-distributions\r\n\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.5.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: version 3.6.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.4\r\n[pip3] torch==1.5.0\r\n[conda] Could not collect\r\n\r\n\n\ncc @malfet @yf225 @glaringlee"},{"labels":[null,"api",null,null],"text":"similar to issue #33192\r\n\r\nif we start with AnyModule a, there is no good way to add a name, \r\nto create a NamedAnyModule\r\ni.e. in  _nn/modules/container/named_any.h_\r\n\r\n```\r\nprivate:\r\n  /// Creates a `NamedAnyModule` from a type-erased `AnyModule`.\r\n  NamedAnyModule(std::string name, AnyModule any_module)\r\n    : name_(std::move(name)), module_(std::move(any_module)) {}\r\n```\r\nI can create the pr to move this method to public if it makes sense.\r\n(similar to pr #34208)\n\ncc @yf225"},{"labels":["api",null],"text":"## 🚀 Feature\r\nI would be able to clone a model into another model. Such as being done in the [Reinforcement Learning (DQN) Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) at **Training**.\r\n\r\nThe requested functions that do exist in python but not C++ are:\r\n```\r\nload_state_dict()\r\nstate_dict()\r\ntarget_net.load_state_dict(policy_net.state_dict())\r\n```\r\n\r\n## Motivation\r\nIt would be neat to be able to follow the pytorch example listed above. However the C++ library are missing the necessary functions for doing this.\r\n\n\ncc @yf225"},{"labels":["api",null],"text":"This is to better match the Python API `ctx.saved_tensors`.\r\n\r\nI believe we originally named it `get_saved_variables()` because there was still a Tensor vs. Variable distinction at that time. Now that Tensor and Variable are the same, we can deprecate `get_saved_variables()` and replace it with `get_saved_tensors()`.\r\n\r\ncc @yf225"},{"labels":["api",null],"text":"## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nCan we remove \"using namespace ...\" from the public headers of libtorch?\r\n\r\n## Motivation\r\nE.g., `torch/csrc/api/include/torch/nn/modules/_functions.h` has this line `using namespace torch::autograd;`. This pollutes the namespace of some applications.\r\n\r\n\n\ncc @yf225"},{"labels":["api",null,null],"text":"In Python, we have the following tensor print behavior:\r\n```python\r\n>>> x=torch.ones(2, 2, requires_grad=True)\r\n>>> x\r\ntensor([[1., 1.],\r\n        [1., 1.]], requires_grad=True)\r\n>>> y=x*2\r\n>>> y\r\ntensor([[2., 2.],\r\n        [2., 2.]], grad_fn=<MulBackward0>)\r\n```\r\nHowever in C++, we have the following behavior:\r\n```cpp\r\nauto x = torch::ones({2, 2}, torch::requires_grad());\r\nstd::cout << x << std::endl;\r\nauto y = x * 2;\r\nstd::cout << y << std::endl;\r\n```\r\n```cpp\r\n 1  1\r\n 1  1\r\n[ CPUFloatType{2,2} ]\r\n 2  2\r\n 2  2\r\n[ CPUFloatType{2,2} ]\r\n```\r\nIt would be nice to show `requires_grad` and `grad_fn` in C++ tensor print as well.\n\ncc @yf225"},{"labels":[null,"api",null],"text":"## To Reproduce\r\n\r\nIn python, accessing the .grad attribute of a non-leaf tensor is a common pitfall users run into. For example:\r\n```\r\nx = torch.randn(3, requires_grad=True).cuda()\r\ny = x ** 2\r\ny.sum().backward()\r\nx.grad\r\n```\r\nLuckily, this raises a nice warning in Python:\r\n```\r\n/scratch/rzou/pt/cudnn_double_Bwd/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\r\n  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\r\n```\r\n\r\nIn the C++ API, no warning happens: https://discuss.pytorch.org/t/very-confused-with-changes-in-autograd/74355 . It would be great for usability to add that warning.\r\n\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225"},{"labels":[null,"api",null,null],"text":"## 🚀 Feature\r\nC++ API should support Famous Open Datasets CIFAR10 and CIFAR100.\r\n**Dataset Reference**: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\r\n\r\n## Motivation\r\nWhile creating PyTorch C++ tutorials [https://github.com/prabhuomkar/pytorch-cpp](https://github.com/prabhuomkar/pytorch-cpp), I came to know that PyTorch has support for these datasets via [vision](https://github.com/pytorch/vision/tree/master/torchvision/datasets). As of now, [C++ API for Datasets](https://github.com/pytorch/pytorch/tree/master/torch/csrc/api/src/data/datasets) only has MNIST.\r\nSupport for standard datasets like CIFAR, COCO, ImageNet, etc out of the box will help developers play with C++ API easily.\r\n\r\n## Pitch\r\n- Add [cifar.h](https://github.com/pytorch/pytorch/tree/master/torch/csrc/api/include/torch/data/datasets) header with _`TORCH_API`_ `CIFAR` class.\r\n- Add [cifar.cpp](https://github.com/pytorch/pytorch/tree/master/torch/csrc/api/src/data/datasets) with both `CIFAR10` and `CIFAR100` support similar to vision for reading images, targets and getting train/test data. \r\n\r\n## Alternatives\r\nN/A\r\n\r\n## Additional context\r\nReference C++ Implementation: [cifar10.h](https://github.com/prabhuomkar/pytorch-cpp/blob/master/tutorials/intermediate/deep_residual_network/include/cifar10.h) and [cifar10.cpp](https://github.com/prabhuomkar/pytorch-cpp/blob/master/tutorials/intermediate/deep_residual_network/src/cifar10.cpp)\r\nReference Python Implementation: [cifar.py](https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py)\r\n\n\ncc @yf225 @fmassa"},{"labels":["api",null],"text":"Instead of\r\n\r\n```\r\nclass TensorOptions {\r\n  TensorOptions device(optional<Device>) { ... } // setter\r\n}\r\n```\r\n\r\nwe should have\r\n\r\n```\r\nclass TensorOptions {\r\n  TensorOptions device(Device) { ... } // setter\r\n  TensorOptions device_opt(optional<Device>) { ... } // setter\r\n}\r\n```\r\n\r\nThis makes the class more user friendly as now `x.device({kCUDA, 1})` works; it also makes it symmetric with `at::device({kCUDA, 1})`.\n\ncc @yf225"},{"labels":["api",null],"text":"The class was explicitly designed to be two words large, we should pass it by value.\n\ncc @yf225"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\n`torch.round` to take an optimal argument which specifies the decimal place to which rounding should occur\r\n\r\n## Pitch\r\n`torch.round` should have the same functionality as `numpy.around`. Specifying a decimal place will round _up to_ that decimal place, using the same rounding strategy as `torch.round` currently uses. The default value of the decimal parameter should be 0 to keep current behaviour.\r\nE.g.\r\n```\r\n>>> tensor = torch.tensor([0.1234, 0.1237])\r\n>>>torch.round(tensor, decimals=3)\r\ntensor([0.123, 0.124])\r\n```\r\n\r\n## Alternatives\r\nYou can use `numpy.around` on the tensor, but I assume this has some performance hit?\r\n\n\ncc @yf225"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAdd the same strong wolfe line search option to the LibTorch interface as the Python counterpart.\r\n\r\nhttps://pytorch.org/docs/master/optim.html\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nUnify the interface between Python and C++\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nImplement the strong wolfe line search algorithm and add a flag to the `LBFGSOptions` struct\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @yf225"},{"labels":["api",null],"text":"## 🚀 Feature\r\ncan these private _push_back_ functions be made public?\r\n\r\n```\r\n/// Adds a type-erased `AnyModule` to the `Sequential`.\r\n  void push_back(AnyModule any_module) {\r\n    push_back(c10::to_string(modules_.size()), std::move(any_module));\r\n  }\r\n\r\n  void push_back(std::string name, AnyModule any_module) {\r\n    modules_.push_back(std::move(any_module));\r\n    const auto index = modules_.size() - 1;\r\n    register_module(std::move(name), modules_[index].ptr());\r\n  }\r\n```\r\n\r\n## Motivation\r\n\r\nIf i already have an `AnyModule a`,\r\nI can't figure out how to add it to a _Sequential_ without something like\r\n`if(auto* m=a.as<torch::nn::Conv2d>()) seq->push_back<torch::nn::Conv2d>(*m)`\r\nwhich will then turn it back into an AnyModule and add it.\r\n\r\nOr making the single AnyModule into a vector:\r\n`seq->extend(std::vector<AnyModule>{a})`\r\n\r\nThanks\n\ncc @yf225"},{"labels":[null,"api",null,null],"text":"## 🚀 Feature\r\n\r\nThe `at::Tensor` datatype does not have `&` overloaded the way it is in the python API, thus not allowing for boolean element-wise operations between tensors.\r\n\r\n## Motivation\r\n\r\nI want to call the `at::where()` function with a boolean tensor that is the outcome of the AND of two conditions.\r\n\r\n## Pitch\r\n\r\nSomething like this should be possible:\r\n``` c++\r\n  auto output = at::where((target > -EPSILON) & (target < EPSILON), all_zeros, whatever_values);\r\n```\r\n## Alternatives\r\n\r\nThis is ugly and tedious IMO:\r\n\r\n``` c++\r\n  auto output = at::where(target > -EPSILON, output_pos, zeros);\r\n  output = at::where(output < EPSILON, zeros, output_pos);\r\n```\r\n\n\ncc @yf225"},{"labels":["api",null,null],"text":"## 🚀 Feature\r\n\r\nDistributedStreamSampler: support stream sampler in distributed setting\r\n\r\n## Motivation\r\n\r\nA new class `torch::data::samplers::DistributedStreamSampler` both works in distributed setting like `torch::data::samplers::DistributedSequentialSampler` and works with `torch::data::datasets::StreamDataset` like `torch::data::samplers::StreamSampler`.\r\n\r\n## Pitch\r\n\r\nworks with StreamDataset in distributed setting.\r\n\r\n## Alternatives\r\n\r\nimplement this by user\r\n\r\n## Additional context\r\n\r\nnone\r\n\n\ncc @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar"},{"labels":[null,"api",null],"text":"## 🚀 Feature\r\nThe `setup.py` should emit the CPP version being used for the compilation along with things like `USE_CUDA`, etc. \r\n\r\n## Motivation\r\n\r\nI recently ran into an issue where I got an error stating that pytorch requires atleast C++14 for compilation. Although I managed to compile pytorch after some changes, it was hard to figure out how exactly it is being compiled since I had to browse the compilation output to check the `-std=c++14` flag being passed to gcc.\r\n\r\n## Pitch\r\n\r\nThere should be a `CPP_VERSION: <version number>` in the compilation summary.\r\n\r\n## Alternatives\r\n\r\nMaybe output the library versions used in some text file for easy reference.\r\n\n\ncc @yf225"},{"labels":[null,"api",null,null],"text":"## 🚀 Feature\r\nParallelization: more balanced work distribution among workers\r\n\r\n## Motivation\r\n\r\nI recently checked the code for `at::parallel_for` method and this is what I observed.\r\nSuppose there are `N` indices and `T` workers, then worker `i (0 <= i < T)` receives\r\nindices `[i * ceil(N/T), min(N, (i+1) * ceil(N/T)) )`.\r\n\r\nNow, suppose `N=10, T=4`, then this scheme will have the following distribution : `3|3|3|1`.\r\nDefinitely, something like `3|3|2|2` looks better. The issues is that the current approach is biased against the last worker(s) - it (them) always receive(s) the least work, which **could be even zero**. To elaborate, the whole situation becomes even worse if, for example, `N=100, T=40`. Then work of size 3 is scheduled for each worker, meaning that only 34 workers are going to do something useful, while 6 workers do nothing. And, in general, the situation gets worse if the number of workers scales up with the size of an input.\r\n\r\n## Pitch\r\nWhat about a slightly different distribution? This one **uses all the workers**!\r\nFor a worker `i: 0 <= i < T` let\r\n```\r\nbegin(i) = ceil(N*i/T),\r\nend(i) = begin(i+1),\r\n```\r\nand the worker receives indices `[begin(i), end(i) )`\r\n\r\nIt can be shown that for any `N >= T >= 1`, any `i,j: 0 <= i,j < T`:\r\n```\r\n|(end(i) - begin(i)) - (end(j) - begin(j))| <= 1, \r\nend(i) - begin(i) >= 1, begin(i) < N,\r\n```\r\nso this new scheme is optimally balanced, and each worker will do at least something!\r\n\r\nThe only issue I see is a more likely chance of overflow in computing `begin`\r\n\r\ncc @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"},{"labels":[null,"api",null,null],"text":"## 🚀 Feature\r\nBuild `libtorch` with AMD support. \r\n\r\n## Motivation\r\n\r\nI will be working on a new machine that has AMD GPUs, and we would like to avoid the overhead of Python.\r\n\r\n## Pitch\r\n\r\nIf possible, I would like to contribute to this effort. I would appreciate any guidance in that process.\r\n\r\n## Alternatives\r\n\r\nThere could be a binary dump and all associated CMAKE files like the current release of libtorch, but I'd prefer to be able to cook it up myself.\r\n\r\n## Additional context\r\n\r\nThis would be a great step in enabling Pytorch to run on the next generation of supercomputers. :wink:\r\n\n\ncc @yf225"},{"labels":[null,"api",null,null],"text":"Not only C + + interface on IOS\r\nPython may also call torch on IOS\r\nThis can be used for demonstration or introduction to basic learning\r\n\r\nI made some changes when compiling. At present, most of the examples are available through\r\nIt's just that I don't know if pytorch is compatible with this pattern\r\n\r\nDownload APP:\r\n- US https://apps.apple.com/us/app/id1471351733\r\n- CN  https://apps.apple.com/cn/app/id1471351733\r\n\r\nScreenshots:\r\n- ![](https://github.com/goodclass/PythonAI/raw/master/image/torch1.jpg)\r\n- ![](https://github.com/goodclass/PythonAI/raw/master/image/torch2.jpg)\n\ncc @yf225"},{"labels":[null,"api",null,null,null,null],"text":"`tensor.type()` gives you a DeprecatedTypeProperties object, but we didn't actually deprecate the call.  We should do this ASAP so we can delete DeprecatedTypeProperties.\r\n\r\nCC @ezyang.\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @yf225 @SsnL"},{"labels":["api",null],"text":"`torch::tensor` currently accepts braced-init-list (such as `{{1}, {2}}`) as multidimensional input. It would be great if `at::tensor` supports the same as well.\r\n\r\nThis would involve moving `torch/csrc/api/include/torch/detail/TensorDataContainer.h` to ATen, and using it from `aten/src/ATen/templates/NativeFunctions.h`.\n\ncc @yf225"},{"labels":["api",null,null],"text":"After https://github.com/pytorch/pytorch/pull/28523 is merged, C++ `torch::tensor(scalar)` behaves the same as Python `torch.tensor(scalar)` and creates a 0-dim tensor. However, C++ `at::tensor(scalar)` still creates a 1-dim tensor, and it would be great to change the behavior of `at::tensor(scalar)` to create a 0-dim tensor.\n\ncc @yf225"},{"labels":[null,null,"api",null,null],"text":"In order to make rpc, remote and dist autograd APIs run in torch script mode, we need to provide C++ APIs of them and register them as Prim::ops. \r\n\r\nThese APIs include:\r\n\r\nrpc_sync(), rpc_async(), remote(), dist_autograd.backward()\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @aazzolini @xush6528"},{"labels":["api",null],"text":"Currently, `torch.optim` optimizers in PyTorch C++ API behave slightly differently from the Python API. In order to achieve parity, we should check the following aspects of an optimizer:\r\n\r\n1. Make sure the C++ optimizer takes the same set of constructor arguments as the Python optimizer.\r\n2. Make sure the C++ optimizer's constructor has the exact same logic as the Python optimizer's `__init__()` function. Particularly, we need to support `param_groups` in C++ optimizers.\r\n2. Make sure the C++ optimizer's `step()` function has the exact same logic as the Python optimizer's `step()` function.\r\n3. Add `state` to all optimizers, which is equivalent to the `state` dict in Python optimizers. In the `serialize` function of each C++ optimizer, make sure to serialize the `state` field into a list of `at::IValue`s, and have tests to make sure we can deserialize the `state` field successfully.\r\n    - You might ask \"what should we do with the existing serialization logic in the `serialize` function, and would the change break backward compatibility of the user's existing serialized optimizers?\" The answer is that we should put a \"version number\" in the serialized optimizer, and use that to identify the version - if we find no version number, we know that it's the old version, if we find version number \"1.5\", we know that it's the second version (aka. the version after our changes). We need to have tests to cover deserialization of the old version as well.\r\n\r\nFor more detailed discussions on the class structure design, see https://github.com/pytorch/pytorch/pull/29581.\r\n\r\n## Optimizers\r\n- [x] Adagrad (https://github.com/pytorch/pytorch/pull/29335)\r\n- [x] Adam (https://github.com/pytorch/pytorch/pull/33730)\r\n- [x] LBFGS (https://github.com/pytorch/pytorch/pull/34564)\r\n- [x] RMSprop (https://github.com/pytorch/pytorch/pull/33450)\r\n- [x] SGD (https://github.com/pytorch/pytorch/pull/32592)\r\n- [ ] Adadelta\r\n- [x] AdamW\r\n- [ ] SparseAdam\r\n- [ ] Adamax\r\n- [ ] ASGD\r\n- [ ] Rprop\r\n\r\ncc @yf225"},{"labels":["api",null],"text":"<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nDocker introduced the `--gpus` option with 19.03, which enables first class GPU support inside docker. Unfortunately this option doesn't seem to be available using the Docker API v1.40 (most likely should be located inside the HostOptions when creating the container, for reference this is what the current docs state [DOCS](https://docs.docker.com/engine/api/v1.40/#operation/ContainerCreate))\r\n\r\nAlso the changelogs for version 1.40, didnt seem to include a change supporting this case.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Try to launch a container using the new GPU option using the Docker API\r\n\r\n**Describe the results you received:**\r\nOption not available using the API\r\n\r\n**Describe the results you expected:**\r\nOption to be available using the API\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n```\r\n\r\nClient:\r\n Version:           19.03.6\r\n API version:       1.40\r\n Go version:        go1.12.17\r\n Git commit:        369ce74a3c\r\n Built:             Fri Feb 28 23:45:43 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          19.03.6\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.12.17\r\n  Git commit:       369ce74a3c\r\n  Built:            Wed Feb 19 01:06:16 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.3.3-0ubuntu1~18.04.2\r\n  GitCommit:        \r\n runc:\r\n  Version:          spec: 1.0.1-dev\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:   \r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 12\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 12\r\n Images: 268\r\n Server Version: 19.03.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: \r\n runc version: \r\n init version: \r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.3.0-61-generic\r\n Operating System: Ubuntu 18.04.4 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 31.29GiB\r\n Name: tanuki\r\n ID: MGDH:HBPR:BX2D:35L4:CRTC:7K34:D6QG:MCJZ:SYJC:GSBV:SWFF:NIEF\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n"},{"labels":["api"],"text":"Currently API routers are responsible for transforming requests based upon API version, however this has lead to host specific details leaking into the http router.\r\nWe also don't really want API version specific details leaking into the implementation.\r\n\r\nI propose that we add a shim here so that transformations can be made after the request is routed.\r\nThis could look something like:\r\n\r\n```go\r\n// the API router\r\nfunc(r *fooRouter) createFoo(ctx context.Context, w http.Response, req *http.Request) {\r\n  // make a fooConfig from the reuqest\r\n  r.backend.Create(ctx, fooConfig)\r\n}\r\n```\r\n\r\n```go\r\nfunc(b *fooBackend) Create(ctx context.Context, fooConfig Config) {\r\n    b.transform.Create(ctx, &fooConfig)\r\n}\r\n```\r\n\r\n```go\r\nfunc(t *fooTransformer) Create(ctx context.Context, fooConfig) {\r\n  ver := getApiVersion(ctx)\r\n  if ver.LessThan(\"1.2\") {\r\n      // set some value\r\n  }\r\n}\r\n```\r\n\r\nThen the transformer can be configured with host specific details rather than relying on the HTTP router to have these details.\r\nIt also seems like it makes it easier to unit test such transformations."},{"labels":["api",null],"text":"A long long time ago (in this galaxy, though), we added support for container stats.\r\nThe API was designed to just push stats at an interval until the client disconnects.\r\n\r\nNot too much later, people were wanting to be able to just fetch a single set of stats and be done with it, so we added a query param `stream=false`, which makes dockerd hang up after a single stat is sent. Not the best naming but it did what it needed to do.\r\n\r\nBut then, oh no... now the docker CLI, when using `--no-stream`, always shows 0% CPU usage because there was only 1 CPU stat and nothing compare CPU usage with. It was decided to collect 2 stats instead of just 1 so that the client can calculate the usage.\r\n\r\nFast forward to where we are now....\r\n\r\nHere we have a problem. Any tool wanting to sample docker takes 2x as long (nearly 2 seconds) to collect per container than before the mentioned change (which admittedly was years ago).\r\nMeanwhile every sampling tool is just going to sample as often as it needs to and calculate usage from there and does not care about the primed stats.\r\n\r\nIt would have been nice if we changed the CLI to make 2 collections rather than expecting the engine to do this for them. It wasn't a breaking change to make the engine do 2 collections, but certainly seems (or at least requiring an API bump) to backtrack and only take 1 collection.\r\n\r\nI was hoping to make a patch to work around this problem but I can't really think of one that doesn't involve bumping the API... save for probably a long shot where we could change the engine to accept more than just true/false from the query param (`?stream=`)... but even this could only be taken advantage of by a custom client lib, not the official one because that is accepting a bool in the function arguments.\r\n\r\nSo opening this as an intention to fix this, and also to solicit feedback for ideas we could do possibly as a back-portable patch."},{"labels":["api",null,null,null],"text":"I noticed that we still have some old release branches in this repository. Some of those branches have changes that never made it into a release (or documentation cherry-picks that were published in the documentation, but not tagged as a release)\r\n\r\n\r\nBranch                                               | Changes not in a release\r\n-----------------------------------------------------|--------------------------------------------------------------\r\n[1.12.x](https://github.com/moby/moby/tree/1.12.x)   | https://github.com/moby/moby/compare/v1.12.6...1.12.x\r\n[1.13.x](https://github.com/moby/moby/tree/1.13.x)   | https://github.com/moby/moby/compare/v1.13.1...1.13.x\r\n[17.03.x](https://github.com/moby/moby/tree/17.03.x) | https://github.com/moby/moby/compare/v17.03.2-ce...17.03.x\r\n[17.04.x](https://github.com/moby/moby/tree/17.04.x) | https://github.com/moby/moby/compare/v17.04.0-ce...17.04.x\r\n[17.05.x](https://github.com/moby/moby/tree/17.05.x) | https://github.com/moby/moby/compare/v17.05.0-ce...17.05.x (no diff)\r\n[docs](https://github.com/moby/moby/tree/docs)       | documentation-only changes\r\n\r\n\r\nSome care should be taken when removing these branches, because the docker documentation fetches some resources from some of these branches; https://github.com/docker/docker.github.io/blob/ec57364ede82a135021e2e762bd0833d92a0cc0f/_scripts/fetch-upstream-resources.sh#L74 https://github.com/docker/docker.github.io/blob/ec57364ede82a135021e2e762bd0833d92a0cc0f/_scripts/fetch-upstream-resources.sh#L89-L95\r\n\r\n\r\nIn addition, with the introduction of the Swagger file (which is used to document current versions of the API), this repository only has a definition of the _current_ version of the API. Older versions of the API need to be fetched from either tags in various repositories, or from release-branches in those repositories. Having those files spread in various locations makes building the docs complicated, but also makes it more difficult to _compare_ changes between API versions, or to make fixes to the API documentation (if there's a bug or missing documentation in existing API versions).\r\n\r\nI'm planning to add copies of the swagger.yml for each API version. in this repository to address that.\r\n\r\n- [x] update docker-ce-packaging to build from upstream repositories instead of the mono-repo; https://github.com/docker/docker-ce-packaging/pull/449\r\n    - note: more changes needed in the private release packaging repositories, but those are tracked separately\r\n- [x] update fetch-upstream-resources.sh script in documentation repository to not depend on these branches (https://github.com/docker/docker.github.io/pull/10101)\r\n    - [x] https://github.com/docker/docker.github.io/pull/10343 Engine API: use template for API reference pages\r\n    - [x] https://github.com/docker/docker.github.io/pull/10348 Simplify use of \"ENGINE_BRANCH\" and \"DISTRIBUTION_BRANCH\"\r\n    - [x] https://github.com/docker/docker.github.io/pull/10344 [WIP] simplify fetching API versions\r\n    - [x] https://github.com/docker/docker.github.io/pull/10578 Engine API: remove per-branch fetching of API docs\r\n- [x] backport fetch-upstream-resources.sh changes to documentation archive branches\r\n    - [x] v18.09 archive branch https://github.com/docker/docker.github.io/pull/10196\r\n    - [x] v18.03 archive branch https://github.com/docker/docker.github.io/pull/10349\r\n    - [x] ~v17.12 archive branch~ won't fix\r\n    - [x] ~v17.09 archive branch~ won't fix\r\n    - [x] ~v17.06 archive branch~ won't fix\r\n    - [x] ~v17.03 archive branch~ won't fix\r\n    - [x] ~v1.13 archive branch~ won't fix\r\n- [x] add a copy of the swagger.yml for each released version of the API\r\n    - [x] v1.40 - docker v19.03 https://github.com/moby/moby/pull/40570 \r\n    - [x] v1.39 - docker v18.09 https://github.com/moby/moby/pull/40570 \r\n    - [x] v1.38 - docker v18.06 https://github.com/moby/moby/pull/40570 \r\n    - [x] v1.37 - docker v18.03 - v18.05 https://github.com/moby/moby/pull/40778\r\n    - [x] v1.36 - docker v18.02 https://github.com/moby/moby/pull/40778\r\n    - [x] v1.35 - docker v17.12 - v18.01 https://github.com/moby/moby/pull/40778\r\n    - [x] v1.34 - docker v17.11 https://github.com/moby/moby/pull/40778\r\n    - [x] v1.33 - docker v17.10 https://github.com/moby/moby/pull/40778\r\n    - [x] v1.32 - docker v17.09 https://github.com/moby/moby/pull/40778\r\n    - [x] v1.31 - docker v17.07 https://github.com/moby/moby/pull/40778\r\n    - [x] v1.30 - docker v17.06 https://github.com/moby/moby/pull/40778\r\n    - [x] v1.29 - docker v17.05 https://github.com/moby/moby/pull/40570 \r\n    - [x] v1.28 - docker v17.04 https://github.com/moby/moby/pull/40570 \r\n    - [x] v1.27 - docker v17.03 https://github.com/moby/moby/pull/40570 \r\n    - [x] v1.26 - docker v1.13.1 https://github.com/moby/moby/pull/40570 \r\n    - [x] v1.25 - docker v1.13.0 https://github.com/moby/moby/pull/40570 \r\n- [x] cherry-pick copy of swagger.yaml to current release branch\r\n    - [x] https://github.com/moby/moby/pull/40575 [19.03 backport] docs: add API versions v1.25 - v1.29, v1.38 - v1.40\r\n    - [x] https://github.com/moby/moby/pull/40779 [19.03 backport] docs: add API versions v1.30 - v1.37\r\n- [x] ~remove unused release-branches~ we can't update all archives, so we may want to keep the old branches (but eventually, \"archive\" the docker/docker-ce repository once it's no longer used in the build pipeline\r\n"},{"labels":["api",null,null,null,null,null],"text":"**Description**\r\n\r\nVolume create event is emitted on `docker create` independently if volume already exists. I would expect a single volume create event since I 've created the volume myself prior to creating the container.\r\n\r\nAccording to my tests, this started happening after 18.03.1 (excluding) and 18.06.3 (including) and it is reproducible up to and including 19.03.2.\r\n\r\n**Steps to reproduce the issue:**\r\n1. `docker events` (on another terminal)\r\n2. `docker create volume foo`\r\n3. `docker run -v foo:/bar busybox` \r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n2019-10-06T17:33:50.529598724+03:00 volume create foo (driver=local)\r\n2019-10-06T17:34:06.642690750+03:00 volume create foo (driver=local)\r\n2019-10-06T17:34:06.673619523+03:00 container create 0a24e7e48aa1b71dfebf73696101e010bf6407b87fca1a0b949acf00cfd34358 (image=busybox, name=great_poitras)\r\n2019-10-06T17:34:06.678983859+03:00 container attach 0a24e7e48aa1b71dfebf73696101e010bf6407b87fca1a0b949acf00cfd34358 (image=busybox, name=great_poitras)\r\n2019-10-06T17:34:06.732641855+03:00 network connect 92b1bfd9f57ce29a27127d07ee8268a04b2ce5c35b3472960f73858fa0642c4b (container=0a24e7e48aa1b71dfebf73696101e010bf6407b87fca1a0b949acf00cfd34358, name=bridge, type=bridge)\r\n2019-10-06T17:34:06.741265314+03:00 volume mount foo (container=0a24e7e48aa1b71dfebf73696101e010bf6407b87fca1a0b949acf00cfd34358, destination=/bar, driver=local, propagation=, read/write=true)\r\n```\r\n\r\n**Describe the results you expected:**\r\n```\r\n2019-10-06T17:33:50.529598724+03:00 volume create foo (driver=local)\r\n2019-10-06T17:34:06.673619523+03:00 container create 0a24e7e48aa1b71dfebf73696101e010bf6407b87fca1a0b949acf00cfd34358 (image=busybox, name=great_poitras)\r\n2019-10-06T17:34:06.678983859+03:00 container attach 0a24e7e48aa1b71dfebf73696101e010bf6407b87fca1a0b949acf00cfd34358 (image=busybox, name=great_poitras)\r\n2019-10-06T17:34:06.732641855+03:00 network connect 92b1bfd9f57ce29a27127d07ee8268a04b2ce5c35b3472960f73858fa0642c4b (container=0a24e7e48aa1b71dfebf73696101e010bf6407b87fca1a0b949acf00cfd34358, name=bridge, type=bridge)\r\n2019-10-06T17:34:06.741265314+03:00 volume mount foo (container=0a24e7e48aa1b71dfebf73696101e010bf6407b87fca1a0b949acf00cfd34358, destination=/bar, driver=local, propagation=, read/write=true)\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.06.3-ce\r\n API version:       1.38\r\n Go version:        go1.10.3\r\n Git commit:        d7080c1\r\n Built:             Wed Feb 20 02:27:18 2019\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\nServer:\r\n Engine:\r\n  Version:          18.06.3-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.3\r\n  Git commit:       d7080c1\r\n  Built:            Wed Feb 20 02:26:20 2019\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 0\r\nServer Version: 18.06.3-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: f9bd8fw4occfpqwk6fccpgy1y\r\n Is Manager: true\r\n ClusterID: p5akkh9kxo9zlt7tmziyv6mu1\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 10\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Autolock Managers: false\r\n Root Rotation In Progress: false\r\n Node Address: 127.0.0.1\r\n Manager Addresses:\r\n  127.0.0.1:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86e\r\nrunc version: a592beb5bc4c4092b1b1bac971afed27687340c5\r\ninit version: fec3683\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.15.0-1037-gcp\r\nOperating System: Ubuntu 16.04.6 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 7.298GiB\r\nName: travis-job-ca6d6f0b-880f-450b-9a4a-5d10e659112b\r\nID: BYBT:L62W:A24Q:UNFC:SG5Q:Z47D:C5QK:DXTM:44FB:QYIO:5IDE:7VLA\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n https://mirror.gcr.io/\r\nLive Restore Enabled: false\r\n```"},{"labels":["api",null,null],"text":"It's unfortunately common for projects to mount the docker socket into (privileged) containers. A configuration option for specifying additional sockets with limited privileges would be fairly simple and would make it easier to do it easily without doing it wrong.\r\n\r\nEssentially, in the docker configuration, users would be able to specify the path of an additional socket with a limited set of permissions:\r\n\r\n`[(socket_path, [permissions,]),]`\r\n\r\nOr maybe also able to specify read-only access:\r\n\r\n`[(socket_path, {principal: 'rw', principal2: 'r'},]`\r\n\r\nI am aware that there are a number of far more complete RBAC solutions to limiting privileges; but I think creating said additional socket(s) with limited docker API privileges would be a fairly simple improvement that could help avoid granting unnecessarily broad privileges (e.g for easy ACME cert reloads and load balancing).\r\n\r\nAn example use case: securing the Traefik docker driver:\r\n\r\n- \"Docker integration: Exposing Docker socket to Traefik container is a serious security risk\" https://github.com/containous/traefik/issues/4174#issuecomment-446600393\r\n  > It seems it only require (read) operations : ServerVersion, ContainerList, ContainerInspect, ServiceList, NetworkList, TaskList & Events. \r\n  - https://github.com/liquidat/ansible-role-traefik\r\n    > This role does exactly that: it launches two containers, a traefik one and another to securely provide limited access to the docker socket. It also provides the necessary configuration.\r\n    - https://github.com/Tecnativa/docker-socket-proxy/issues/13\r\n      - Creates a HAproxy container that proxies limited access to the docket socket\r\n \r\nWith such a proposed configuration option for additional docker sockets with limited privileges, such an additional docker socket proxy container would be unnecessary."},{"labels":["api",null,null],"text":"**Description**\r\n\r\nI have noticed that, while a service has a task that is running, that task has a status with `ContainerStatus.ExitCode: 0`. That is a misleading value. \r\n\r\nI believe `0` is being used here as a stand-in for `null` (or `nil`). But those values are distinct. A `null` value (or `nil`, or no value at all) should mean \"the container has not yet reported an exit value\". Any non-`null` value should mean \"the container *has* exited, and its process has reported this exit code\". `0` in particular is a special value because it is the only value indicating success.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a service with a command that will continue running for at least a few seconds.\r\n```\r\n$ docker service create --name service_name busybox:latest sleep 120\r\neeest5s97viw1qzoy8pvllbkm\r\noverall progress: 1 out of 1 tasks\r\n1/1: running   [==================================================>]\r\nverify: Service converged\r\n```\r\n2. Get the task status. I am not aware of a way to do this with the CLI, so I use the API.\r\n```\r\n$ curl -s --unix-socket /var/run/docker.sock http://localhost/tasks?filters%3D%5B%22service%22%3A%22service_name%22%5D | jq '.[].Status'\r\n{\r\n  \"Timestamp\": \"2018-08-01T15:41:54.007243253Z\",\r\n  \"State\": \"running\",\r\n  \"Message\": \"started\",\r\n  \"ContainerStatus\": {\r\n    \"ContainerID\": \"a14116edc1b8e8222efde70af7b7531bb71c702b813e5a8d81f848b8dfa1013c\",\r\n    \"PID\": 6947,\r\n    \"ExitCode\": 0\r\n  },\r\n  \"PortStatus\": {}\r\n}\r\n```\r\n\r\n**Describe the results you received:**\r\nThe task is in `\"State\": \"running\"` but the `\"ContainerStatus.ExitCode\"` value is `0`.\r\n\r\n**Describe the results you expected:**\r\nEither no `\"ExitCode\"` property within `\"ContainerStatus\"`, or one with a `null` value.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           18.06.0-ce\r\n API version:       1.38\r\n Go version:        go1.10.3\r\n Git commit:        0ffa825\r\n Built:             Wed Jul 18 19:11:02 2018\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.06.0-ce\r\n  API version:      1.38 (minimum version 1.12)\r\n  Go version:       go1.10.3\r\n  Git commit:       0ffa825\r\n  Built:            Wed Jul 18 19:09:05 2018\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n```"},{"labels":["api",null,null,null],"text":"https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md#memory\r\n\r\n`.linux.resources.memory.kernelTCP` is defined in runtime spec and supported by runc.\r\n\r\nHowever, Docker/Moby does not expose API/CLI for setting this value currently, unlike [`.linux.resources.memory.kernel`](https://github.com/moby/moby/blob/3a633a712c8bbb863fe7e57ec132dd87a9c4eff7/api/types/container/host_config.go#L331).\r\n"},{"labels":["api",null],"text":"The `api/types` package should only contain the types used to serialize/deserialize requests and responses for the HTTP API. This is the case for most of the types in this package and sub packages.\r\n\r\nThere are two exceptions:\r\n* `api/types/client.go` contains types that are used by `APIClient` interface in the `client` package\r\n* `api/types/backend` contains types that are used by the backend interfaces between the API routers and the server implementation\r\n\r\nThese two categories of types are never used for serialization/deserialization.\r\n\r\nI propose we move these types to a more appropriate place:\r\n* `api/types/client.go` should move to `client/types`\r\n* `api/types/backend` could be split up and moved to `api/server/router/<component>` or moved to `api/server/backend/types`\r\n\r\ncc @tiborvass @vdemeester @cpuguy83 "},{"labels":["api",null],"text":"**Description**\r\n\r\nDocker attach API do use `application/vnd.docker.raw-stream` content type, which has two implementations:\r\n- multiplexed stdout/stderr streams\r\n- raw tty output\r\n\r\nfrom API response there's no way (afaict) to determine which one is used\r\nI suggest a header is introduced to document which mode is being used\r\n\r\n**Steps to reproduce the issue:**\r\n1. ask a friend to create a container, setting tty= to a random value kept secret\r\n2. invoke containers/{id}/attach\r\n3. try to guess the raw.stream format in use\r\n\r\n**Describe the results you received:**\r\n\r\nhave to try parsing a multiplexed frame, then admit I was wrong.\r\n\r\n**Describe the results you expected:**\r\n\r\nsimple way to determine the parsing logic to use\r\n"},{"labels":["api"],"text":"The [Swagger spec](https://github.com/moby/moby/blob/a21654c/api/swagger.yaml) for endpoints like [`/images/create`](https://github.com/moby/moby/blob/a21654c/api/swagger.yaml#L6246) advertise a `produces` MIME type of `\"application/json\"`. In reality, the endpoint produces line delimited JSON, which is not valid JSON per se, and for which an appropriate MIME type might be [`\"application/x-ndjson\"`](http://ndjson.org/).\r\n\r\nDifferentiating such endpoints from regular `application/json` endpoints will allow consumers such as codegenerators to generate suitable client implementations for handling the streaming output, instead of just trying to parse the response as JSON and exploding."},{"labels":["api",null,null],"text":"There is already a websocket endpoint to attach to a container.\r\nNow that service logs are available in the API, I'd be happy to be able to use websocket to fetch them.\r\nHijacking a TCP connection is not something we can easily do in a browser.\r\n\r\n"},{"labels":["api",null,null],"text":"Hi\r\n\r\nMaybe it has been asked before, but I haven't been able to find it.\r\n\r\nActually docker inspect on an exited container returns:\r\n```\r\n        ...\r\n        \"State\": {\r\n            \"Status\": \"exited\",\r\n            \"Running\": false,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": true,\r\n            \"Dead\": false,\r\n            \"Pid\": 0,\r\n            \"ExitCode\": 137,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2017-10-12T05:50:23.819164201Z\",\r\n            \"FinishedAt\": \"2017-10-12T06:45:33.520387397Z\"\r\n        },\r\n        ...\r\n```\r\n\r\nIt would be possible to preserve the PID after container exited, in order to easily match logs with atop/kernel OOM and so?\r\n\r\nThanks."},{"labels":["api",null,null,null],"text":"**Description**\r\n\r\nDocker stopped rejecting API requests with version numbers greater than the maximum version supported by the daemon. This happened in https://github.com/moby/moby/commit/e98e4a71110fd33852bb755a9b8b4ebc9df904db#diff-14051df1b5de1608aaba3a983f2a87e3 as part of https://github.com/moby/moby/pull/27745 and has affected every version of Docker since 1.13.0.\r\n\r\nThis change breaks the capability detection logic used by the Amazon ECS agent. The Amazon ECS agent is designed to work with all Docker versions >= 1.5.0, and we enable or disable features depending on the capabilities of the Docker version used. We use Remote API version as we expected this to be more stable than the daemon version; the daemon version has gone through a few changes since we started (roughly semver-like to year-month plus edition). We currently call the `/_ping` API with a range of different versions to discover which API versions are supported by the Docker daemon, but this change makes it so that every request with an API version >= 1.12 will result in success. This is causing incorrect behavior where features are being enabled with incompatible Docker versions; see https://github.com/aws/amazon-ecs-agent/issues/1008 .\r\n\r\nIn https://docs.docker.com/engine/api/version-history/#v125-api-changes , I see a note about a new header returned that specifies the maximum API version of the daemon. However, I do not see a note about the change in behavior to start accepting requests that have a specified version greater than the maximum.\r\n\r\n**Steps to reproduce the issue:**\r\nRun `curl --verbose --unix-socket /var/run/docker.sock http://localhost/v9999.9999/_ping`\r\n\r\n**Describe the results you received:**\r\nOn Docker versions prior to 1.13.0, you'll see an HTTP 400 with an error message \"client is newer than server\"\r\nOn Docker versions 1.13.0 or greater, you'll see an HTTP 200 OK.\r\n\r\n**Describe the results you expected:**\r\nI expected all versions of Docker to reject the API call because the specified API version was too new."},{"labels":["api",null,null],"text":"The responsible of the `client` package is to transform a go struct into an HTTP request, perform the HTTP request, and convert the HTTP response into a go struct + error.\r\n\r\n`client/service_update.go` and `client/service_create.go` have a bunch of application logic that does not belong in this package. We should remove it and move it into a separate package.\r\n\r\nIt looks like this logic was first introduced in #32388 and subsequently updated in #33239, and #33575.\r\n\r\nThis is related to #34242 and https://github.com/docker/cli/pull/386 \r\n\r\ncc @thaJeztah @nishanttotla @aaronlehmann "},{"labels":["api",null],"text":"**Motivation:**\r\nCurrently an application that would like to integrate with Docker by calling it's APIs has to either rely on docker listening on the default socket or be (manually) configured with the socket in case it differs from the default. \r\n\r\nSuch an application can try detecting the socket by parsing a running Docker process command line parameters and might need to parse the Docker configuration file. However, this duplicates Docker's logic inside the application and it's very brittle. Inspecting Docker's open file descriptors (e.g. through /proc) is not optimal either as file descriptors other than the API socket/s are held open. \r\n\r\nExposing the API sockets may also facilitate Docker CLI such that it will be able to communicate with a Docker host that is not listening on the default socket (changes to Docker CLI will go into a separate issue).\r\n\r\n**Implementation:**\r\nThe idea is to publish the socket info into a file, say similar to publishing the Docker PID through the docker.pid file.\r\n"},{"labels":["api",null],"text":"Now that the websocket endpoint is returning binary frames, it should include an header to differentiate the different streams, similarly to what is done for the /attach endpoint when tty is disabled.\r\nAs frames are atomic and contain a whole log line, a single byte header would probably be sufficient (ie no need to get the frame length, it is already known)\r\n\r\nUnless Im missing something, there is currently no way to differentiate stderr from stdout."},{"labels":["api",null],"text":"**Description**\r\n\r\nI'm in the process of interfacing with the Docker Engine API. I'm using the json-output of `/events`, and I noticed the exec-related events are a bit strange. I'm not sure I should call it a bug or a feature request, but I'm leaning towards the former.\r\n\r\nIn the docs at https://docs.docker.com/engine/api/v1.28/#operation/SystemEvents it states that it can report certain container events: `attach, ... exec_create, exec_detach, exec_start, ..., update`.\r\nThe example json response shows these values are being returned in the `Action`-field at the root of the json-object. \r\n\r\nFor the exec-related events, the value in that Action field is suffixed with the command being executed, which makes it harder to parse. For example, `docker exec [container] touch /tmp/foobar` will result in `{ ..., \"Action\":\"exec_create: touch /tmp/foobar\", ...}` and `{..., \"Action\":\"exec_start: touch /tmp/foobar\", ...}`. The `status`-field shows the same data.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nThis works with every image / exec-command, as far as I know:\r\n\r\n1. Run `docker run --name eventstuff -d phusion/baseimage`\r\n2. In a second terminal run `curl --no-buffer --unix-socket /var/run/docker.sock http:/v1.28/events`\r\n3. In the first terminal run `docker exec eventstuff touch /tmp/foobar`\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n{\"status\":\"exec_create: touch /tmp/foobar\",\"id\":\"[...]\",\"from\":\"phusion/baseimage\",\"Type\":\"container\",\"Action\":\"exec_create: touch /tmp/foobar\",\"Actor\":{\"ID\":\"[...]\",\"Attributes\":{\"image\":\"phusion/baseimage\",\"name\":\"eventstuff\"}},\"time\":1493976336,\"timeNano\":1493976336680190415}\r\n{\"status\":\"exec_start: touch /tmp/foobar\",\"id\":\"[...]\",\"from\":\"phusion/baseimage\",\"Type\":\"container\",\"Action\":\"exec_start: touch /tmp/foobar\",\"Actor\":{\"ID\":\"[...]\",\"Attributes\":{\"image\":\"phusion/baseimage\",\"name\":\"eventstuff\"}},\"time\":1493976336,\"timeNano\":1493976336680626546}\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\n```\r\n{\"status\":\"exec_create\",\"id\":\"[...]\",\"from\":\"phusion/baseimage\",\"Type\":\"container\",\"Action\":\"exec_create\",\"Actor\":{\"ID\":\"[...]\",\"Attributes\":{\"image\":\"phusion/baseimage\",\"name\":\"eventstuff\"}},\"time\":1493976336,\"timeNano\":1493976336680190415}\r\n{\"status\":\"exec_start\",\"id\":\"[...]\",\"from\":\"phusion/baseimage\",\"Type\":\"container\",\"Action\":\"exec_start\",\"Actor\":{\"ID\":\"[...]\",\"Attributes\":{\"image\":\"phusion/baseimage\",\"name\":\"eventstuff\"}},\"time\":1493976336,\"timeNano\":1493976336680626546}\r\n```\r\n\r\nThe main problem is that the suffixed command should not be in the `Action`-field. There are a few other remarks I have about the exec-events:\r\n\r\n- the exec events could have their own type, so they can return their instance-id (to be used with `GET /exec/[id]/json`) and the container-id.\r\n- the exec events could also include an event for when the exec-instance has died, so the full lifecycle is covered. Ideally, this event will also return the exit-code of the instance.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.04.0-ce\r\n API version:  1.28\r\n Go version:   go1.8\r\n Git commit:   4845c567eb\r\n Built:        Sat Apr  8 18:55:45 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.04.0-ce\r\n API version:  1.28 (minimum version 1.12)\r\n Go version:   go1.8\r\n Git commit:   4845c567eb\r\n Built:        Sat Apr  8 18:55:45 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 36\r\n Running: 28\r\n Paused: 0\r\n Stopped: 8\r\nImages: 17\r\nServer Version: 17.04.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: \r\ncontainerd version: 422e31ce907fd9c3833a38d7b8fdd023e5a76e73\r\nrunc version: 9c2d8d184e5da67c95d601382adf14862e4f2228\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.10.13-1-ARCH\r\nOperating System: Arch Linux\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 15.56GiB\r\nName: thebringeroflight\r\nID: M552:GIKC:2YPD:PVD2:GRRY:QFCA:6XPE:FXHB:6JYK:N4NN:BCBI:JWAL\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nProbably not relevant, but I'm running this on my development workstation, a physical machine.\r\n"},{"labels":["api",null],"text":"A couple of metrics we have are not following Prometheus naming conventions.\r\n\r\nSince they're still experimental, we can fix those:\r\n```\r\n$ curl http://localhost:3000/metrics | promtool check-metrics\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  155k  100  155k    0     0  9472k      0 --:--:-- --:--:-- --:--:-- 9692k\r\nengine_daemon_events_subscribers_total: non-counter metrics should not have \"_total\" suffix\r\nhttp_request_duration_microseconds: use base unit \"seconds\" instead of \"microseconds\"\r\n```\r\n\r\n/cc @crosbymichael  @juliusv @thaJeztah "},{"labels":["api",null,null,null],"text":"The docker CLI is moving to it's own repository, `docker/cli` See https://github.com/moby/moby/pull/32694\r\n\r\nThe current plan is to leave the HTTP engine API in the moby repo. It will work for now because \"docker engine\" is still a monolith, but it will get problematic as soon as we start splitting it into components.\r\n\r\nWe need a plan moving forward. Suggestions welcome :D"},{"labels":["api",null,null],"text":"I've looked through tickets, code, google, but do not see any procedure for calling dockerd through the latest API and running a similar command as:\r\n```\r\ndocker stack deploy -c docker-compose-stack.yml foo\r\n```\r\n\r\nI'd like to be able to build my own service running in a container then call the hosts docker service (if appropriate) to deploy into swarm mode. I'd like to do this without something hacky like SSHing back into the host and running a shell. That would unnecessary require additional pem and account configuration on the host.\r\n\r\nIf you know a work around or a best practice, please advise. Else this should be added as a feature to the API."},{"labels":["api",null],"text":"Basically https://github.com/docker/docker/blob/master/api/server/httputils/errors.go#L52 has gone on far too long.\r\n\r\nTo replace this, we've tried a couple of different approaches which were pretty disastrous:\r\n\r\n- errcode, which while there's nothing wrong with the errcode package, we used it horribly wrong and for the wrong reasons.\r\n- HTTP errors package - https://github.com/docker/docker/blob/master/api/errors/errors.go#L13\r\n\r\nerrcode was a neat concept but in practice was misused and made the entire codebase just became absolutely horrible to work on where as before the horribleness was contained to just the API layer.\r\n\r\nThis lead to the http errors package currently in use for certain cases. This also seems like an \"ok\" thing, but in practice is also not really used correctly and generally more of a shortcut to correct error handling. We end up with low-level bits worried about HTTP status codes.\r\n\r\nWe need to standardize on a way to handle errors in the API. This potentially needs to be regardless of how the request came in (HTTP, GRPC, or even a direct function call...e.g. cluster->daemon).\r\n\r\nThe goal of good error handling is that the caller knows, or at least knows how to find out, the class of error that occurred so that the error can be dealt with appropriately. This is true of function calls and it is true across the \"remote\" API barrier.\r\n\r\n### Goals\r\n\r\n- API backends (e.g. `daemon`) should ideally not be concerned with HTTP.\r\n- API clients should be able to read a returned error and be able to know *what* an error is.\r\n- Reduce the cognitive overhead in trying to determine how to return an error\r\ni. Example, what status code? (400, 409, 422, 412)\r\nii. How do I get the API to produce that status code I expect?\r\niii. How would non-remote API callers handle my error?\r\n- Ensure status codes don't change because of either a change to the error text, or hopefully even a change to the low-level error type.\r\n\r\nThe way I see it there are really two ways to deal with this (in no particular order):\r\n\r\n### Error class defined in common API package\r\n\r\nThis means in the API package we define several error classes like so (psuedo-code):\r\n\r\n```go\r\npackage api\r\n\r\ntype Error interface {\r\n    error\r\n    NotFound() bool\r\n    // other fn's for conflict, invalid arg, etc\r\n}\r\n\r\ntype apiError struct { \r\n    err error\r\n    notFound bool\r\n    conflict bool\r\n    invalidArg bool\r\n}\r\n\r\nfunc (e *apiError) Error() string { return e.err.Error() }\r\nfunc (e *apiError) NotFound() bool { return e.notFound }\r\n\r\nfunc NewNotFoundError(err error) Error {\r\n    return &apiError{err: err, notFound: true}\r\n}\r\n```\r\n\r\nFrom here the HTTP layer can determine both the error message and the class of error (not found) and the status code to return.\r\n\r\nThis approach is similar to the current approach we have except it's moving the http handling to the http layer and non-http code can just deal with this error type.\r\n\r\nAnother slight alternative but generally the same thing to this would be to use the well defined grpc error codes instead of defining our own. This could fit in well since part of the daemon is already using grpc anyway.\r\n\r\n### Define error handling interfaces on backends\r\n\r\nIn this method, backends would be required to define a set of functions for error handling:\r\n\r\nExample:\r\n\r\n```go\r\npackage api\r\n\r\ntype ErrorInspector interface {\r\n    IsNotFound(error) bool\r\n    IsConflict(error) bool\r\n    IsInvalidArgument(error) bool\r\n}\r\n```\r\n\r\n```go\r\npackage container // container router\r\n\r\nimport \"github.com/docker/docker/api\"\r\n\r\ntype ContainerBackend interface {\r\n    CreateContainer(...) error\r\n    DeleteContainer(...) error\r\n    api.ErrorInspector\r\n}\r\n\r\nfunc(r *containerRouter) getContainer(w http.ResponseWriter, req *http.Request) error {\r\n    err := r.backend.DeleteContainer(...)\r\n    if err != nil {\r\n        if r.backend.IsNotFound(err) {\r\n           // make an HTTP 404\r\n       }\r\n       // make an HTTP 500 probably\r\n    }\r\n\r\n    // normal\r\n}\r\n```\r\n\r\nWriting out the above it seems a little risky since it requires the router to know the failure-modes of the backend. Maybe could be cleaned up a bit.\r\n\r\n\r\nInterested in your thoughts, other alternatives, etc.\r\n\r\nping @docker/core-engine-maintainers "},{"labels":["api",null,null],"text":"Debugging the daemon can be quite gruesome work.\r\nEven one of the simplest things like collecting a goroutine stack dump (most common request) is difficult to explain and even more difficult to collect.\r\n\r\nWe already have some API endpoints to aid in debugging but it's only available if debug mode is actually enabled.... forcing a daemon reload (also difficult to explain and perform) to enable it can even mess up the results.\r\n\r\nI propose:\r\n\r\n1. Enable debug endpoints (namely `/debug/pprof/*`) by default.\r\n2. Add a new CLI subcommand `docker debug` which includes a suite of subcommands for pulling the desired information\r\ni. `docker debug stack` -> generates a goroutine stack dump\r\nii. `docker debug profile <blah>` -> generates pprof formatted reports (whatever happens to be supported by the language runtime)\r\nii. `docker debug profile ls` -> List available profiles (this requires a call to the daemon, otherwise would include in `--help` output)\r\n\r\nThese commands either send output directly to stdout or optionally to an output file.\r\n\r\nPotentially also interesting is grabbing a trace or a cpu profile (explicitly not supported above), perhaps by adding a `--trace` or `--cpu-profile` to a `docker` command.... but this may be left as another exercise.\r\n\r\n**note**: not tied to the layout/naming of those commands\r\n\r\nAnything under `/debug` or `docker debug` should be considered non-formal API that is subject to change outside the scope of API versioning.\r\n\r\nIn the future these endpoints can be used to generate something like a support tarball via `docker debug support`."},{"labels":["api",null],"text":"(Note that the title is a joke, referring to [swagger.yaml line 818](https://github.com/docker/docker/blob/master/api/swagger.yaml#L818): `\"TODO check is correct\"`.)\r\n\r\nIn the swagger spec for inspecting a container (`operationId: \"ContainerInspect\"`), one of the object properties is `NetworkSettings`. For that value, the spec refers to the object definition at `#/definitions/NetworkConfig`. However, that definition is not complete.\r\n\r\nHere is the incomplete `NetworkConfig` swagger spec:\r\n```yaml\r\nNetworkConfig:\r\n  description: \"TODO: check is correct\"\r\n  type: \"object\"\r\n  properties:\r\n    Bridge:\r\n      type: \"string\"\r\n    Gateway:\r\n      type: \"string\"\r\n    Address:\r\n      type: \"string\"\r\n    IPPrefixLen:\r\n      type: \"integer\"\r\n    MacAddress:\r\n      type: \"string\"\r\n    PortMapping:\r\n      type: \"string\"\r\n    Ports:\r\n      type: \"array\"\r\n      items:\r\n        $ref: \"#/definitions/Port\"\r\n```\r\n\r\nThe spec for inspecting a container gives an example `NetworkSettings` with many more properties than are defined in the `NetworkConfig` spec that supposedly defines it:\r\n```yaml\r\nNetworkSettings:\r\n  Bridge: \"\"\r\n  SandboxID: \"\"\r\n  HairpinMode: false\r\n  LinkLocalIPv6Address: \"\"\r\n  LinkLocalIPv6PrefixLen: 0\r\n  SandboxKey: \"\"\r\n  SecondaryIPAddresses: null\r\n  SecondaryIPv6Addresses: null\r\n  EndpointID: \"\"\r\n  Gateway: \"\"\r\n  GlobalIPv6Address: \"\"\r\n  GlobalIPv6PrefixLen: 0\r\n  IPAddress: \"\"\r\n  IPPrefixLen: 0\r\n  IPv6Gateway: \"\"\r\n  MacAddress: \"\"\r\n  Networks:\r\n    bridge:\r\n      NetworkID: \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\"\r\n      EndpointID: \"7587b82f0dada3656fda26588aee72630c6fab1536d36e394b2bfbcf898c971d\"\r\n      Gateway: \"172.17.0.1\"\r\n      IPAddress: \"172.17.0.2\"\r\n      IPPrefixLen: 16\r\n      IPv6Gateway: \"\"\r\n      GlobalIPv6Address: \"\"\r\n      GlobalIPv6PrefixLen: 0\r\n      MacAddress: \"02:42:ac:12:00:02\"\r\n```\r\n\r\nThe properties in the example should be added to the `NetworkConfig` definition. This should be mostly straightforward, with a couple exceptions. The `Networks` property will follow the same spec from `ContainerSummary`:\r\n```yaml\r\nNetworks:\r\n  type: \"object\"\r\n  additionalProperties:\r\n    $ref: \"#/definitions/EndpointSettings\"\r\n```\r\n\r\nI'm not sure about the two `null` properties from the example: `SecondaryIPAddresses` and `SecondaryIPv6Addresses`. Are they lists of strings? Some object?"},{"labels":["api",null,null,null],"text":"Currently, dockerd itself supports a \"single\" and \"static\" way to protect /run/docker.sock using a tlskey file. But it is not suitable for swarm.\r\n\r\nJust like swarm token has its expired time, tlskey should also have, especially when the key has been exposed to public.\r\n\r\n\"single\" means the key takes effect for only 1 node, thus, swarm cluster requires people to update key on each of active swarm nodes once the key is changed.\r\n\r\n\"static\" indicates we have to restart all daemons once the key is changed to apply for changes.\r\n\r\nTaking advantage of built-in etcd storage, Swarm node should record a key-value pair in it which enables a \"non-single\" way to solve the problem. For example, assume there are 5 swarm nodes, post a request to 1 node about to change the password, swarm stores the new password, and then all other swarm nodes will get the updated automatically. Besides, all swarm nodes needn't restart to apply for changes if we provide standard WWW-Auth like Basic-Auth.\r\n\r\n\r\n"},{"labels":["api",null,null],"text":"Working with the API and Swarm, inspecting a service or listing services on a swarm does not include the node that the service is currently running on.\r\n\r\nCould this be included in a future update? Or is there already a mechanism for that, besides just running through all nodes and listing containers?"},{"labels":["api",null,null],"text":"I'd like to print image name from my current running containers, while showing statistics. The issue si similar to https://github.com/docker/docker/issues/20973, but referring to image name:\r\n\r\ndocker stats --format \"table {{.Container}}\\t{{.Name}}\\t{{**.ImageName**}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.BlockIO}}\\t{{.NetIO}}\"\r\n\r\nRequest is: property .ImageName should be available to format shown statistics\r\n"},{"labels":["api"],"text":"#25820 added metrics to the engine (great).\r\n\r\nAt that point it was decided to expose metrics on an entirely different endpoint, controlled via `--metrics-addr` (an experimental flag).\r\n\r\nThis raises a few concerns regarding exposing an unprotected port. For some exposing a plain text port is fine, for others leaking container IDs etc is bad.\r\n\r\nThere are a few ways to get around that, one would be to use the TLS configuration but it feels pretty meh to me.\r\n\r\nWhat I'd suggest is to expose the `/metrics` endpoint on the regular Docker API. One can easily achieve the `--metrics-addr` behavior by running a container on the machine that listens to a port and exposes just the `/metrics` endpoint of the API (by querying the local daemon).\r\n\r\nOne could also chose to deploy this same service with an overlay network, thus not requiring to bind the external port.\r\n\r\nLong story short: I believe exposing metrics under the same API server leads to a more consistent exposure level (TLS, authentication, ...) while enabling the same use case as `--metrics-addr` and then some.\r\n\r\nThoughts?\r\n\r\n/cc @crosbymichael @stevvooe @dhiltgen @vieux @icecrime "},{"labels":["api"],"text":"In this issue, we are proposing a simple pagination system for the Docker remote API. The need for pagination of resource collection responses is growing as the Docker API expands from being a single-node API to a cluster-wide API where the number of objects in a collection may increase by many orders of magnitude depending on the size of the cluster.\r\n\r\nThe current List APIs include:\r\n\r\n- Containers\r\n- Images\r\n- Networks\r\n- Volumes\r\n- Secrets\r\n- Nodes\r\n- Services\r\n- Tasks\r\n- Plugins\r\n\r\nThe proposed method of pagination is mostly server-driven:\r\n\r\n- The set of resources to be listed should have a stable ordering. For most resources in Docker, this would either be the resource name or ID.\r\n- The server may decide on a default page size (number of records to return) if the client does not specify a `pageSize` query parameter. The server should also impose a max page size.\r\n- If there are additional pages of results available, the server should return a `X-Next-Page-Start` response header indicating the point in the stable ordering of resources where the next page begins. The absence of this header indicates that no additional pages are available.\r\n- If the client wishes to fetch the next page of results, it need only set a `pageStart` query parameter to the value in the returned `X-Next-Page-Start` header. This instructs the server to skip to the first value in the stable ordering of resources where the ordering key (either name or ID) is greater than or equal to the  `nextPageStart` value.\r\n\r\nConsiderations:\r\n\r\n- Any filtering parameters should not affect the ordering of results.\r\n- If multiple orderings of the result set are available, the API should use an `orderBy` query parameter to specify the desired ordering. Pagination of results is consistent as long as the desired ordering is also consistent between successive page requests.\r\n- If the client would like to know the size of the entire resource collection, the client should specify a `count=true` query parameter to indicate to the server that it should return the entire size of the resource set in the response. This MAY be done in either a `X-Resource-Count` response header or as a `resourceCount` field in the response payload itself (many of the existing Docker API endpoints return a JSON array and would use the response header). The server may also simply choose to always return the resource count regardless of whether the client specified a `count=true` query parameter. The returned resource collection count should *not* consider any filtering parameters in the request as doing so may be an expensive operation."},{"labels":["api",null,null,null],"text":"**Description**\r\nI am trying to invoke Docker Remote API using Elixirl as explained at:\r\nhttps://docs.docker.com/engine/reference/api/docker_remote_api/\r\n\r\nHere is the exact command used:\r\n\r\n```\r\n$ curl https://188.166.86.204:2376/_ping \\\r\n  --cert ~/docker_workspaces/certs/cert.pem \\\r\n  --key ~/docker_workspaces/certs/key.pem \\\r\n  --cacert ~/docker_workspaces/certs/ca.pem\r\n\r\nOK\r\n\r\niex>\r\noptions = [] \\\r\n  |> Keyword.merge([\r\n  hackney: [\r\n    ssl_options: [\r\n      certfile: \"~/docker_workspaces/certs/cert.pem\",\r\n      keyfile: \"~/docker_workspaces/certs/key.pem\",\r\n      cacertfile: \"~/docker_workspaces/certs/ca.pem\"\r\n    ]\r\n  ]\r\n])\r\nHTTPoison.request(:get, \"https://188.166.86.204:2376/_ping\", \"\", [], options)\r\n\r\n[error] SSL: :certify: tls_connection.erl:619:Fatal error: handshake failure - malformed_handshake\r\n{:error, %HTTPoison.Error{id: nil, reason: {:tls_alert, 'handshake failure'}}}\r\n```\r\nThis works fine when I connect to v1.12.5, but for some reason v1.13.0 is not working.\r\n\r\n\r\n**Relevant versions:**\r\n```\r\n$ curl -v\r\ncurl 7.52.1 (x86_64-apple-darwin16.1.0) libcurl/7.52.1 OpenSSL/1.0.2j zlib/1.2.8\r\nProtocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtsp smb smbs smtp smtps telnet tftp \r\nFeatures: IPv6 Largefile NTLM NTLM_WB SSL libz TLS-SRP UnixSockets HTTPS-proxy \r\n```\r\n\r\n```\r\n$ erl --version\r\nErlang/OTP 19 [erts-8.2] [source] [64-bit] [smp:8:8] [async-threads:10] [hipe] [kernel-poll:false] \r\n```\r\n\r\n```\r\n$ elixir --version\r\nErlang/OTP 19 [erts-8.2] [source] [64-bit] [smp:8:8] [async-threads:10] [hipe] [kernel-poll:false] [dtrace]\r\n\r\nElixir 1.4.0\r\n```\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a new docker machine using the cli\r\n2. curl the rest-api with the correct certs (should return ok)\r\n3. Use Hackey/HTTPoison and the same certs to query the rest api.\r\n\r\n**Output of `docker version`:**\r\nWorking:\r\n```\r\nroot@7396d938:~# docker version\r\nClient:\r\n Version:      1.12.5\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   7392c3b\r\n Built:        Fri Dec 16 02:42:17 2016\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.12.5\r\n API version:  1.24\r\n Go version:   go1.6.4\r\n Git commit:   7392c3b\r\n Built:        Fri Dec 16 02:42:17 2016\r\n OS/Arch:      linux/amd64\r\n```\r\n\r\nNot working:\r\n```\r\nroot@6f3778bf:~# docker version\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:58:26 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:58:26 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\nroot@6f3778bf:~#\r\n```\r\n"},{"labels":["api",null,null],"text":"I spent a bunch of time debugging a [bearer token](https://github.com/docker/docker/issues/29257) issue.  A lot of it was wasted because I hadn't introspected the bearer token to discover that my `access` section was empty.  Perhaps there's an easier way, but I ended up downloading the [JWT Debugger](https://jwt.io/) and manually copying the long tokens.\r\n\r\nTo reduce the complexity of debugging, it'd be nice if there was an API endpoint that returned a simple JSON structure showing the access rights granted by the request."},{"labels":["api",null,null,null,null],"text":"Currently, `docker service logs` expects a service name or ID and will stream the combined output of every task that is part of the service.\r\n\r\nIt would be neat to support `docker service logs <task ID>`.\r\n\r\nThis is already implemented in the SwarmKit backend, so it's really a matter of plumbing it into the API/CLI.\r\n\r\nRelevant bits: In `daemon/cluster/cluster.go` there is:\r\n```go\r\n\tstream, err := state.logsClient.SubscribeLogs(ctx, &swarmapi.SubscribeLogsRequest{\r\n\t\tSelector: &swarmapi.LogSelector{\r\n\t\t\tServiceIDs: []string{service.ID},\r\n\t\t},\r\n\t\tOptions: &swarmapi.LogSubscriptionOptions{\r\n\t\t\tFollow: config.Follow,\r\n\t\t},\r\n\t})\r\n```\r\n\r\nIn order to stream logs for a task using SwarmKit, one would need to use the exact same code, using a different selector: `TaskIDs: []string{task.ID}`.\r\n\r\nI think the workflow would look like:\r\n1) The user calls `docker service logs <something>`\r\n2) The CLI attempts to call `/services/<something>/inspect`\r\n3) If that succeeds, then it continues with `/services/<something>/logs`, and the rest of the code is the same\r\n4) If it fails, then it attempts to call `/tasks/<something>/inspect`\r\n5) If that works, it calls `/tasks/<something>/logs` and continues with the rest of the code\r\n6) if that also fails, the CLI errors out saying <something> is neither a service nor a task\r\n\r\n/cc @thaJeztah @aaronlehmann "},{"labels":["api",null],"text":"When using the docker stats api currently each request to the API will take up to 2 seconds. As explained in https://github.com/docker/docker/issues/23188 the docker API needs two measurement points to calculate an average. \r\n\r\nFor stats collectors like [Metricbeat](https://github.com/elastic/beats/tree/master/metricbeat) it would be useful to get the raw cpu values and either do the calculations on the collector side or directly when viewing the data on the storage system (in our case Elasticsearch).\r\n\r\nI'm proposing to add a flag/param to the docker stats API request that skips average calculations and return only raw values instead. This will speed up the API request and will potentially also reduce the load on the docker service as the request is open for a much shorter time.\r\n\r\nCollecting stats for a large number of containers has currently to be done one by one which leads to lots of open http requests and is quite slow because of the above issue. This will potentially be solved with https://github.com/docker/docker/pull/25361 by @WeiZhang555 but also as part of this bulk request it would be nice to have the flag to disable calculations."},{"labels":["api"],"text":"There is an API endpoint which exists for updating metadata for a secret (i.e., labels) but there is not yet a method on the Go client in the `github.com/docker/docker/client` package.\r\n\r\nNote: this issue is **not** requesting that we add a CLI command for updating a secret."},{"labels":["api",null,null],"text":"# Problem statement\r\n\r\nDocker 1.12.0 and 1.13.0 introduce new syntaxes for:\r\n\r\n- Mount specifications (`docker run --mount` and `docker service create --mount`)\r\n- Port specifications (`docker service create --port`)\r\n- Secret specifications (`docker service create --secret`)\r\n\r\nThis raises several questions regarding consistency:\r\n\r\n1. What are the guidelines in terms of short syntax (\"porcelain\") versus long syntax (\"plumbing\")?\r\n2. How many flags should be exposed: one for porcelain and one for plumbing, or a single one with two possible syntaxes?\r\n3. Where those flags overlap, do they use the same naming scheme for long syntax option names?\r\n\r\n## Current state\r\n\r\n<table>\r\n  <tbody>\r\n    <tr>\r\n      <th colspan=\"2\">Area</th>\r\n      <th>Mounts</th>\r\n      <th>Ports</th>\r\n      <th>Secrets</th>\r\n    </tr>\r\n    <tr>\r\n      <th align=\"left\" rowspan=\"2\">Syntax</th>\r\n      <th align=\"left\">Short</th>\r\n      <td><a href=\"https://docs.docker.com/engine/reference/run/#/volume-shared-filesystems\"><code>/src:/dest:rw</code></a></td>\r\n      </td>\r\n      <td><a href=\"https://github.com/docker/docker/pull/27917#issuecomment-258556920\"><code>80:8080/tcp</code></a></td>\r\n      <td><code>secret_name</code></td>\r\n    </tr>\r\n<tr>\r\n      <th align=\"left\">Long</th>\r\n      <td><a href=\"https://docs.docker.com/engine/reference/run/#/volume-shared-filesystems\"><code>src=/src,dst=/dest,readonly=true</code></a></td>\r\n      <td><code>target=80,published=8080,protocol=tcp</code></td>\r\n      <td><code>source=secret_name,target=alias</code></td>\r\n</tr>\r\n    <tr>\r\n      <th align=\"left\" rowspan=\"2\">Flags</th>\r\n      <th align=\"left\">Short</th>\r\n      <td><code>-v, --volume</code></td>\r\n      <td><code>-p, --publish</code></td>\r\n      <td><code>--secret</code></td>\r\n    </tr>\r\n<tr>\r\n      <th align=\"left\">Long</th>\r\n      <td><code>--mount</code></td>\r\n      <td><code>--port</code></td>\r\n      <td><code>--secret</code></td>\r\n</tr>\r\n    <tr>\r\n      <th align=\"left\" colspan=\"2\">Named options</th>\r\n      <td>\r\n<ul>\r\n<li>type</li>\r\n<li>src, source</li>\r\n<li>dst, destination, target</li>\r\n<li>readonly, ro</li>\r\n<li>bind-propagation</li>\r\n<li>volume-driver</li>\r\n<li>volume-label</li>\r\n<li>volume-nocopy</li>\r\n<li>volume-opt</li>\r\n</ul>\r\n</td>\r\n<td>\r\n<ul>\r\n<li>target</li>\r\n<li>published</li>\r\n<li>protocol</li>\r\n<li>mode</li>\r\n</ul>\r\n</td>\r\n<td>\r\n\r\n<ul>\r\n<li>source</li>\r\n<li>target</li>\r\n<li>uid</li>\r\n<li>gid</li>\r\n<li>mode</li>\r\n</ul>\r\n</td>\r\n    </tr>\r\n</tbody>\r\n</table>\r\n\r\nWhat we can see from the table above is that:\r\n\r\n- Some areas have a same flag support the short and long syntax.\r\n- Some areas have overlapping named options, but not necessarily the same short versions.\r\n- Some areas have overlapping named options, but not necessarily the same semantic (*to be verified*).\r\n\r\n# Proposal\r\n\r\n*The proposal is to ~burn it all~ homogenize according to well defined guidelines that we can reuse in the future for every feature with the same requirement. What follows is an attempt at defining such guidelines.*\r\n\r\nAll features which require both a rich way to express complex things in a non-ambiguous manner, while preserving a nice \"porcelain\" syntax for the 80% use case should:\r\n\r\n1. Use the same flag for both the short and long syntax.\r\n2. The long syntax should be a **comma separated** list of **`arg=value`** specifications (e.g., `arg1=val1,arg2=val2`).\r\n3. The short syntax should be a separated list of values (**NOTE:** separator TBD, although `--publish` and `--volume` use `:` today), which defines positional arguments that map directly to an expanded long term equivalent. For example, given the order `arg1`, `arg2`, `arg3`, the short syntax `val1:val2` is equivalent to the long form `arg1=val1,arg2=val2`. This is inspired by the way Python function invocation syntax deals with optional and keyword-only arguments.\r\n4. Wherever possible, consistency of named arguments in the long form across different features should be enforced, including in any short term aliases that those named arguments can take.\r\n\r\n# What it means for 1.13.0\r\n\r\nIf we agree to move forward with this, then we need to review the current set of flags and act accordingly:\r\n\r\n- [x] The newly introduced `--port` flag should likely disappear in favor of `--publish`, ensuring that the short form maps cleanly to the long form. #28943\r\n- [x] We know that the `-v` syntax is likely already too overloaded to map cleanly to the long-syntax equivalent introduced by mount, and we might need to drop support for `--mount` in `run` until we can figure this out.\r\n- [ ] We need to reconcile named arguments in secrets with those in mounts (especially with regard to the aliases that `source` supports in mounts).\r\n\r\n# References\r\n\r\n- #26825 - Add `--mount` flag to `docker create` and `docker run`\r\n- #27794 - Add secret management\r\n- #27917 - Add support for host port `PublishMode` in services using the `--port` option in docker service create\r\n\r\nPing @joaofnfernandes @aluzzardi @thaJeztah @mstanleyjones @vieux @dhiltgen @ehazlett @dnephin :cactus:"},{"labels":["api",null],"text":"#26276 introduced a backwards incompatible api change in the docker info api. If the client reports a lower API version than 1.25, we need to return a response using the old format."},{"labels":["api",null,null],"text":"Networks don't support updates. While this makes sense for the most part (it's impossible to change a driver midway), there are some fields which would be harmless to update such as labels.\n\nThere's also some other fields which could be considered such as `attachable`.\n\n/cc @diogomonica @mrjana @mavenugo \n"},{"labels":["api",null,null],"text":"**Description**\n\nWhen using the remote API to create a service but in the params an invalid network ID is send, the API returns 404. A 404 error signals in REST semantics that an inexistent resource has been tryied to be accessed. However, in this case we are creating a new resource, the fact that the new resource has a property that \"links\" to an invalid resource should not be relevant in this case.\nBesides, the documentation https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/create-a-volume\ndoes not mention that the call can return 404:\n\n**Steps to reproduce the issue:**\nJust use the remote API to create a service with an invalid network ID\n\n**Describe the results you received:**\n404 { message: 'network xxxxxx not found' }\n\n**Describe the results you expected:**\n406 { message: 'network xxxxxx not found' }\n\n**Output of `docker version`:**\n\n```\nClient:\n Version:      1.12.0\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   8eab29e\n Built:        Thu Jul 28 23:54:00 2016\n OS/Arch:      darwin/amd64\n\nServer:\n Version:      1.12.1\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   23cf638\n Built:        Thu Aug 18 05:33:38 2016\n OS/Arch:      linux/amd64\n```\n"},{"labels":["api",null],"text":"For `/nodes`, it looks like the DELETE and GET methods can take either an ID, a full name or an ID prefix, but POST can only use the actual ID. This seems inconsistent (and doesn't match the docs). Let's standardize on one or the other.\n"},{"labels":["api",null,null],"text":"While working on https://github.com/docker/docker/pull/25548, I noticed some things in the plugin API that may need improvement; putting them in an issue so that I don't forget, and we can discuss.\n1. Plugins can only be referenced by _name_ on the API (not by ID). Referencing by ID may be more consistent with other API endpoints, and also keeps control over possibly \"ambiguous\" URLs (see other points below)\n2. Due to using names in the URL we may run into issues with plugins installed from third-party registries (`my-registry:5000/my/plugin:latest`). I'm not sure routing those URL's will always work; they look a bit \"scary\".\n3. Potentially ambiguous URLs (for example, inspecting a plugin called `helloworld/enable` would produce URL `/plugins/helloworld/enable`). This is not a _direct_ issue, because inspect uses `GET`, and enable `POST`, but just posting in case.\n4. Some endpoints don't return a message in the response. I didn't check if other endpoints consistently do this, but a \"success\" message may be nice (e.g. `/plugins/(plugin)/enable` only responds with `HTTP/1.1 200 OK`, but no message (e.g. `{message: \"successfully enabled plugin foo\"}`\n\n/cc @anusha-ragunathan @tiborvass @bfirsh\n"},{"labels":["api",null],"text":"The `image/load` endpoint returns either `text/plain` or `application/json`, depending on if the `?quiet` query-parameter is set to true/1 or false/0;\n\n```\ncat my-image.tar | curl --data \"@-\" --verbose -H \"Content-Type: application/x-tar\" -X POST http://127.0.0.1:2375/v1.25/images/load?quiet=0\n\n(snip)\n\nHTTP/1.1 100 Continue\nHTTP/1.1 200 OK\nContent-Type: application/json\nTransfer-Encoding: chunked\n\n{\"status\":\"Loading layer\",\"progressDetail\":{\"current\":32768,\"total\":1292800},\"progress\":\"[=                                                 ] 32.77 kB/1.293 MB\",\"id\":\"8ac8bfaff55a\"}\n{\"status\":\"Loading layer\",\"progressDetail\":{\"current\":65536,\"total\":1292800},\"progress\":\"[==                                                ] 65.54 kB/1.293 MB\",\"id\":\"8ac8bfaff55a\"}\n{\"status\":\"Loading layer\",\"progressDetail\":{\"current\":98304,\"total\":1292800},\"progress\":\"[===                                               ]  98.3 kB/1.293 MB\",\"id\":\"8ac8bfaff55a\"}\n{\"status\":\"Loading layer\",\"progressDetail\":{\"current\":131072,\"total\":1292800},\"progress\":\"[=====                                             ] 131.1 kB/1.293 MB\",\"id\":\"8ac8bfaff55a\"}\n...\n{\"stream\":\"Loaded image: busybox:latest\\n\"}\n```\n\n```\ncat my-image.tar | curl --data \"@-\" --verbose -H \"Content-Type: application/x-tar\" -X POST http://127.0.0.1:2375/v1.25/images/load?quiet=1\n\n(snip)\n\nHTTP/1.1 100 Continue\nHTTP/1.1 200 OK\nContent-Length: 29\nContent-Type: text/plain; charset=utf-8\n\nLoaded image: busybox:latest\n```\n\nThe endpoint should return JSON in both cases to be consistent\n\n/cc @bfirsh \n"},{"labels":["api",null],"text":"## Why we should do this\n\nThere are various ways we can clean up the API to fit HTTP conventions. This will make it easier to comprehend, easier to use, and generally better organised.\n\nHere are some things that could be changed:\n1. We could remove `/json` from URLs because:\n   - They're all JSON anyway.\n   - If we want to add different formats, we can use parameters or the `Accept` header.\n   - Consistency with new `/networks` and `/volumes` endpoints.\n2. We could remove `/create` from API URLs because `POST` implies you are creating.\n3. We could remove `/update` from API URLs and use `PUT` instead.\n## Why we shouldn't do this\n\nArguably, this is a pretty big change for a minor improvement in usability and aesthetics. It wouldn't be a breaking change because we can use a new API version, but it _is_ breaking in the sense that people will be forced to update their client libraries to use this API version.\n\nAlthough this is a minor improvement on the surface, I think people who see this crufty API design will perceive Docker as poor quality, and it is a [broken window](https://en.wikipedia.org/wiki/Broken_windows_theory) that encourages more bad design.\n## How\n\nThis is non-trivial, because some other API endpoints need renaming (e.g. `/images/search` will collide with an image called `search`). See the failed attempt in [#23219](https://github.com/docker/docker/pull/23219) for full details.\n\nThe trivial ones:\n- Rename `GET /containers/json` to `GET /containers`\n- Rename `GET /containers/(id or name)/json` to `GET /containers/(id or name)`\n- Rename `POST /containers/(id or name)/update` to `PUT /containers/(id or name)`\n- Rename `GET /images/json` to `GET /images`\n- Rename `POST /images/create` to `POST /images`\n- Rename `GET /images/json` to `GET /images`\n- Rename `GET /images/(name)/json` to `GET /images/(name)`\n- Rename `POST /images/create` to `POST /images`\n- Rename `GET /exec/(id)/json` to `GET /exec/(id)`\n- Rename `POST /volumes/create` to `POST /volumes`\n- Rename `POST /networks/create` to `POST /networks`\n- Rename `POST /services/create` to `POST /services`\n- Rename `POST /services/(id or name)/update` to `PUT /services/(id or name)`\n\nThe ones that are a bit more complex: \n- Do something to `GET /images/search` because it collides with `GET /images/(name)`. We could either:\n  - Move it client side, because all it does is query Docker Hub.\n  - Give it a different name. e.g. `GET /image-search?q=...` It's a different resource to daemon images, anyway.\n- Rename `GET /images/(name)/get` to `GET /images/(name)?format=tar` (`GET .../get` is not particularly descriptive)\n- Rename `POST /images/load` to `POST /images?format=tar`\n- Rename `GET /images/get?names=(name1)&names=(name2)` to `GET /images?format=tar&name=(name1)&name=(name2)`. Alternatively, we could remove this entirely because it's probably pretty simple to concatenate the tarballs client-side.\n## See also\n- #5893\n- #23219\n\n/cc @justincormack @thaJeztah @vdemeester @cpuguy83 \n"},{"labels":["api",null,null,null],"text":"Stopping multiple containers is currently implemented in the client.\n\nAs a result, stopping multiple containers takes longer than needed, and also can result in only _some_ of the containers being stopped if the client disconnects during the process.\n\nFor example, having three containers, and stopping them all through the CLI;\n\n``` bash\ndocker run -dit --name one busybox\ndocker run -dit --name two busybox\ndocker run -dit --name three busybox\n\ndocker stop one two three\n```\n\nResults in:\n\n```\nDEBU[0683] Calling POST /v1.25/containers/one/stop\nDEBU[0683] Sending 15 to 4abc5dc9add562469a8aeda45c6955453c6615ca0a6d897ce2aff01f33acd04e\nINFO[0693] Container 4abc5dc9add562469a8aeda45c6955453c6615ca0a6d897ce2aff01f33acd04e failed to exit within 10 seconds of signal 15 - using the force\nDEBU[0693] Sending 9 to 4abc5dc9add562469a8aeda45c6955453c6615ca0a6d897ce2aff01f33acd04e\nDEBU[0693] containerd: process exited                    id=4abc5dc9add562469a8aeda45c6955453c6615ca0a6d897ce2aff01f33acd04e pid=init status=137 systemPid=734\nDEBU[0693] received containerd event: &types.Event{Type:\"exit\", Id:\"4abc5dc9add562469a8aeda45c6955453c6615ca0a6d897ce2aff01f33acd04e\", Status:0x89, Pid:\"init\", Timestamp:0x578a7d0e}\nDEBU[0693] Revoking external connectivity on endpoint one (9aad9a405dde09368cf35444ffa3b65a664856f8786ef8aecd59594f6a5caf19)\nDEBU[0693] Releasing addresses for endpoint one's interface on network bridge\nDEBU[0693] ReleaseAddress(LocalDefault/172.18.0.0/16, 172.18.0.2)\nDEBU[0693] Calling POST /v1.25/containers/two/stop\nDEBU[0693] Sending 15 to de985018aa64897c1ec5d5b985143403450559741ef05c103e74784c06397941\nINFO[0703] Container de985018aa64897c1ec5d5b985143403450559741ef05c103e74784c06397941 failed to exit within 10 seconds of signal 15 - using the force\nDEBU[0703] Sending 9 to de985018aa64897c1ec5d5b985143403450559741ef05c103e74784c06397941\nDEBU[0703] containerd: process exited                    id=de985018aa64897c1ec5d5b985143403450559741ef05c103e74784c06397941 pid=init status=137 systemPid=775\nDEBU[0703] received containerd event: &types.Event{Type:\"exit\", Id:\"de985018aa64897c1ec5d5b985143403450559741ef05c103e74784c06397941\", Status:0x89, Pid:\"init\", Timestamp:0x578a7d18}\nDEBU[0703] Revoking external connectivity on endpoint two (8453045aeb96f59a41e32cac1351835df8820e11460bac6c666e7378440f5cd6)\nDEBU[0703] Releasing addresses for endpoint two's interface on network bridge\nDEBU[0703] ReleaseAddress(LocalDefault/172.18.0.0/16, 172.18.0.3)\nDEBU[0703] Calling POST /v1.25/containers/three/stop\nDEBU[0703] Sending 15 to 744058945ea8424a1be8ba640ce5fe722edf6e0815714720bae8a3d34f41f459\nINFO[0713] Container 744058945ea8424a1be8ba640ce5fe722edf6e0815714720bae8a3d34f41f459 failed to exit within 10 seconds of signal 15 - using the force\nDEBU[0713] Sending 9 to 744058945ea8424a1be8ba640ce5fe722edf6e0815714720bae8a3d34f41f459\nDEBU[0713] containerd: process exited                    id=744058945ea8424a1be8ba640ce5fe722edf6e0815714720bae8a3d34f41f459 pid=init status=137 systemPid=817\nDEBU[0713] received containerd event: &types.Event{Type:\"exit\", Id:\"744058945ea8424a1be8ba640ce5fe722edf6e0815714720bae8a3d34f41f459\", Status:0x89, Pid:\"init\", Timestamp:0x578a7d23}\nDEBU[0713] Revoking external connectivity on endpoint three (8c14e18ecee41b2ff8aef42678ddee73a5b522e0be5442782cf0da92d189b15b)\nDEBU[0714] Releasing addresses for endpoint three's interface on network bridge\nDEBU[0714] ReleaseAddress(LocalDefault/172.18.0.0/16, 172.18.0.4)\n```\n\nThe client calls `stop` for the first container, and doesn't stop the second container\nuntil stopping the first container was completed. Stopping these containers therefore\ntakes 30 seconds.\n\nOn the other hand, stopping the _daemon_ (dockerd), will execute this in parallel, so only taking 10 seconds;\n\n```\n^CINFO[0748] Processing signal 'interrupt'\nDEBU[0748] starting clean shutdown of all containers...\nDEBU[0748] stopping de985018aa64897c1ec5d5b985143403450559741ef05c103e74784c06397941\nDEBU[0748] Sending 15 to de985018aa64897c1ec5d5b985143403450559741ef05c103e74784c06397941\nDEBU[0748] stopping 744058945ea8424a1be8ba640ce5fe722edf6e0815714720bae8a3d34f41f459\nDEBU[0748] Sending 15 to 744058945ea8424a1be8ba640ce5fe722edf6e0815714720bae8a3d34f41f459\nDEBU[0748] stopping 4abc5dc9add562469a8aeda45c6955453c6615ca0a6d897ce2aff01f33acd04e\nDEBU[0748] Sending 15 to 4abc5dc9add562469a8aeda45c6955453c6615ca0a6d897ce2aff01f33acd04e\nINFO[0758] Container 4abc5dc9add562469a8aeda45c6955453c6615ca0a6d897ce2aff01f33acd04e failed to exit within 10 seconds of signal 15 - using the force\nDEBU[0758] Sending 9 to 4abc5dc9add562469a8aeda45c6955453c6615ca0a6d897ce2aff01f33acd04e\nINFO[0758] Container 744058945ea8424a1be8ba640ce5fe722edf6e0815714720bae8a3d34f41f459 failed to exit within 10 seconds of signal 15 - using the force\nDEBU[0758] Sending 9 to 744058945ea8424a1be8ba640ce5fe722edf6e0815714720bae8a3d34f41f459\nINFO[0758] Container de985018aa64897c1ec5d5b985143403450559741ef05c103e74784c06397941 failed to exit within 10 seconds of signal 15 - using the force\nDEBU[0758] Sending 9 to de985018aa64897c1ec5d5b985143403450559741ef05c103e74784c06397941\n```\n\nWe could improve here, and have an API endpoint that allows stopping _multiple_ containers, so that\nstopping occurs in parallel.\n"},{"labels":["api",null,null,null,null],"text":"Both `docker info` and `docker swarm inspect` display information regarding the swarm.\n\nThey do so in a different way.\n\nThere are also other inconsistencies in `swarm inspect` (#23689, #24122).\n\n/cc @stevvooe @aanand @dnephin @tonistiigi \n"},{"labels":["api",null,null],"text":"Swarm mode internally tracks removed nodes with a cert blacklist recording the CN of the nodes cert.  It would be useful to expose this list via an API for visibility/management purposes.\n"},{"labels":["api",null,null],"text":"Barring a resolution to https://github.com/docker/swarmkit/issues/696 there should be a programmatic way to restart the swarm mode manager after making configuration changes via the API.\n"},{"labels":["api",null,null,null],"text":"Hi, all,\n\nI use two APIs to get one volume details\n\nAs I call `192.168.0.11:2376/volumes` against docker engine :\nNo labels in results.\n\n```\n{\n  \"Volumes\": [\n    {\n      \"Name\": \"allen\",\n      \"Driver\": \"local\",\n      \"Mountpoint\": \"/var/lib/docker/volumes/allen/_data\",\n      \"Labels\": null\n    },\n    {\n      \"Name\": \"test5\",\n      \"Driver\": \"local\",\n      \"Mountpoint\": \"/var/lib/docker/volumes/test5/_data\",\n      \"Labels\": null\n    }\n  ],\n  \"Warnings\": null\n}\n```\n\nwhile calling `192.168.0.11:2376/volumes/test5` has label details:\n\n```\n{\n  \"Name\": \"test5\",\n  \"Driver\": \"local\",\n  \"Mountpoint\": \"/var/lib/docker/volumes/test5/_data\",\n  \"Labels\": {\n    \"owner\": \"ryan\"\n  }\n}\n```\n\nIt is inconsistency. Since Swarm uses /volumes to get volumes, there is no way for user to use `docker volumes inspect xxx` to get volume labels.\n\nrelated https://github.com/docker/swarm/issues/2245\n\ndocker version\n\n```\nroot@ubuntu:~# docker version\nClient:\n Version:      1.11.1\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   5604cbe\n Built:        Tue Apr 26 23:30:23 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.11.1\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   5604cbe\n Built:        Tue Apr 26 23:30:23 2016\n OS/Arch:      linux/amd64\n```\n\ndocker info:\n\n```\nroot@ubuntu:~# docker info\nContainers: 2\n Running: 0\n Paused: 0\n Stopped: 2\nImages: 46\nServer Version: 1.11.1\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 364\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge null host\nKernel Version: 3.19.0-25-generic\nOperating System: Ubuntu 14.04.3 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 1.954 GiB\nName: ubuntu\nID: HRBI:EV6M:3NAC:CXZ3:BDS7:UOKL:PD6Y:ZCK4:V3PF:V7MR:CU2Y:3LGG\nDocker Root Dir: /var/lib/docker\nDebug mode (client): false\nDebug mode (server): true\n File Descriptors: 15\n Goroutines: 33\n System Time: 2016-05-20T02:09:44.861951674+08:00\n EventsListeners: 1\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\n```\n\nuname -a\n\n```\nroot@ubuntu:~# uname -a\nLinux ubuntu 3.19.0-25-generic #26~14.04.1-Ubuntu SMP Fri Jul 24 21:16:20 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n```\n"},{"labels":["api",null,null],"text":"Hi there,\n\nDo you guys think it could be a good idea to have in the API an endpoint which allows to get the stats of all the running containers at once?\n\nAt the moment I am using this collectd plugin (https://github.com/lebauce/docker-collectd-plugin) to get the stats out of my server. \n\nThe problem I am seeing is that when I am running 100+ container the server starts loading because I am making too many requests too often to the docker daemon (aka one call per container to the /stats endpoint every 20s).\n\nIf we could get all the stats at once that would fix this problem, for now I have implemented some rate limit on that plugin but not sure if it is the best as I do  not have a real time info anymore.\n\nCheers.\n"},{"labels":["api",null],"text":"### Problem Statement\n\nFor the docker Remote API there is a sound versioning scheme in place even with [history](https://docs.docker.com/engine/reference/api/docker_remote_api/#version-history).\n\nFor the plugin API only a [single mention](https://docs.docker.com/engine/extend/plugin_api/#api-design) of versioning can be found:\n\n_The API is versioned via an Accept header, which currently is always set to `application/vnd.docker.plugins.v1+json`._\n\nAccording to [this comment](https://github.com/docker/docker/issues/21857#issuecomment-207057067) there are possible changes in how the API works and/or is used. Furthermore, in contrast to the Remote API which is only implemented once (by docker engine, and of course swarm) the plugin API is implemented by a variety of services provided by different vendors.  \n\nTherefore I see the need to introduce versioning information for this API.\n### Requirements\n1. API Client (docker daemon) should indicate which version of the API it is supporting.\n2. API Service (plugin) should inidicate which version of the API it is supporting.\n3. For every API call it should be clear, which API version it refers to.\n4. A history for the versions should be maintained.\n### Possible implementation\n\nWhen the Plugin gets activated, the payload (currently empty) specifies, which plugin types for which API version is supported.\n**Request**\n\n```\n{\n    [ \n        { \n            \"PluginType\" : string, \n            \"APIVersion\" : string\n        }\n   ]\n}\n```\n\nThe following `PluginType`s are currently defined:  `authz` - `NetworkDriver` - `VolumeDriver`.\nIf for a given `PluginType` more then one APIVersion is supported it is listed multiple times. \n\n**Response**\nThe response indicates which Docker subsystems the plugin implements.\n\n```\n{\n    \"Implements\": [ \n    {\n        \"PluginType\" : string, \n        \"APIVersion\" : string\n    }\n    ]\n}\n```\n\nAt its own discretion the plugin can return only those `APIVersion` that the daemon also supports or it can return all versions.\nDocker Daemon can pick any of the supported `APIVersion` to communicate with the plugin. The daemon should stick with the version selected at the beginning of communication, changing API version during lifetime of plugin may result in undefined behaviour.\nIf the plugin supports more then one Docker subsystem,  the plugin may support different API versions for different subsystems. The order of entries in the list indicates plugin's preferences.\nSimilarily Docker daemon may use different API version when talking to different subsystems the plugin implements.\n\n**API-Calls**\nThe Version of the API-Call is encoded similiar the Remote API. A call gets the API version prepended, e.g. `/v1/VolumeDriver.List`. A call without the version prefix means the version of the first list entry for this subsystem returned by the plugin as responss to plugin activation. \n_This is not necessary the highest version currently specified or the highest version supported by daemon!_.\n\n**History**\nChanges between different versions should be tracked either in the [plugin documentation overview](https://docs.docker.com/engine/extend/plugin_api/) or in the documentation of the various subsystems ([volume](https://docs.docker.com/engine/extend/plugins_volume/), [network](https://docs.docker.com/engine/extend/plugins_network/)).\n\nSince the API comprises alls subsystems the API version is comprises all subsystems to.. Thus it is possible, that for a given subsystem no change exists between subsequent version.\n### Notes\n\nWhile this proposal could be part of #20363 too, I feel the discussion there is to wide and partly orthogonal. Thus I present this as a separate issue.\n"},{"labels":["api",null],"text":"When `docker load`ing a `tar` file with image(s), the [(current v1.22) API](https://docs.docker.com/engine/reference/api/docker_remote_api_v1.22/#load-a-tarball-with-a-set-of-images-and-tags-into-docker) should return the list of images it loaded.\n"},{"labels":["api",null],"text":"Every part of the remote API that returns a collection of objects (`GET /images/json`, `GET /containers/json`, `GET /events`, etc.) returns its values as a JSON list of objects: `[{...}, {...}, ...]`. Every part, that is, except `GET /volumes`. That endpoint returns an object of one element whose value is a list: `{ \"Volumes\": [{...}, {...}, ...] }`. This is still the case in the [most recent dev Remote API Docs](https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.24.md#list-volumes).\n\nIs there a reason `GET /volumes` is inconsistent with the rest of the API? If there is no reason, I ask that it be made to return a flat list.\n"},{"labels":["api",null],"text":"Just a quick placeholder, following a discussion on Slack;\n\nCurrently we accept both `/something` (\"latest\" API version), and `/vXX.YY/something` (API version \"XX.YY\").\n\nThe \"latest\" API version can be troublesome, because software using the \"latest\" version (and not explicitly defining a version), can break after an upgrade of the daemon.\n\nInstead, we can deprecated the \"latest\" URLs, and require consumers of the API to explicitly specify the API version they're using.\n\n/cc @tiborvass @dgageot @nishanttotla \n"},{"labels":["api",null,null],"text":"Currently docker merges labels with some container attributes in here: https://github.com/docker/docker/blob/master/daemon/events.go#L17\n\nThe way it is right now the user can start a container with the label \"image=this-is-my-label\" and that label won't show up in the docker events.\n\nSo my my proposal is:\n- Somehow split engine-api message with labels in one map and remaining attributes in the other.\n- Use the special labels assigned to docker e.g. \"com.docker.container.image\" and \"com.docker.container.name\"\n\nThoughts?\n\nPing @calavera \n"},{"labels":["api"],"text":"Currently, the `/info` endpoint for the Swarm API uses `DriverStatus` field in the Engine API. We would like to create a new field exclusively for Swarm to use.\n\nCorresponding `docker/swarm` issue: https://github.com/docker/swarm/issues/1625\n\ncc @calavera @vieux\n"},{"labels":["api",null],"text":"Currently the [https://docs.docker.com/engine/reference/api/docker_remote_api_v1.21/#list-containers](`GET /containers/json`) endpoint returns a human readable `Status` field, which is useful for the `docker ps` cli, but not very useful for other API clients, since it's not machine readable. For example, this is what it has returned for me (simplified to only include the Id and Status fields from the returned JSON): \n\n```\nId                                                               Status\n9080c3d4e54b1367cd659d5841953f0a0b0ed789d52805491ba752744ca7ed33 Created\nda837d8058c56c0c038d4580f5f8c2197a7c383be7de106dfbc6d17849975f2a Exited (137) 12 days ago\n624d6deaf83091659d563d2063c6fcf0275a4b945b02e5b2262557a64e6953f8 Up 8 minutes (Paused)\n2a6e659fe99f33a5f43605e313fbee17c2eb91e317567d6fd98d69c5d30504db Up 5 seconds\n```\n\nIt would be more useful if the Status field (or a new field, for backwards compatibility) returned the states from this state diagram:\n\n[![State diagram from Remote API documentation](https://docs.docker.com/engine/reference/api/images/event_state.png)](https://docs.docker.com/engine/reference/api/docker_remote_api/#docker-events)\n\nI therefore propose a new `State` field to the result of calling `GET /containers/json` which will have one of the values from that diagram, namely `created`, `running`, `paused` or `stopped` (I don't think it can ever have `deleted`, since, well, it's deleted).\n"},{"labels":["api",null,null],"text":"The current implementation of `update`(#15078), and thus the endpoint `/containers/(id)/update`, is using `HostConfig` struct although it's only updating `Resources`. \n\nThis raises few concerns :\n- We decided that `update` should only _update_ stuff that are in `Resources` struct, but it's possible to send a complete `HostConfig` struct which could lead to confusion — like why is some fields not taken care of, etc….\n- If a complete `HostConfig` struct is passed, it will completely and **silently ignore** all attributes except `Resources`. It should at least put a warning.\n\nI think we should _reduce_ what we send and change the API `/containers/(id)/update` to just take `Resoures` struct instead of `HostConfig` — it would make it clear that only stuff _in_ this struct are supported (and it's simpler to implement than doing some diff or something and warn that something that is not in `Resources` has been wrongly set).\n\n/cc @hqhq @tiborvass @thaJeztah @runcom @duglin @unclejack @icecrime @calavera \n"},{"labels":["api",null,null],"text":"I have a container which needs to be restarted when the machine reboots.  If the process crashes or exits with an error I would like it to stay down so it can be diagnosed.\n\nExisting restart policies:\n- `on-failure` will not achieve either requirement.\n- `always` will achieve the first requirement but not the second.\n- `unless-stopped` is slightly better than `always`, but not quite what I want.\n\nI apologise if this is a misguided request; I searched around quite a lot but didn't find an open discussion of the topic.  I found [this](https://www.pivotfreight.com/devblog/on-docker-restart-policies) which asserts that `on-failure` will restart containers on a daemon restart, but this does not match my experiments or my reading of the code.  On a hard reset of the machine, the exit code will be recorded as zero so no restart.\n\nIn #7586 it is [asserted](https://github.com/docker/docker/issues/7586#issuecomment-57061376) that `always` has a max-retry count, and a max-retry count of 1 would be good enough for my purposes, but again this does not match what I find in the code.\n"},{"labels":["api",null],"text":"Currently the networking API always report 500 on failures (except in [special cases when the error message matches certain patterns](https://github.com/docker/docker/blob/e75da4b6fffbcf9dab86cd581e962a19d3efa35a/api/server/httputils/httputils.go#L129-L151)). Should HTTP handlers in the networking API use the [errcode package](https://godoc.org/github.com/docker/distribution/registry/api/errcode)? If so, I can prepare a patch for that.\n"},{"labels":["api",null,null],"text":"I see the need for a docker run --volumes-no option that enables me to run a normal production image and start a container instance of it so that it saves the content on docker commit even in the directorys that are normal defined as volume by the source image.\n"},{"labels":["api",null],"text":"There are some information that can only be reliably retrieved using the container inspection.\n\nWhen you get normal container information (e.g. through container listing), one cannot find out in what network is a container in. Before api 1.21, one could not find the imageID of the containers without inspecting it too - sometimes it returned the image repo+tag and sometimes the id.\n\nWhen a single host has thousands of containers, getting network information for every single container is very expensive, since it demands a get request per container.\n\nI would like to suggest an API extension where all the containers' inspection information will be retrieved in a single request. Although the response may be considerable large (several jsons), it is still better than a restful request per inspection.\n"},{"labels":["api",null,null],"text":"Instead of leaving a dangling image around or potentially purging common layers to early, it would be nice if there was an option to cleanly replace an image when pulling or building and tagging so there will not be a dangling image left behind.\n"},{"labels":["api",null,null],"text":"I like how, when manipulating containers, I can type the first few non-colliding characters of the container's ID. This doesn't seem to be true for volumes. Could these also be matched in the same way that container/image IDs are? At the moment I have to cut-and-paste the entire volume ID, which is annoying.\n\nThanks.\n"},{"labels":["api",null,null],"text":"`docker inspect`  should also return:\n- subnet\n- ip-range\n- gateway\n\nThey should show with the default or empty if they have nothing.\n"},{"labels":["api",null,null],"text":"In many cases, it might be required for the user to be aware of the NUMA topology of the docker host in order to specify valid 'cpuset' for the containers.Currently NUMA topology information is not available via docker APIs. It's available by using commands like numactl or lscpu or via sysfs. \nThis proposal is for considering addition of NUMA topology info to docker Info API so that the same can be used by users or swarm in future.\n\nHere is an example on how the extended info API might look like:\n\n```\nroot@ubuntu:~/docker/bundles/1.8.0/binary# echo -e \"GET /info HTTP/1.0\\r\\n\" | nc -U /var/run/docker.sock\nHTTP/1.0 200 OK\nContent-Type: application/json\nServer: Docker/1.8.0 (linux)\nDate: Mon, 05 Oct 2015 06:52:06 GMT\nContent-Length: 1136\n\n{\"ID\":\"KFL7:6UJ7:QM6U:F3JV:VHPQ:4QKX:KLAB:YRLU:N25F:PD57:PYWZ:XM32\",\"Containers\":17,\"Images\":60,\"Driver\":\"aufs\",\"DriverStatus\":[[\"Root Dir\",\"/var/lib/docker/aufs\"],[\"Backing Filesystem\",\"extfs\"],[\"Dirs\",\"94\"],[\"Dirperm1 Supported\",\"true\"]],\"MemoryLimit\":true,\"SwapLimit\":false,\"CpuCfsPeriod\":true,\"CpuCfsQuota\":true,\"IPv4Forwarding\":true,\"BridgeNfIptables\":true,\"BridgeNfIp6tables\":true,\"Debug\":false,\"NFd\":34,\"OomKillDisable\":true,\"NGoroutines\":27,\"SystemTime\":\"2015-10-05T06:52:06.053388928Z\",\"ExecutionDriver\":\"native-0.2\",\"LoggingDriver\":\"json-file\",\"NEventsListener\":0,\"KernelVersion\":\"3.19.0-25-generic\",\"OperatingSystem\":\"Ubuntu 14.04.3 LTS\",\"IndexServerAddress\":\"https://index.docker.io/v1/\",\"RegistryConfig\":{\"InsecureRegistryCIDRs\":[\"127.0.0.0/8\"],\"IndexConfigs\":{\"docker.io\":{\"Name\":\"docker.io\",\"Mirrors\":null,\"Secure\":true,\"Official\":true}},\"Mirrors\":null},\"InitSha1\":\"\",\"InitPath\":\"/root/docker/bundles/1.8.0/binary/docker\",\"NCPU\":4,\"MemTotal\":4144381952,\"DockerRootDir\":\"/var/lib/docker\",\"HttpProxy\":\"\",\"HttpsProxy\":\"\",\"NoProxy\":\"\",\"Name\":\"ubuntu\",\"Labels\":null,\"ExperimentalBuild\":false,\"NumaNodes\":1,\"NodeCpus\":\"0-3\\n\"}\n```\n\nHere is what \"numactl -H\" output looks like:\n\n```\nroot@ubuntu:~/docker/bundles/1.8.0/binary# numactl -H\navailable: 1 nodes (0)\nnode 0 cpus: 0 1 2 3\nnode 0 size: 3952 MB\nnode 0 free: 1450 MB\nnode distances:\nnode   0 \n  0:  10 \n```\n\nThe API will result in showing number of numa nodes and the cpus in each node. The following code(a rough draft) can be added to daemon/info.go in order to scan sysfs and retrieve the numa topology:\n\n```\n       nodes, _ := filepath.Glob(\"/sys/devices/system/node/\" + \"/node*\")\n        num := len(nodes)\n\n        val := \"\"\n        for i:=0; i<len(nodes); i++ {\n\n                new, _ := filepath.Glob(nodes[i] + \"/cpulist\")\n                read, _ := ioutil.ReadFile(new[0])\n                val += string(read)\n        }\n```\n\nThe info structure can be modified by adding NumaNodes and NodeCpus:\n\n```\nv := &types.Info{\n                ID:                 daemon.ID,\n                Containers:         len(daemon.List()),\n                Images:             imgcount,\n                Driver:             daemon.GraphDriver().String(),\n                DriverStatus:       daemon.GraphDriver().Status(),\n                IPv4Forwarding:     !daemon.SystemConfig().IPv4ForwardingDisabled,\n                BridgeNfIptables:   !daemon.SystemConfig().BridgeNfCallIptablesDisabled,\n                BridgeNfIp6tables:  !daemon.SystemConfig().BridgeNfCallIp6tablesDisabled,\n                Debug:              os.Getenv(\"DEBUG\") != \"\",\n                NFd:                fileutils.GetTotalUsedFds(),\n                NGoroutines:        runtime.NumGoroutine(),\n                SystemTime:         time.Now().Format(time.RFC3339Nano),\n                ExecutionDriver:    daemon.ExecutionDriver().Name(),\n                LoggingDriver:      daemon.defaultLogConfig.Type,\n                NEventsListener:    daemon.EventsService.SubscribersCount(),\n                KernelVersion:      kernelVersion,\n                OperatingSystem:    operatingSystem,\n                IndexServerAddress: registry.IndexServer,\n                RegistryConfig:     daemon.RegistryService.Config,\n                InitSha1:           dockerversion.INITSHA1,\n                InitPath:           initPath,\n                NCPU:               runtime.NumCPU(),\n                MemTotal:           meminfo.MemTotal,\n                DockerRootDir:      daemon.Config().Root,\n                Labels:             daemon.Config().Labels,\n                ExperimentalBuild:  utils.ExperimentalBuild(),\n                NumaNodes:          num,\n                NodeCpus:           val,\n        }\n```\n\nIt further can be extended to scan more options other that NUMA topology to help users make better decisions.\n"},{"labels":["api",null],"text":"This is a quick write up of a design to enable programmatic bootstrapping of a Docker cluster.  I will quickly submit a PR if we can decide this is generally the right direction.  The motivation of this is to allow bootstrapping a cluster in a more user friendly fashion and additionally allow scripts and other programs to setup a cluster without having to deal with distro specific configuration files to handle daemon flags.\n## API\n\n`POST /join`\n\n``` json\n{\n  \"DiscoveryBackend\": \"...\",\n  \"DiscoveryAddress\": \"...\",\n  \"KvStore\": \"...\",\n}\n```\n## CLI\n\n`docker join --backend ... --address .. --kv-store ...`\n## Design\n\nIf no options are passed as daemon flags for `--discovery-backend`, `--discovery-address`, and `--kv-store` then the `/join` API can be used.  If any of those daemon flags are set the `/join` API should fail.  When POST-ing to `/join` the three arguments will be passed to `pkg/discovery` to initialize the discovery backend.  Additionally these parameters will be saved in `/var/lib/docker/discovery/config.json` to be used if the daemon restarts.\n\nOn start of the daemon discovery info will be read first from the CLI and then if not available will look in `/var/lib/docker/cluster/config.json`\n"},{"labels":["api",null,null,null],"text":"Hi guys,\n\nWe use swarm as orchestration tools. And we add labels on docker daemon to let swarm to do some specific scheduling, such as schedule redis container to the docker daemon which has `ssd` label.\n\nBut in some case, we need to change the docker daemon labels, such as we want add MapR-ability label to some daemon in the midnight, so the map reduce container could be scheduled to this daemon. \n\nThere's no API to change the daemon labels on the fly, and we can't afford to restart daemon. So we want add API to do that, such as:\n\n```\nUsage: docker labels [COMMAND] [OPTION]\n\nManage docker daemon labels:\n\nlist - show the docker daemon labels.\nadd - add new labels to docker daemon, docker labels add --label=[k=v]\nremove - remove any exist labels from docker daemon, docker labels remove --label=[k]\n```\n"},{"labels":["api",null],"text":"It would be useful to have a list of error codes and their corresponding cause. This could be in a tabular format such as:\n\n| Error Code | Meaning |\n| --- | --- |\n| NNN | Text |\n| NNN | Text |\n"},{"labels":["api",null,null],"text":"Right now the API will send an error when someone tries to call `stop` or `kill` on a paused container.\nThis is because signals alone don't do anything to processes that have been frozen by cgroup freezer.\n\nHowever once the process is unfrozen the kernel will go ahead and send those signals.\nWe should be able to update the API to go ahead and send the requested signal to the container, then automatically unpause it.\n\nThis is better than asking the user to unpause it first as the container may be paused due to some bad behavior in the container.\n\nIntroducing this behavior would also allow us to remove the work-around added in #13304\n"},{"labels":["api",null],"text":"In [pulp](https://pulp-docker.readthedocs.org/en/latest/) you can store docker images and serve them via readonly registry API. The way v1 content is uploaded is that you `docker save` the image and upload it to pulp via pulp's API. With v2, we would like to upload blobs and manifest directly. Unfortunately there is only a single way to get those: via v2 registry API. That's pretty cumbersome.\n\nWould it possible for docker client to output manifest and blobs?\n\nI have in mind something like:\n\n```\n$ docker manifest <image>\n{\n   \"name\": \"image\",\n   \"tag\": \"latest\",\n   \"architecture\": \"amd64\",\n   \"fsLayers\": [\n      {\n         \"blobSum\": \"sha256:074533a2d610d82f28097af30284da4077614e30cb0171f0d8d859d2bd294d74\"\n      },\n...\n```\n\nand\n\n```\n$ docker export -o blob.tar.gz --blob sha256:074533a2d610d82f28097af30284da4077614e30cb0171f0d8d859d2bd294d74\n```\n\nThis is _not_ a dupe of https://github.com/docker/distribution/issues/727\n"},{"labels":["api",null,null,null],"text":"Currently the output of `docker info` looks like:\n\n```\n$ docker info\nContainers: 1362\nImages: 145\nStorage Driver: overlay\n Backing Filesystem: extfs\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 4.1.4-1-ARCH\nOperating System: Arch Linux\nCPUs: 8\nTotal Memory: 15.6 GiB\nName: archbox\nID: OUQD:OBQE:7KQO:2FQ5:YJIC:ILXS:O7SE:PWEK:3ZWT:PG7I:RUPA:2EIW\n```\n\nThis does not really help at a first glance to assess how many containers are running on a machine. Indeed on those **1362** containers, I only have **10** _running_ containers, the rest are _stopped_. I might have a few _paused_ containers too.\n\nThis would be useful in `docker info` to show more informations on the current state of containers:\n\n```\n$ docker info\nContainers: 1362\n Running: 8\n Paused: 2\n Stopped: 1352\nImages: 145\nStorage Driver: overlay\n Backing Filesystem: extfs\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 4.1.4-1-ARCH\nOperating System: Arch Linux\nCPUs: 8\nTotal Memory: 15.6 GiB\nName: archbox\nID: OUQD:OBQE:7KQO:2FQ5:YJIC:ILXS:O7SE:PWEK:3ZWT:PG7I:RUPA:2EIW\n```\n\nThis also means adding this to the remote API on `/info`. This way we can also use it in Swarm as a quick way to assess the load on machines (see related issue on the Swarm side docker/swarm#1140)\n\nI agree that this information is provided somehow by `docker ps` but this could be an added convenience to have this on `info` :smiley:\n"},{"labels":["api",null,null],"text":"Hi All,\n\nNow that volume plugins were added to Docker experimental channel, I would like to suggest adding server-side plugins for handling Docker commands. This will allow Docker users to achieve similar capabilities as supplied by [PowerStrip](https://github.com/ClusterHQ/powerstrip).\n\nThe server side command handler plugins are pre/post extension to Docker command handling, allowing to modify input commands before processed by Docker engine, and output data after processed by Docker Engine (but before sent to client). It will allow adding new capabilities into Docker engine, implemented as out-of-process plugins.\n\nI've implemented a similar mechanism for a project I am working on. I'll be glad to get your feedback on the design and contribute the code if it makes sense.\n## Plugin Mechanics\n\nThe Command Handler plugins are similar to [Volume Plugins](https://github.com/docker/docker/blob/master/experimental/plugin_api.md): \n- Processes running on same host as Docker Engine. \n- Registered by placing a file in `/usr/share/docker/plugins/srv/cmd`. \n- There can be two type of plugin files:\n  - `.sock` files are UNIX domain sockets.\n  - `.spec` files are text files containing a URL, such as unix:///other.sock.\n- The name of the file (excluding the extension) determines the plugin name.\n- Plugins should be started before Docker, and stopped after Docker.\n- The Plugin API is RPC-style JSON over HTTP. Requests flow from the Docker daemon to the plugin.\n## Plugin Initialization\n\nUnlike volume plugins, the command handler plugins should be initialized upon Docker Engine startup. \nDocker Engine sends a /Plugin.Activate request and receives list of commands and type of handling (input or output) that plugin is interested in.\n### /Plugin.Activate\n\n**Request:** empty body\n\n**Response:**\n\n```\n{\n    \"Implements\": [\n        {\"cmd\":\"run\", \"type\":\"input\"}, \n        {\"cmd\":\"stop\", \"type\":\"input\"}, \n        {\"cmd\":\"inspect\", \"type\":\"output\"}\n    ]\n}\n```\n## Plugin Invocation\n\nOnce a plugin is registered for set of commands, Docker Engine calls the plugin before processing the command (\"input\" command handler) and before sending the output to client (\"output\" command handler). \n\nPlugin invocation is done by calling /Plugin.Execute request, while passing type of processing (input or output) and the associated payload (complete input command or output message).\n### /Plugin.Execute\n\n**Request:** \n\n```\n{\n    \"Command\": \"run\",\n    \"Type\": \"input\" or \"output\"\n    \"Payload\": { COMMAND PAYLOAD }\n}\n```\n\n**Response:**\n\n```\n{\n    \"Action\": \"continue\" or \"stop\"\n    \"Payload\": {UPDATED PAYLOAD}\n    \"Return_Code\": 0\n    \"Error_Msg\": {OPTIONAL ERROR MESSAGE}        \n}\n```\n\nDocker Engine should block until plugin responds back (timeouts?). It will continue processing the command in case the plugin responded with \"continue\" response action. It should stop processing the command (and returning return code and message) in case the response action is \"stop\".\n\nIn case of \"continue\" action, Docker Engine should use the Payload returned from the plugin response instead of the original command payload.\nThis allows plugins to modify command input parameters or output data.\n\nNote that multiple command handler plugins can be registered for the same command. Docker Engine will continue calling plugins for the same command as long as it did not get \"stop\" action from one of the handlers. After processing all plugins registered for a command, Docker Engine should continue handling the modified command (as returned from the last plugin).\n## Example Plugins\n\nExample for capabilities we would be able to implement using command handler plugins:\n- Run additional commands before/after container is started/stopped. E.g. when containerA is started/stopped, automatically start/stop containerB.\n- Add missing parameters to command line. E.g. add security parameters (cap-add, cap-drop) even if not specified in command line.\n- Prevent running command with certain parameters.\n- Modify Docker output to include additional data. E.g. modifying 'docker inspect' output to include container start time (or anything else which is useful). Note that this one is tricky as the CLI might be expecting certain outputs.\n"},{"labels":["api",null,null],"text":"Currently the filter for `ps`, `images`, and `events` only work with `key=value`. I was very surprised to learn that there is no `key!=value` expression for exclusion unlike affinity/constraint filters you can set for Docker Swarm. My suggestion is for filter should have the ability to include as well as exclude. Thoughts?\n"},{"labels":["api",null,null],"text":"For day-to-day use and simple scripting purposes It'd be great if the --filter function was extended with filters on more columns.\n\nSpecifically:\n\n```\n$ docker ps --filter image=myname/test \n```\n\nWould be great so we can kill all our tests in one go like so:\n\n```\n$ docker ps --filter image=myname/test -q | xargs docker rm -f\n```\n\nThis is helpful specifically in this combination because I want only image id's to be returned, and |grep thus doesn't work.\n\nOther filters that would make sense to me:\n\n_For images_\n- Repository (with tag)\n- Age? (--filter agemore=4h --filter ageless=2w)\n\n_For containers_\n- Image (with tag)\n- Name (this is implemented by the way https://github.com/docker/docker/issues/10897, but undocumented)\n- Age? (as above)\n- Status (running, exited, etc)\n\n**references**\nI found this ticket: https://github.com/docker/docker/pull/11904 which describes something that sounds similar but (I think) is actually for the bash completion (only).\n"},{"labels":["api",null,null],"text":"As a frequent exporter of docker filesystems, I want an option to tell `docker cp` to copy symlinks as literals, so that my filesystem archives receive the symlinks that my containers contain.\n\nMaybe `docker cp -a`?\n"},{"labels":["api",null,null],"text":"Generally, a docker command instantiation makes several http requests to a running registry. Processing these requests in the registry server to sessionize the request flow is a non-trivial problem. While we have a proposed `Docker-Command` header in https://github.com/dmcgowan/docker/pull/18, it would be smart to include a unique token for each command execution that is added in a header to support trivial sessionization.\n\nUse cases of these session identifiers include producing usable metrics for private registries or added to outgoing webhooks (docker/distribution#42) for later processing.\n\nThe header would consist of the command that was run (not the entire cli, ie \"push\" or \"pull\") and a session id generated at the start of each instantiation of the docker client process. An example of such a header might be as follows:\n\n```\nDocker-Command: push 3eed4669-d1e8-4753-9420-998e530386b1\n```\n\nThe header could be added to all requests, from the client to the engine and beyond. If taken that far, such a header could support distributed debugging or begin to enable [dapper](http://research.google.com/pubs/pub36356.html)-like analysis.\n"},{"labels":["api",null,null,null,null],"text":"The `docker export` command and corresponding API endpoint allow the client to get an archive of the container's root filesystem. This is useful for squashing an entire set of image layers into one, but there's currently no way to get just the file system diff for a container.\n\nThis could be achieved by adding an optional parameter to the container export API endpoint and an optional flag to the `docker export` command: `diff=1` This would instruct the runtime to only return the layer diff for the container rather than the entire filesystem.\n## Why is this useful?\n\nIt would allow for additional functionality in tools which are used to build images like the `docker build` command. Layer archives could be collected by the client, checksums computed for each layer archive, and for images to be built using a new content-addressable format.\n"},{"labels":["api",null],"text":"Currently the `docker cp` command supports copying a file/directory from inside container to the client. I propose that we create a complementary command, perhaps `docker add` or overloading of the `docker cp` command, which does the opposite: copy from `HOSTPATH` to `CONTAINER:PATH`\n## Why is this useful?\n1. It would clear the way for the `docker build` subsystem to be completely removed from the\n   Docker daemon.\n   \n   Most of the functionality of `docker build` is already possible with the API, as every line of a\n   Dockerfile simply creates a container with the current build config, modifies the filesystem of the\n   container in some way (either by ADD/COPY of some resource or by executing a command), then\n   commits the changes as a new Image. All of this is possible with the current Docker Remote API\n   except for ADDing resources to the container.\n   \n   In a brave new world, builds of new images could be orchestrated remotely over the API and\n   ultimately give more power to users who want/need extra functionality in their builds.\n2. Allows for updating containers with miscellaneous data.\n   \n   Such functionality would make it possible for files to be dynamically loaded into a newly created\n   or running container. Things like TLS certificates, keys, or other configuration files could be added\n   to a container before it starts or while it is running, adding all sorts of new possibilities to\n   configuration management tools and scripts.\n"},{"labels":["api",null],"text":"Currently we can only inspect a single image or container. It would be great as part of the listing of all images or containers to be able to specify a parameter, something like '/images/json?inspect=true' which will return all images/containers in their 'inspected format' instead of the summary we get now. Thoughts?\n"},{"labels":["api",null,null],"text":"It would be nice if \"docker images -f\" could also filter by images that were not pushed to the repository after its creation whether by commit, build or import / load.\n\n``` sh\ndocker images -f 'not-pushed=true'\n```\n"},{"labels":["api",null,null],"text":"Docker Remote API already provides a WebSocket endpoint to attach to a container's tty with a WebSocket connection (`/containers/(id)/attach/ws` — although not documented).\n\nIt would be great to have this functionality for execs as well. It would let people attach to exec ttys with standard WebSocket libraries and totally avoid HTTP hijacking.\n"},{"labels":["api",null,null],"text":"Currently there is no unique event for when a container is automatically restarted by the daemon.  Such an event _can_ be determined from the sequence of events for a container:\n- restart - die, start, restart\n- stop - die, stop\n- start - start\n- abend - die, start\n\nTrick is to simply track \"die, start\" sequences and check for at some later time the absence or presence of a restart event for the container in question (ugly).  A unique event would be nice, eg:\n- abend - die, start, _autorestarted_ or whatever\n\nUse case is that docker does a nice job of hiding containers that abend by just restarting them.  It would be nice to trap that event to get an idea that it is happening.\n\nDiscussed here on forum:\n\nhttps://forums.docker.com/t/docker-events-when-container-automatically-restarted/356\n"},{"labels":["api"],"text":"The API to pull an image is a bit inconsistent.\nIE POST /images/create?fromImage=xxx.xx/asd/qwe\nWhen the pull fails it still returns http status 200, and the response body seems like some sort of pseudo-json like\n{\"status\":\"Pulling repository aaa.com/ns/name\"}\n{\"errorDetail\":{\"message\":\"Error: image ns/name not found\"},\"error\":\"Error: image ns/name not found\"}\n\nwhere it should probably be like \n{\"status\":\"Pulling repository aaa.com/ns/name\",\n\"errorDetail\":{\"message\":\"Error: image ns/name not found\"},\"error\":\"Error: image ns/name not found\"}\n\nand the response status probalby a 422\n\n\"The 422 (Unprocessable Entity) status code means the server understands the content type of the request entity (hence a 415(Unsupported Media Type) status code is inappropriate), and the syntax of the request entity is correct (thus a 400 (Bad Request) status code is inappropriate) but was unable to process the contained instructions.\"\n"},{"labels":["api",null],"text":"After reading through everyone's feedback on https://github.com/docker/docker/issues/8664, I propose adding to docker the metadata equivalent of a \"time last-modified\" metadata common on filesystems. This wouldn't require additional logging or filtering.\n\nThe current state metadata is like this:\n\n```\n\"State\": {\n    \"ExitCode\": 0,\n    \"FinishedAt\": \"0001-01-01T00:00:00Z\",\n    \"Paused\": false,\n    \"Pid\": 19967,\n    \"Restarting\": false,\n    \"Running\": true,\n    \"StartedAt\": \"2014-10-20T10:53:50.649532096Z\"\n},\n```\n\nSo with my proposal here, we could have a new state datapoint for \"ModifiedAt\":\n\n```\n\"State\": {\n    \"ExitCode\": 0,\n    \"FinishedAt\": \"0001-01-01T00:00:00Z\",\n    \"Paused\": false,\n    \"Pid\": 19967,\n    \"Restarting\": false,\n    \"Running\": true,\n    \"StartedAt\": \"2014-10-20T10:53:50.649532096Z\",\n    \"ModifiedAt\": \"2014-10-20T10:55:50.649532096Z\"\n},\n```\n\nOn container start, ModifiedAt would be set to the same at StartedAt. And docker exec calls on a running container would update the ModifiedAt field.\n\nthoughts? @jpetazzo @crosbymichael smells ok? :-)\n"},{"labels":["api",null,null],"text":"Right now the remote API does not allow running clients with a newer API version than the daemon.\n\n```\n$ docker ps\n2014/08/26 14:21:03 Error response from daemon: client and server don't have same version (client : 1.13, server: 1.12)\n```\n\nThis message is misleading/incorrect. The code in question is [daemon.go:1057](https://github.com/docker/docker/blob/c4a190db0c7020ee39672421d1bbca3678f86683/api/server/server.go#L1057), which is checking to see that the client API is **newer** than the server API, not different. That code would happily allow running a client which is older than the server.\n\nThis brings me to the next point, why does it allow running an older client, but not a newer one?\nThis is extremely irritating to work with as I have to keep a separate client which is older than all the remote servers I end up working with.\nThe remote API changes between versions are usually so trivial, that in my opinion, this restriction is not warranted.\n\nCan we either:\n1. Fix the message to indicate that the client API is newer than the server API.\n2. Remove the restriction altogether and just display a warning.\n\nI would much prefer # 2.\n"},{"labels":["api",null],"text":"In addition to work in #5893 and related issues. \n# Refactoring Docker Remote API Client\n## What's wrong\n\nThe docker remote api isn't easy to use because:\n- There's no official client to the API\n- Much of the validation the CLI provides is in the CLI code, not the\n  API layer\n- The documentation doesn't always show a proper mapping of CLI commands\n  to API calls (e.g. `docker run` involves multiple steps)\n- The CLI code is cluttered with many concerns and makes understanding\n  how to use the API properly by example difficult\n- Marshaling/Unmarshaling in Go should use the same structures as the\n  internal code for easier code sharing (i.e. dont require consumers to\n  implement a `type Containers struct` for unmarshalling the output of\n  `docker ps`)\n## How to fix\n\nPropose:\n- Creating an official API client for use in the CLI tool\n- Exposing proper validation from the API layer or in the API client\n- Simplifying the CLI code and breaking commands down into smaller files\n  so they're easier to understand.\n- Leverage pre-existing types when (un)serializing calls from the API\n## Proposed implementation\n\nSee https://gist.github.com/mcculloughsean/5c1dd3cc674e6fd583f5#file-proposal-go for example code. \n### Creating an official client\n\nComposes ['Call'](https://github.com/docker/docker/blob/403df1765abfd3ac62b7f6b8bd709fb0d691d8c8/api/client/utils.go#L42-L42) with commands to create a fetcher per endpoint in the API with proper unmarshalling.\n### Reimplement CLI in terms of API client\n\nUse existing types for marshaling/unmarshaling JSON data for use in Go\ncode.\n\nMove formatting to methods for CLI stringification.\n\nCreate helper methods to generate valid query strings.\n### Push validations down to API as much as possible\n\nRemove all checks from the CLI level that can easily live in the API\nlayer.\n\nCreate a common pattern for passing validation errors back up through\nthe API pipeline (maybe by switching unmarshaling strategy based on http\nstatus code)\n\nEnsure that validation failure messages are verbose. e.g.:\n\n``` bash\n$curl -v -H \"Content-Type: application/json\" -d \"{\\\"Hostname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"Memory\\\":0,\\\"MemorySwap\\\":0,\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":true,\\\"AttachStderr\\\":true,\\\"PortSpecs\\\":null,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":null,\\\"Cmd\\\":[\\\"date\\\"],\\\"Image\\\":\\\"repository.snc1/candyland/echo-fedora\\\",\\\"Volumes\\\":{\\\"/tmp\\\":{}},\\\"WorkingDir\\\":\\\"\\\",\\\"DisableNetwork\\\":false,\\\"ExposedPorts\\\":{\\\"22/tcp\\\":{}}}\" http://localhost:12345/containers/create?name=foo\n* About to connect() to localhost port 12345 (#0)\n*   Trying ::1... Connection refused\n*   Trying 127.0.0.1... connected\n* Connected to localhost (127.0.0.1) port 12345 (#0)\n> POST /containers/create?name=foo HTTP/1.1\n> User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\n> Host: localhost:12345\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 340\n>\n< HTTP/1.1 201 Created\n< Content-Type: application/json\n< Date: Thu, 31 Jul 2014 21:54:33 GMT\n< Content-Length: 90\n<\n{\"Id\":\"2d3a1faaa6d83680ddb4164dc39755ef18fab37b4bf66e88f62bd00168547faa\",\"Warnings\":null}\n* Connection #0 to host localhost left intact\n* Closing connection #0\n\n$curl -v -H \"Content-Type: application/json\" -d \"{\\\"Hostname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"Memory\\\":0,\\\"MemorySwap\\\":0,\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":true,\\\"AttachStderr\\\":true,\\\"PortSpecs\\\":null,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":null,\\\"Cmd\\\":[\\\"date\\\"],\\\"Image\\\":\\\"repository.snc1/candyland/echo-fedora\\\",\\\"Volumes\\\":{\\\"/tmp\\\":{}},\\\"WorkingDir\\\":\\\"\\\",\\\"DisableNetwork\\\":false,\\\"ExposedPorts\\\":{\\\"22/tcp\\\":{}}}\" http://localhost:12345/containers/create?name=foo\n* About to connect() to localhost port 12345 (#0)\n*   Trying ::1... Connection refused\n*   Trying 127.0.0.1... connected\n* Connected to localhost (127.0.0.1) port 12345 (#0)\n> POST /containers/create?name=foo HTTP/1.1\n> User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\n> Host: localhost:12345\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 340\n>\n< HTTP/1.1 500 Internal Server Error\n< Content-Type: text/plain; charset=utf-8\n< Date: Thu, 31 Jul 2014 21:54:35 GMT\n< Content-Length: 53\n<\nAbort due to constraint violation: constraint failed\n```\n\nThe second error message isn't useful. The dockerd logs themselves\naren't useful either:\n\n```\n[debug] server.go:999 Calling POST /containers/create\n2014/07/31 21:54:33 POST /containers/create?name=foo\n[ead17ee3] +job create(foo)\n[error] mount.go:11 [warning]: couldn't run auplink before unmount:\nexec: \"auplink\": executable file not found in $PATH\n[ead17ee3] -job create(foo) = OK (0)\n[debug] server.go:999 Calling POST /containers/create\n2014/07/31 21:54:35 POST /containers/create?name=foo\n[ead17ee3] +job create(foo)\nAbort due to constraint violation: constraint failed\n[ead17ee3] -job create(foo) = ERR (1)\n[error] server.go:1025 Error: Abort due to constraint violation:\nconstraint failed\n[error] server.go:90 HTTP Error: statusCode=500 Abort due to constraint\nviolation: constraint failed\n```\n"},{"labels":["api"],"text":"\"docker inspect mycontainer\" does not show container's IPv6 address:\n\n```\n\"NetworkSettings\": {\n    \"Bridge\": \"docker0\",\n    \"Gateway\": \"172.17.42.1\",\n    \"IPAddress\": \"172.17.0.8\",\n    \"IPPrefixLen\": 16,\n    \"PortMapping\": null,\n    \"Ports\": {}\n},\n```\n\nthe ifconfig output inside the container:\n\n```\nroot@5f54af0a7db0:/# ifconfig\neth0  Link encap:Ethernet  HWaddr 96:79:64:33:6f:17\n      inet addr:172.17.0.9  Bcast:0.0.0.0  Mask:255.255.0.0\n      inet6 addr: 2a11:4f8:140:2425:9479:64ff:fe33:6f17/64 Scope:Global\n      inet6 addr: fe80::9479:64ff:fe33:6f17/64 Scope:Link\n      UP BROADCAST RUNNING  MTU:1500  Metric:1\n      RX packets:13 errors:0 dropped:4 overruns:0 frame:0\n      TX packets:7 errors:0 dropped:0 overruns:0 carrier:0\n      collisions:0 txqueuelen:1000\n      RX bytes:1490 (1.4 KB)  TX bytes:586 (586.0 B)\n```\n\nIt would be nice to have a key \"IPv6Address\" under \"NetworkSettings\".\n"},{"labels":["api",null],"text":"There are a few issues that ask for specific hooks, but none of them have apparently been fully addressed. The ones that are closed were promised a \"generic\" hooks mechanism for all commands/events.\n\nA few usecases from reading the issues:\n- shutdown hook (EXITPOINT ?) for gracefulness (#2100)\n- start hook (#252 is a bit old, and #3317)\n\nFrom https://github.com/GoogleCloudPlatform/kubernetes/issues/140#issuecomment-48602271:\n\n> [...] we felt there were two cases we wanted to handle:\n> - hooks that either need the container context (and as such executing outside the process namespace would be pointless), or if interrupted by container shutdown would not be internally inconsistent. Pre-termination is a good example\n> - hooks that should be outside of a container, because they need to continue to run even if a container fails. Deploy across multiple containers is a good example, or post-termination.\n\nPlease feel free to comment with more.\n\nIt makes sense to offer hooks for greater customization, however, this might affect the current way of doing things (see the `exec` discussion in https://github.com/dotcloud/docker/issues/3317#issuecomment-31273101)\n\nFor solutions, there was a mention of \"predefined paths\" for executables.\n\nSome hooks (start and shutdown) would be more useful if they were executed synchronously (unlike how `docker events` fires currently).\n\nWe need to define what hooks should be implemented in docker and what should be done externally via docker events.\n"},{"labels":["api"],"text":"When a build fails, the http request returns a 200 response.  A failure response would be more appropriate.  An example http response from a failure:\n\n``` clj\n{:status 200,\n :body \"{\\\"stream\\\":\\\"Step 0 : FROM java\\\\n\\\"}\n        {\\\"status\\\":\\\"Pulling repository java\\\"}\n        {\\\"errorDetail\\\":{\\\"message\\\":\\\"HTTP code: 404\\\"},\\\"error\\\":\\\"HTTP code: 404\\\"}\"}\n```\n"},{"labels":["api"],"text":"This is an edge case, but it should be more consistent on the returns of an out-of-sync client and server.  `docker build` seems to run regardless of version difference.  `docker run` just silently fails because it tries to fetch from the index.  `docker images` finally gives an error that should have been encountered on the first interaction with the docker server.\n\nOutput of the three commands (when docker client was 1.13 and server was 1.12):\n\n``` console\n$ docker build -t infosiftr/jbilling .\nSending build context to Docker daemon 328.6 MB\nSending build context to Docker daemon \nStep 0 : FROM java\n ---> f14d9b4cde6f\nStep 1 : ADD jbilling-community-3.1.0/ /usr/local/jbilling/\n ---> Using cache\n ---> 1520b79e1eb8\nStep 2 : WORKDIR /usr/local/jbilling/bin\n ---> Running in 0036197d8bfc\n ---> 9085b16f76c5\nRemoving intermediate container 0036197d8bfc\nStep 3 : RUN chmod +x *.sh\n ---> Running in 81604d05b1a5\n ---> 9f7f442c623c\nRemoving intermediate container 81604d05b1a5\nStep 4 : CMD [\"./startup.sh\"]\n ---> Running in ef39f2dc039e\n ---> 85cf958acdf1\nRemoving intermediate container ef39f2dc039e\nSuccessfully built 85cf958acdf1\n$ docker run -it --rm --name jbill infosiftr/jbilling\nUnable to find image 'infosiftr/jbilling' locally\nPulling repository infosiftr/jbilling\n2014/07/07 15:49:41 HTTP code: 404\n$ docker images\n2014/07/07 15:49:56 Error response from daemon: client and server don't have same version (client : 1.13, server: 1.12)\n```\n\nRelevant version stuffs (after restarting docker server):\n\n``` console\n$ docker version\nClient version: 1.1.0\nClient API version: 1.13\nGo version (client): go1.3\nGit commit (client): 79812e3\nServer version: 1.1.0\nServer API version: 1.13\nGo version (server): go1.3\nGit commit (server): 79812000\n$ docker -D info\nContainers: 2\nImages: 188\nStorage Driver: btrfs\nExecution Driver: native-0.2\nKernel Version: 3.12.21-gentoo-r1\nDebug mode (server): false\nDebug mode (client): true\nFds: 10\nGoroutines: 10\nEventsListeners: 0\nInit SHA1: 51b8dd8c22b5c2bbfdc550b0a6bb1cb2df838169\nInit Path: /usr/libexec/docker/dockerinit\nSockets: [unix:///var/run/docker.sock]\n$ uname -a\nLinux minas-morgul 3.12.21-gentoo-r1 #1 SMP Fri Jun 6 15:17:15 MDT 2014 x86_64 AMD Phenom(tm) II X6 1090T Processor AuthenticAMD GNU/Linux\n```\n"},{"labels":["api"],"text":"Sent a POST /container/create with an empty body.. Its sending HTTP 500 (server error) back, which isn't correct imho.\n\nShould send something like bad request.\n\nI'd like to help. Where can this be fixed?\n"},{"labels":["api",null],"text":"The current Remote API has a number issues (#5722, #5278, #2786, #3037, #1011, #3622, #2949, etc) that make it award to use. I propose that the following issues be addressed in a new version of the API. This new API will breaking but I believe going forward the benefit of these changes will outweigh the cost IMHO. \n1. Change POST request that don't have payload to GET requests.\n2. Utilize Server-Side-Events for blocking operations and use JSON data for event data.\n3. Standardize error response and return a JSON object that encapsulates error messages.\n4. Avoid using \"/json\" in request URL and instead rely on \"Accept\" header.\n5. Return appropriate Content-Type header information.\n6. Use \"camelCase\" or \"PascalCase\" for JSON property names but not both.\n7. For PascalCase, two letter property names should not be capitalized (Change \"ID\" to \"Id\", and \"OS\" to \"Os\"). Doing so would make the JSON easier to process.\n8. Stream content type should be specific. If returning octet-stream that's a gzip file then use \"application/gzip) ([RFC 6713](http://www.rfc-editor.org/rfc/rfc6713.txt))\n9. Break the Remote API documentation page into smaller pages.\n10. Add JSON schema definitions for JSON objects.\n11. Add HATEOAS support?\n\nI have started a new repository [wiki page](https://github.com/saden1/docker-api/wiki) to track this proposal. I have also done a first take on revamping the List Containers request and would love feedback. \n\nhttps://github.com/saden1/docker-api/wiki/List-Containers\n\nIs this something the docker team is interested in doing?\n"},{"labels":["api",null,null],"text":"Since I can't seem to figure out how to add this support, here's the idea:\n\nIt would be great if the /info endpoint would include any data related to the following:\n- BridgeIface\n- BridgeIP\n- BridgeNetwork\n- BridgeNetmask\n\nGiven the fact that both BridgeIface and BridgeIP are represented in config.go, I figure this is doable. From there, I'd assume the remaining attributes could be determined and published.\n"},{"labels":["api",null],"text":"Right now the docker push command has lots of great output when using the CLI, but sometimes you don't need that much information, like when you are scripting something, or you are using the API. When this happens it would be nice if there was a  less verbose output that you could use. Docker build already has a similar command, so maybe we can use something similar.\n\n/cc @vieux \n"}
]
    