[{"text": "I think it's a bug\r\n![image](https://user-images.githubusercontent.com/7847271/84676251-f498bf00-af35-11ea-8bd4-845697e447e0.png)\r\n"},
{"text": " padded_batch() missing 1 required positional argument: 'padded_shapes' in line train_data = train_data.padded_batch(BATCH_SIZE)"},
{"text": "This tutorials is currently broken (also needs a `%tensorflow_version 2.x`):\r\n\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/load_data/csv.ipynb"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/api_docs/cc\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe C++ API references for TFLite are missing many important components on tensorflow.org. For example, the docs for `tflite::FlatBufferModel` and `tflite::InterpreterBuilder` are completely gone, which used to be there the last time I checked, about two months ago.\r\n"},
{"text": "The issue is for this page: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/resource-apply-centered-r-m-s-prop\r\n\r\n`momentum` is missing from the list of arguments.\r\n\r\nCurrent doc has this:\r\n\r\n```\r\nscope: A Scope object\r\nvar: Should be from a Variable().\r\nmg: Should be from a Variable().\r\nms: Should be from a Variable().\r\nmom: Should be from a Variable().\r\nlr: Scaling factor. Must be a scalar.\r\nrho: Decay rate. Must be a scalar.\r\nepsilon: Ridge term. Must be a scalar.\r\ngrad: The gradient.\r\n```\r\n\r\nThis should be instead like this (momentum added after rho):\r\n\r\n```\r\nscope: A Scope object\r\nvar: Should be from a Variable().\r\nmg: Should be from a Variable().\r\nms: Should be from a Variable().\r\nmom: Should be from a Variable().\r\nlr: Scaling factor. Must be a scalar.\r\nrho: Decay rate. Must be a scalar.\r\nmomentum: momentum scale. Must be a scalar.\r\nepsilon: Ridge term. Must be a scalar.\r\ngrad: The gradient.\r\n```\r\n\r\n### Clear description\r\nThis could be due to a bug in the scripts that auto-generate the api-def.\r\n\r\nThis is the op registration from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/training_ops.cc#L1056-L1069 \r\n\r\n```\r\nREGISTER_OP(\"ResourceApplyCenteredRMSProp\")\r\n    .Input(\"var: resource\")\r\n    .Input(\"mg: resource\")\r\n    .Input(\"ms: resource\")\r\n    .Input(\"mom: resource\")\r\n    .Input(\"lr: T\")\r\n    .Input(\"rho: T\")\r\n    .Input(\"momentum: T\")\r\n    .Input(\"epsilon: T\")\r\n    .Input(\"grad: T\")\r\n    .Attr(\"T: numbertype\")\r\n    .Attr(\"use_locking: bool = false\")\r\n    .SetShapeFn(\r\n        ApplyCenteredRMSPropShapeFn</*is_sparse=*/false, /*is_resource=*/true>);\r\n```\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\n\"Note that it is a negative quantity between -1 and 0\" should be changed to \"Note that it is a negative quantity between -1 and 1\"\r\n\r\n### Correct links\r\n\r\n### Parameters defined\r\n\r\n### Returns defined\r\n\r\n### Raises listed and defined\r\n\r\n### Usage example\r\n\r\n### Request visuals, if applicable\r\n\r\n### Submit a pull request?\r\n\r\nDon't plan to submit pull request\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\nUnder Load using Keras.preprocessing > Create Dataset, the percentage under training_ds for validation_split is set to 0.2. I believe is supposed to be 0.8.\r\n\r\n\r\n### Clear description\r\n\r\nWhen you used 0.8 on the number of images in the directory, it will result to 2936, but 0.2 is way smaller. \r\n\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: https://www.tensorflow.org/guide/data_performance_analysis\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/guide/data_performance_analysis#3_are_you_reaching_high_cpu_utilization\r\n\r\n## Description of issue (what needs changing):\r\n\r\nReferring to the following line:\r\n\r\n> tf.data achieves high throughput by trying to make the best possible use of available resources. In general, even when running your model on an accelerator like a GPU or TPU, the tf.data pipelines are run on the CPU. You can check your utilization with tools like sar and htop, or in the cloud monitoring console if you\u2019re running on GCP.\r\n\r\nThe link attached to \"cloud monitoring console\" is invalid.\r\n"},
{"text": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nCurrently https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder says it expects input to be logits, whereas it acutally expects softmax already applied.\r\n\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_decode, which expects output of softmax, and it directly passes the input to https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder - see https://github.com/tensorflow/tensorflow/blob/7b301123019d2b4bbd9c597916ba032f05854074/tensorflow/python/keras/backend.py#L6037-L6088\r\n\r\n**Describe the expected behavior**\r\n\r\nThe documentation of https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder should say it expects softmax output.\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://tensorflow.google.cn/guide/autodiff\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWhen listing the reason of the \"None\" gradient (Chapter of \"Getting a gradient of None\"), the text  \"**3. Took gradients through an integer or string**\" is followed by order number \"5\", which is \"**5. Took gradients through a stateful object**\". Suggest to correct it to  \"**4. Took gradients through a stateful object**\"\r\n\r\n"},
{"text": "This is the same issue mentioned [here](https://github.com/tensorflow/tensorflow/issues/41413).\r\n\r\nCurrently [here](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_text_classification.ipynb#scrollTo=zXXx5Oc3pOmN) under 'Loss function and optimizer' it says:\r\n\r\n```\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n```\r\n\r\nThis needs to be corrected to:\r\n\r\n\r\n`model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])`\r\n"},
{"text": "I'm following the [GPU install instructions](https://www.tensorflow.org/install/gpu) for Ubuntu 18.04 and getting the following after running this:\r\n```\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-1 \\\r\n    libcudnn7=7.6.5.32+cuda10.1  \\\r\n    libcudnn7-dev=7.6.5.32-1+cuda10.1\r\n```\r\nI'm getting this:\r\n```\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Version '7.6.5.32+cuda10.1' for 'libcudnn7' was not found\r\n```\r\n `7.6.5.32+cuda10.1` may need to be changed to  `7.6.5.32-1+cuda10.1`  since the latter works.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\n- https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0\r\n- https://www.tensorflow.org/tutorials/distribute/input#partial_batches\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe release note states \r\n> tf.distribute.experimental.MultiWorkerMirroredStrategy adds support for partial batches.\r\n\r\nHowever the documentation/tutorial states\r\n> Currently this is supported for all strategies except tf.distribute.experimental.MultiWorkerMirroredStrategy.\r\n> [...] Partial batches are supported for all strategies except tf.distribute.experimental.MultiWorkerMirroredStrategy.\r\n\r\nThis sounds like those contradict each other. What is the actual reality? Can the documentation or the release notes be updated to clarify?"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose]()\r\n\r\n## Description of issue (what needs changing):\r\nConv1DTranspose - Dilation - Does not inform uses that dilation doesn't work for any value  of `Dilation>1` because it isn't implemented yet. \r\n### Clear description\r\nCurrently documentation says:\r\n_\"an integer, specifying the dilation rate to use for dilated convolution. Currently, specifying a dilation_rate value != 1 is incompatible with specifying a stride value != 1.\"_\r\n\r\nThis may not be implemented yet in the newest of nightly build, but with my tf-nightly==2.5.0dev20200629 build this didn't work. I fear updating to new nightly builds in case in breaks my research code which relies on nightly builds until Conv1DTranspose is released in a supported build.\r\n```\r\nInvalidArgumentError:  Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.\r\n\t [[node test1_AE/decoder/conv1d_transpose/conv1d_transpose (defined at D:\\Users\\[username]\\Desktop\\libs_python\\nn4n_autoencoder4.py:120) ]] [Op:__inference_train_function_2185]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\nThis is with stride == 1.\r\n### Correct links\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py\r\n\r\n### Parameters defined\r\n\r\nYes, setting my dilation to 1 gets rid of the issue.\r\n\r\n### Returns defined\r\n\r\nNot necessary. (I'm not sure if you are asking if I have define returns in my code or if my code returns a defined value, or if the documentation claims to return something)\r\n\r\n### Raises listed and defined\r\n```\r\nInvalidArgumentError:  Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.\r\n\t [[node test1_AE/decoder/conv1d_transpose/conv1d_transpose (defined at D:\\Users\\[username]\\Desktop\\libs_python\\nn4n_autoencoder4.py:120) ]] [Op:__inference_train_function_2185]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\n### Usage example\r\n\r\nNightly build, so no.\r\n\r\n### Request visuals, if applicable\r\n No.\r\n\r\n### Submit a pull request?\r\nI will not be doing so.\r\n\r\n\r\n### Test Code\r\n\r\nNote 1: This is built with tf-nightly==2.5.0dev20200626 which was removed from the PyPi archive for unknown reasons.\r\n\r\nNote 2: model.fit must be called for the error to occur. Simpy constructing and compiling the network was not enough to reproduce the error.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as krs\r\nimport numpy as np\r\n\r\ntrain_data = np.random.uniform(-1,1,(20,20))\r\n\r\ninputs = krs.Input((20,1))\r\n\r\nx = inputs\r\n\r\nx = krs.layers.Conv1D(1,3,strides = 1,padding='same',dilation_rate=2,activation='relu')(x)\r\nx = krs.layers.Flatten()(x)\r\nx = krs.layers.Dense(10,activation='relu')(x)\r\nx = krs.layers.Dense(2,activation='relu')(x)\r\nx = krs.layers.Dense(10,activation='relu')(x)\r\nx = krs.layers.Dense(20,activation='relu')(x)\r\nx = krs.layers.Reshape(target_shape=(20,1))(x)\r\nx = krs.layers.Conv1DTranspose(1,3,strides=1,dilation_rate=2,padding='same',activation='relu',output_padding=0)(x)\r\noutput = krs.layers.Flatten()(x)\r\n\r\nmodel = krs.Model(inputs,output,name='test')\r\n\r\nmodel.compile(optimizer='adam',loss='MSE')\r\n\r\nmodel.summary()\r\n\r\nmodel.fit(train_data,train_data)\r\n```"},
{"text": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\nUbuntu 18.04, system with two GPUs, tensorflow 2.2\r\n\r\n### Describe the problem\r\n\r\nThe guide, near the top, includes the following:\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nif physical_devices:\r\n  tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\nand on a multi-GPU system, this will lead to problems later because memory growth will only be managed on one of the GPUs.  It should instead be:\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor device in physical_devices:\r\n  tf.config.experimental.set_memory_growth(device, True)\r\n\r\nI am aware that this is really a minor documentation issue, and would prefer to simply upload the fix myself.  Unfortunately, I am not willing to sign the Contributor License Agreement.  I have many friends at Google with whom I have many technical discussions.  I don't want those discussions to automatically grant licenses to Google if I forget to say \"not a contribution\" at the beginning of them.\r\n### Source code / logs\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/inv\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe first paragraph says\r\n\r\n> Computes the inverse of one or more square invertible matrices or their\r\n\r\ninstead of \r\n\r\n> Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).\r\n\r\n![image](https://user-images.githubusercontent.com/552629/88276649-b2eef700-ccdf-11ea-9b32-1eb450ffc57a.png)\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/inv\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe first paragraph says\r\n\r\n> Computes the inverse of one or more square invertible matrices or their\r\n\r\ninstead of \r\n\r\n> Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).\r\n\r\n![image](https://user-images.githubusercontent.com/552629/88276649-b2eef700-ccdf-11ea-9b32-1eb450ffc57a.png)\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/tensor\r\nhttps://www.tensorflow.org/guide/tensor#multi-axis_indexing\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe left side image in the figure called \"Selecting the last feature across all locations in each example in the batch\" has batches 1 (blue) and 2 (green) swapped. The right side image and the surrounding code example show the correct order.\r\n\r\nThe image that needs an edit is: https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/index1.png\r\n\r\n### Submit a pull request?\r\n\r\nNo. I do not have a way to edit the image."},
{"text": "https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_yuv\r\n\r\nThe documentation says this function is only well defined if the values are between 0 and 1, but the example uses an input with values greater than 1."},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/tutorials/model_maker_image_classification\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe tutorial for Tensorflow Model Maker says that on export, there will be a model.tflite file and then a labels.txt file. However, when I export the model using the instructions, it only outputs a single model.tflite. The console output says that it is stored in a temp directory (which appears to be subsequently deleted), and that the labels are merged into the model.tflite file. Would I still be able to use this on mobile or is there any way I can extract labels.txt?\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "This is mainly a documentation bug (official tensorflow tutorial), but it is a \"dangerous trap\" and might also happen in general to users, so see below my last sentence this could also be fixed in Tensorflow that it detects this automatically.\r\n\r\nIn this [tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning) raw prediction values (form_logit=True) are used. So we have negative values and positive values, while \r\n\r\n> \r\n\"prediction will be treated as a logit, or a raw prediction value. **Positive numbers predict class 1, negative numbers predict class 0.**\"\r\n\r\nHowever, the model.compile statement is as follows:\r\n\r\n```\r\nbase_learning_rate = 0.0001\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\r\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\n```\r\n\r\nThis is wrong, as per default, threshold value to classify is 0.5:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/binary_accuracy\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy\r\n```\r\ntf.keras.metrics.binary_accuracy(\r\n    y_true, y_pred, threshold=0.5\r\n)\r\n```\r\n```\r\ntf.keras.metrics.BinaryAccuracy(\r\n    name='binary_accuracy', dtype=None, threshold=0.5\r\n)\r\n```\r\n\r\n> threshold | (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0.\r\n> -- | --\r\n\r\n\r\n\r\n\r\nThis leads to the wrong classifications. model.evaluate will also give false accuracy measure. Reason is that predicted values in range [0,0.49999] are wrongly classified as 0 (I am not sure what happens to a value of exactly 0.5), whereas they actually should be classified as 1!\r\n\r\nSo it needs to be corrected to:\r\n\r\n> model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\r\n>               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n>               metrics=tf.keras.metrics.BinaryAccuracy(threshold=0.0))\r\n\r\nWould be even better if this is corrected inside Tensorflow that it automatically detects that from_logits=True was set and then assumes that default threshold is not 0.5 anymore, but 0.0 (and maybe additional WARNING output)."},
{"text": "I think the doc-string [here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/math_ops.py#L2758):\r\n```python\r\n  `output[i, j, k, ..., l] = trace(x[i, j, i, ..., l, :, :])`\r\n```\r\n\r\nshould be:\r\n\r\n```python\r\n  `output[i, j, k, ..., l] = trace(x[i, j, k, ..., l, :, :])`\r\n```\r\n\r\n\r\n"},
{"text": "A tracking bug for migrating the [TF1 Speech Recognition example](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md) to TF 2"},
{"text": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Permute\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nPermute layer is quite picky about its dims argument despite docs clearly saying \r\n\r\n`dims: Tuple of integers. Permutation pattern, does not include the samples dimension. Indexing starts at 1. For instance, (2, 1) permutes the first and second dimensions of the input. `\r\n\r\n It does not actually follow from this documentation that the dims input MUST have all the dimensions clearly listed without misses -  as per check actually present in the code:\r\n``` python\r\n   if sorted(dims) != list(range(1, len(dims) + 1)):\r\n      raise ValueError(\r\n          'Invalid permutation `dims` for Permute Layer: %s. '\r\n          'The set of indices in `dims` must be consecutive and start from 1.' %\r\n          (dims,))\r\n```\r\nThe next hurdle is that the error message thrown by runtime is kind of inconsistent:\r\n\r\n``` python\r\nkeras.layers.Permute((3, 2), input_shape=[30, 6, 8], name=f\"Permute_layer\")\r\n>>> Invalid permutation `dims` for Permute Layer: (3, 2). The set of indices in `dims` must be consecutive and start from 1.\r\n```\r\nIt does not say in the doc that the dims must start with 1, it just says indexing starts with 1! Ok now the user knows it must start with 1, but how does one actually get dimensions 2 and 3 swapped? At that point it's just a mess from there on.\r\n\r\n### Suggest following changes:\r\nto the docs: please make example use 3D tensor, rather than 2D. Then it would be clear that all dims must be listed, even if they are not to be permuted, e.g.\r\n``` python\r\ntmp = keras.layers.Permute((1, 3, 2), name=f\"Permute_input\")(tmp)\r\n```\r\n\r\nAnother note that docs do not make is that the len(ndim) must match the  input_shape dimensions, which is however checked by the __init__, again possibly conflicting with the doc that says that input_shape can be whatever (clearly not whatever, it is coupled with ndim argument.\r\n\r\n\r\n"},
{"text": "https://www.tensorflow.org/resources/learn-ml\r\n-> The four areas of machine learning education\r\n-> Build your own projects\r\n-> [colab]\r\n-> https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/keras/basic_classification.ipynb\r\n\r\nGives me \"Notebook not found\"\r\nFetch for https://api.github.com/repos/tensorflow/docs/contents/site/en/r2/tutorials/keras?per_page=100&ref=r2.0rc failed: {\r\n  \"message\": \"No commit found for the ref r2.0rc\",\r\n  \"documentation_url\": \"https://developer.github.com/v3/repos/contents/\"\r\n}\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/community/contribute/code_style\r\n\r\n## Description of issue (what needs changing):\r\n\r\nDocs say to install `clang-tidy` but the example given says to run `clang-format`. Is this intended? I would have expected to run `clang-tidy`."},
{"text": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/where#args_1\r\n\r\n## Description of issue (what needs changing):\r\nFor Args \"x\".\r\nOriginal:\r\nIf provided, a Tensor which is of the same type as y, and has a shape broadcastable with condition and y.\r\nShould be:\r\nIf provided, a Tensor which is of the same type as **x**, and has a shape broadcastable with condition and y.\r\n"},
{"text": "If we search `tf.argmax` in [tensorflow r1.15 documentation](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/argmax), no result get for tf 1.x\r\n\r\nHowever, if search \"tf.math.argmax\", you will get it.\r\n\r\nBut they're both annations:\r\n```Python\r\n# pylint: disable=redefined-builtin\r\n@tf_export(v1=[\"math.argmax\", \"argmax\"])         # !!!! This line\r\n@deprecation.deprecated_args(None, \"Use the `axis` argument instead\",\r\n                             \"dimension\")\r\n@_set_doc(\r\n    gen_math_ops.arg_max.__doc__.replace(\"dimensions\", \"axes\").replace(\r\n        \"dimension\", \"axis\"))\r\ndef argmax(input,\r\n           axis=None,\r\n           name=None,\r\n           dimension=None,\r\n           output_type=dtypes.int64):\r\n  axis = deprecation.deprecated_argument_lookup(\r\n      \"axis\", axis, \"dimension\", dimension)\r\n  return argmax_v2(input, axis, output_type, name)\r\n```\r\n\r\nWhy only show the first aliased annotation in documentation? "},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nThis link is broken https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n\r\n![image](https://user-images.githubusercontent.com/6630197/85917788-7c979680-b85d-11ea-8d93-0fae5b8d9bf2.png)\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\nYes, this one [https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb)\r\n\r\n\r\n### Submit a pull request?\r\n\r\nNo.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/models/model_from_config\r\n\r\n## Description of issue (what needs changing):\r\n\r\ntf.keras.models.model_from_config function can create only layers, not the complete model as it is described in documentation. Correct usage is mentioned at https://www.tensorflow.org/guide/keras/save_and_serialize, but not described in the main documenation for tf.keras.Model class.\r\n\r\n> Calling config = model.get_config() will return a Python dict containing the configuration of the model. The same model can then be reconstructed via Sequential.from_config(config) (for a Sequential model) or Model.from_config(config) (for a Functional API model).\r\n\r\n### Clear description\r\nThe behavior of tf.keras.models.model_from_config does not correspond to the documentation.\r\nMoreover, it is even more confusing when compared with similar methods, like\r\n\r\n`tf.keras.models.model_from_json(model.to_json())`\r\n\r\n> <tensorflow.python.keras.engine.training.Model at 0x7fa2e443aa20>\r\n\r\n`tf.keras.models.model_from_yaml(model.to_yaml())`\r\n\r\n> <tensorflow.python.keras.engine.training.Model at 0x7fa2e443ca40>\r\n\r\nwhile model_from_config \r\n\r\n`tf.keras.models.model_from_config(model.get_config())`\r\n\r\n> > ---------------------------------------------------------------------------\r\n> KeyError                                  Traceback (most recent call last)\r\n> <ipython-input-99-f3b4bb685ac8> in <module>\r\n> ----> 1 tf.keras.models.model_from_config(model.get_config())\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)\r\n>      53                     '`Sequential.from_config(config)`?')\r\n>      54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n> ---> 55   return deserialize(config, custom_objects=custom_objects)\r\n>      56 \r\n>      57 \r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n>      99   globs['SequenceFeatures'] = sfc.SequenceFeatures\r\n>     100 \r\n> --> 101   layer_class_name = config['class_name']\r\n>     102   if layer_class_name in _DESERIALIZATION_TABLE:\r\n>     103     config['class_name'] = _DESERIALIZATION_TABLE[layer_class_name]\r\n> \r\n> KeyError: 'class_name'\r\n> \r\n\r\n### Correct usage\r\n\r\n`tf.keras.Model().from_config(model.get_config())`\r\n\r\n> <tensorflow.python.keras.engine.training.Model at 0x7fa2e4480080>"},
{"text": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\ncolab tensorflow 2\r\n\r\n\r\n`v2.2.0-0-g2b96f3662b 2.2.0`\r\ntested on both CPU and GPU (GPU is much worse!)\r\n\r\n**Describe the current behavior**\r\n\r\nwhen timing a simple tf.function that uses a loop, `tf.range` is much slower than using `range`.\r\nBUT `tf.range` is recommended in the docs, moreover is says that looping over non-tensor will be rolled during tracing (which does not happen. normal `range` is being traced as a loop)\r\n\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])\r\ndef test(x):\r\n  for i in tf.range(100):\r\n    x = x * tf.constant(1.1)\r\n  return x\r\n```\r\n\r\n```\r\n%%timeit\r\ntest(tf.range(1000, dtype='float32'))\r\n```\r\nprints `100 loops, best of 3: 2.12 ms per loop`\r\nprints `10 loops, best of 3: 70.2 ms per loop` on GPU!\r\nwhile using `range` or `np.arange` is about 300 micro seconds in both CPU and GPU\r\n\r\n**Describe the expected behavior**\r\n\r\n1 there is a documentation issue where it currently always recommends `tf.range`.\r\n2 the documentation should specify when python loops are not rolled\r\n3 tf.range performance should be the same as range when used in a traced loop\r\n(also note the `np.arange` is faster than `tf.range` and comparable to `range`)"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/rfft\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/rfft2d\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/rfft3d\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the `Args` section, there are inputs `input` and `Tcomplex`. `Treal` no longer exist, and `input` should be `input_tensor`.\r\n\r\nRunning code\r\n\r\n~~~python\r\ntf.signal.rfft(1, Tcomplex=tf.complex64)\r\n~~~\r\n\r\ngot exception:\r\n\r\n~~~python\r\nTypeError: _rfft() got an unexpected keyword argument 'Tcomplex'\r\n~~~\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nNo\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2\r\n\r\n\r\n## Related Issue:\r\n#39520"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/irfft\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/irfft2d\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/irfft3d\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the `Args` section, there are inputs `input` and `Treal`. `Treal` no longer exist, and `input` should be `input_tensor`.\r\n\r\nRunning code:\r\n\r\n~~~python\r\ntf.signal.irfft(1, Treal=tf.float32) \r\n~~~\r\n\r\ngot exception:\r\n\r\n~~~python\r\nTypeError: _irfft() got an unexpected keyword argument 'Treal'\r\n~~~\r\n\r\nAnd if run code:\r\n\r\n~~~python\r\ntf.signal.irfft(input=1)\r\n~~~\r\n\r\ngot exception:\r\n\r\n~~~python\r\nTypeError: _irfft() got an unexpected keyword argument 'input'\r\n~~~\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nNo\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/swish\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the \"Args\" section, there is an input ` name`, but it is not in the signature, and the function doesn't accept the argument.\r\n\r\nRunning code:\r\n\r\n~~~python\r\ntf.nn.swish(1, name=None)\r\n~~~\r\n\r\ngot exception:\r\n\r\n~~~python\r\nTypeError: swish() got an unexpected keyword argument 'name'\r\n~~~\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nNo\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2\r\n"},
{"text": "Link to 'Tensorflow Roadmap' in README is broken: https://www.tensorflow.org/community/roadmap\r\n\r\nThis template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n"},
{"text": "OS: Ubuntu 18.04\r\nGraphics card: Nvidia 1050Ti\r\n\r\n**Problem**\r\nFollowing the instructions under https://www.tensorflow.org/install/gpu#install_cuda_with_apt gives the following error: \r\n\r\n```ssh\r\n...\r\n\r\nUnpacking libcudnn7-dev (7.6.4.38-1+cuda10.1) ...\r\nErrors were encountered while processing:\r\n /tmp/apt-dpkg-install-fjAi3S/55-libnvidia-compute-450_450.36.06-0ubuntu1_amd64.deb\r\nE: Sub-process /usr/bin/dpkg returned an error code (1)\r\n```\r\nafter executing this step\r\n```ssh\r\nsudo apt-get install --no-install-recommends \\\r\n>     cuda-10-1 \\\r\n>     libcudnn7=7.6.4.38-1+cuda10.1  \\\r\n>     libcudnn7-dev=7.6.4.38-1+cuda10.1\r\n```\r\n\r\n**Additional Info**\r\nThe complete message after running the above command is\r\n\r\n```ssh\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nThe following packages were automatically installed and are no longer required:\r\n  libnvidia-common-440 libnvidia-extra-440\r\nUse 'sudo apt autoremove' to remove them.\r\nThe following additional packages will be installed:\r\n  cuda-command-line-tools-10-1 cuda-compiler-10-1 cuda-cudart-10-1\r\n  cuda-cudart-dev-10-1 cuda-cufft-10-1 cuda-cufft-dev-10-1 cuda-cuobjdump-10-1\r\n  cuda-cupti-10-1 cuda-curand-10-1 cuda-curand-dev-10-1 cuda-cusolver-10-1\r\n  cuda-cusolver-dev-10-1 cuda-cusparse-10-1 cuda-cusparse-dev-10-1\r\n  cuda-demo-suite-10-1 cuda-documentation-10-1 cuda-driver-dev-10-1\r\n  cuda-drivers cuda-drivers-450 cuda-gdb-10-1 cuda-gpu-library-advisor-10-1\r\n  cuda-libraries-10-1 cuda-libraries-dev-10-1 cuda-license-10-1\r\n  cuda-license-10-2 cuda-memcheck-10-1 cuda-misc-headers-10-1 cuda-npp-10-1\r\n  cuda-npp-dev-10-1 cuda-nsight-10-1 cuda-nsight-compute-10-1\r\n  cuda-nsight-systems-10-1 cuda-nvcc-10-1 cuda-nvdisasm-10-1 cuda-nvgraph-10-1\r\n  cuda-nvgraph-dev-10-1 cuda-nvjpeg-10-1 cuda-nvjpeg-dev-10-1\r\n  cuda-nvml-dev-10-1 cuda-nvprof-10-1 cuda-nvprune-10-1 cuda-nvrtc-10-1\r\n  cuda-nvrtc-dev-10-1 cuda-nvtx-10-1 cuda-nvvp-10-1 cuda-runtime-10-1\r\n  cuda-samples-10-1 cuda-sanitizer-api-10-1 cuda-toolkit-10-1 cuda-tools-10-1\r\n  cuda-visual-tools-10-1 default-jre default-jre-headless libcublas-dev\r\n  libcublas10 libnvidia-cfg1-450 libnvidia-common-450 libnvidia-compute-450\r\n  libnvidia-decode-450 libnvidia-encode-450 libnvidia-fbc1-450\r\n  libnvidia-gl-450 libnvidia-ifr1-450 nsight-compute-2019.5.0\r\n  nsight-systems-2019.5.2 nvidia-compute-utils-450 nvidia-dkms-450\r\n  nvidia-driver-450 nvidia-kernel-common-450 nvidia-kernel-source-450\r\n  nvidia-modprobe nvidia-settings nvidia-utils-450 openjdk-11-jre\r\n  openjdk-11-jre-headless xserver-xorg-video-nvidia-450\r\nSuggested packages:\r\n  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\r\n  | fonts-wqy-zenhei\r\nThe following packages will be REMOVED:\r\n  libnvidia-cfg1-440 libnvidia-compute-440 libnvidia-decode-440\r\n  libnvidia-encode-440 libnvidia-fbc1-440 libnvidia-fbc1-440:i386\r\n  libnvidia-gl-440 libnvidia-ifr1-440 nvidia-compute-utils-440 nvidia-dkms-440\r\n  nvidia-driver-430 nvidia-driver-440 nvidia-kernel-common-440\r\n  nvidia-kernel-source-440 nvidia-utils-440 xserver-xorg-video-nvidia-440\r\nThe following NEW packages will be installed:\r\n  cuda-10-1 cuda-command-line-tools-10-1 cuda-compiler-10-1 cuda-cudart-10-1\r\n  cuda-cudart-dev-10-1 cuda-cufft-10-1 cuda-cufft-dev-10-1 cuda-cuobjdump-10-1\r\n  cuda-cupti-10-1 cuda-curand-10-1 cuda-curand-dev-10-1 cuda-cusolver-10-1\r\n  cuda-cusolver-dev-10-1 cuda-cusparse-10-1 cuda-cusparse-dev-10-1\r\n  cuda-demo-suite-10-1 cuda-documentation-10-1 cuda-driver-dev-10-1\r\n  cuda-drivers cuda-drivers-450 cuda-gdb-10-1 cuda-gpu-library-advisor-10-1\r\n  cuda-libraries-10-1 cuda-libraries-dev-10-1 cuda-license-10-1\r\n  cuda-license-10-2 cuda-memcheck-10-1 cuda-misc-headers-10-1 cuda-npp-10-1\r\n  cuda-npp-dev-10-1 cuda-nsight-10-1 cuda-nsight-compute-10-1\r\n  cuda-nsight-systems-10-1 cuda-nvcc-10-1 cuda-nvdisasm-10-1 cuda-nvgraph-10-1\r\n  cuda-nvgraph-dev-10-1 cuda-nvjpeg-10-1 cuda-nvjpeg-dev-10-1\r\n  cuda-nvml-dev-10-1 cuda-nvprof-10-1 cuda-nvprune-10-1 cuda-nvrtc-10-1\r\n  cuda-nvrtc-dev-10-1 cuda-nvtx-10-1 cuda-nvvp-10-1 cuda-runtime-10-1\r\n  cuda-samples-10-1 cuda-sanitizer-api-10-1 cuda-toolkit-10-1 cuda-tools-10-1\r\n  cuda-visual-tools-10-1 default-jre default-jre-headless libcublas-dev\r\n  libcublas10 libcudnn7 libcudnn7-dev libnvidia-cfg1-450 libnvidia-common-450\r\n  libnvidia-compute-450 libnvidia-decode-450 libnvidia-encode-450\r\n  libnvidia-fbc1-450 libnvidia-gl-450 libnvidia-ifr1-450\r\n  nsight-compute-2019.5.0 nsight-systems-2019.5.2 nvidia-compute-utils-450\r\n  nvidia-dkms-450 nvidia-driver-450 nvidia-kernel-common-450\r\n  nvidia-kernel-source-450 nvidia-modprobe nvidia-settings nvidia-utils-450\r\n  openjdk-11-jre openjdk-11-jre-headless xserver-xorg-video-nvidia-450\r\n0 upgraded, 79 newly installed, 16 to remove and 239 not upgraded.\r\nNeed to get 0 B/2,205 MB of archives.\r\nAfter this operation, 4,855 MB of additional disk space will be used.\r\nDo you want to continue? [Y/n] \r\nExtracting templates from packages: 100%\r\n(Reading database ... 294935 files and directories currently installed.)\r\nRemoving nvidia-driver-430 (440.59-0ubuntu0.18.04.1) ...\r\nRemoving nvidia-driver-440 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving xserver-xorg-video-nvidia-440 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving libnvidia-cfg1-440:amd64 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving libnvidia-encode-440:amd64 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving libnvidia-decode-440:amd64 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving nvidia-utils-440 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving libnvidia-fbc1-440:i386 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving libnvidia-fbc1-440:amd64 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving libnvidia-ifr1-440:amd64 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving libnvidia-gl-440:amd64 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving nvidia-compute-utils-440 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving nvidia-dkms-440 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving all DKMS Modules\r\nDone.\r\nINFO:Disable nvidia\r\nDEBUG:Parsing /usr/share/ubuntu-drivers-common/quirks/dell_latitude\r\nDEBUG:Parsing /usr/share/ubuntu-drivers-common/quirks/put_your_quirks_here\r\nDEBUG:Parsing /usr/share/ubuntu-drivers-common/quirks/lenovo_thinkpad\r\nupdate-initramfs: deferring update (trigger activated)\r\nRemoving nvidia-kernel-common-440 (440.82-0ubuntu0~0.18.04.2) ...\r\nupdate-initramfs: deferring update (trigger activated)\r\nRemoving nvidia-kernel-source-440 (440.82-0ubuntu0~0.18.04.2) ...\r\nRemoving libnvidia-compute-440:amd64 (440.82-0ubuntu0~0.18.04.2) ...\r\nSelecting previously unselected package cuda-license-10-1.\r\n(Reading database ... 294368 files and directories currently installed.)\r\nPreparing to unpack .../00-cuda-license-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-license-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-misc-headers-10-1.\r\nPreparing to unpack .../01-cuda-misc-headers-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-misc-headers-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvcc-10-1.\r\nPreparing to unpack .../02-cuda-nvcc-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvcc-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-cuobjdump-10-1.\r\nPreparing to unpack .../03-cuda-cuobjdump-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cuobjdump-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvprune-10-1.\r\nPreparing to unpack .../04-cuda-nvprune-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvprune-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-compiler-10-1.\r\nPreparing to unpack .../05-cuda-compiler-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-compiler-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvdisasm-10-1.\r\nPreparing to unpack .../06-cuda-nvdisasm-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvdisasm-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-gdb-10-1.\r\nPreparing to unpack .../07-cuda-gdb-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-gdb-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvprof-10-1.\r\nPreparing to unpack .../08-cuda-nvprof-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvprof-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-sanitizer-api-10-1.\r\nPreparing to unpack .../09-cuda-sanitizer-api-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-sanitizer-api-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-memcheck-10-1.\r\nPreparing to unpack .../10-cuda-memcheck-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-memcheck-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-cudart-10-1.\r\nPreparing to unpack .../11-cuda-cudart-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cudart-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-driver-dev-10-1.\r\nPreparing to unpack .../12-cuda-driver-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-driver-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-cudart-dev-10-1.\r\nPreparing to unpack .../13-cuda-cudart-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cudart-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-cupti-10-1.\r\nPreparing to unpack .../14-cuda-cupti-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cupti-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-gpu-library-advisor-10-1.\r\nPreparing to unpack .../15-cuda-gpu-library-advisor-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-gpu-library-advisor-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvtx-10-1.\r\nPreparing to unpack .../16-cuda-nvtx-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvtx-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-command-line-tools-10-1.\r\nPreparing to unpack .../17-cuda-command-line-tools-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-command-line-tools-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package openjdk-11-jre-headless:amd64.\r\nPreparing to unpack .../18-openjdk-11-jre-headless_11.0.7+10-2ubuntu2~18.04_amd64.deb ...\r\nUnpacking openjdk-11-jre-headless:amd64 (11.0.7+10-2ubuntu2~18.04) ...\r\nSelecting previously unselected package default-jre-headless.\r\nPreparing to unpack .../19-default-jre-headless_2%3a1.11-68ubuntu1~18.04.1_amd64.deb ...\r\nUnpacking default-jre-headless (2:1.11-68ubuntu1~18.04.1) ...\r\nSelecting previously unselected package openjdk-11-jre:amd64.\r\nPreparing to unpack .../20-openjdk-11-jre_11.0.7+10-2ubuntu2~18.04_amd64.deb ...\r\nUnpacking openjdk-11-jre:amd64 (11.0.7+10-2ubuntu2~18.04) ...\r\nSelecting previously unselected package default-jre.\r\nPreparing to unpack .../21-default-jre_2%3a1.11-68ubuntu1~18.04.1_amd64.deb ...\r\nUnpacking default-jre (2:1.11-68ubuntu1~18.04.1) ...\r\nSelecting previously unselected package cuda-nsight-10-1.\r\nPreparing to unpack .../22-cuda-nsight-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nsight-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvvp-10-1.\r\nPreparing to unpack .../23-cuda-nvvp-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvvp-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvrtc-10-1.\r\nPreparing to unpack .../24-cuda-nvrtc-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvrtc-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvrtc-dev-10-1.\r\nPreparing to unpack .../25-cuda-nvrtc-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvrtc-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-cusolver-10-1.\r\nPreparing to unpack .../26-cuda-cusolver-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cusolver-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-cusolver-dev-10-1.\r\nPreparing to unpack .../27-cuda-cusolver-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cusolver-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-license-10-2.\r\nPreparing to unpack .../28-cuda-license-10-2_10.2.89-1_amd64.deb ...\r\nUnpacking cuda-license-10-2 (10.2.89-1) ...\r\nSelecting previously unselected package libcublas10.\r\nPreparing to unpack .../29-libcublas10_10.2.2.89-1_amd64.deb ...\r\nUnpacking libcublas10 (10.2.2.89-1) ...\r\nSelecting previously unselected package libcublas-dev.\r\nPreparing to unpack .../30-libcublas-dev_10.2.2.89-1_amd64.deb ...\r\nUnpacking libcublas-dev (10.2.2.89-1) ...\r\nSelecting previously unselected package cuda-cufft-10-1.\r\nPreparing to unpack .../31-cuda-cufft-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cufft-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-cufft-dev-10-1.\r\nPreparing to unpack .../32-cuda-cufft-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cufft-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-curand-10-1.\r\nPreparing to unpack .../33-cuda-curand-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-curand-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-curand-dev-10-1.\r\nPreparing to unpack .../34-cuda-curand-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-curand-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-cusparse-10-1.\r\nPreparing to unpack .../35-cuda-cusparse-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cusparse-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-cusparse-dev-10-1.\r\nPreparing to unpack .../36-cuda-cusparse-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-cusparse-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-npp-10-1.\r\nPreparing to unpack .../37-cuda-npp-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-npp-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-npp-dev-10-1.\r\nPreparing to unpack .../38-cuda-npp-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-npp-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvml-dev-10-1.\r\nPreparing to unpack .../39-cuda-nvml-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvml-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvjpeg-10-1.\r\nPreparing to unpack .../40-cuda-nvjpeg-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvjpeg-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvjpeg-dev-10-1.\r\nPreparing to unpack .../41-cuda-nvjpeg-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvjpeg-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package nsight-compute-2019.5.0.\r\nPreparing to unpack .../42-nsight-compute-2019.5.0_2019.5.0.14-1_amd64.deb ...\r\nUnpacking nsight-compute-2019.5.0 (2019.5.0.14-1) ...\r\nSelecting previously unselected package cuda-nsight-compute-10-1.\r\nPreparing to unpack .../43-cuda-nsight-compute-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nsight-compute-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package nsight-systems-2019.5.2.\r\nPreparing to unpack .../44-nsight-systems-2019.5.2_2019.5.2.16-b54ef97_amd64.deb ...\r\nUnpacking nsight-systems-2019.5.2 (2019.5.2.16-b54ef97) ...\r\nSelecting previously unselected package cuda-nsight-systems-10-1.\r\nPreparing to unpack .../45-cuda-nsight-systems-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nsight-systems-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvgraph-10-1.\r\nPreparing to unpack .../46-cuda-nvgraph-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvgraph-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-nvgraph-dev-10-1.\r\nPreparing to unpack .../47-cuda-nvgraph-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-nvgraph-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-visual-tools-10-1.\r\nPreparing to unpack .../48-cuda-visual-tools-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-visual-tools-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-tools-10-1.\r\nPreparing to unpack .../49-cuda-tools-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-tools-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-samples-10-1.\r\nPreparing to unpack .../50-cuda-samples-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-samples-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-documentation-10-1.\r\nPreparing to unpack .../51-cuda-documentation-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-documentation-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-libraries-dev-10-1.\r\nPreparing to unpack .../52-cuda-libraries-dev-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-libraries-dev-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-toolkit-10-1.\r\nPreparing to unpack .../53-cuda-toolkit-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-toolkit-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package libnvidia-common-450.\r\nPreparing to unpack .../54-libnvidia-common-450_450.36.06-0ubuntu1_all.deb ...\r\nChecking for existing driver runfile install\r\n/var/lib/dpkg/tmp.ci/preinst: 6: /var/lib/dpkg/tmp.ci/preinst: [[: not found\r\nUnpacking libnvidia-common-450 (450.36.06-0ubuntu1) ...\r\nPreparing to unpack .../55-libnvidia-compute-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking libnvidia-compute-450:amd64 (450.36.06-0ubuntu1) ...\r\ndpkg: error processing archive /tmp/apt-dpkg-install-fjAi3S/55-libnvidia-compute-450_450.36.06-0ubuntu1_amd64.deb (--unpack):\r\n trying to overwrite '/usr/lib/x86_64-linux-gnu/libnvidia-allocator.so', which is also in package libnvidia-extra-440:amd64 440.82-0ubuntu0~0.18.04.2\r\nSelecting previously unselected package libnvidia-decode-450:amd64.\r\nPreparing to unpack .../56-libnvidia-decode-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking libnvidia-decode-450:amd64 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package libnvidia-encode-450:amd64.\r\nPreparing to unpack .../57-libnvidia-encode-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking libnvidia-encode-450:amd64 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package libnvidia-fbc1-450:amd64.\r\nPreparing to unpack .../58-libnvidia-fbc1-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking libnvidia-fbc1-450:amd64 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package libnvidia-gl-450:amd64.\r\nPreparing to unpack .../59-libnvidia-gl-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking libnvidia-gl-450:amd64 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package libnvidia-ifr1-450:amd64.\r\nPreparing to unpack .../60-libnvidia-ifr1-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking libnvidia-ifr1-450:amd64 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package nvidia-compute-utils-450.\r\nPreparing to unpack .../61-nvidia-compute-utils-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking nvidia-compute-utils-450 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package nvidia-kernel-source-450.\r\nPreparing to unpack .../62-nvidia-kernel-source-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking nvidia-kernel-source-450 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package nvidia-kernel-common-450.\r\nPreparing to unpack .../63-nvidia-kernel-common-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking nvidia-kernel-common-450 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package nvidia-dkms-450.\r\nPreparing to unpack .../64-nvidia-dkms-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking nvidia-dkms-450 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package nvidia-utils-450.\r\nPreparing to unpack .../65-nvidia-utils-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking nvidia-utils-450 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package libnvidia-cfg1-450:amd64.\r\nPreparing to unpack .../66-libnvidia-cfg1-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking libnvidia-cfg1-450:amd64 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package xserver-xorg-video-nvidia-450.\r\nPreparing to unpack .../67-xserver-xorg-video-nvidia-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking xserver-xorg-video-nvidia-450 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package nvidia-driver-450.\r\nPreparing to unpack .../68-nvidia-driver-450_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking nvidia-driver-450 (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package nvidia-modprobe.\r\nPreparing to unpack .../69-nvidia-modprobe_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking nvidia-modprobe (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package nvidia-settings.\r\nPreparing to unpack .../70-nvidia-settings_450.36.06-0ubuntu1_amd64.deb ...\r\nUnpacking nvidia-settings (450.36.06-0ubuntu1) ...\r\nSelecting previously unselected package cuda-drivers-450.\r\nPreparing to unpack .../71-cuda-drivers-450_450.36.06-1_amd64.deb ...\r\nUnpacking cuda-drivers-450 (450.36.06-1) ...\r\nSelecting previously unselected package cuda-drivers.\r\nPreparing to unpack .../72-cuda-drivers_450.36.06-1_amd64.deb ...\r\nUnpacking cuda-drivers (450.36.06-1) ...\r\nSelecting previously unselected package cuda-libraries-10-1.\r\nPreparing to unpack .../73-cuda-libraries-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-libraries-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-runtime-10-1.\r\nPreparing to unpack .../74-cuda-runtime-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-runtime-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-demo-suite-10-1.\r\nPreparing to unpack .../75-cuda-demo-suite-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-demo-suite-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package cuda-10-1.\r\nPreparing to unpack .../76-cuda-10-1_10.1.243-1_amd64.deb ...\r\nUnpacking cuda-10-1 (10.1.243-1) ...\r\nSelecting previously unselected package libcudnn7.\r\nPreparing to unpack .../77-libcudnn7_7.6.4.38-1+cuda10.1_amd64.deb ...\r\nUnpacking libcudnn7 (7.6.4.38-1+cuda10.1) ...\r\nSelecting previously unselected package libcudnn7-dev.\r\nPreparing to unpack .../78-libcudnn7-dev_7.6.4.38-1+cuda10.1_amd64.deb ...\r\nUnpacking libcudnn7-dev (7.6.4.38-1+cuda10.1) ...\r\nErrors were encountered while processing:\r\n /tmp/apt-dpkg-install-fjAi3S/55-libnvidia-compute-450_450.36.06-0ubuntu1_amd64.deb\r\nE: Sub-process /usr/bin/dpkg returned an error code (1)\r\n```"},
{"text": "**System information**\r\n- Have I written custom code: NO\r\n- TensorFlow installed from: pip (tf-nightly)\r\n- TensorFlow version: 2.3.0-dev20200605\r\n\r\n**Describe the current behavior**\r\nTPU won't initialize in colab using the nightly build.\r\n\r\n**Describe the expected behavior**\r\nRun the https://www.tensorflow.org/guide/tpu notebook successfully, as one would if using TF 2.2.\r\n\r\n**Standalone code to reproduce the issue**\r\n1. Load https://www.tensorflow.org/guide/tpu in colab.\r\n2. Run `!pip install tf-nightly` in a new cell before running anything else.\r\n3. Run the TPU initialization section of the notebook.\r\n4. Observe the following error:\r\n\r\n```InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}\r\n```\r\n\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/setdiff1d\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the \"Args\" section, there is an input `out_idx`, but it is not in the signature, and the function doesn't accept the argument.\r\n\r\nRunning code: \r\n\r\n~~~python\r\ntf.compat.v1.setdiff1d([1],[1],out_idx=tf.dtypes.int32, name=None)\r\n~~~\r\n\r\ngot exception:\r\n\r\n~~~python\r\nTypeError: setdiff1d() got an unexpected keyword argument 'out_idx'\r\n~~~\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nNo\r\n\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2"},
{"text": "I'm reading this tutorial page from the documentation: https://www.tensorflow.org/tutorials/text/text_generation\r\n\r\nThe GRU layer is stateful so it remembers its state between batches. But the batches are shuffled. Therefore I think the stateful parameter should be `False`. "},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/hessians\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the \"Args\" section, there is an input `colocate_gradients_with_ops`, but it is not in the signature, and the function doesn't accept the argument.\r\n\r\nRunning code:\r\n\r\n~~~python\r\ntf.hessians(1,1,colocate_gradients_with_ops=None)\r\n~~~\r\n\r\ngot exception:\r\n\r\n~~~python\r\nTypeError: HessiansV2() got an unexpected keyword argument 'colocate_gradients_with_ops'\r\n~~~\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nYes\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/maximum#returns\r\n\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe example of tf.math.maximum is not written correctly.\r\nIt's written in this manner,\r\nExample: \r\n``` x = tf.constant([0., 0., 0., 0.]) y = tf.constant([-2., 0., 2., 5.]) tf.math.maximum(x, y) ```\r\nInstead of this it should've written in this manner,\r\n``` \r\n    x = tf.constant([0., 0., 0., 0.])\r\n    y = tf.constant([-2., 0., 2., 5.])\r\n    tf.math.maximum(x, y)\r\n    -> tf.Tensor([0. 0. 2. 5.], shape=(4,), dtype=float32)\r\n```    \r\n\r\n\r\n\r\n\r\n\r\n### Submit a pull request?\r\nno"},
{"text": "\r\n## URL(s) with the issue: https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/optimizers/RMSprop\r\n\r\n## Description of issue (what needs changing): the sub-indexes in the formula are not displayed correctly.\r\n\r\n### Clear description\r\nAs an example, meansquared_{t} appears like mean_{s}quaredt and so on.\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/lite/guide/build_rpi#compile_natively_on_raspberry_pi\r\n\r\n## Description of issue (what needs changing):\r\nGuide on natively compiling says \"tested on Raspberry Pi Zero\", but following the instructions on a Raspberry Pi Zero leads to a build for armv7l, not armv6 as specified.\r\nAdding TARGET_ARCH=armv6 to the command would likely work, as has been done in the cross-compile section to separate newer models from the RPi 1 / Zero. However, since me following these cross-compiling instructions resulted in a armv7l target as well (hard-float VFP ABI linking errors on Zero), I directly followed the tips from #30181 to be on the safe side."},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe description of the keras `Model.fit` function is either ambiguous, or incorrect, regarding the accepted datatypes for the parameter `validation_data`. Specifically, some data types *are* accepted, even when the documentation states that they are not. For example, the datatype `keras.utils.Sequence` *is* accepted as a possible datatype, and (as far as I can tell) behaves as one would expect. \r\n\r\nAs far as I can tell, this is primarily true when the user passes a generator/Sequence as `x`. In this case, the function `Model.fit` dispatches to the (deprecated) function `Model.fit_generator`, which *does* accept a generator or Sequence for the `validation_data` parameter.\r\n\r\nThis documentation should be corrected to unambiguously state one of the following: \r\n\r\n- *exactly* the list of datatypes that are accepted (e.g. numpy arrays, lists, pandas dataframes, etc). Note, this may require more work in order to fully test this set of datatypes. \r\n- *an approximation* of the list of datatypes that are accepted, with a cavaeat that some may be untested/only sometimes valid\r\n\r\nIf the types accepted are dependent on the type of `x`, then this should also be documented. \r\n\r\n### Submit a pull request?\r\n\r\nIf necessary, I am happy to open a PR, however I think that given this is clear user-facing code, and the primary interface for most tensorflow users it woudl be best to have this fix spearheaded by an internal developer."},
{"text": "[https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/preprocess_input](url)\r\nThe documentation says in the 'Returns' paragraph that\r\n> The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling. \r\n\r\nHowever, according to [the function definition](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/applications/resnet_v2.py#L125-L139) it's not true anymore  since the **mode parameter** can't be set and it is always equal to **'tf'**.\r\n\r\nTherefore, this docs part must be corrected to \r\n>will scale pixels between -1 and 1,  sample-wise\r\n\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/agents/tutorials/0_intro_rl\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe equation after \"The optimal Q-function obeys the following Bellman optimality equation:\" is not rendering correctly. This is what I see on my screen:\r\n\r\n```\r\n$\\begin{equation} Q^(s, a) = \\mathbb{E}\\left[ r + \\gamma \\max_{a'} Q^(s', a')\\right] \\end{equation}$\r\n```\r\n"},
{"text": "#38076  URL(s) with the issue:\r\nhttps://www.tensorflow.org/resources/tools#\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/resources/tools#\r\n\r\n## Description of issue (what needs changing):\r\nLink to what-if-tool is broken, as what-if moved to new repo.\r\n\r\n### Clear description\r\nCurrent broken link, when someone clicks on \"Get Started\" link at \r\nhttps://www.tensorflow.org/resources/tools\r\n\r\nBroken link ->\r\nhttps://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/interactive_inference/What_If_Tool_Notebook_Usage.ipynb\r\n\r\nNew Correct link should be ->\r\nhttps://github.com/PAIR-code/what-if-tool/blob/master/What_If_Tool_Notebook_Usage.ipynb\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\nhttps://github.com/PAIR-code/what-if-tool/blob/master/What_If_Tool_Notebook_Usage.ipynb\r\n\r\nI wanted to submit a pull-request for this\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/profiler#install_the_profiler_and_gpu_prerequisites\r\n\r\n## Description of issue (what needs changing):\r\nThe documentation says to do `ldconfig -p | grep libcupti` to check that CUPTI exists on the path, and to do `export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH` to fix it if it is not on the path.  However, the documentation can be misleading in situations where an old install of CUDA 10.0 has been replaced with 10.1 (at least on my installation).\r\n\r\n\r\nMy output when checking the path is as below:\r\n```console\r\ntyler@lambda2:/usr/local/cuda/bin$ ldconfig -p | grep libcupti\r\n\tlibcupti.so.10.0 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcupti.so.10.0\r\n\tlibcupti.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcupti.so\r\n```\r\n\r\nReading the documentation, this suggested to me that I did indeed have a version of libcupti on the path, and that everything should work. However, when I trained my model with the profiler on I saw the following error logs in the console.\r\n\r\n```\r\n2020-05-13 15:49:23.364143: I tensorflow/core/profiler/lib/profiler_session.cc:163] Profiler session started.\r\n2020-05-13 15:49:23.364212: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs\r\n2020-05-13 15:49:23.364588: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64\r\n2020-05-13 15:49:23.364606: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n```\r\n\r\nAfter double checking that I had CUDA 10.1 installed and not 10.2, I did the below\r\n```console\r\ntyler@lambda2:~/$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\r\ntyler@lambda2:~/$ echo $LD_LIBRARY_PATH\r\n/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n```\r\n\r\nThis then allows the profiler to load CUPTI\r\n```\r\n2020-05-13 18:18:51.560268: I tensorflow/core/profiler/lib/profiler_session.cc:163] Profiler session started.\r\n2020-05-13 18:18:51.560338: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs\r\n2020-05-13 18:18:51.561266: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\r\n```\r\n\r\nHowever, rerunning the command from the documentation for checking that CUPTI is on the path gives the same output as before\r\n\r\n```console\r\ntyler@lambda2:~/$ ldconfig -p | grep libcupti\r\n\tlibcupti.so.10.0 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcupti.so.10.0\r\n\tlibcupti.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcupti.so\r\n```\r\n\r\n### Desired fixes\r\nAfter updating my path, I would expect that `ldconfig -p | grep libcupti` would update to show that `usr/local/cuda/extras/CUPTI/lib64` with version 10.1 is available. \r\n\r\nAdditionally, I believe the documentation should explicitly state that running `ldconfig -p | grep libcupti` should show `libcupti.so.10.1` or greater\r\n\r\n\r\n### Submit a pull request?\r\n\r\nNo, I'm not sure of what the best way to check for 10.1 or 10.2 would be\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/rfft\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe table of `Args` in the documentation includes `Tcomplex`:\r\n\r\n```\r\nTcomplex | An optional\u00a0tf.DType\u00a0from:\u00a0tf.complex64,\u00a0tf.complex128. Defaults to\u00a0tf.complex64.\r\n```\r\n\r\nBut the function does not accept this argument. Calling `tf.signal.rfft(..., Tcomplex=...)` results in the error:\r\n\r\n```\r\nTypeError: _rfft() got an unexpected keyword argument 'Tcomplex'\r\n```\r\n\r\nThis makes sense given the signature in the documentation:\r\n\r\n```\r\ntf.signal.rfft(\r\n    input_tensor, fft_length=None, name=None\r\n)\r\n```\r\n\r\nand https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/signal/fft_ops.py#L114-L140\r\n\r\n### Submit a pull request?\r\n\r\nNo. I could not find where this table was generated in the code."},
{"text": "[Here](https://github.com/tensorflow/tensorflow/blob/fedc6d951faa73936a1154d6507d54240614d416/tensorflow/python/eager/backprop.py#L532) a minor typo in the source code: **rturns** ==> **returns**\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\nhttps://codelabs.developers.google.com/codelabs/digit-classifier-tflite/index.html?index=..%2F..index#7\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThere is an error in the code of step 4 of the codelabs.\r\nget a null safety error when implementing the code\r\n\r\n"},
{"text": "Could you add the website link in the following url\uff1a\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/contrib/quantize\r\n\r\n![image](https://user-images.githubusercontent.com/35229624/80811968-97e97b00-8bf9-11ea-9752-8b65e886e8d5.png)\r\nthe link marked in blue is 404 not found.\r\n\r\nThank you!\r\n\r\n"},
{"text": "The docstring says \" it is a negative quantity between -1 and 0, where 0 indicates orthogonality and values closer to -1 indicate greater similarity\". Although it is true that the function reverses the sign of the classic cosine similarity so that -1 will denote \"similarity\" instead of 1 in the original formula, the actual range is still -1 to 1 (not -1 to 0 as misleading by the docstring). \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/losses.py#L1073"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/gpu\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe tf-nightly pip package does not support GPU. It should be: tf-nightly-gpu\r\n\r\nOn the Windows Setup section, the path:\r\n\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\libx64;%PATH%\r\n\r\nshould be:\r\n\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64;%PATH% \r\n\r\nSince the CUDA toolkit generates this path with lib64 and not libx64.\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://github.com/tensorflow/docs/edit/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb\r\n\r\n## Description of issue (what needs changing):\r\nWhen rendered, the literal \\_\\_init\\_\\_ is replaced with init\r\n### Clear description\r\n\\_\\_init\\_\\_ is a \"built-in\" python function for classes.  In the ipynb source code it is correct.  However, when rendered at: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras , it is incorrect.\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\nYes, please see the notebook in the docs.  The text is:\r\n```\r\nNote: TF_CONFIG is parsed and TensorFlow's GRPC servers are started at the time MultiWorkerMirroredStrategy.init() is called, so TF_CONFIG environment variable must be set before a tf.distribute.Strategy instance is created.\r\n```\r\nvs\r\n```\r\nNote: TF_CONFIG is parsed and TensorFlow's GRPC servers are started at the time MultiWorkerMirroredStrategy.__init__() is called, so TF_CONFIG environment variable must be set before a tf.distribute.Strategy instance is created.\r\n```\r\nNotice that .init() is shown when .\\_\\_init\\_\\_() should be shown.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\nNo:  I didn't know how to escape the underscores, but I believe it is as in this report using a backslash before each underscore.\r\n"},
{"text": "## URL with the issue: https://www.tensorflow.org/lite/examples\r\n\r\n## Description of issue:\r\nThe digit classifier example card has the wrong description : \r\n\"Generate reply suggestions to input conversational chat messages.\"\r\n\r\n## Screenshot\r\n\r\n![image](https://user-images.githubusercontent.com/23613193/80188049-4441c500-862e-11ea-998d-b66c246de3d2.png)\r\n"},
{"text": "ksizes should be sizes on:\r\nhttps://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/ops/array_ops.py#L4806\r\n\r\nalso on lines 4805 and 4835 the call needs updating to\r\n\r\n`tf.image.extract_patches`\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/ops/array_ops.py#L4805\r\n\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/ops/array_ops.py#L4835\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/experimental/CosineDecay\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/experimental/CosineDecayRestarts\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/experimental/LinearCosineDecay\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/experimental/NoisyLinearCosineDecay\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/InverseTimeDecay\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PiecewiseConstantDecay\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay\r\n\r\nMay be present on others, but I'm not going to scour the web site looking.\r\n\r\n## Description of issue (what needs changing):\r\nThe general documentation of these classes does not show up on the web site. This leads to confusion as many of them have argument documentation containing statements such as *\"See the decay computation above\"*, when there is no documented computation above.\r\n\r\nInstead you have to open the source code to see the documentation that is mentioned.\r\n\r\nHere's an example:\r\n![image](https://user-images.githubusercontent.com/1826947/79771096-4f2dea00-82fc-11ea-8373-ec3572b00f3b.png)\r\nAnd here's the missing documentation:\r\n![image](https://user-images.githubusercontent.com/1826947/79771213-77b5e400-82fc-11ea-9d0d-2dc9efc02232.png)\r\n"},
{"text": "The examples in the documentations of `tf.linalg.tensor_diag_part` and `tf.linalg.tensor_diag`\r\nare showing the non-tensor version of these functions, e.g.\r\n```\r\n# 'diagonal' is [1, 2, 3, 4]\r\ntf.diag(diagonal) ==> [[1, 0, 0, 0]\r\n                       [0, 2, 0, 0]\r\n                       [0, 0, 3, 0]\r\n                       [0, 0, 0, 4]]\r\n```\r\n\r\nSee \r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/tensor_diag\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/tensor_diag_part"},
{"text": "In this doc https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl the math is messed up\r\n\r\nsee https://snipboard.io/my0uSb.jpg\r\n\r\nusing chrome browser"},
{"text": "## URL(s) with the issue: \r\nhttps://www.tensorflow.org/api_docs/python/tf/saved_model/tag_constants\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tfx/serving/serving_basic#train_and_export_tensorflow_model\r\n\r\n## Description of issue (what needs changing):\r\nIn the explanation corresponding to **`tags`** in [this link](https://www.tensorflow.org/tfx/serving/serving_basic#train_and_export_tensorflow_model), the Hyperlink corresponding to the Text, related [TensorFlow API documentation](https://www.tensorflow.org/api_docs/python/tf/saved_model/tag_constants) is broken.\r\n\r\n### Clear description\r\n\r\nThis link is useful for the community to understand the purpose of different Tags used while Saving a Model.\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? : Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : N/A"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/text_classification_with_hub#build_the_model\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/text_classification_with_hub#build_the_model\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe last layer in the model is `model.add(tf.keras.layers.Dense(1))`. However, in the following description, it says `The last layer is densely connected with a single output node. Using the sigmoid activation function, ...`\r\n\r\nI check the api doc and find that the default activation is none for dense layer.\r\n\r\n- Without `activation='sigmoid'`, the predictions are not in the range of (0, 1) as shown below, which is not interpretable.\r\n\r\n```\r\npred = model.predict(test_data.batch(512))\r\nprint(pred)\r\n\r\n[[-0.29496038]\r\n [ 1.2088487 ]\r\n [ 0.11580676]\r\n ...\r\n [-1.610341  ]\r\n [-0.8496179 ]\r\n [ 1.3117154 ]]\r\n```\r\n\r\nSo shall the example code be `model.add(tf.keras.layers.Dense(1, activation='sigmoid'))`?"},
{"text": "I've found a little mistake in the documentation. On the following website, the order of y_true and y_pred are reversed:\r\n\r\nhttps://www.tensorflow.org/tutorials/customization/custom_training\r\n\r\n```\r\ndef loss(predicted_y, target_y):\r\n  return tf.reduce_mean(tf.square(predicted_y - target_y))\r\n```\r\n\r\nIt's usually the other way around:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/\r\n\r\n```\r\nkeras.losses.mean_squared_error(y_true, y_pred)\r\n```\r\nIt makes no difference for MSE since this loss is symmetric. It does make a difference for MMSE (Masked MSE) where random values of the target are mapped to zero.\r\n"},
{"text": "in [tensorflow.org](https://www.tensorflow.org/community) website the below roadmap link is  not working\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/cosine_similarity\r\n\r\n## Description of issue (what needs changing):\r\nDocumentation states that tf.keras.losses.cosine_similarity() \"is a negative quantity between -1 and 0, where 0 indicates orthogonality and values closer to -1 indicate greater similarity.\"\r\nBut it is actually not true. tf.keras.losses.cosine_similarity() can return positive values.\r\n\r\n### Usage example\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.keras.losses.cosine_similarity([[1., 1.]], [[-1., -1.]])\r\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.99999994], dtype=float32)>\r\n```\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nOn the third line of the second code box, origin probability is [.9, .05, .05], [.5, .89, .6], [.05, .01, .94], it should be [.9, .05, .05], [.05, .89, .06], [.05, .01, .94]. "},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started.md#validate-input-shape\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nAt following lines of the second code of [this section](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started.md#validate-input-shape), it is claimed that input is a 2D tensor.\r\n\r\n```c++\r\n// The property \"dims\" tells us the tensor's shape. It has one element for\r\n// each dimension. Our input is a 2D tensor containing 1 element, so \"dims\"\r\n// should have size 2.\r\nTF_LITE_MICRO_EXPECT_EQ(2, input->dims->size);\r\n```\r\n\r\nHowever, based on [the notebook where the model defined](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb), it should have 1D tensor for input.\r\n\r\n### Submit a pull request?\r\n\r\nNo."},
{"text": "## URL(S) with issue:\r\nhttps://github.com/tensorflow/federated/blob/master/docs/federated_core.md\r\nIn the above readme file [MapsReduce](https://research.google/pubs/pub62.pdf/)\r\n\r\n### Pull Request\r\nI've corrected the link and by successfully merging [#813](https://github.com/tensorflow/federated/pull/813) it'll resolve this issue.\r\nHey, @mihaimaruseac Kindly review it."},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/upgrade#recommended_upgrade_process\r\n\r\n\r\n## Description of issue (what needs changing):\r\nthere is two consecutive \"will\" in the point `Run the upgrade script`. \r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? \r\nYes. I'll be opening a PR soon."},
{"text": "\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/federated/blob/master/docs/deployment.md\r\n```tf.backends``` link is missing .\r\n##Pull request\r\n[#812](https://github.com/tensorflow/federated/pull/812) by sucessfully merging this PR will closw this issue.\r\n\r\n\r\n"},
{"text": "\r\n## URL(s) with the issue:\r\nIn this repo the ```tf.framework.Executor``` link is broken.\r\n### Pull Request\r\nSuccessfully merging of [#811](https://github.com/tensorflow/federated/pull/811) will close this issue.\r\nHey, @MarkDaoust , @lamberta, and @mihaimaruseac would you please review this pull request."},
{"text": "\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/community/forums.md\r\n\r\n## Description of the issue (what needs changing):\r\nIn forums.ms  [how to get help](https://github.com/tensorflow/docs/blob/master/community/#get_help) is broken and while clicking it's showing 404 error.\r\n\r\n### submit a pull request\r\nHey, @MarkDaoust and @lamberta would you please assign me this issue and please provide details to fix this issue I'll love to fix this issue."},
{"text": "The codelab in question is [Build a handwritten digit classifier app with TensorFlow Lite\r\n](https://codelabs.developers.google.com/codelabs/digit-classifier-tflite/index.html?index=..%2F..index#3).\r\n\r\nStep 4.6 introduces the following snippet of code:\r\n\r\n```kotlin\r\n// Read input shape from model file\r\nval inputShape = interpreter.getInputTensor(0).shape()\r\ninputImageWidth = inputShape[1]\r\ninputImageHeight = inputShape[2]\r\nmodelInputSize = FLOAT_TYPE_SIZE * inputImageWidth * inputImageHeight * PIXEL_SIZE\r\n\r\n// Finish interpreter initialization\r\nthis.interpreter = interpreter\r\n```\r\n\r\n`interpreter` is a class field that has not been initialized in any previous code snippets and the code fails to compile.\r\n\r\nLooking at the finalized code in the `finish` directory of the downloadable sample, 4.6 should've probably included this code snippet to initialize `interpreter`:\r\n\r\n```kotlin\r\n// Initialize TF Lite Interpreter with NNAPI enabled.\r\nval options = Interpreter.Options()\r\noptions.setUseNNAPI(true)\r\nval interpreter = Interpreter(model, options)\r\n```"},
{"text": "Hello tf team. I was trying the embedding tutorial as mentioned on the tf docs webpage.\r\n[https://www.tensorflow.org/tutorials/text/word_embeddings](https://www.tensorflow.org/tutorials/text/word_embeddings)\r\n\r\nThe below two lines do give me error while I am following the same documentation as mentioned in the webpage.\r\n\r\ntrain_batches = train_data.shuffle(1000).padded_batch(10)\r\ntest_batches = test_data.shuffle(1000).padded_batch(10)\r\n\r\n![image](https://user-images.githubusercontent.com/47158509/77058288-3e3b4180-69fb-11ea-932a-df913d43c385.png)\r\n\r\nI have manually tried to fix the error by putting padded_shapes as [None] or [None, None] but both of them have thrown error.\r\n\r\n"},
{"text": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add\r\n\r\n## Description of issue (what needs changing):\r\nThese sentences: \r\n> `indices` is an integer tensor containing indices into a new tensor of shape `shape`. The last dimension of `indices` can be at most the rank of `shape`:\r\n\r\nSeem to be direct copy-paste from the docs of [`scatter_nd`](https://www.tensorflow.org/api_docs/python/tf/scatter_nd). However, there is no `shape` args in `scatter_nd_add`.\r\n\r\n### Clear description\r\n\r\nThe docs should instead refer to `tensor.shape`.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? There is no link (that's another issue though I guess), only to the tf v1\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? Yes \r\n\r\n### Returns defined\r\n\r\nAre return values defined? Yes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? No errors raised apparently\r\n\r\n### Usage example\r\n\r\nIs there a usage example? Yes\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content? No, but there are some in `scatter_nd`, I guess it's enough\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n\r\nNot right now\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nThe issue affects many pages, here is one example:\r\n- TF 2.2: https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay\r\n- TF 2.1: https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay\r\n- TF 2.0: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay\r\n\r\n## Description of issue (what needs changing):\r\nThe generated docs for TF 2.1 and TF 2.2 is missing important information. Notably, the whole documentation of `__init__` method containing information like detailed learning rate computation\r\n```python\r\ndef decayed_learning_rate(step):\r\n  return initial_learning_rate * decay_rate ^ (step / decay_steps)\r\n```\r\nis missing in TF 2.1 and TF 2.2. However, the information is still present in the source, see \r\nhttps://github.com/tensorflow/tensorflow/blob/3c1e8c03419266bb6ba379d303d3e03a380617a8/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py#L64-L134\r\n\r\n## Further examples\r\nFor example Adam optimizer is also affected, see\r\n- TF 2.2: https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/optimizers/Adam\r\n- TF 2.0: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/Adam\r\nThe TF 2.0 version contains a lot of math describing how Adam works, which is not present in TF 2.2 docs."},
{"text": "## URL(s) with the issue:  \r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/index.md\r\n## Description of the issue (what needs changing):\r\nThere are lots of links are broken, some of them are:\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf/dtypes/DType\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf/data/experimental/get_structure\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf/debugging/assert_same_float_dtype\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf/estimator/ModeKeys\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf/fill\r\n\r\n### Clear description\r\nThere are lots of the link is broken in r2.0/site/en/api_docs/python/index.md. when someone clicks on these links it's showing 404 ERROR.\r\n\r\n## Solution \r\nWhen I checked  https://www.tensorflow.org/versions/r2.0/api_docs/python . I found out every links\r\nare correct. For example\r\n* tf.DType link is broken in Github but working fine in TensorFlow website\r\nAs the numbers of broken links are big. I'll suggest adding a message like\r\n\" Our TensorFlow 2.0 RC docs is moved to https://www.tensorflow.org/versions/r2.0/api_docs/python. Kindly checkout there\"\r\nI have seen this type of message in some of the TensorFlow docs.\r\n## Pull Request\r\nHey, @dynamicwebpaige , @lamberta, @MarkDaoust  . Please assign me for doing this. I'll love to address this issue.\r\n"},
{"text": "\r\n\r\n## URL(s) with the issue:\r\nREADME.md  Resources's roadmap link is broken.\r\nhttps://www.tensorflow.org/community/roadmap\r\n\r\n## Description of issue (what needs changing):\r\nIt is occurred Page not found errer.\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\na link to the documentation entry\r\nhttps://tensorflow.google.cn/tutorials/estimator/boosted_trees_model_understanding\r\n\r\n## Description of issue (what needs changing):\r\nA bug I found in \"TensorFlow > Learn > TensorFlow Core > Tutorials > Gradient Boosted Trees: Model understanding\" as below:\r\n\r\nmatplotlib.mlab has been removed @ version 3.1.0 \"https://matplotlib.org/3.1.0/api/api_changes.html\"\r\nbut the tutorials still uses _griddata_ class for plotting, i can not get the result follow this.\r\n\r\nso would you mind adjust this for correct code? i am a beginner for tensorflow and python, so fix it by myself is difficult for me ,thanks \r\n\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue / ## Description of issue (what needs changing):\r\n\r\nInside the Friction Log Document: https://docs.google.com/document/d/1HVG3t-mgGZKU4iMeguTWGejbnQ54qUTXwdCFkA5xHG0/edit\r\n\r\nThere is a broken link to the \"Bug / Performance\" template:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/.github/ISSUE_TEMPLATE/00-bug-performance-issue.md\r\n\r\n### Correct links\r\n\r\nI see there are two templates - \r\n**Performance:** https://github.com/tensorflow/tensorflow/issues/new?labels=type%3Aperformance&template=80-performance-issue.md\r\n\r\n**Bug**: https://github.com/tensorflow/tensorflow/issues/new?labels=type%3Abug&template=00-bug-issue.md\r\n\r\nThe friction log should update these links in the google doc, \r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/Module\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI extract the example codes, run it and got Error. It seems caused by inconsistant kwargs name:\r\n\r\n```python\r\n class Dense(tf.Module):\r\n   def __init__(self, in_features, output_features, name=None):\r\n     super(Dense, self).__init__(name=name)\r\n     self.w = tf.Variable(\r\n         tf.random.normal([input_features, output_features]), name='w')\r\n     self.b = tf.Variable(tf.zeros([output_features]), name='b')\r\n\r\n   def __call__(self, x):\r\n     y = tf.matmul(x, self.w) + self.b\r\n     return tf.nn.relu(y)\r\n\r\n\r\nclass MLP(tf.Module):\r\n  def __init__(self, input_size, sizes, name=None):\r\n    super(MLP, self).__init__(name=name)\r\n    self.layers = []\r\n    with self.name_scope:\r\n      for size in sizes:\r\n        self.layers.append(Dense(input_size=input_size, output_size=size))\r\n        input_size = size\r\n\r\n  @tf.Module.with_name_scope\r\n  def __call__(self, x):\r\n    for layer in self.layers:\r\n      x = layer(x)\r\n    return x\r\n\r\nmlp = MLP(input_size=100, sizes=[30, 30])\r\n```\r\n\r\noutput:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-137-269f8996957a> in <module>()\r\n----> 1 mlp = MLP(input_size=100, sizes=[30, 30])\r\n\r\n<ipython-input-135-066c337c5b7a> in __init__(self, input_size, sizes, name)\r\n     17    with self.name_scope:\r\n     18      for size in sizes:\r\n---> 19        self.layers.append(Dense(input_size=input_size, output_size=size))\r\n     20        input_size = size\r\n     21 \r\n\r\nTypeError: __init__() got an unexpected keyword argument 'input_size'\r\n```\r\n\r\n\r\n### Submit a pull request?\r\n\r\n I think just need to modify several lines to fix it:\r\n\r\n```python\r\n class Dense(tf.Module):\r\n   def __init__(self, input_size, output_size, name=None):\r\n     super(Dense, self).__init__(name=name)\r\n     self.w = tf.Variable(\r\n         tf.random.normal([input_size, output_size]), name='w')\r\n     self.b = tf.Variable(tf.zeros([output_size]), name='b')\r\n\r\n   def __call__(self, x):\r\n     y = tf.matmul(x, self.w) + self.b\r\n     return tf.nn.relu(y)\r\n\r\n\r\nclass MLP(tf.Module):\r\n  def __init__(self, input_size, sizes, name=None):\r\n    super(MLP, self).__init__(name=name)\r\n    self.layers = []\r\n    with self.name_scope:\r\n      for size in sizes:\r\n        self.layers.append(Dense(input_size=input_size, output_size=size))\r\n        input_size = size\r\n\r\n  @tf.Module.with_name_scope\r\n  def __call__(self, x):\r\n    for layer in self.layers:\r\n      x = layer(x)\r\n    return x\r\n```\r\n\r\nShould I submit a PR to fix this?\r\n"},
{"text": "The tutorial example for tf.keras.experimental.WideDeepModel instantiated the class with first input augment: dnn_model, and second augment: linear_model as shown in :\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/experimental/WideDeepModel#example_4\r\n\r\nHowever, the class should be instantiated by linear_model then dnn_model as shown in:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/premade/wide_deep.py#L72\r\n\r\nPlease update the tutorial example.\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes\r\n\r\n## Description of issue (what needs changing):\r\nThe documentation for the `shapes` argument states that is expects a dict, but in fact in many circumstances it is not possible to assemble such a dict because eager tensors are unhashable.\r\nSo while it is possible to submit a dict here in GraphMode, Eager expects a list of (key, value) pairs.\r\n\r\nIt should also state whether `tf.Variable`s can be used as keys.\r\n\r\n### Usage example\r\nThe usage example seems to confuse the two concepts, and provides a mixup of both in invalid python syntax:\r\n```\r\ntf.assert_shapes([\r\n  (x: ('N', 'Q')),\r\n  (y: ('N', 'D')),\r\n  (param: ('Q',)),\r\n  (scalar: ()),\r\n])\r\n```\r\n*This seems to be fixed already in master*\r\n\r\n\r\nFurther, `tf.assert_shapes` should probably be changed to `tf.debugging.assert_shapes`\r\n\r\n### Submit a pull request?\r\nYes"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Nadam\r\n\r\n## Description of issue (what needs changing):\r\nOptional name attribute is said to default as \"Adamax\" rather than \"Nadam\"\r\n\r\n### Clear description\r\n\r\nname: Optional name for the operations created when applying gradients. Defaults to \"Adamax\".\r\n\r\n### Correct links\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/optimizer_v2/nadam.py#L33-L238\r\n\r\nSource code correctly shows default as name=\"Nadam\". Therefore documentation issue.\r\n\r\n### Submit a pull request?\r\n\r\nNot at the moment"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/encode_png\r\n\r\n## Description of the issue (what needs changing):\r\n\r\nCurrently, all Image decoding and encoding functions are a part of the `tf.io` module but `encode_png` function is still a part of the `tf.image` module.\r\n\r\n## Changes required\r\n\r\nChange `tf.image.encode_png`  to `tf.io.encode_png`\r\n\r\n\r\n### Submit a pull request?\r\nI will be happy to help.\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\r\n\r\n## Description of issue (what needs changing):\r\nTypo was reintroduced in v2.1\r\nSee https://github.com/tensorflow/tensorflow/issues/34809 \r\n\r\nmust be \r\n\r\n```\r\nvalue_embeddings = token_embedding(value_input)\r\n```\r\n\r\n@rabitt"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/xlog1py\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThis documentation describes `tf.math.xlog1py`, shown as a part of \"stable\" version of TF 2.1 but I believe this API has not been released yet. `tf.math.xlog1py` is only available on nightly at this point, added in https://github.com/tensorflow/tensorflow/commit/19986377f2a3c560418f42f2323733564a7303eb (Jan 2020).\r\n\r\nFYI: I ran into this issue as I was using `tfp-nightly` which depends on `tf-nightly` (module 'tensorflow_core._api.v2.math' has no attribute 'xlog1py').\r\n\r\nThe documentation should have not published under \"TensorFlow Core v2.1.0\". Why was it the case? If it was due to a mistake, could we improve on the process so we can have a \"nightly\" doc and a \"stable\" doc?"},
{"text": "## URL(s) with the issue:\r\nhttps://keras.io/models/model/\r\nand \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#predict\r\n\r\n## Description of issue (what needs changing):\r\nDocumentation for model predict says \"batch_size: Integer or None. Number of samples per gradient update.\", but unless I'm missing something, there are no gradient updates while predicting.\r\n\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/profiler/Profiler?hl=zh-TW\r\n\r\n## Description of issue (what needs changing):\r\nThe README link for this function is 404, so there is no support for usage. The sample code is not complete either.\r\n\r\n### Clear description\r\n![image](https://user-images.githubusercontent.com/33815430/75243362-b64f8680-5804-11ea-8bf6-2888306b1a38.png)\r\nFor example\r\n```python\r\nprofiler.profile_name_scope(options=(option_builder.ProfileOptionBuilder\r\n          .trainable_variables_parameter()))\r\n```\r\nNo declarence of option_builder. It's hard for me to reproduce the result by this sample code.\r\nCould tensorflow provide new support to this function??"},
{"text": "\r\n## URL(s) with the issue: \r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe first reference to \"optimizer configuration\" is unqualified.\r\nThe second reference to \"optimizers\" is kind of qualified with \"(from tf.train)\".\r\nDoes this mean, tf.keras.optimizer states are stored but not tf.optimizer or does this mean no optimizer states are stored?\r\n\r\nfull text as follows:\r\n\r\n```\r\nThis technique saves everything:\r\n\r\n- The weight values\r\n- The model's configuration(architecture)\r\n- The optimizer configuration\r\n\r\nKeras saves models by inspecting the architecture. Currently, it is not able to save TensorFlow optimizers (from tf.train). When using those you will need to re-compile the model after loading, and you will lose the state of the optimizer.\r\n```"},
{"text": "It is a documentation issue, but embedded in the code. This seems most appropriate issue template.\r\n\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/layers/dense_attention.py#L187-L313\r\n\r\n## Description of issue (what needs changing):\r\nThere is an error in the provided example (L236 - 276):\r\n\r\nHere is a code example for using `Attention` in a CNN+Attention network:\r\n  ```python\r\n  value_input = tf.keras.Input(shape=(None,), dtype='int32')\r\n  value_embeddings = token_embedding(query_input)\r\n  ```\r\n\r\nThis last one should be ```value_embeddings = token_embedding(value_input)```\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\nthe spelling of placeholder is incorrect in line 53\r\n\r\n### Clear description\r\nthe spelling in placehoolder and should be placeholder\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nMaybe.. If you accept the request \r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/text/text_generation#the_prediction_loop\r\n\r\n## Description of issue (what needs changing):\r\nConsider the previously generated letters for subsequent generations. Right now only the lastly generated letter is considered to generate the immediate next letter. The result reads very far from natural as you can imagine.\r\n\r\n### Clear description\r\n\r\nThe `generate_text()` method is supposed to generate text, letter by letter, by taking into account a starting string and any letter that has been generated so far. Right now, it's generating the first letter by taking into account the starting string, then from then on, it only considers the last generated letter. \r\n\r\nSo in this:\r\n```\r\ndef generate_text(model, start_string):\r\n  # Evaluation step (generating text using the learned model)\r\n\r\n  # Number of characters to generate\r\n  num_generate = 1000\r\n\r\n  # Converting our start string to numbers (vectorizing)\r\n  input_eval = [char2idx[s] for s in start_string]\r\n  input_eval = tf.expand_dims(input_eval, 0)\r\n\r\n  # Empty string to store our results\r\n  text_generated = []\r\n\r\n  # Low temperatures results in more predictable text.\r\n  # Higher temperatures results in more surprising text.\r\n  # Experiment to find the best setting.\r\n  temperature = 1.0\r\n\r\n  # Here batch size == 1\r\n  model.reset_states()\r\n  for i in range(num_generate):\r\n      predictions = model(input_eval)\r\n      # remove the batch dimension\r\n      predictions = tf.squeeze(predictions, 0)\r\n\r\n      # using a categorical distribution to predict the character returned by the model\r\n      predictions = predictions / temperature\r\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n\r\n      # We pass the predicted character as the next input to the model\r\n      # along with the previous hidden state\r\n      input_eval = tf.expand_dims([predicted_id], 0)\r\n\r\n      text_generated.append(idx2char[predicted_id])\r\n\r\n  return (start_string + ''.join(text_generated))\r\n```\r\n\r\nThe end of the for-loop needs to be updated to match the comment description, with something like this:\r\n```\r\n# We pass the predicted character as the next input to the model\r\n# along with the previous hidden state\r\ninput_eval = tf.concat([input_eval, tf.expand_dims([predicted_id], 0)], -1)\r\n```\r\nWhere we add the newly predicted ID to the end of what is currently `input_eval`\r\n\r\n### Submit a pull request?\r\n\r\nI plan on submitting a pull request with the quick fix.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\n[Software requirements](https://www.tensorflow.org/install/gpu#software_requirements)\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe requirement may lack the dependency of [Microsoft Visual C++ Redistributable for Visual Studio](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) runtime.\r\n\r\n### Clear description\r\n\r\nAfter installation, TF raises an error with `ImportError: DLL load failed: The specified module could not be found.`\r\n\r\nFixed after installing [Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019](https://aka.ms/vs/16/release/vc_redist.x64.exe) via [error page](https://www.tensorflow.org/install/errors) and issue https://github.com/tensorflow/tensorflow/issues/22794 & https://github.com/tensorflow/tensorflow/issues/22512\r\n\r\nWanna confirm whether Microsoft Visual C++ Redistributable for Visual Studio must be installed and its version. If true, please add this information to [software requirements](https://www.tensorflow.org/install/gpu#software_requirements).\r\n\r\n### Submit a pull request?\r\n\r\nIf confirmation, I can PR to [tensorflow/docs](https://github.com/tensorflow/docs/blob/master/site/en/install/gpu.md).\r\n\r\n---\r\nPS.. here comes system info to trace that issue.\r\nHardware:\r\n```\r\nProcessor: Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz (4 CPUs), ~2.9GHz\r\nMemory: 16384MB RAM\r\nCard name: NVIDIA GeForce 940MX\r\nManufacturer: NVIDIA\r\nChip type: GeForce 940MX\r\n```\r\nSoftware:\r\n```\r\nOperating System: Windows 10 Pro 64-bit (10.0, Build 18363) (18362.19h1_release.190318-1202) \r\n(conda)TensorFlow: 2.1.0 (install via pypi)\r\n(conda)cudatoolkit: 10.1.243 (install via anaconda)\r\n(conda)cudnn: 7.6.5 (install via anaconda)\r\nNvidia Driver Version: 441.87\r\n```\r\nIt can be reproduced in `conda install` or `native install`.\r\n\r\nHope those info helps.\r\n\r\nThanks in advance! :-)"},
{"text": "## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/cf7fcf164c9846502b21cebb7d3d5ccf6cb626e8/tensorflow/python/ops/signal/shape_ops.py#L55-L199\r\n\r\nDocumentation:\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/frame\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIn the following example, the count of frames generated with pad_end=False is incorrect\r\n    \r\n    # A batch size 3 tensor of 9152 audio samples.\r\n    audio = tf.random.normal([3, 9152])\r\n\r\n    # Compute overlapping frames of length 512 with a step of 180 (frames overlap\r\n    # by 332 samples). By default, only 50 frames are generated since the last\r\n    # 152 samples do not form a full frame.\r\n    frames = tf.signal.frame(audio, 512, 180)\r\n    frames.shape.assert_is_compatible_with([3, 50, 512])\r\n\r\nIn fact, only 49 frames are generated with pad_true=False (the default).\r\n\r\n### Clear description\r\n\r\nSuppose we are given a tensor x with x.shape[-1] == N, a frame_length == K, and a frame_step == k.\r\n\r\nTo compute tf.signal.frame(x, frame_length, frame_step) (here default axis=-1 and pad_end=False), we could equivalently stack along axis -2 the following list of slices:\r\n\r\n    [x[..., 0:K],\r\n     x[..., k:K+k],\r\n     x[..., 2*k:K+2*k],\r\n     x[..., 3*k:K+3*k],\r\n     ...\r\n     x[..., j*k:K+j*k]]\r\n\r\nwhere j is the maximum integer such that K+j*k <= N.\r\n\r\nWe can compute that j = (N - K) // k, so the number of slices in this list is j+1 = 1 + (N - K) // k. \r\n\r\nIn the example here, N = 9152, K=512, k=180, so:\r\n\r\n    j+1 = 1 + (9152-512) // 180 = 1 + 8640 // 180 = 1+48 = 49\r\n\r\nThus in the example given, the return shape should be [3, 49, 512], not [3, 50, 512]. Attached is a screenshot showing that this is indeed the behavior of tf.signal.frame, so it is an error in the documentation not in the code.\r\n\r\n![tf signal frame](https://user-images.githubusercontent.com/47697814/74045729-497e6480-499b-11ea-9107-12305554ee18.png)\r\n\r\n### Submit a pull request?\r\n\r\nI am planning to submit a pull request."},
{"text": "\r\nThe documentation for pre-made estimators such as tensorflow.estimator.LinearRegressor says that each of the methods evaluate, predict and train take a parameter called \"hooks\", which is a list of tensorflow.train.SessionRunHooks. However, no such class exist in the v2.1 API; instead, the source code says that it expects instances of tensorflow.compat.v1.train.SessionRunHook.\r\n\r\nIn the example of LinearRegressor (other pre-made estimators share the same problem), this is the API doc, which I believe is generated directly from the source code (see the documentation of the parameters for the train, evaluate and predict methods):\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#evaluate\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#predict\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#train\r\n\r\nThis is the source code that says it expects a class from the compat.v1 API:\r\nhttps://github.com/tensorflow/estimator/blob/a7ba3b45d07dd517a0e6ff38e90ae3aa240f424b/tensorflow_estimator/python/estimator/estimator.py#L1947\r\n\r\n\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nNumerous links of the older TF APIs, listing only a few index pages as examples here:\r\n\r\n- Links under \"Classes\" and \"Functions\" within https://github.com/tensorflow/docs/blob/r1.11/site/en/api_docs/python/tf.md\r\n- Links under \"Classes\" and \"Functions\" within https://github.com/tensorflow/docs/blob/r1.11/site/en/api_docs/python/tfdbg.md\r\n- Links within https://github.com/tensorflow/docs/blob/r1.5/site/en/api_docs/python/index.md\r\n- \"JAVA\" link within https://github.com/tensorflow/docs/blob/r1.7/site/en/api_docs/index.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\nA large amount of links on index pages (e.g. for functions and classes) are currently broken (404). This appears to affect only the older TF versions (multiple versions affected), whose index pages are Markdown files within Github repositories.\r\n\r\n### Clear description\r\n\r\nOutbound links from these indexing pages are currently broken. Some might be fixed by adding a .md after the current URL to point to the correct Markdown available in the repositories, but not all of them can be fixed through this way.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n- Incorrect.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue?\r\n- Currently no plan.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\n[`tf.keras.optimizers.Optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer), specifically the section [Write a customized optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#write_a_customized_optimizer_2).\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe instructions for creating a custom optimizer seem to be inconsistent with how `tf.keras.optimizers.Optimizer` subclasses are defined in TensorFlow and other projects.\r\n\r\n### Clear description\r\n\r\nThis originated as a [question on Stack Overflow](https://stackoverflow.com/q/58772846/1917160), which is reproduced below.\r\n\r\nSuppose I want to write a custom optimizer class that conforms to the `tf.keras` API (using TensorFlow version>=2.0). I am confused about the documented way to do this versus what's done in implementations.\r\n\r\nThe documentation for `tf.keras.optimizers.Optimizer` states,\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L217-L224\r\n\r\nHowever, the current `tf.keras.optimizers.Optimizer` implementation does not define a `resource_apply_dense` method, but it *does* define a private-looking [`_resource_apply_dense` method stub](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L916-L928). Similarly, there are no `resource_apply_sparse` or `create_slots` methods, but there are a [`_resource_apply_sparse` method stub](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L958-L977) and a [`_create_slots` method call](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L434).\r\n\r\nIn official `tf.keras.optimizers.Optimizer` subclasses (using `tf.keras.optimizers.Adam` as an example), there are [`_resource_apply_dense`](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/adam.py#L192-L227), [`_resource_apply_sparse`](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/adam.py#L229-L267), and [`_create_slots`](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/adam.py#L150-L159) methods, and there are no such methods without the leading underscore.\r\n\r\nThere are similar leading-underscore methods in slightly-less-official `tf.keras.optimizers.Optimizer` subclasses (e.g., `tfa.optimizers.MovingAverage` from TensorFlow Addons: [`_resource_apply_dense`](https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L73-L76), [`_resource_apply_sparse`](https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L78-L82), [`_create_slots`](https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/moving_average.py#L92-L95)).\r\n\r\nAnother confounding point for me is that some of the TensorFlow Addons optimizers *also* override the `apply_gradients` method (e.g., [`tfa.optimizers.MovingAverage`](https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L55-L57)), whereas the `tf.keras.optimizers` optimizers do not.\r\n\r\nMoreover, I noticed that the `apply_gradients` method of `tf.keras.optimizers.Optimizer` method calls `_create_slots`, but the base `tf.keras.optimizers.Optimizer` class does not have a `_create_slots` method. So, it seems that a `_create_slots` method *must* be defined in an optimizer subclass if that subclass does not override `apply_gradients`.\r\n\r\n#### Questions\r\n\r\nWhat is the correct way to subclass a `tf.keras.optimizers.Optimizer`? Specifically,\r\n\r\n1. Does the `tf.keras.optimizers.Optimizer` documentation listed at the top simply mean to override the leading-underscore versions of the methods they mention (e.g., `_resource_apply_dense` instead of `resource_apply_dense`)? If so, are there any API guarantees about these private-looking methods not changing their behavior in future versions of TensorFlow? What are the signatures of these methods?\r\n2. When would one override `apply_gradients` in addition to the `_apply_resource_[dense|sparse]` methods?"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/source?hl=en\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIt's said that tensorflow 2.1.0 is built by bazel 0.26.1, but when I use bazel 0.26.1 to build, I get the following error:\r\n```\r\nPlease upgrade your bazel installation to version 0.27.1 or higher to build TensorFlow!\r\n```\r\n\r\nSo, what is the correct bazel version?"},
{"text": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\r\n\r\n## Description of issue (what needs changing):\r\n\r\nCurrently, the page https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback says `validation_data: Deprecated. Do not use.`, but if I attempt to access the attribute `validation_data` inside the callback I get the error `AttributeError: 'MyCustomCallbackClass' object has no attribute 'validation_data'.`. You should change the documentation to remove the attribute `validation_data`, which apparently was removed.\r\n\r\nSee also this issue https://github.com/tensorflow/tensorflow/issues/27318."},
{"text": "**System information**\r\n- OS Platform and Distribution: Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): [binary](https://pypi.org/project/tensorflow/1.15.2/#files)\r\n- TensorFlow version: 1.15.2\r\n- Python version: python2, python3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n**Describe the problem**\r\n\r\nAs [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0) Tensorflow 1.15 contained GPU support by default. Tensorflow 1.15.2 no longer has GPU support.\r\n"},
{"text": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): terminal\r\n- TensorFlow version: tensorflow                         2.1.0              \r\n                                   tensorflow-estimator               2.1.0\r\n- Python version: 3.6.7\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nam done by following steps in this link https://www.tensorflow.org/tensorboard/get_started  but it gives an error message like that \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ntensorboard --logdir logs/fit\r\n\r\n\r\n**Any other info / logs**\r\n2020-01-29 15:42:56.124384: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-01-29 15:42:56.124520: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-01-29 15:42:56.124541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nTraceback (most recent call last):\r\n  File \"/home/kuppa/anaconda3/bin/tensorboard\", line 8, in <module>\r\n    sys.exit(run_main())\r\n  File \"/home/kuppa/anaconda3/lib/python3.6/site-packages/tensorboard/main.py\", line 59, in run_main\r\n    default.get_plugins() + default.get_dynamic_plugins(),\r\n  File \"/home/kuppa/anaconda3/lib/python3.6/site-packages/tensorboard/default.py\", line 115, in get_dynamic_plugins\r\n    for entry_point in pkg_resources.iter_entry_points('tensorboard_plugins')\r\n  File \"/home/kuppa/anaconda3/lib/python3.6/site-packages/tensorboard/default.py\", line 115, in <listcomp>\r\n    for entry_point in pkg_resources.iter_entry_points('tensorboard_plugins')\r\n  File \"/home/kuppa/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2443, in load\r\n    self.require(*args, **kwargs)\r\n  File \"/home/kuppa/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2466, in require\r\n    items = working_set.resolve(reqs, env, installer, extras=self.extras)\r\n  File \"/home/kuppa/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 792, in resolve\r\n    raise VersionConflict(dist, req).with_context(dependent_req)\r\npkg_resources.VersionConflict: (grpcio 1.16.1 (/home/kuppa/anaconda3/lib/python3.6/site-packages), Requirement.parse('grpcio>=1.24.3'))"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD#apply_gradients\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe formatting in the \"References\" section is wrong; there is a ``nesterov = True,`` that has nothing to do with the reference."},
{"text": "Not sure if this is a documentation issue or a functional bug. \r\n\r\n## Description of issue:\r\nThe layer names in 'tf.keras.applications' are not consistent with the layer names in 'keras.applications'.\r\n\r\n## Example:\r\n`keras.applications.resnet50.ResNet50(weights='imagenet').summary()` prints the following layers:\r\n\r\n> (...)\r\n> __________________________________________________________________________________________________\r\n> **conv1_pad** (ZeroPadding2D)       (None, 230, 230, 3)  0           input_3[0][0]                    \r\n> __________________________________________________________________________________________________\r\n> **conv1** (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \r\n> __________________________________________________________________________________________________\r\n> **bn_conv1** (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \r\n> __________________________________________________________________________________________________\r\n> **activation_50** (Activation)      (None, 112, 112, 64) 0           bn_conv1[0][0]                   \r\n> __________________________________________________________________________________________________\r\n> **pool1_pad** (ZeroPadding2D)       (None, 114, 114, 64) 0           activation_50[0][0]              \r\n> __________________________________________________________________________________________________\r\n> (...)\r\n\r\n                           \r\n`tf.keras.applications.resnet50.ResNet50(weights='imagenet').summary()` prints the following layers:\r\n\r\n> (...)\r\n> __________________________________________________________________________________________________\r\n> **conv1_pad** (ZeroPadding2D)       (None, 230, 230, 3)  0           input_6[0][0]                    \r\n> __________________________________________________________________________________________________\r\n> **conv1_conv** (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \r\n> __________________________________________________________________________________________________\r\n> **conv1_bn** (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \r\n> __________________________________________________________________________________________________\r\n> **conv1_relu** (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \r\n> __________________________________________________________________________________________________\r\n> **pool1_pad** (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \r\n> __________________________________________________________________________________________________\r\n> (...)\r\n\r\nThis can lead to errors if code relying on layer names is migrated from `keras` to `tf.keras`. Even after looking for it for quite a bit, I have not found any documentation explaining the change of layer names. Also, I manually needed to map `keras `layer names to `tf.keras` layer names, which would be avoidable with some nice documentation.\r\n\r\n## URL(s) with the issue:\r\nProbably this should be mentioned in the migration docs or the applications docs.\r\nhttps://www.tensorflow.org/guide/migrate#a_note_on_slim_contriblayers\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications\r\n\r\n\r\n## Usage example\r\n\r\nThe following code snippet runs when applications is imported from `keras `but not with `tf.keras`, as the layer is not found.\r\n\r\n```\r\nloaded_model = applications.ResNet50(weights='imagenet')\r\n\r\npartial_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer('res5c_branch2c').output)\r\n```\r\n\r\nSome easily findable documentation should be explaining why the layer is not found (renamed layer names) and where to find the mapping from `keras` layer names to `tf.keras` layer names (i.e., the mapping from `res5c_branch2c ` to `conv5_block3_3_conv`, which is the layer name used in tf.keras).\r\n\r\n## Used versions in these examples\r\ntensorflow 2.1.0\r\nKeras 2.3.1\r\n\r\n## Final Remark\r\nI'm not sure if renaming the layers was that useful... While the tf.keras layer names might be better readable, papers and tutorials often refer to the keras label names - and users now seem to have to find a mapping themself. \r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThis button \r\n![image](https://user-images.githubusercontent.com/2697890/73086114-09ba7600-3ee1-11ea-9b27-1f75816b4e94.png)\r\n\r\nlinks to 403 page\r\n![image](https://user-images.githubusercontent.com/2697890/73086137-1a6aec00-3ee1-11ea-9fa1-0ace2995a872.png)"},
{"text": "https://www.tensorflow.org/tutorials/images/classification\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThere's a bug in code\r\n\r\n`history = model.fit_generator(\r\n    train_data_gen,\r\n    steps_per_epoch=total_train // batch_size,\r\n    epochs=epochs,\r\n    validation_data=val_data_gen,\r\n    validation_steps=total_val // batch_size\r\n)`\r\n\r\nwhere\r\n`    steps_per_epoch=total_train // batch_size,\r\n`\r\nwill not compile because the , is behind the comment. The correct code should be\r\n`history = model.fit_generator(\r\n    train_data_gen,\r\n    steps_per_epoch=total_train, // batch_size\r\n    epochs=epochs,\r\n    validation_data=val_data_gen,\r\n    validation_steps=total_val // batch_size\r\n)`"},
{"text": "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/toco\r\n\r\nTflite converter is missing the usage documentation exmaple links :\r\n[link 1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/convert/cmdline_reference.md)\r\n[link 2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/convert/cmdline_examples.md)\r\n\r\nthe flags are needed to run such command : \r\n\r\n```\r\nbazel run --config=opt tensorflow/lite/toco:toco -- \\\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\r\n--inference_type=FLOAT \\\r\n--allow_custom_ops\r\n```\r\n\r\nPlease update the links or some one points me at the file that contains these flags to investigate more other possibilities and methods.\r\n\r\n\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/text/text_classification_rnn\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe sample for LSTM-based text classification doesn't run under Tensorflow 2.1 anymore.\r\n\r\nThe line:\r\n\r\n```\r\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)\r\n```\r\n\r\nDoes fail with the error:\r\n\r\n```\r\nAttributeError: 'ShuffleDataset' object has no attribute 'output_shapes'\r\n```\r\n"},
{"text": "The current GPU Support instructions for CUDA 10 on Ubuntu 16.04 refers to a package version that does not exist in the [NVIDIA ML repo](https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu1604/x86_64/): `libnvinfer5=6.0.1-1+cuda10.1`\r\n\r\n#### Ubuntu 16.04 (CUDA 10)\r\n\r\n<pre class=\"prettyprint lang-bsh\">\r\n...\r\n# Install TensorRT. Requires that libcudnn7 is installed above.\r\n<code class=\"devsite-terminal\">sudo apt-get install -y --no-install-recommends libnvinfer5=6.0.1-1+cuda10.1 \\\r\n    libnvinfer-dev=6.0.1-1+cuda10.1\r\n</code>\r\n</pre>\r\n\r\nThis results in an error when trying to install:\r\n```\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Version \u20186.0.1-1+cuda10.1\u2019 for \u2018libnvinfer5\u2019 was not found\r\n```\r\n\r\nChanging this to `libnvinfer6=6.0.1-1+cuda10.1` seems to work and run happily."},
{"text": "**System information**\r\n- OS Platform and Distribution: macOS 10.15 and iPadOS 13.3\r\n- Mobile device: iPad 2018\r\n- TensorFlow installed from (source or binary): pod\r\n\r\n**Describe the problem**\r\n\r\nI did all the steps from the readme of https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/ios \r\n\r\nbut got next error:\r\n\r\n> [!] CocoaPods could not find compatible versions for pod \"TensorFlowLiteSwift\":\r\n>  In snapshot (Podfile.lock):\r\n>    TensorFlowLiteSwift (= 0.0.1-nightly)\r\n\r\n> In Podfile:\r\n>    TensorFlowLiteSwift (= 0.0.1-nightly)\r\n\r\n> None of your spec sources contain a spec satisfying the dependencies: `TensorFlowLiteSwift (= 0.0.1-nightly), TensorFlowLiteSwift (= 0.0.1-nightly)`.\r\n\r\n>You have either:\r\n> * out-of-date source repos which you can update with `pod repo update` or with `pod install --repo-update`.\r\n> * mistyped the name or version.\r\n> * not added the source repo that hosts the Podspec to your Podfile.\r\n\r\n**The solution**\r\n\r\nTo make it work I just removed the Podfile.lock file and run again the command\r\n`pod install`\r\n\r\nMaybe the source repo should also be updated.\r\n\r\nThanks!\r\n\r\nP.S.: Initially it was posted there https://github.com/tensorflow/tensorflow/issues/35803 "},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c02_transfer_learning.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\n![Screenshot from 2020-01-13 12-53-58](https://user-images.githubusercontent.com/29497701/72238480-d7b53400-3603-11ea-847d-0eb7c0eb0716.png)\r\n\r\nIn description, it should be 'cats_vs_dogs'\r\n\r\n### Submit a pull request?\r\n\r\nYes"},
{"text": "## URL(s) with the issue: \r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/diag_part?version=nightly\r\n\r\n## Description of issue (what needs changing):\r\nThe `diag_part` documentation contains old examples of non-existing APIs, it uses `tf.matrix_diag_part` in the examples which does not exist.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c05_forecasting_with_machine_learning.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWindow size should be 30 instead of 20 in the description under \"Forecasting With Machine Learning\".\r\n\r\n### Clear description\r\n\r\n![Screenshot from 2020-01-10 12-05-13](https://user-images.githubusercontent.com/29497701/72131252-b1905980-33a1-11ea-8cf5-11d089d5316e.png)\r\n\r\n--------------------\r\nAs we can see under \"Linear Model\", `window_size=30` while in description it is mentioned as model forecasts, given previous 20 steps.\r\n\r\n### Submit a pull request?\r\n\r\nYes..."},
{"text": "## URL(s) with the issue:\r\n\r\nFor example,\r\nhttps://www.tensorflow.org/versions/r1.14/api_docs/python/tf/train\r\nhttps://www.tensorflow.org/versions/r1.14/api_docs/python/tf/estimator/Estimator\r\nhttps://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/Model\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe links above currently redirect to GitHub. The 1.15 links work:\r\n\r\nhttps://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train\r\nhttps://www.tensorflow.org/versions/r1.15/api_docs/python/tf/estimator/Estimator\r\nhttps://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/Model\r\n\r\nI have projects using TensorFlow 1.14, so I would like to use the 1.14 docs for reference.\r\n\r\nWill the 1.14 docs be back up?\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy?version=stable\r\n\r\n## Description of issue (what needs changing):\r\n\r\nOn the documentation page for tf.distribute.experimental.TPUStrategy, the Stable documentation is shown as the raw text of the documentation (looks like it's a combination of HTML and Markdown?).\r\n\r\nExample below:\r\n\r\n<img width=\"1664\" alt=\"Screen Shot 2020-01-09 at 1 22 11 PM\" src=\"https://user-images.githubusercontent.com/11432284/72094036-87786200-32e3-11ea-89ca-253b45a5ad7b.png\">\r\n\r\nClicking \"See Nightly\" it renders correctly, but clicking \"See Stable\" again it still shows the raw text again.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c03_moving_average.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIt should be 'mean absolute error' instead of squared error while Naive Forecasting\r\n\r\n![Screenshot from 2020-01-09 12-10-09](https://user-images.githubusercontent.com/29497701/72044240-4bd89a80-32d9-11ea-937f-a189784b83b0.png)\r\n\r\n### Submit a pull request?\r\n\r\nYes, I'll be submitting one shortly"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb\r\n\r\n## Description of issue (what needs changing):\r\nThe example script on how to make lstm layers ready for tf lite is outdated and not working anymore, because the requested tf-nightly package causes issues. \r\n\r\n\r\nI would like to get an updated tutorial or a better alternative. To use the TFLite converter with the experimental_flag set to True works with lstm layers, but does not allow post training quantization. As a general question I would like to know, if this would be possible with the the model that is build in the example script?\r\n\r\n"},
{"text": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(2)\r\na.assign(5)\r\nassert a.numpy() == 5\r\n\r\n# ValueError: Shapes () and (2,) are incompatible\r\na.assign([1,2])  \r\n\r\n# TypeError: assign() got an unexpected keyword argument 'validate_shape'\r\na.assign([1,2], validate_shape=False)\r\n\r\n# ValueError: Shapes () and (2,) are incompatible\r\ntf.compat.v1.assign(a, [1,2], validate_shape=False)  \r\n\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Linux\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0, 2.1.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.assign` had a `validate_shape` parameter that `Variable.assign` seems to be missing.\r\n\r\nIn addition, the docs say:\r\n> If you want to change the shape of a variable later you have to use an `assign` Op with `validate_shape=False`.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/Variable\r\n\r\nHow should one change the shape of a variable?\r\n\r\n**Code to reproduce the issue**\r\nSee above.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\n* lower: https://www.tensorflow.org/api_docs/python/tf/strings/lower?version=stable\r\n* upper: https://www.tensorflow.org/api_docs/python/tf/strings/upper?version=stable\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe first line of the docstrings for these functions is the auto-generated \"TODO: add doc.\" instead of an actual summary.\r\n\r\n### Submit a pull request?\r\n\r\nYes, I'll be submitting one shortly."},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://github.com/ManishAradwad/examples/blob/9f7d80aff8214b358e4aea0b83f2648748990c4b/courses/udacity_intro_to_tensorflow_for_deep_learning/l07c01_saving_and_loading_models.ipynb#L579\r\n\r\n`The differnece in output should be zero:`\r\n\r\n## Description of issue (what needs changing):\r\n\r\ndiffernece should be difference\r\n\r\n### Submit a pull request?\r\n\r\nYes\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tfx/tutorials/transform/census#python_check_imports_and_globals\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe text says \"First we'll make sure that we're using Python 2, and then go ahead and install and import the stuff we need.\" but the code below indicates we need to use Python 3.\r\n\r\n### Clear description\r\n\r\nThe text states:\r\n\r\n> First we'll make sure that we're using Python 2, and then go ahead and install and import the stuff we need.\r\n\r\nThe code example says:\r\n\r\n```python\r\nimport sys\r\n\r\n# Confirm that we're using Python 3\r\nassert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'\r\n```\r\n"},
{"text": "The codelab still links to the experimental folder for the makefile, which is incorrect.\r\n\r\nVisual:\r\n![image](https://user-images.githubusercontent.com/997157/71787651-3bc27180-2fe0-11ea-8318-424dc887efc5.png)\r\n\r\nUpdate the codelab at this link https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3\r\n\r\nCode should read:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile \\                                    \r\nTARGET=sparkfun_edge micro_speech_bin\r\n```\r\n\r\n## URL(s) with the issue:\r\nhttps://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe \"Report a Mistake\" link at the bottom left does not work, and goes to a GitHub 404 instead.\r\n![image](https://user-images.githubusercontent.com/997157/71787589-711a8f80-2fdf-11ea-8c54-03f8fc68b8d0.png)\r\n\r\nCurrently it links to: https://github.com/tensorflow/tensorflow/issues/new/title=[sparkfun-tensorflow-codelab]:"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c03_exercise_flowers_with_data_augmentation.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIn the directory structure, it should be \"daisy\" instead of \"diasy\"\r\n\r\n![Screenshot from 2020-01-03 18-39-11](https://user-images.githubusercontent.com/29497701/71725075-a8aaff80-2e58-11ea-9ac8-076078500026.png)\r\n\r\n### Submit a pull request?\r\n\r\nYes\r\n"},
{"text": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe code given in the guide does not run if the latent dimension is set 1, it runs fine for every latent dimension >1!\r\n\r\n### Clear description\r\n\r\nwhen the model is built and trained with code:\r\n```\r\nvae = VariationalAutoEncoder(784, 64, 1)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n\r\nvae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\nvae.fit(x_train, x_train, epochs=3, batch_size=64)\r\n```\r\nit throws the error:\r\n\r\n**ValueError: The last dimension of the inputs to Dense should be defined. Found None**\r\n\r\n### Correct links\r\n\r\nn/a\r\n\r\n### Parameters defined\r\n\r\nlatent_dim = 1\r\n(vae = VariationalAutoEncoder(784, 64, 1) )\r\n\r\n### Returns defined\r\n\r\nn/a\r\n\r\n### Raises listed and defined\r\n\r\n**ValueError: The last dimension of the inputs to Dense should be defined. Found None**\r\n\r\n### Usage example\r\n\r\ncode in the guide:\r\n\r\n```\r\nclass Sampling(layers.Layer):\r\n  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\r\n\r\n  def call(self, inputs):\r\n    z_mean, z_log_var = inputs\r\n    batch = tf.shape(z_mean)[0]\r\n    dim = tf.shape(z_mean)[1]\r\n    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\r\n\r\n\r\nclass Encoder(layers.Layer):\r\n  \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\r\n\r\n  def __init__(self,\r\n               latent_dim=32,\r\n               intermediate_dim=64,\r\n               name='encoder',\r\n               **kwargs):\r\n    super(Encoder, self).__init__(name=name, **kwargs)\r\n    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\r\n    self.dense_mean = layers.Dense(latent_dim)\r\n    self.dense_log_var = layers.Dense(latent_dim)\r\n    self.sampling = Sampling()\r\n\r\n  def call(self, inputs):\r\n    x = self.dense_proj(inputs)\r\n    z_mean = self.dense_mean(x)\r\n    z_log_var = self.dense_log_var(x)\r\n    z = self.sampling((z_mean, z_log_var))\r\n    return z_mean, z_log_var, z\r\n\r\n\r\nclass Decoder(layers.Layer):\r\n  \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\r\n\r\n  def __init__(self,\r\n               original_dim,\r\n               intermediate_dim=64,\r\n               name='decoder',\r\n               **kwargs):\r\n    super(Decoder, self).__init__(name=name, **kwargs)\r\n    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\r\n    self.dense_output = layers.Dense(original_dim, activation='sigmoid')\r\n\r\n  def call(self, inputs):\r\n    x = self.dense_proj(inputs)\r\n    return self.dense_output(x)\r\n\r\n\r\nclass VariationalAutoEncoder(tf.keras.Model):\r\n  \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\r\n\r\n  def __init__(self,\r\n               original_dim,\r\n               intermediate_dim=64,\r\n               latent_dim=32,\r\n               name='autoencoder',\r\n               **kwargs):\r\n    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\r\n    self.original_dim = original_dim\r\n    self.encoder = Encoder(latent_dim=latent_dim,\r\n                           intermediate_dim=intermediate_dim)\r\n    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\r\n\r\n  def call(self, inputs):\r\n    z_mean, z_log_var, z = self.encoder(inputs)\r\n    reconstructed = self.decoder(z)\r\n    # Add KL divergence regularization loss.\r\n    kl_loss = - 0.5 * tf.reduce_mean(\r\n        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\r\n    self.add_loss(kl_loss)\r\n    return reconstructed\r\n\r\n\r\n\r\nvae = VariationalAutoEncoder(784, 64, 1)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n\r\nvae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\nvae.fit(x_train, x_train, epochs=3, batch_size=64)\r\n```\r\n### Request visuals, if applicable\r\n\r\nn/a\r\n\r\n### Submit a pull request?\r\n\r\nn/a\r\n"},
{"text": "The link to the [EMNIST page](https://www.tensorflow.org/datasets/catalog/emnist) on Tensorflow's docs is broken.\r\n\r\nThe existing link is -\r\nhttps://www.nist.gov/node/1298471/emnist-dataset\r\n\r\nIt should be replaced by -\r\nhttp://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes?version=stable\r\n\r\n## Description of issue (what needs changing):\r\n\r\nUpdate Documentation or implementation of `tf.debugging.assert_shapes`.     \r\n`Shapes` in its argument requires LIST OF TUPLE, not DICTIONARIES.\r\n\r\n### Clear description\r\n\r\nYour Documentation (Python 4's syntax?)\r\n```python\r\nx = tf.random.normal([128, 32, 32, 1])\r\ntf.debugging.assert_shapes(\r\n    [(x: (128, 32, 32, 1))]\r\n)\r\n# => \r\n#     [(x: (128, 32, 32, 1))]\r\n#       ^\r\n# SyntaxError: invalid syntax\r\n```\r\n\r\nWith dictionary (Python 3's syntax)\r\n```python\r\ntf.debugging.assert_shapes(\r\n    {x: (128, 32, 32, 1)}\r\n)\r\n# =>\r\n# ... ... Traceback (most recent call last):\r\n#   File \"<stdin>\", line 2, in <module>\r\n#  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 713, in __hash__\r\n#     raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\r\n# TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\r\n```\r\nWith list of tuple (python 3's syntax)(NOT DICTIONARIES)\r\n```python\r\ntf.debugging.assert_shapes([\r\n    (x, (128, 32, 32, 1))\r\n]\r\n)\r\n# => nothing (correct)\r\n```\r\n"},
{"text": "Line 194 is missing the square bracket:\r\n\r\n- **wrong**\r\n`Use distribution to create a linear combination of value with shape batch_size, Tq, dim]:`\r\n\r\n\r\n- **correct**\r\n`Use distribution to create a linear combination of value with shape [batch_size, Tq, dim]:`\r\n\r\nLink to the line code:\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/dense_attention.py#L194\r\n"},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/constant_initializer\r\n\r\n## Description of issue (what needs changing):\r\nThe [example code](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/init_ops_v2.py#L152) uses `>>>`, but on the website, this is incorrectly converted to `&gt;&gt;&gt;`\r\n\r\nTF docs link: https://www.tensorflow.org/api_docs/python/tf/constant_initializer\r\n\r\n### Submit a pull request?\r\n\r\nThis is more of a problem with how documentation is generated from comments in the python file. I don't mind taking a look if someone can point out where to get started. "},
{"text": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and no.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): I used `pip install tensorflow-gpu==2.0.0`\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1080 TI and 11170 MiB\r\n\r\n**Describe the current behavior**\r\nFirst as discussed in this [issue](https://github.com/tensorflow/tensorflow/issues/18257). There is a bug in the first example of the documentation of `tf.while_loop`. \r\n\r\nThen, the parallel_iterations argument doesn't seem to parallelize the loop. There is no difference between the run time with `parallel_iterations = 1` or `parallel_iterations = 10`. \r\nI have a question opened on [Stackoverflow](https://stackoverflow.com/questions/59299060/tf-2-0-while-loop-and-parallel-iterations) .\r\n\r\n**Describe the expected behavior**\r\nIf the function in iteration n doesn't depend on previous iterations, then I expect that by setting `parallel_iterations = 10,` the loop should finish about 10 times faster than setting `parallel_iterations = 1`\r\n**Code to reproduce the issue**\r\nThe code is posted on the [Stackoverflow](https://stackoverflow.com/questions/59299060/tf-2-0-while-loop-and-parallel-iterations). \r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\r\n\r\n\r\n> on_epoch_end: logs include `acc` and `loss`, and\r\n>    optionally include `val_loss`\r\n>    (if validation is enabled in `fit`), and `val_acc`\r\n>    (if validation and accuracy monitoring are enabled).\r\n\r\nand\r\n\r\n> on_batch_end: logs include `loss`, and optionally `acc`\r\n\r\nThis is correct for the original Keras implementation. However, in tf2.keras callbacks get `accuracy` and `val_accuracy` instead of the short documented versions `acc`/`val_acc`. Either the implementation is wrong or the documentation.\r\n"},
{"text": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/losses.py#L493\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe example will cause errors:\r\n\r\ncce = tf.keras.losses.SparseCategoricalCrossentropy()\r\nloss = cce(\r\n  [0, 1, 2],\r\n  [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\r\nprint('Loss: ', loss.numpy())  # Loss: 0.3239\r\n\r\nNeed to change to\r\n\r\ncce = tf.keras.losses.SparseCategoricalCrossentropy()\r\nloss = cce(\r\n  [0, 1, 2],\r\n  tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]]))\r\nprint('Loss: ', loss.numpy())  # Loss: 0.3239\r\n\r\nIn addition [.5, .89, .6] should be [0.05, 0.89, 0.06] to be consistent with similar example (\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy?hl=en). Thus Loss should be updated to 0.0945\r\n\r\n"},
{"text": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.1-16986-g6c32a22 2.1.0-dev20191029\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n`tf.keras.models.Sequential` doesn't support `run_eagarly` as mentioned in the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#run_eagerly).\r\n\r\n**Describe the expected behaviour**\r\nEither Sequential model accepts `run_eagarly` as a param and changes its behaviour, or we modify the docs. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential(\r\n    layers=[tf.keras.layers.Dense(input_shape=(3, ), units=1)], \r\n    run_eagerly=True)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tst.py\", line 5, in <module>\r\n    run_eagerly=True)\r\n  File \"/home/squadrick/.local/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'run_eagerly'\r\n```"},
{"text": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tensorboard/tensorboard_profiling_keras#other_ways_for_profiling\r\n\r\n## Description of the issue (what needs changing):\r\n\r\nThe guide presents the profiler API and service as other ways of profiling.\r\nBoth of these use modules inside the `tensorflow.python` package.\r\nHowever, this package does not seem to exist as exhibited by the error message:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-14-c7b3d51d1060> in <module>()\r\n----> 1 with tf.python.eager.profiler.Profiler('logdir_path'):\r\n      2   # do your training here\r\n      3   pass\r\n      4 \r\n      5 \r\n\r\nAttributeError: module 'tensorflow' has no attribute 'python'\r\n```\r\n\r\n### Submit a pull request?\r\n\r\nI would be pleased to submit a pull request, but do not know if there is any TF2.0 compatible way to use profile API or service."},
{"text": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\r\n\r\n## Description of issue (what needs changing):\r\nIn the code example, it currently reads:\r\n```python\r\n# Variable-length int sequences.\r\nquery_input = tf.keras.Input(shape=(None,), dtype='int32')\r\nvalue_input = tf.keras.Input(shape=(None,), dtype='int32')\r\n\r\n# Embedding lookup.\r\ntoken_embedding = tf.keras.layers.Embedding(max_tokens, dimension)\r\n# Query embeddings of shape [batch_size, Tq, dimension].\r\nquery_embeddings = token_embedding(query_input)\r\n# Value embeddings of shape [batch_size, Tv, dimension].\r\nvalue_embeddings = token_embedding(query_input)\r\n```\r\n\r\nThe last line should instead be:\r\n```\r\nvalue_embeddings = token_embedding(value_input)\r\n```"},
{"text": "\r\n##  URL(s) with the issue:\r\nhttps://www.tensorflow.org/install/docker\r\n\r\n## Documentation page with the issue:\r\nhttps://www.tensorflow.org/install/docker\r\n\r\n## Description of issue (what needs changing):\r\nSince the latest docker builds are now tensorflow v2.x instead of v1.x the python example script on this page doesn't work out of the box. If the user copy and pastes the commands from this page they'll get a \"AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'\" error when trying to verify their docker container.\r\nThe python example script can be changing to this to fix it:\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.enable_eager_execution();\r\nprint(tf.reduce_sum(tf.random_normal([1000, 1000])))\r\n```\r\n\r\nA better alternative however would be to change it to a TF v2.0 native example.\r\n\r\n"},
{"text": "This [line](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/layers/dense_attention.py#L241) and this [one](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/layers/dense_attention.py#L368) should be:\r\n```\r\nvalue_embeddings = token_embedding(value_input)\r\n```\r\n"},
{"text": "When I go to the API Docs > Chain state (runtime), it has a \"contract\" section https://polkadot.js.org/api/METHODSSTORAGE.html#contract in the menu but no methods associated when I click the link, and there isn't a \"substrate\" section at all (differs from Substrate UI at https://polkadot.js.org/apps/next/#/chainstate which does have that section)"},
{"text": "#### Location of the documentation\r\n\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\r\n\r\n#### Documentation problem\r\n\r\nHow should I get the correlation among the rows? I can obviously do `df.T.corr()` but this strikes me as a workaround rather than a nice way to do things. I wrote a `df.corr(axis=1)` assuming that would work. Curious what people feel is idiomatic, and would recommend some mention in the docs. "},
{"text": "#### Location of the documentation\r\n\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.skew.html\r\n\r\n#### Documentation problem\r\n\r\nIn the function signature it says `skipna=None` even though the parameter documentation underneath says `skipna bool, default True`.\r\n\r\n#### Suggested fix for documentation\r\n\r\nThe `skipna=None` in function signature should be changed to `skipna=True`.\r\n"},
{"text": "Hello,\r\n\r\nI noticed in doc https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.strftime.html that `DatetimeIndex` have a `dt` attribute which can help to convert a `Timestamp` to string.\r\n\r\n```python\r\n>>> rng = pd.daterange(pd.Timestamp(\"2018-03-10 09:00\"),\r\n...                     periods=3, freq='s')\r\n>>> rng.strftime('%B %d, %Y, %r')\r\nIndex(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\r\n       'March 10, 2018, 09:00:02 AM'],\r\n      dtype='object')\r\n```\r\n\r\n(or `rng.toseries().dt.strftime('%B %d, %Y, %r')`)\r\n\r\nA similar method should exist for `TimedeltaIndex` to help to convert it to string (it's especially useful to output an Excel files without \"0 days\" for each index when each timedelta is below 1 day)\r\n\r\n```python\r\n>>> rng = pd.timedeltarange(start=\"0h\", freq=\"15min\", periods=3)\r\nTimedeltaIndex(['00:00:00', '00:15:00', '00:30:00'], dtype='timedelta64[ns]', freq='15T')\r\n>>> rng.dt.strftime(\"%M:%S\")\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-48-1f408f75ecf6> in <module>\r\n----> 1 rng.dt.strftime(\"%m:%s\")\r\n\r\nAttributeError: 'TimedeltaIndex' object has no attribute 'dt'\r\n\r\n```\r\n\r\nI have found a bad method to do this:\r\n\r\n```python\r\nrng.map(lambda td: time.strftime(\"%M:%S\", time.gmtime((td.topytimedelta().totalseconds()))))\r\n```\r\n\r\nunfortunatelly it doesn't support the \"f\" specifier for microsecond (I only need in fact millisecond)\r\n\r\n```python\r\n>>> rng = pd.totimedelta([\"00:01:02.345\", \"00:02:03.456\"])\r\nTimedeltaIndex(['00:01:02.345000', '00:02:03.456000'], dtype='timedelta64[ns]', freq=None)\r\n>>> rng.map(lambda td: time.strftime(\"%M:%S.%f\", time.gmtime((td.topytimedelta().totalseconds()))))\r\nIndex(['01:02.f', '02:03.f'], dtype='object')\r\n```\r\n\r\nit could be nice to simply be able to do\r\n\r\n```python\r\nrng.toseries().dt.strftime(\"%M:%S.%f\")\r\n```\r\n\r\nor naming attribute `td` (instead of `dt`) to avoid confusion between `DatetimeIndex` and `TimedeltaIndex`\r\n\r\n```python\r\nrng.toseries().td.strftime(\"%M:%S.%f\")\r\n```\r\n\r\nor even simplier\r\n\r\n```python\r\nrng.strftime(\"%M:%S.%f\")\r\n```\r\n\r\nI think a workaround to achieve this is (with positive `Timedelta`) to add epoch and so manage `Timestamp` instead.\r\n\r\n```python\r\n>>> rng = pd.totimedelta([\"00:01:02.345\", \"00:02:03.456\"])\r\nTimedeltaIndex(['00:01:02.345000', '00:02:03.456000'], dtype='timedelta64[ns]', freq=None)\r\n>>> rng = rng + pd.todatetime(0)\r\nDatetimeIndex(['1970-01-01 00:01:02.345000', '1970-01-01 00:02:03.456000'], dtype='datetime64[ns]', freq=None)\r\nrng = rng.strftime(\"%H:%M.%f\")\r\nIndex(['00:01.345000', '00:02.456000'], dtype='object')\r\n>>> rng.str[:-3]\r\nIndex(['00:01.345', '00:02.456'], dtype='object')\r\n```\r\n\r\nBut I'm still wondering how to manage negative `Timedelta` such as\r\n\r\n```python\r\n>>> rng = pd.totimedelta([\"00:01:02.345\", \"-00:02:03.456\"])\r\nTimedeltaIndex(['00:01:02.345000', '-1 days +23:57:56.544000'], dtype='timedelta64[ns]', freq=None)\r\n```\r\n\r\nAny idea?\r\n\r\nKind regards\r\n\r\n\r\nPS : related issue https://github.com/pandas-dev/pandas/issues/17232 (about display of negative Timedelta)"},
{"text": "Python 3.6 added a `fold` argument in `datetime.datetime` to disambiguate DST transition times that occur twice (in wall time).\r\n\r\nhttps://docs.python.org/3/library/datetime.html#datetime-objects.\r\n\r\nTechnically `Timestamp` will accept the argument in 3.6, but it's not formally documented or tested.\r\nAdditionally since we will still be supporting 3.5 after dropping 2.7, we can add/handle a fold argument directly in the `Timestamp` constructor as well.\r\n"},
{"text": "```python\r\nIn[2]: import pandas as pd\r\n  ...: import numpy as np\r\n  ...: pd.version\r\nOut[2]: u'0.23.4'\r\n\r\nIn[3]: ts = pd.Series([np.nan, 1., 2., 3., np.nan, 4., np.nan])\r\nIn[4]: ts.pctchange(fillmethod = None)\r\nOut[4]: \r\n0    NaN\r\n1    NaN\r\n2    1.0\r\n3    0.5\r\n4    NaN\r\n5    NaN\r\n6    NaN\r\ndtype: float64\r\n\r\nIn[5]: ts.pctchange(fillmethod = 'pad')\r\nOut[5]: \r\n0         NaN\r\n1         NaN\r\n2    1.000000\r\n3    0.500000\r\n4    0.000000\r\n5    0.333333\r\n6    0.000000\r\ndtype: float64\r\n\r\nIn[6]: ts.pctchange(fillmethod = 'pad').mask(ts.isnull())\r\nOut[6]: \r\n0         NaN\r\n1         NaN\r\n2    1.000000\r\n3    0.500000\r\n4         NaN\r\n5    0.333333\r\n6         NaN\r\ndtype: float64\r\n```\r\n\r\nHello, \r\n\r\nAfter recently updating my version, I noticed a change in behavior of pctchange with missing data. This is related to https://github.com/pandas-dev/pandas/issues/19873 . \r\n\r\nFirst example without fillmethod is as expected. The second example is the result now and the third is what it used to be. I think the user should be able to choose if she prefers the second or third behavior. I agree that the second example is correct, as it forward fills as expected, but if the time series is a stock price for example, returns on missing days (holidays) were not 0, which can bias some statistics. \r\n\r\nI would suggest adding a new parameter, like skipna. I could not find any solution with existing parameters, if I missed something please let me know.\r\n\r\nThanks\r\n\r\n\r\n"},
{"text": "Creating an issue of my comment on the PR that deprecated PeriodIndex add/sub with integers (https://github.com/pandas-dev/pandas/pull/22535):\r\n\r\n@jbrockmendel what was actually the rationale of deprecating it for Periods as well? \r\nThe issue that is closed by this (https://github.com/pandas-dev/pandas/issues/21939, cc @jreback @mroeschke) discusses it for DatetimeIndex/TimedeltaIndex, but I think says to keep it for PeriodIndex for now. \r\n\r\nIn any case, for PeriodIndex, this was an explicitly documented behaviour (in several places throughout the docs), and the documentation is now no longer up to date (you can check the doc build to see the deprecation warnings being raised). \r\n\r\nBut instead of updating the docs, I think we should maybe also reconsider the deprecation (given that is was a clearly documented behaviour). Or at least provide a clear alternative way.\r\n\r\n(and to be clear: I fully agree with the deprecation for DatetimeIndex / TimedeltaIndex!)\r\n"},
{"text": "In #23167, I'm trying consistently infer the dtype of the underlying Series/Index while calling the constructor of the `.str`-accessor. For testing this thoroughly, I wanted to build a parametrized fixture that returns an ndarray for all the dtypes that `lib.inferdtype` can infer. I based myself on the list in the docstring, but found the following:\r\n* the docstring mentions `'complex'` as a possible outcome, but this does not work (instead returning `'mixed'`)\r\n```\r\n>>> lib.inferdtype([1+1j, 2+2j])\r\n'mixed'\r\n>>> lib.inferdtype([np.complex128(1+1j)])\r\n'mixed'\r\n```\r\nand I don't believe it's actually possible to achieve this, given the code.\r\n* the docstring mentions `'timedelta64'`, but this similarly does not work (returning `'timedelta'`; and can't be hit either, IMO)\r\n```\r\n>>> lib.inferdtype([np.timedelta64(1, 'D')])\r\n'timedelta'\r\n```\r\n* the docstring *does not* mention `'interval'`, but that *is* a possible outcome:\r\n```\r\n>>> lib.inferdtype([pd.Interval(0, 1), pd.Interval(0, 2)])\r\n'interval'\r\n```\r\n\r\nSo it needs to be discussed if `'complex'`/`'timedelta64'` should be added to the code or removed from the docstring, and vice versa for `'interval'`."},
{"text": "I'm trying to create a contact via the API on the hosted version at app.monicahq.com, following the documentation here: https://www.monicahq.com/api/contacts\r\n\r\nAccording to the docs, I can send `\"gender\" : \"male\"` to POST /contacts but the API expects a genderid instead. Since there's no API to retrieve available gender IDs I don't know any valid value for this, so I cannot create a contact through the API.\r\n\r\nAlso, the API expects isstarred, which is not documented.\r\n\r\nAlso, the error messages for missing genderid and isstarred are in Arabic."},
{"text": "Hi.\r\nIs there Swagger / OpenAPI json file available? Or endpoint with it? I found discussion about generating docs only but no json file with API description. \r\n\r\nThanks "},
{"text": "We just moved our documentation for writing bots into /api in #7392; this lists some CSS bugs \r\n\r\n* [ ] The multi-line bold section here is oddly not wrapped automatically:\r\n![image](https://user-images.githubusercontent.com/2746074/32851853-0b1664fe-c9eb-11e7-8a4d-b7f47eaa2270.png)\r\n* [ ] The pagedown key on /api/writing-bots moves much less than one would expect.\r\n* [ ] This multi-line code block looks wrong:\r\n![image](https://user-images.githubusercontent.com/2746074/32851837-fc989118-c9ea-11e7-889b-6790e5700c8d.png)\r\n* [x] Including a code block in the heading looks wrongs: \r\n![image](https://user-images.githubusercontent.com/2746074/32851917-446dc10c-c9eb-11e7-95e3-9e63ceb8e520.png)\r\n(This one we might want to just fix by changing the text to \"Zulip bots package\".\r\n\r\n"},
{"text": "As i understand the documentation, calls to:\n/V1/products/attributes/{attributeCode}\n/V1/products/attributes/\nshould return frontend labels for the attribute and for options.\nLike this for options:\n\n```\n\"options\": [\n    {\n      \"label\": \"string\",\n      \"value\": \"string\",\n      \"sortOrder\": 0,\n      \"isDefault\": true,\n      \"storeLabels\": [\n        {\n          \"storeId\": 0,\n          \"label\": \"string\"\n        }\n      ]\n    }\n  ]\n```\n\nand like this for attribute label\n\n```\n\"frontendLabels\": [\n    {\n      \"storeId\": 0,\n      \"label\": \"string\"\n    }\n  ]\n```\n\nIn my tries i only get the default store view labels and values for options and null for the attribute labels like this:\n\n```\n \"options\" : [\n    {\n      \"label\" : \"string\",\n      \"value\" : \"string\"\n    }\n  ],\n.\n.\n.\n\"frontendlabels\" : null\n```\n\nMagento version is 2.0.2\n\n/Thomas\n"},
{"text": "I'm making a call to REST API configurableProductLinkManagementV1. The first simple product is associated correctly. Any simple product after that is not. The error I receive states \"option values are not specified\".  I'm passing in the following JSON:\n\n'{\"childSKU\": \"my SKU here\"}'\n\nI'm running Magento 2.1.0, PHP version 5.6.25-1+deb.sury.org~trusty+1, Apache/2.4.7, Ubuntu Linux 14.04, MySQL 5.6.33-0ubuntu0.14.04.1. \n### Steps to reproduce\n1.  I create all of the simple products using  catalogProductRepositoryV1.\n2.  I then create the configurable product using the same REST API.\n3.  Next I add the configurable product options using configurableProductOptionRepositoryV1.\n4.  Finally I associate all of the simple products to the configurable product using configurableProductLinkManagementV1.\n### Expected result\n1.  I'm expecting for the configurable product to have its associated simple products. \n### Actual result\n1. Only the first simple product has been added to the configurable product.  No other simple products are added to the configurable product.\n\nWhat are the correct steps when creating simple/configurable products and associating the products to one another?  And is there any where that I can find what the different JSON objects represent?  The REST API documentation doesn't give enough detailed information.\n"},
{"text": "<!--- Provide a general summary of the issue in the Title above -->\n\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\n### Preconditions\n\n<!--- Provide a more detailed information of environment you use -->\n\n<!--- Magento version, tag, HEAD, etc., PHP & MySQL version, etc.. -->\n1. Magento version 2.1\n2. \n### Steps to reproduce\n\n<!--- Provide a set of unambiguous steps to reproduce this bug include code, if relevant  -->\n1. Create a 'product' using /V1/products API with name and sku.\n2. It should throw an error.\n3. \n   ![image](https://cloud.githubusercontent.com/assets/17979940/19347900/dc7fc358-9168-11e6-8940-e48d0f0c0d10.png)\n### Expected result\n\n<!--- Tell us what should happen -->\n1. As per the swagger JSON, only sku is mandatory field. So it should have created a product with sku and name.\n### Actual result\n\n<!--- Tell us what happens instead -->\n1. It throws an error\n   ![image](https://cloud.githubusercontent.com/assets/17979940/19347916/f9115e78-9168-11e6-8a5e-e0565da97c0a.png)\n\n<!--- (This may be platform independent comment) -->\n\nDoes Magento have comprehensive API documentation or swagger JSON returned should be considered as the API doc?\n"},
{"text": "According to the documentation, `\\Magento\\Cms\\Api\\Data\\PageSearchResultsInterface::getItems()` should return an array of `\\Magento\\Cms\\Api\\Data\\PageInterface` objects, while it currently returns an array of arrays.\n### Preconditions\n\nI am using Magento 2.1.\n### Steps to reproduce\n\n```\n$sortOrder      = $this->sortOrderBuilder->setField(\\Magento\\Cms\\Model\\Page::TITLE)\n                                         ->setAscendingDirection()\n                                         ->create();\n$searchCriteria = $this->searchCriteriaBuilder->addFilter('storeid', $storeId)\n                                              ->addSortOrder($sortOrder)\n                                              ->create();\n\n$pageList = $this->pageRepository->getList($searchCriteria);\n$pages    = $pageList->getItems();\n```\n### Expected result\n\nAccording to the documentation, I would expect `$pages` to be an array of `\\Magento\\Cms\\Api\\Data\\PageInterface` objects. \n### Actual result\n\n`$pages` is array of data arrays:\n\n```\nArray\n(\n    [0] => Array\n        (\n            [identifier] => [page identifier]\n            [title] => [page title]\n            [pagelayout] => [page title]\n            [metakeywords] => \n            [metadescription] => \n            [content] => [page content]\n            [creationtime] => 2016-10-06 14:22:20\n            [updatetime] => 2016-10-06 14:29:20\n            [sortorder] => 0\n            [layoutupdatexml] => \n            [customtheme] => \n            [customroottemplate] => \n            [customlayoutupdatexml] => \n            [customthemefrom] => 2016-10-06\n            [customthemeto] => 2016-10-06\n            [active] => 1\n        )\n)\n```\n\n<!--- (This may be platform independent comment) -->\n"},
{"text": "<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Before adding new issues, please, check this article https://github.com/magento/magento2/wiki/Issue-reporting-guidelines-->\r\n\r\n### Preconditions\r\n<!--- Provide a more detailed information of environment you use -->\r\n<!--- Magento version, tag, HEAD, etc., PHP & MySQL version, etc.. -->\r\n1. Magento 2.1.3, using Sample Data\r\n2. Environment using [magento2-docker-compose](https://github.com/mageinferno/magento2-docker-compose)\r\n\r\n### Steps to reproduce\r\n<!--- Provide a set of unambiguous steps to reproduce this bug include code, if relevant  -->\r\n1. Create new integration\r\n2. Create a new website with code \"test\"\r\n3. Create a new store, assign to the \"test\" website\r\n4. Create a new store view with code \"test\", assign to the new store\r\n5. (Optional) - Don't assign any products to the website to make test easier to verify\r\n6. Perform REST API GET request using the store code in the URL, eg:\r\n\r\n```\r\nGET /rest/test/V1/products/?searchCriteria[pageSize]=1&searchCriteria[currentPage]=50 \r\nHTTP/1.1\r\nHost: m2.localhost:8000\r\nAuthorization: OAuth oauthconsumerkey=\"sitoq7tu4b1aj7kikj4irkog8tggh1ch\",oauthtoken=\"kr3bly2kbbnrr9flyws9r4mxdaagxngo\",oauthsignaturemethod=\"HMAC-SHA1\",oauthtimestamp=\"1484095699\",oauthnonce=\"TivkWr\",oauthversion=\"1.0\",oauthsignature=\"Q02C2wk4AgkDPkYAbACUoLMoXa4%3D\"\r\nCache-Control: no-cache\r\n```\r\n\r\n### Expected result\r\n<!--- Tell us what should happen -->\r\n1. Only products assigned to the test website are returned (or no products if none assigned)\r\n\r\n### Actual result\r\n<!--- Tell us what happens instead -->\r\n1. All products in the Magento instance are returned  \r\n\r\n<!--- (This may be platform independent comment) -->\r\n\r\n---\r\n\r\n### Comments \r\n\r\nIs it intended that using a non-\"default\" store code exposes a global product repository? Reading the [documentation](http://devdocs.magento.com/guides/v2.0/rest/restendpoints.html) it sounds like the request should be limited to the store that's specified:\r\n\r\n> The value of `storecode` can be one of the following:\r\n> - default\r\n> - The assigned store code\r\n>  - all. This value only applies to the CMS and Product modules. If this value is specified, the API call affects all the merchant's stores. GEToperations cannot be performed when you specify all.\r\n\r\nI have also attempted a search criteria filter against the following fields: store, storeid, storecode, website, websiteid, websitecode, all of which return `\"Invalid attribute name: %1\"`. \r\n\r\nI'm unable to find any other alternatives in the documentation. Is there currently no multistore support in the product API, or have I missed something obvious? "},
{"text": "It would be nice if the documentation made it clear what the requirements are around end tags for amp components.\n\nLooks like all the examples (except <a href=\"https://github.com/ampproject/amphtml/blob/f201cde9492142355de0a242c73512ec94567456/spec/amp-html-format.md#sample-document\n\">the sample document</a>) use end tags (i.e. `<amp-img></amp-img>`) but this doesn't seem to be explicitly stated as a requirement anywhere.\n\nNot including the end tag can cause issues in some instances, like #360 \n"},
{"text": "The AMP docs (https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#svg documentation) says that it allows the deprecated  \"glyph\", \"glyphRef\", \"tref\", \"hkern\", \"vkern\" elements from SVG 1.1. I'd suggest not allowing these elements.\n"},
{"text": "https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#svg doesn't list the \"svg\" element itself.\n"},
{"text": "According to the [documentation](https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#custom-fonts) states that custom fonts can be included via HTTP or HTTPS: \n\n> Authors are free to include all custom fonts via a @font-face CSS instruction via their custom CSS. Fonts included via @font-face must be fetched via HTTP or HTTPS scheme.\n\nHowever, custom fonts are not shown in the preview if they are not served via HTTPS (at least if the fonts are served from a different domain). \n\nIt would be great if we could make this more explicit in the documentation. \n"},
{"text": "In some cases, AMP documents are importing extended components they're not actually using in the document.  Even though the async tag and caching are used, this can still negatively impact performance.\n\nThe simplest mitigation effort here is to add a validation rule to invalidate docs with these tags, but this can be hard to use in cases where templating engines use a \"header\" and \"footer\" files that are the same throughout the site.\n\nIs there a way to request import of these extended components without actually making the network request unless the tag is used?\n"},
{"text": "https://github.com/ampproject/amphtml/blob/master/spec/amp-html-layout.md\n\nis currently missing a simple table saying: \"responsive does this\".\n"},
{"text": "It's not clear from the docs\n"},
{"text": "The folder at https://github.com/ampproject/amphtml/tree/master/examples/metadata-examples does not have any examples of OpenGraph meta-data. OpenGraph is required to properly share things on platforms such as Facebook.\n"},
{"text": "The folder at https://github.com/ampproject/amphtml/tree/master/examples/metadata-examples does not have any examples of Twitter card meta-data. Twitter cards are required to properly share things on Twitter.\n"},
{"text": "I noticed some things about the `<amp-img>` `srcset`:\n\nThe [amp documentation](https://www.ampproject.org/docs/reference/amp-img.html#attributes) currently states that the `srcset` attribute is the same as on an `<img>` and that \"The behavior will be polyfilled where not natively supported\".\n\nHowever: the native `srcset` is never used and the behavior differs in that the `sizes` attribute is not responsible for the image lookup (`getLayoutRect` is used if i read the code correctly). This is a mayor advantage over the native implementation in my opinion since this doesn't force me to hardcode my styles in the markup.\n\nThe documentation should mention the different implementation or the implementation should enforce the `sizes` attribute and use it to decide which image to use.\n\nPersonally: i think the current implementation is more advanced than the native one and since an amp document has a more stable layout than vanilla html there won't be any problems with it except that browser zooming doesn't change the used image.\n"},
{"text": "There's a small typo under [Iframe Click-To-Play](https://github.com/ampproject/amphtml/blob/master/extensions/amp-iframe/amp-iframe.md#iframe-click-to-play):\n-  It is possible to have an `amp-iframe` appear on the top of a document when the ~~amp-ifame~~ `amp-iframe` has a `placeholder` element as shown in the example below.\n"},
{"text": "Hello\n\nI can't find any support for paginated AMP content. The [HTML standard](https://html.spec.whatwg.org/#sequential-link-types), supported by [Google indexing](https://support.google.com/webmasters/answer/1663744?hl=en) and others, is to use link elements with rel=next and rel=prev.\n\nThis usage of <link> is explicitly disallowed for AMPs as per [AMP HTML spec](https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md).\n"},
{"text": "The `<amp-iframe>` spec, and the validation code, reference the 600px/75% rule.  However there is no description of what width that applies to.  For example 600px down on a iPhone 4 is a completely different proposition to the same content on an iPad pro or a desktop.   What width does the CDN use when validating?  Do publishers have to guess based on the user agent what width they are dealing with and adjust content?  Always use worst case?\n"},
{"text": "https://github.com/ampproject/amphtml/blob/spec/amp-iframe-origin-policy.md is 404 - linked from https://github.com/ampproject/amphtml/blob/master/extensions/amp-iframe/amp-iframe.md\n\nlink should be https://github.com/ampproject/amphtml/blob/master/spec/amp-iframe-origin-policy.md\n"},
{"text": "The list in the documentation found here\n\nhttps://github.com/ampproject/amphtml/tree/master/extensions\n\nis for example missing `<amp-facebook>`.\n"},
{"text": "It was tricky to test the intro and validation tables to get them exactly right across the docs. Now that they are live, I'm seeing bugs, and I should fix these. Really, it requires an end-to-end test, and sweep.\n\nThree main intro table bugs I want to sweep in a PR:\n- The intro to the validation tables is missing an end parenthesis across the tags.\n- The links to validation errors are relative based on the ampproject.org site, which means they are broken if accessed from the specifications. Make the specific.\n- We've got new tags that are missing intro and validation tables. Check the extensions list and update.\n"},
{"text": "Sample code at https://www.ampproject.org/docs/get_started/create/include_image.html and https://www.ampproject.org/docs/guides/amp_replacements.html fails to include <noscript> fallbacks for images, resulting in missing images when js is disabled.\n\nFor example:\n`<amp-img src=\"welcome.jpg\" alt=\"Welcome\" height=\"400\" width=\"800\"></amp-img>\n`\n\nShould change to:\n\n```\n<amp-img src=\"welcome.jpg\" alt=\"Welcome\" height=\"400\" width=\"800\">\n  <noscript>\n    <img src=\"welcome.jpg\" alt=\"Welcome\" height=\"400\" width=\"800\" />\n  </noscript>\n</amp-img>\n```\n\nOr:\n\n```\n<amp-img src=\"welcome.jpg\" alt=\"Welcome\" height=\"400\" width=\"800\"></amp-img>\n<noscript>\n  <img src=\"welcome.jpg\" alt=\"Welcome\" height=\"400\" width=\"800\" />\n</noscript>\n```\n\n_edit:_ fixed multiline formatting of code samples\n"},
{"text": "In https://github.com/ampproject/amphtml/blob/master/extensions/amp-analytics/amp-analytics.md the link currently goes here:\n\nhttp://support.chartbeat.com/docs/\n\nbut it should go to something more specific.\n"},
{"text": "This is a follow up of issue #2340. I would like to clarify when pingback requests are fired. According with the spec, I was assuming:\n- Pingbacks are fired when a user sees a content. It doesn't matter if the user is anonymous or authenticated. It doesn't matter if (based on the authorization response) the full version of the content is show to the user or if a login page or teaser of the content is shown. In all cases a pingback request is fired and can be used in the server side to update meters, feed server side metrics, etc.\n- Further accesses of the same user to the same content will trigger additional pingback requests.\n\nIs this correct? Thanks!\n"},
{"text": "The documentation says that a layout of type `responsive` is assumend:\n- if width or height attributes are present along with sizes and/or heights attribute, responsive layout is assumed;\n\nBut this only happens, when: `width` **and** `height` **and** (`heights` **or** `sizes`) is present.\nOtherwise it will become `fixed-height`.\n\nSee here the condition which applies `fixed-height` when `height` is set and `width` is unset or 'auto'.\nThis condition comes before the `responsive` condition:\nhttps://github.com/ampproject/amphtml/blob/fd2fce8d8086f7953f65d816e509905e17fd0d54/src/custom-element.js#L175\n\nand here is the condition for `responsive` layout which unlike the documentation specifies, does not allow only `width` **or** `height` but needs both **and** (`heights` **or** `sizes`): https://github.com/ampproject/amphtml/blob/fd2fce8d8086f7953f65d816e509905e17fd0d54/src/custom-element.js#L177\n\nWhat is the correct behaviour? As specified in the docs or the implemented behaviour?\n\nSee the documentation is question here:\nhttps://github.com/ampproject/amphtml/blob/master/spec/amp-html-layout.md#layout\n"},
{"text": "https://www.ampproject.org/docs/reference/extended/amp-dynamic-css-classes.html says they are added to the \"HTML element\", implying `<html>`. There was some discussion that it's actually `<body>`, so this needs to be confirmed one way or the other.\n\ncc @ericlindley-g \n"},
{"text": "In: https://github.com/ampproject/amphtml/blob/master/extensions/amp-social-share/amp-social-share.md\n\nLine 127 should be `text` not `title` for the linkedin type.\n\nConfig is at: https://github.com/ampproject/amphtml/blob/master/extensions/amp-social-share/0.1/amp-social-share-config.js#L109\n"},
{"text": "a9.js link here: https://github.com/ampproject/amphtml/blob/master/ads/integration-guide.md is pointing to an unavailable resource. Throwing 404. \n## How do we reproduce the issue?\n\nGo to the link and click on the hyperlink a9.js (see screenshot attached)\n![a9_missin_resource](https://cloud.githubusercontent.com/assets/11433063/14722572/f733beb6-07c0-11e6-84c7-32437ddc10a1.png)\n\nRepro steps:\n1. https://github.com/ampproject/amphtml/blob/master/ads/integration-guide.md\n2. Click on a9.js\n3. See 404 at redirect URL\n## What browsers are affected?\n\nBrowser independent\n## Which AMP version is affected?\n\nN/A\n\nNot sure how long it has been 404, it recently came to our attention. \nAppropriate resource here: https://github.com/ampproject/amphtml/blob/master/ads/a9.js\n"},
{"text": "@sriramkrish85 \u2014 opening this bug from the convo in #3046, to track the task to improve the documentation for amp-iframe resize (or change the origin check)\n"},
{"text": "In the refactor from a builtin to an extension, amp-ad docs got left behind.  I can see rationale for that, since the lazy script loading allows publishers to treat it as a builtin.  But there should be at least some docs in extensions/amp-ad.\n\n@zhouyx ?\n"},
{"text": "amp-social-share button does not work for Facebook. It is not picking up the `app_id` which is already specified in the document:\n\n```\n    <meta property=\"fb:app_id\" content=\"12345678901234\"></meta>\n    <meta property=\"fb:pages\" content=\"1234567890\"></meta>\n```\n\nClicking on the facebook share button results in the following dialog:\n![screen shot 2016-06-22 at 3 04 13 pm](https://cloud.githubusercontent.com/assets/128726/16279816/b6d2eef0-388a-11e6-96ed-017c2aa748d7.png)\n## How do we reproduce the issue?\n\njsbin: [here](http://jsbin.com/cuhogoxusa/)\n## What browsers are affected?\n\nAll of them.\n## Which AMP version is affected?\n\nAt least: `Powered by AMP HTML \u2013 Version 1466016447690`\n## Workaround\n\nAdd `data-param-app_id` parameter to the `<amp-social-share type=\"facebook\" />` component.\n## Remarks\n\nEither documentation should be updated to mark `data-param-app_id` as required. Or the `<amp-social-share>` component should pick up the appId from the meta tags.\n"},
{"text": "The `amp-video` docs state that:\n\n> The autoplay attribute allows the author to specify when - if ever - the animated image will autoplay.\n> \n> The presence of the attribute alone implies that the animated image will always autoplay. The author may specify values to limit when the animations will autoplay. Allowable values are desktop, tablet, or mobile, with multiple values separated by a space. The runtime makes a best-guess approximation to the device type to apply this value.\n\nHowever, the validator complains if you set `autoplay=\"desktop\"` or any of the variations noted above. Using just `autoplay` or `autoplay=\"autoplay\"` or `autoplay=\"\"` do validate.\n\nThere was some discussion on this in #2381. Do the docs need to be updated for this attribute or is still considered a bug?\n"},
{"text": "Under Embed https://github.com/ampproject/amphtml/blob/master/3p/README.md#embeds, provide an example of developing direct iframe embeds instead of using amp's 3p iframe mechanism for embeds.\n\n\"Direct iframe embeds not using our 3p iframe mechanism (used e.g. for ads) are preferred.\"\n\nThis statement can be interpreted incorrectly to suggest using amp-iframe instead of amp-ad.\n"},
{"text": "`amp-html-format.md` documents `on` attribute and actions. However, it omits the fact that mutliple events can be defined this way via `;` separator. Please confirm this is the case and update the docs.\n"},
{"text": "## Short description of your issue:\n\nOn the [page of `amp-img.md`](https://github.com/ampproject/amphtml/blob/master/builtins/amp-img.md) we can find a `FLEX_ITEM` in the list of _Supported Layouts_, but It is not described in the [online documentation](https://www.ampproject.org/docs/guides/responsive/control_layout.html).\n## How do we reproduce the issue?\n\nRead the doc!\n## What browsers are affected?\n\nAll browsers (I gess!).\n## Which AMP version is affected?\n\nCurrent website version, that is `1468521759997`.\n"},
{"text": "## Short description of your issue:\n\nDocumentation is in Markdown, but when this is published to the site (where Mustache.js is included) the tags are interpreted and therefore don't show in the docs.\n## How do we reproduce the issue?\n\nhttps://www.ampproject.org/docs/reference/extended/amp-mustache.html\nShould display:\n\n```\nHello {{world}}!\n```\n\nRenders as:\n\n```\nHello !\n```\n\nhttps://www.ampproject.org/docs/reference/extended/amp-access.html\nShould display:\n\n```\nYou are reading article {{views}} out of {{maxViews}}.\n```\n\nRenders as:\n\n```\nYou are reading article  out of .\n```\n"},
{"text": "https://github.com/ampproject/amphtml/blob/master/tools/experiments/README.md is probably the most helpful.\n\nCurrent issues are:\n- amp-experiment points at https://cdn.ampproject.org/experiments.html, which doesn't shed light on the console method for enabling experiments that must be done on on pages that aren't served from the CDN.\n- Other extensions like amp-form don't link to any help page.\n"},
{"text": "The `amp-a4a`, `amp-ad-network-adsense-impl`, `amp-ad-network-doubleclick-impl`, and `amp-ad-network-fake-impl` tags are not part of AMP's publisher-facing API, they're implementation details of the runtime. Including them in the API reference isn't necessary and may mislead publishers.\n"},
{"text": "Files `amp-ad.md` and `amp-embed.md` are in both folders: `builtins/` and `extensions/amp-ad/`.\n\nWhich is not only confusing both publishers and 3P contributors (some of them are still updating old place but not new place), but also becomes a blocker for us to improve the `amp-ad` documentation in general.\n\nIt was due to the migration of `amp-ad` from builtin to extension. However, `builtins/*` still exists because [amp-docs](https://github.com/ampproject/docs) is using them to generate web pages.\n\nSo we have to remove this dependency from amp-docs. However, the blocker is that `amp-embed` is such an outlier (it is not a real extension), which doesn't follow the pattern `/extensions/amp-abc/amp-abc.md` that amp-docs relies on to auto-generate the web pages. \n\n**Solutions**\n1. Make a special case for `amp-embed` in amp-docs\n2. Merge the content of `amp-embed.md` into `amp-ad.md`\n\nI prefer 2 because:\n- They are essentially the same thing, `amp-embed` is a pure alias of `amp-ad`\n- Easy for amp-docs.\n\n@jasti thoughts?\n"},
{"text": "In the amp-access spec, under the [\"Login Page\"](https://github.com/ampproject/amphtml/blob/master/extensions/amp-access/amp-access.md#login-page) section, the following sentence can be found:\n\n> See the \"Login Flow\" section for more details.\n\nHowever this section doesn't seem to exist.\n\nIs this a section that was forgotten? Or is the name that should be referred here wrong?\n"},
{"text": "https://www.ampproject.org/docs/reference/spec.html\nvs https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#boilerplate\n\n> `contain the [AMP boilerplate code ('head > style[amp-boilerplate]' and 'noscript > style[amp-boilerplate]')](amp-boilerplate.md) in their head tag. \ud83d\udd17`\n# \n\nhttps://www.ampproject.org/docs/reference/spec.html#common-attributes\nvs https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#common-attributes\n\n> The value for the syntax is a simple domain specific language of the form:\n> `[sourcecode] eventName:targetId[.methodName[(arg1=value, arg2=value)]] [/sourcecode]`\n"},
{"text": "@mikepmtl Would you be able to update the reach-player.amp.html example? The current one is broken.\n\nWould also like your input on #5325 for Reach Player?\n"},
{"text": "Found that line at the docs for [extended templates](https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md#extended-templates). Is it a placeholder? (otherwise a link would be better) https://github.com/ampproject/amphtml/blob/master/spec/amp-html-templates.md#templates says \"None yet. Coming soon.\"\n"},
{"text": "At https://www.ampproject.org/docs/reference/components/amp-live-list#server-side-filtering, the link to `Server side filtering` is broken - https://www.ampproject.org/docs/reference/components/components/amp-live-list-server-side-filtering.html\n\nHowever, the Markdown appears correct at https://github.com/ampproject/amphtml/blob/master/extensions/amp-live-list/amp-live-list.md#server-side-filtering.\n\nHow is that link generated?\n"},
{"text": "## Short description of your issue:\r\n\r\nLink for Ad Network documentation at this page is broken: https://github.com/ampproject/amphtml/blob/master/ads/google/adsense.md.\r\n\r\n\r\n## How do we reproduce the issue?\r\n\r\n1. Go to https://github.com/ampproject/amphtml/blob/master/ads/google/adsense.md\r\n2. Click on hyperlinked text \"ad network documentation\"\r\n3. Redirects to a broken AdSense help center page\r\n\r\n## What browsers are affected?\r\n\r\nBrowser independent \r\n\r\n## Which AMP version is affected?\r\n\r\nN/A"},
{"text": "Hello,\r\n\r\nNot sure if this repo is right place to submit this issue (didn't find a repo for the website), but this definitely a weird one.\r\n\r\nI'm author of [stylelint-config-amp](https://github.com/tinovyatkin/stylelint-config-amp) and while I was looking for examples of correct CSS test code for an AMP page then [ampproject.org](https://ampproject.com) definitely should be a good place. However, I was badly surprised to find that it's mostly violates it's own rules from https://www.ampproject.org/docs/guides/responsive/style_pages, particularly:\r\n\r\n- Uses `transition: all` everywhere\r\n- Uses blacklisted `filter` property (inside a `@media` rule, but docs says nothing it can be an exception)\r\n\r\nSo, is it docs outdated or CSS at project main page erroneous? \r\n"},
{"text": "The [AMP Forms](https://ampbyexample.com/components/amp-form/) documentation recommends the following markup for rendering success / error states in form responses: \r\n\r\n  ```\r\n<div submit-success>\r\n      <template type=\"amp-mustache\">\r\n        Success! Thanks {{name}} for trying the\r\n        <code>amp-form</code> demo! Try to insert the word \"error\" as a name input in the form to see how\r\n        <code>amp-form</code> handles errors.\r\n      </template>\r\n    </div>\r\n    <div submit-error>\r\n      <template type=\"amp-mustache\">\r\n        Error! Thanks {{name}} for trying the\r\n        <code>amp-form</code> demo with an error response.\r\n      </template>\r\n    </div>\r\n```\r\n\r\nThis markup does not validate either when browsing the URL and appending `#development=1`, or when running a page through [https://validator.ampproject.org](https://validator.ampproject.org)\r\n\r\nThe response is `The attribute 'submit-success' may not appear in tag 'div'.` and `The attribute 'submit-error' may not appear in tag 'div'.`\r\n\r\n## To reproduce the issue, simply include\r\n``<script async custom-element=\"amp-form\" src=\"https://cdn.ampproject.org/v0/amp-form-0.1.js\"></script>``\r\nand \r\n``<script async=\"\" custom-template=\"amp-mustache\" src=\"https://cdn.ampproject.org/v0/amp-mustache-0.1.js\"></script>``\r\nin your head tag.\r\n\r\nThen include the recommended markup from the the AMP Forms doc page mentioned above. It will consistently through an error.\r\n\r\nTrue in at least Chrome, Firefox and Safari, AMP version: 1480633190770"},
{"text": "Custom fonts are documented at:\r\n\r\n> https://www.ampproject.org/docs/guides/responsive/style_pages#the-custom-fonts-exception\r\n\r\nYou CAN reference an external file for fonts.\r\n\r\nThe current example (link above) shows a link to a font style sheet:\r\n\r\n>   <link href=\"https://fonts.googleapis.com/css?family=Roboto:200,300,400,500,700\" rel=\"stylesheet\" type=\"text/css\">\r\n\r\nBest practice for `<link>` is to use Subresource Integrity. SRI is documented at:\r\n\r\n> https://www.w3.org/TR/SRI/\r\n\r\nTherefore, the example should be updated to:\r\n\r\n> `<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Roboto:200,300,400,500,700\" integrity=\"sha384-kPC23LXBnM90mw9ntWjNWxvMZKlAYFEUjK6bS6TSSeBWU5S0GdcVib2uYrpffb2y\" crossorigin=\"anonymous\">`\r\n"},
{"text": "Common industry practice is that a the `canonical` header should point to the original source of the content, which in the case of syndicated content may be a completely different site.    However the AMP documentation says the canonical header in an AMP doc is a \r\n\r\n> [tag inside their head that points to the regular HTML version of the AMP HTML document or to itself if no such HTML version exists.](https://github.com/ampproject/amphtml/blob/eda679bb3ca77908cb0cfb40e3fee91f0da4a37d/spec/amp-html-format.md#canon) \r\n\r\nThis change in meaning of the `canonical` header is confusing.  It also presents unsolvable edge cases (ex when a site is pure AMP the spec says canonical should be self referential but if the content is syndicated from another site industry practice would be to point to the other site). \r\n\r\nThis issue impacts numerous AMP features (analytics, ads, issues #7058 #6210 amongst others).\r\n\r\nIf the documentation is accurate and the `canonical` header in an AMP doc should refer to the regular HTML source this implies a chained model where several hops may be needed to reach the original source.  It's unclear if search engines see it this way and publishers are very wary of moving away from standard practice for fear of attracting penalties in the search rank.\r\n\r\nPlease clarify the official position.\r\n\r\ncc: @jridgewell @cramforce "},
{"text": "## Short description of your issue:\r\n\r\nSimple documentation error on https://www.ampproject.org/docs/reference/components/amp-mustache \r\n\r\nOn triple-mustache section, `{{{unescaped}}}` is accidentally written as `{{unescaped}}}`.\r\n\r\n![screen shot 2017-02-09 at 8 35 23 pm](https://cloud.githubusercontent.com/assets/766864/22781764/62e45f3e-ef07-11e6-891f-cfc03c25618e.png)\r\n\r\n## How do we reproduce the issue?\r\n\r\n1. Navigate to https://www.ampproject.org/docs/reference/components/amp-mustache\r\n\r\n## What browsers are affected?\r\n\r\nAll browsers\r\n\r\n## Which AMP version is affected?\r\n\r\nAMP website issue."},
{"text": "## Short description of your issue:\r\n\r\nAccording to [the docs](https://www.ampproject.org/docs/reference/components/amp-list), it should be possible to place an overflow button at the end of the list with `position: absolute`. Due to [this change](https://github.com/ampproject/amphtml/pull/7061/files) we had to hack around the specificity, to achieve this, since the overflow-button inherits `position: relative` from `[overflow]` now.\r\n\r\n## What browsers are affected?\r\n\r\nAll browsers\r\n\r\n## Which AMP version is affected?\r\n\r\n1487358851767\r\n"},
{"text": "I'm pulling the release schedule docs into a separate file in PR #7769.  Once this change goes in links in Great First Issues that use the GFI template will require updating."},
{"text": "https://github.com/ampproject/amphtml/blob/master/validator/README.md does not contain Mac OSX and Windows installation steps for dependencies "},
{"text": "1) Go here: https://cdn.ampproject.org/experiments.html\r\n2) Click on ad-type-custom\r\n3) Note 'Not found' at https://github.com/ampproject/amphtml/ads/custom.md"},
{"text": "Here: https://github.com/ampproject/amphtml/blob/master/ads/README.md#1st-party-cookies\r\nabiity -> ability\r\n\r\nHere: https://github.com/ampproject/amphtml/blob/master/ads/README.md#optimizing-ad-performance\r\n3p.js links to 404 here: https://github.com/ampproject/amphtml/blob/master/src/3p.js"},
{"text": "Hello,\r\n\r\nI'd like to clarify the URL rewrite behavior for images as described in the [AMP cache modifications best practices](https://github.com/ampproject/amphtml/blob/master/spec/amp-cache-modifications.md#all-image-urls-are-rewritten-as-amp-cache-urls-except-those-in-amp-mustache-template) document.\r\n\r\nIn each of the four examples, none of the rewritten URLs contain the origin domain. Regardless of whether the rewritten AMP cache URL is absolute or relative, I'd expect the URI of the content to be fetched to contain the origin domain.\r\n\r\nFor example,\r\n\r\n\t<amp-img src=https://example.com/foo.png></amp-img>\r\n\r\nwould be rewritten per the document as:\r\n\r\n\t<amp-img src=/i/s/foo.png></amp-img>\r\n\r\nMy expectation is that the URL would actually be rewritten as:\r\n\r\n\t<amp-img src=/i/s/example.com/foo.png></amp-img>\r\n\r\nAlternatively, it might also be rewritten according to the AMP Cache URL format descrbed\r\n[here](https://developers.google.com/amp/cache/overview#amp-cache-url-format):\r\n\r\n\t<amp-img src=\"https://example-com.cdn.ampproject.org/i/s/example.com/foo.png\"></amp-img>\r\n\r\nIs this a typo in the the best practices document?\r\n\r\nThanks!\r\n"},
{"text": "\r\n## What's the issue?\r\n\r\nI've found bag in case-studies page on https://www.ampproject.org/docs/create/. The bag is that Page Not Found. This file does not exist and there was no index.html found in the current directory or 404.html in the root directory. There is screenshot about it http://joxi.ru/Grq1M0RuQELNEr\r\n\r\n## How do we reproduce the issue?\r\n\r\nYou can easily reproduce bag by following to https://www.ampproject.org/docs/create/\r\n\r\n## What browsers are affected?\r\n\r\nChrome Browser 59.0.3071.115. Desktop Version on MacBook Pro 2016\r\n\r\nMy contact azat.success@gmail.com"},
{"text": "## What's the issue?\r\n\r\nhttps://stackoverflow.com/questions/38339156/can-amp-errors-containing-the-string-do-not-worry-about-it-be-ignored/38346568\r\n\r\n## How do we reproduce the issue?\r\n\r\nIn Mobile Safari AMP pages that include `<iframe>`s appear to be making network requests to the follow path \"/amp_preconnect_polyfill_404_or_other_error_expected._Do_not_worry_about_it\". This can be observed by monitoring network traffic on the device. It will also show with great frequency in the network logs of servers delivering the `<iframe>`d experience.\r\n\r\nThis preconnect polyfill is not documented (except for a few Stack Overflow questions) and may come as a surprise to those hosting an `<iframe>`d experience on an AMP page. The \"_Do_not_worry_about_it\" portion of the path does not allay any suspicions and does not provide any additional information about the feature.\r\n\r\nThis feature should be documented in AMP with enough detail for developers to understand and anticipate this behavior. I would also suggest that the preconnect URL reference the documentation so the documentation is more discoverable when someone does observer the URL pattern.\r\n\r\n## What browsers are affected?\r\n\r\nObserved in Mobile Safari.\r\n\r\n## Which AMP version is affected?\r\n\r\n1506041909031\r\n"},
{"text": "## What's the issue?\r\nA link in one of the tutorials' page go to \"Page Not Found\"\r\n\r\n\r\n## How do we reproduce the issue?\r\n1. Go to the Italian version of the site\r\n2. From the main menu select \"DOCUMENTAZIONE\" > \"Tutorials\" > \"Crea la tua prima pagina AMP\"\r\n3. From the left menu select \"Aggiunta di un\u2019immagine\"\r\n4. In the body of the page click \"Includere Iframe ed elementi multimediali\"\r\n\r\n--\r\nFaster way: \r\n1. Go to https://www.ampproject.org/it/docs/tutorials/create/include_image\r\n2. Change language to Italian \r\n3. Click \"Includere Iframe ed elementi multimediali\"\r\n\r\n\r\n## What browsers are affected?\r\nTested with Chrome and Safari \r\n"},
{"text": "## What's the issue?\r\n\r\nThe documentation for amp-ad-exit says that all layouts are supported because the extension is hidden but the validation spec for amp-ad-exit does not allow the layout attribute to be present with any value.\r\n\r\n## How do we reproduce the issue?\r\n\r\nAdd an amp-ad-exit tag with a layout attribute to an A4A document and then try to validate the document.\r\n\r\n## What browsers are affected?\r\nN/A\r\n\r\n## Which AMP version is affected?\r\nN/A"},
{"text": "The `amp-video` component [requires the \"poster\" attribute if nested within `amp-story`](https://github.com/ampproject/amphtml/blob/master/extensions/amp-video/validator-amp-video.protoascii#L128).\r\n\r\nHowever, the `amp-video` docs [do not mention this requirement](https://www.ampproject.org/docs/reference/components/amp-video#poster)."},
{"text": "The documentation as it currently stands lacks some tips and tools on how to debug test failures. \r\n\r\nE.g. I got bit by: the fetchMock API. \r\n\r\ncc @mrjoro "},
{"text": "## What's the issue?\r\nThe tag `amp-bind-macro` does not support the attribute `name`.\r\n\r\n## Briefly describe the bug/feature request.\r\nI was following the examples in https://www.ampproject.org/docs/reference/components/amp-bind related to amp bind macro and found out that I cannot use the attribute `name`.\r\n\r\n## How do we reproduce the issue?\r\nGo to the link https://www.ampproject.org/docs/reference/components/amp-bind, scroll to the section named `Defining macros with amp-bind-macro` and then try that example.\r\n\r\n## What browsers are affected?\r\nAll browsers, all devices\r\n\r\n## Which AMP version is affected?\r\nVersion 1516850240342\r\n\r\n## What worked for me?\r\nI used an `id` attribute instead of the `name` attribute and then I was able to get the example working.\r\n\r\nPlease fix this."},
{"text": "The documentation for `layout=\"fill\"` doesn't say a lot and can make someone think that just using that attribute alone will make the element fill its parent, whereas in reality it fills the element with `position: realtive`. I think that this should be mentioned in the docs."},
{"text": "According to the docs related to styles, -moz-binding and behavior are banned styles, though I can use them and they are not flagged as so.\r\n\r\n## How do we reproduce the issue?\r\n\r\nhttps://validator.ampproject.org/\r\n\r\n1. Add the following style in the validator web ui\r\n\r\n <style amp-custom>\r\n   .test {\r\n     -moz-binding: inherited;\r\n     behavior: url(http://www.msn.com/blah.htc);\r\n    }\r\n  </style>\r\n\r\n"},
{"text": "Recently our client has feedback us about their amp-ad above the fold didn't collapse when it's non-filled.\r\n<img width=\"446\" alt=\"screen shot 2018-03-08 at 1 37 25 pm\" src=\"https://user-images.githubusercontent.com/4546841/37135132-c61f76d4-22d6-11e8-9083-cf0850061875.png\">\r\n\r\n\r\nAnd the document is confused to me as it wrote: \r\n(https://www.ampproject.org/docs/reference/components/amp-ad#no-ad-available)\r\n> If there is no fallback element available, the amp-ad element is collapsed (that is, set to display: none) if the ad sends a message that the ad slot cannot be filled and AMP determines that this operation can be performed without affecting the user's scroll position.\r\n\r\nI've looked through some other issues search for relative imformation, and got the conclusion of `if the amp-ad is in the viewport and got nonfilled, it won't collapse.`\r\n- https://github.com/ampproject/amphtml/issues/3182#issuecomment-218501388\r\n- https://github.com/ampproject/amphtml/issues/4841#issuecomment-245124144\r\n\r\nAs a suggestion could we clearly mention that if the amp-ad is in the viewport, when no-ad available the ad area would not collapse, no matter there isn't a default fallback or not?\r\nThe origin document `AMP determines that this operation can be performed without affecting the user's scroll position` is a kind of vague explanation (at least for me).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"},
{"text": "The amp-story documentation at https://www.ampproject.org/docs/reference/components/amp-story.html refers to the 1.0 version of `amp-story` component instead of the 0.1 version.\r\n\r\nThe documentation should refer to 0.1 until 1.0 is ready."},
{"text": "## What's the issue?\r\n\r\nIn the documentation https://www.ampproject.org/docs/reference/components/amp-bind the \"A slightly more complex example\" demo seems to be buggy. \r\n\r\nIt's very very frustrating to try to learn it when the doc is not cristal clear :( \r\n\r\n## How do we reproduce the issue?\r\n\r\n\r\nTo reproduce : \r\n\r\nuse the dev mod ```#development=1```\r\n\r\n```html \r\n\r\n\r\n<!doctype html>\r\n\r\n<html =\"\" lang=\"en\">\r\n\r\n    <head>\r\n        <meta charset=\"utf-8\">\r\n        <title>starter</title>\r\n\t    <link rel=\"canonical\" href=\"https://www.ampstart.com/lol\">\r\n\r\n\r\n        <meta name=\"viewport\" content=\"width=device-width,minimum-scale=1,initial-scale=1\">\r\n    \r\n        <script async=\"\" src=\"https://cdn.ampproject.org/v0.js\"></script>\r\n        <script async custom-element=\"amp-bind\" src=\"https://cdn.ampproject.org/v0/amp-bind-0.1.js\"></script>\r\n    \r\n        <style amp-boilerplate=\"\">body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate=\"\">body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>\r\n    \r\n\r\n\r\n    </head>\r\n\r\n\r\n    <body>\r\n        \r\n        \r\n       <!-- Store complex nested JSON data in <amp-state> elements. -->\r\n        <amp-state id=\"myAnimals\">\r\n        <script type=\"application/json\">\r\n            {\r\n            \"dog\": {\r\n                \"style\": \"greenBackground\"\r\n            },\r\n            \"cat\": {\r\n                \"style\": \"redBackground\"\r\n            }\r\n            }\r\n        </script>\r\n        </amp-state>\r\n\r\n        <p [text]=\"'This is a ' + currentAnimal + '.'\">This is a dog.</p>\r\n\r\n        <!-- CSS classes can also be added or removed with [class]. -->\r\n        <p class=\"greenBackground\" [class]=\"myAnimals[currentAnimal].style\">\r\n        Each animal has a different background color.\r\n        </p>\r\n\r\n\r\n\r\n        <button on=\"tap:AMP.setState({currentAnimal: 'cat'})\">Set to Cat</button>\r\n\r\n    </body>\r\n\r\n</html>\r\n\r\n\r\n``` \r\n\r\nerros \r\n\r\n\r\n``` \r\ncurrentAnimal is not defined; returning null.\r\ncurrentAnimal is not defined; returning null.\r\nCannot read property null of null; returning null.\r\nblob:http://localhost:8009/6c555f15-1cbc-4a20-9d49-bccc79fbe883:1 Cannot read property \"style\" of \"style\"; returning null.\r\n\r\nlog.js:162 Default value for <P [text]=\"'This is a ' + currentAnimal + '.'\"> does not match first result (This is a null.). We recommend writing expressions with matching default values, but this can be safely ignored if intentional.\r\n\r\namp-bind: \"null\" is not a valid result for [class].\u200b\u200b\u200b \r\n\r\nDefault value for <P [class]=\"myAnimals[currentAnimal].style\"> does not match first result (null). We recommend writing expressions with matching default values, but this can be safely ignored if intentional.\r\n\r\n```\r\n\r\n## What browsers are affected?\r\n\r\nAll\r\n\r\n## Which AMP version is affected?\r\n\r\nPowered by AMP HTML \u2013 Version 1524862012167\r\n"},
{"text": "Looks like we need to update our dev docs with a section on how to get our closure compiler to work with the out-of-box version of Java found on new Macbooks.\r\n\r\nRunning `gulp dist` yields the error:\r\n```\r\nCompiler error for v0.js:\r\nNo Java runtime present, requesting install.\r\n```\r\n\r\nInstalling Java from [here](https://support.apple.com/kb/DL1572?locale=en_US) and rerunning `gulp dist` yields the error:\r\n```\r\nCompiler error for v0.js:\r\nException in thread \"main\" java.lang.UnsupportedClassVersionError: org/ampproject/AmpCommandLineRunner : Unsupported major.minor version 52.0\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:621)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:283)\r\n\tat java.net.URLClassLoader.access$000(URLClassLoader.java:58)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:197)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:190)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:306)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:247)\r\n\tat java.lang.Class.forName0(Native Method)\r\n\tat java.lang.Class.forName(Class.java:249)\r\n\tat org.eclipse.jdt.internal.jarinjarloader.JarRsrcLoader.main(JarRsrcLoader.java:56)\r\n```\r\n\r\nThere is a Java install on the path:\r\n```\r\n~/src/amphtml$ which java\r\n/usr/bin/java\r\n```\r\n\r\nAnd here is the version info:\r\n```\r\n~/src/amphtml$ java -version\r\njava version \"1.6.0_65\"\r\nJava(TM) SE Runtime Environment (build 1.6.0_65-b14-468)\r\nJava HotSpot(TM) 64-Bit Server VM (build 20.65-b04-468, mixed mode)\r\n```"},
{"text": "On this doc [https://www.ampproject.org/docs/reference/components/amp-install-serviceworker](https://www.ampproject.org/docs/reference/components/amp-install-serviceworker) it says:\r\n\r\n> This ServiceWorker runs whenever the AMP file is served from the origin where you publish the AMP file. The ServiceWorker will not be loaded when the document is loaded from an AMP cache.\r\n\r\nIs that statement correct?"},
{"text": "The current example in the `amp-date-countdown` sample relies on `.hide` and `.show` which require user action and thus cause an error (see #16567). \r\n\r\nhttps://github.com/ampproject/amphtml/blob/703aad74ebe0c045389c69e2d43041ffb2bbe9f8/extensions/amp-date-countdown/amp-date-countdown.md#L177\r\n\r\nThe sample under `examples` uses `amp-animation` instead:\r\n\r\nhttps://github.com/ampproject/amphtml/blob/703aad74ebe0c045389c69e2d43041ffb2bbe9f8/examples/amp-date-countdown.amp.html#L102\r\n\r\nWe should either update the documentation to use the same approach as the sample or, preferably, wait until #13973 is done, since (if I understand correctly) this will effectively allow for a less-hacky approach."},
{"text": "## What's the issue?\r\n\r\nSeeing a lot of broken CORS requests in some of our content due to the publisher subdomain being one-way hashed by the Google AMP CDN.\r\n\r\nFor example, this AMP doc:\r\n\r\nhttps://es-us.vida-estilo.yahoo.com/amphtml/thalia-festeja-los-69-anos-de-su-marido-y-fans-creen-que-se-puso-botox-mira-que-joven-luce-033930503.html\r\n\r\nAccording to the encoding documented in the Google AMP cache docs (https://developers.google.com/amp/cache/overview), the expected Google AMP cache subdomain for our example should be:\r\n\r\nhttps://es--us-vida--estilo-yahoo-com.cdn.ampproject.org/v/s/es-us.vida-estilo.yahoo.com/amphtml/thalia-festeja-los-69-anos-de-su-marido-y-fans-creen-que-se-puso-botox-mira-que-joven-luce-033930503.html?amp_js_v=0.1\r\n\r\nBut this is being redirected to this one-way hashed subdomain:\r\n\r\nhttps://7ai3pvhopvh4fo4mm3sukbcq2wp6ubqw5ndoini2u3d6epbxubaa.cdn.ampproject.org/v/s/es-us.vida-estilo.yahoo.com/amphtml/thalia-festeja-los-69-anos-de-su-marido-y-fans-creen-que-se-puso-botox-mira-que-joven-luce-033930503.html?amp_js_v=0.1\r\n\r\nAccording to the docs this is done in certain cases, but for unclear reasons:\r\n\r\n> Subdomains created by the Google AMP Cache will be human-readable when character limits and technical specs allow, and will closely resemble the publisher's own domain.\r\n\r\n> Where technical limitations prevent a human readable subdomain, a one-way hash will be used instead.\r\n\r\nNot clear why this domain is subject to \"technical limitations\".  Bug?\r\n\r\nAnyway, in this example, the `Origin` is now presented as `7ai3pvhopvh4fo4mm3sukbcq2wp6ubqw5ndoini2u3d6epbxubaa.cdn.ampproject.org`, which does not pass our CORS header validation as specified by the AMP docs.\r\n\r\nSo how should publishers be validating these one-way hashed publisher subdomains?\r\n"},
{"text": "The `controls` attribute is valid for `amp-anim` but it is not documented in [amp-anim.md](https://github.com/ampproject/amphtml/blob/master/extensions/amp-anim/amp-anim.md)"},
{"text": "I used the command provided in https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md/CONTRIBUTING.md to install Node.js but it didn't work:\r\n\r\n```\r\n$ nvm install --lts\r\n--lts.0.0\r\nNode.js v--lts.0.0 is only available in 32-bit.\r\n```\r\n\r\nOS: Windows 10 64-bit\r\n\r\nWorkaround:\r\n\r\nRun `nvm list available` to list available Node.js versions, then install the latest LTS version using `nvm install x.y.z` (8.11.4 in my case)."},
{"text": "## What's the issue?\r\n\r\nThe correct way to handle a form within a list in unintuitive and the docs/examples are out of date. It appears that:\r\n\r\n- the runtime accepts nested templates ([example](https://github.com/ampproject/amphtml/blob/master/examples/amp-list-with-form.amp.html#L38))\r\n- the validator does not [issue 16056](https://github.com/ampproject/amphtml/issues/16056)\r\n- the [current docs](https://www.ampproject.org/docs/troubleshooting/validation_errors#disallowed-tag-ancestor) agree with the validator\r\n\r\n## How do we reproduce the issue?\r\n1. create an amp-list that contains a form with \r\n```\r\n<div submit-success>\r\n    <template type=\"amp-mustache\">\r\n    </template>\r\n  </div>\r\n  <div submit\r\n```\r\n2. Fail validation\r\n\r\nThe current workaround is to name the template and reference it that way.\r\n`<div submit-success template=\"template-name\"></div>`\r\nbut this isn't well documented.\r\n\r\n## What browsers are affected?\r\n\r\nAll\r\n\r\n## Which AMP version is affected?\r\n\r\n1534366245210\r\n"},
{"text": "Currently if a contributor runs `gulp validator` after following the instructions in `contributing/getting-started-e2e.md`, they are greeted with the following error:\r\n\r\nhttps://github.com/ampproject/amphtml/blob/af504f3f42d414b5c78b55ab969d3147b2d0d9f9/validator/build.py#L77\r\n\r\nThat's a good instruction if the contributor develops on a Debian-based Linux distro, but on other platforms it is confusing. This error message should direct users to the correct documentation to resolve the error. And we should provide step-by-step instructions for installing the validator's dependencies.\r\n\r\n\r\n\r\n/cc @aghassemi since we discussed docs updates for before the Contributor Summit"},
{"text": "## What's the issue?\r\n\r\nThe error message for using `AMP.setState` without importing `amp-bind` is a stacktrace.\r\n\r\n## How do we reproduce the issue?\r\n\r\nJust put the following button in the doc without having a script tag to import `amp-bind`:\r\n\r\n```js\r\n <button on=\"tap:AMP.setState({})\">a asdf asdf sdf</button>\r\n```\r\n\r\nThis results in the following error:\r\n\r\n```\r\ndocument-register-element.patched.js:501 Uncaught TypeError: Cannot read property 'obj' of undefined\r\n    at Kc (document-register-element.patched.js:501)\r\n    at sc (document-register-element.patched.js:449)\r\n    at qc (document-register-element.patched.js:427)\r\n    at yd (document-register-element.patched.js:1413)\r\n    at Rk.f.handleAmpTarget (resources-impl.js:1517)\r\n    at Fh (v0.js:189)\r\n    at m (v0.js:188)\r\n    at v0.js:188\r\n    at Array.forEach (<anonymous>)\r\n    at Eh (v0.js:188)\r\n```\r\n\r\nConsider making the error more friendly."},
{"text": "## What's the issue?\r\nStep `11` of the [Getting Started: Quick Start](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-quick.md) is missing the command to fetch the `upstream/master` ref.\r\n \r\n## What you will need to do:\r\n\r\nTo correct this, we need to add `git fetch upstream master` above `git branch -u upstream/master master`\r\n\r\nTo introduce this change, please follow the steps below.\r\n\r\n- [ ] Claim this issue by adding a comment below. Please only claim this bug if you plan on starting work in the next day or so. (If you [join the AMP Project](https://goo.gl/forms/T65peVtfQfEoDWeD3) we'll be able to assign this issue to you after you've claimed it.)\r\n- [ ] If you aren't too familiar with Git/GitHub, see the [Getting Started End-to-End Guide](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md) for [an intro to Git & GitHub](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#intro-to-git-and-github), and [how to get a copy of the code](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#get-a-copy-of-the-amphtml-code). You can also refer to the [Quick Start Guide](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-quick.md) for the necessary setup steps with less explanation than the End-to-End guide.\r\n- [ ] Follow the instructions for [building AMP](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#building-amp-and-starting-a-local-server).\r\n- [ ] [Create a Git branch](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#create-a-git-branch) for making your changes.\r\n- [ ] [Sign the Contributor License Agreement](https://github.com/ampproject/amphtml/blob/master/CONTRIBUTING.md#contributor-license-agreement) before creating a Pull Request. (If you are contributing code on behalf of a corporation start this process as early as possible.)\r\n- [ ] [Commit your changes](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#edit-files-and-commit-them) frequently.\r\n- [ ] [Push your changes to GitHub](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#push-your-changes-to-your-github-fork).\r\n- [ ] [Create a Pull Request](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#send-a-pull-request-ie-request-a-code-review). Mention `closes Issue <this issue number>` in the description.\r\n- [ ] [Respond to your reviewer's comments](https://github.com/ampproject/amphtml/blob/master/contributing/getting-started-e2e.md#respond-to-pull-request-comments) (if any).\r\n\r\n## Questions?\r\nIf you have questions ask in your Pull Request (if you've created one) or see the How to get help section of the Getting Started guide."},
{"text": "[Following the tutorial](https://codelabs.developers.google.com/codelabs/creating-your-first-amp-component/#3) and running `gulp test --files extensions/amp-hello-world/0.1/test/test-amp-hello-world.js` on the 4th step  throws `TypeError: element.build is not a function` when running `element.build();`\r\n"},
{"text": "During a discussion about clearing a form that uses amp-bind in https://github.com/ampproject/amphtml/issues/7368#issuecomment-432385515 I noticed that amp-bind updates an input element's value attribute. I want to make sure this is the desired behavior and not a bug. \r\n\r\nShould the attribute be updated, given that the attribute is the initial value? I believe the current behavior is not what we want: pressing an `<input type=\"reset\">` will assign all input elements their `value` attribute, which if bind mutated, will not be the original empty value that a document author expects.\r\n\r\nIf this is the desired behavior, I think we should document it specifically."},
{"text": "There is a demo site `google-ads-amp-demos.com` linked in `ads/google/csa.md` but the demo site appears to be down. This issue tracks the effort to remove or replace the linked example after a conversation.\r\n\r\n/cc @jasti \r\n"},
{"text": "There is a demo site `google-ads-amp-demos.com` linked in `ads/google/csa.md` but the demo site appears to be down. This issue tracks the effort to remove or replace the linked example after a conversation.\r\n\r\n/cc @jasti \r\n"},
{"text": "## What's the issue?\r\n\r\nThe [AMP HTML URL Variable Substitutions](https://github.com/ampproject/amphtml/blob/master/spec/amp-var-substitutions.md) page is missing definitions for \"VARIANT(experiment)\" and \"VARIANTS\" as suggested by the link on the [amp-experiment Reporting section](https://www.ampproject.org/docs/reference/components/amp-experiment#reporting).\r\n\r\n## Steps to Reproduce\r\n\r\n1. Navigate to the [amp-experiment documentation, Reporting section](https://www.ampproject.org/docs/reference/components/amp-experiment#reporting)\r\n2. Read that \"VARIANT(experiment)\" and \"VARIANTS\" are substitution variables, but the link provided-[URL substitution variable](https://github.com/ampproject/amphtml/blob/master/spec/amp-var-substitutions.md)-does not contain details on these variables as suggested."},
{"text": "This is not really a feature request but not really a bug tho, there is just something in the docs that is not as clear as it should be imo.\r\n\r\nThe `amp-img` tag is different from the `img` (HTML) tag as in the amp version we can specify a ratio which scales up to natural size of the image or up to a boundary like `max-width` (CSS) [1].\r\nHTML5 img allows `%` or no unit only (*), otherwise it uses px by default [2].\r\n\r\n**It is unclear to me which units in `amp-img` are valid.**\r\n\r\nPlease make it clear on [1] which units are allowed or at least how exactly the values differ from HTML5.\r\n\r\nThanks in advance\r\n\r\n*) The German and English page of [2] and [3] differ heavily concerning attributes. The English version only mentions px, the German version also mentions `%`.\r\n\r\n[1] https://www.ampproject.org/docs/reference/components/amp-img\r\n[2] https://developer.mozilla.org/de/docs/Web/HTML/Element/img\r\n[3] https://developer.mozilla.org/en/docs/Web/HTML/Element/img\r\n\r\n----\r\n\r\nMy guess is that those formats are invalid:\r\n\r\n    <amp-img width=\"200px\" ....\r\n\r\n    <amp-img width=\"50%\" ....\r\n\r\n    <amp-img width=\"50em\" ....\r\n"},
{"text": "The [How AMP HTML is deployed](https://github.com/ampproject/amphtml/blob/master/build-system/SERVING.md) article was written 3 years ago, and is outdated.\r\n\r\nOnce the new release workflow is in place, this file should either be updated or deleted."},
{"text": "When I'm trying to use `<picture>` element\u2026 \r\n\r\n```html\r\n<picture>\r\n  <source src=\"https://satyr.io/160x90/red\" \r\n    media=\"(max-width: 599px)\">\r\n  <amp-img src=\"https://satyr.io/160x90/green\" \r\n    width=\"16\" height=\"9\" layout=\"responsive\"\r\n    media=\"(min-width: 600px)\">\r\n</picture>    \r\n```\r\n\r\nhttps://jsbin.com/voresus/edit?html,output\r\n\r\n\u2026I'm getting this message from AMP validator:\r\n\r\n```\r\nThe parent tag of tag 'picture' is 'body', but it can only be 'noscript'\r\n```\r\n\r\nI have read some texts saying something like \"`<picture>` is replaced by inheriting `<amp-img>` and `media=\"\"` attribute\"\u2026 \r\n\r\nBut why? The answer is impossible to find.\r\n\r\n### Suggested steps\r\n\r\n1. If `<picture>` is disallowed, it should be here: https://amp.dev/documentation/guides-and-tutorials/learn/spec/amphtml#html-tags\r\n2. There should be section about `<picture>` replacement in he AMP docs for [amp-img](https://amp.dev/documentation/components/amp-img) and [Responsive images with srcset, sizes & heights](https://amp.dev/documentation/guides-and-tutorials/develop/style_and_layout/art_direction) both mentions all `<picture>` use cases and how to solve them in AMP.\r\n"},
{"text": "## What's the issue?\r\nSome accents appear when converting markdown to amp.dev docs\r\n\r\nthis:\r\n![image](https://user-images.githubusercontent.com/4308648/59971317-a7168b80-9547-11e9-9861-069ed94eff3a.png)\r\n\r\nbecome this:\r\n![image](https://user-images.githubusercontent.com/4308648/59971194-76355700-9545-11e9-882a-39613d099184.png)\r\n\r\n![image](https://user-images.githubusercontent.com/4308648/59971246-36bb3a80-9546-11e9-9f8d-54ac7c8cc372.png)\r\n\r\n## How do we reproduce the issue?\r\nVisit some documentation pages, like:\r\n- https://amp.dev/documentation/components/amp-user-location?format=websites\r\n- https://amp.dev/documentation/components/amp-accordion?format=websites\r\n\r\n## What browsers are affected?\r\n\r\nAll browsers\r\n"},
{"text": "## What's the issue?\r\n\r\nI am following the AMP-BIND tutorial and getting this error: The attribute '[src]' may not appear in tag 'AMP-IMG (AMP4EMAIL)'\r\n\r\n## How do we reproduce the issue?\r\nWhen following the amp-bind tutorial https://amp.dev/documentation/components/amp-bind?format=email in the \"A slightly more complex example\" it give me the error of [src] not being supported.\r\n\r\n```\r\n<!doctype html>\r\n<html 4email>\r\n<head>\r\n  <meta charset=\"utf-8\">\r\n  <script async src=\"https://cdn.ampproject.org/v0.js\"></script>\r\n  <script async custom-element=\"amp-bind\" src=\"https://cdn.ampproject.org/v0/amp-bind-0.1.js\"></script>\r\n  <style amp4email-boilerplate>body{visibility:hidden}</style>\r\n</head>\r\n<body>\r\n  <!-- Store complex nested JSON data in <amp-state> elements. -->\r\n<amp-state id=\"myAnimals\">\r\n  <script type=\"application/json\">\r\n    {\r\n      \"tiger\": {\r\n        \"imageUrl\": \"https://raw.githubusercontent.com/ampproject/docs/future/pages/static/img/docs/tutorials/firstemail/photo_by_nick_karvounis.jpg\"\r\n      },\r\n      \"cat\": {\r\n        \"imageUrl\": \"https://raw.githubusercontent.com/ampproject/docs/future/pages/static/img/docs/tutorials/firstemail/photo_by_caleb_woods.jpg\"\r\n      }\r\n    }\r\n  </script>\r\n</amp-state>\r\n\r\n<p [text]=\"'This is a ' + currentAnimal + '.'\">This is a tiger.</p>\r\n\r\n\r\n<!-- Change an image's src with the [src] binding. -->\r\n<amp-img width=\"300\" height=\"200\" src=\"https://raw.githubusercontent.com/ampproject/docs/future/pages/static/img/docs/tutorials/firstemail/photo_by_nick_karvounis.jpg\"\r\n    [src]=\"myAnimals[currentAnimal].imageUrl\">\r\n</amp-img>\r\n\r\n<button on=\"tap:AMP.setState({currentAnimal: 'cat'})\">Set to Cat</button>\r\n</body>\r\n</html>\r\n```\r\n\r\nOutput error in playground : _The attribute '[src]' may not appear in tag 'AMP-IMG (AMP4EMAIL)'._\r\n\r\n## Which AMP version is affected?\r\nEmail playground\r\n<script async custom-element=\"amp-bind\" src=\"https://cdn.ampproject.org/v0/amp-bind-0.1.js\"></script>\r\n"},
{"text": "It seems that this line from the `amp-animation` docs contains a broken link:\r\n\r\nhttps://github.com/ampproject/amphtml/blob/0937333cb3f4d1b09bd41f86db565c2dcda7ed3a/extensions/amp-animation/amp-animation.md#L646\r\n\r\nSpecifically, the link is to: [CSS transforms (+ feature detection) not working on SVG elements](https://developer.microsoft.com/en-us/microsoft-edge/platform/issues/1173754/) ([archived version](https://web.archive.org/web/20171213084125/https://developer.microsoft.com/en-us/microsoft-edge/platform/issues/1173754/)).\r\n\r\nThis makes Travis CI fail, so I'll add it as an exception in `check-links.js`. We should do one of these things:\r\n\r\n-   Find an alternate source for this issue\r\n-   Link to archived page\r\n-   Remove the reference to this issue all together"},
{"text": "On the current documentation page: https://amp.dev/documentation/components/amp-experiment/\r\n\r\nIt has no mention of 'none'. This was noticed by a client, and fix this."},
{"text": "## What's the issue?\r\nI am using `amp-bind` to update the href attribute of an anchor, however, I am getting an error message when input my code into amp.gmail.dev/playground. According to the documentation at https://amp.dev/documentation/components/amp-bind/?format=email#element-specific-attributes [href] is a valid attribute to use with binding.\r\n\r\n## Briefly describe the bug.\r\nWhen I input code with binding on the href attribute of an anchor tag I get an error message that reads \"The attribute '[href]' may not appear in tag 'A (AMP4EMAIL)'. \"\r\n\r\n## How do we reproduce the issue?\r\nTake this gist: https://gist.github.com/ChrisThorn10/7e0e0b5c5b717a8c3159879480b2f1b8\r\nand paste it into the playground at amp.gmail.dev/playground\r\n\r\nYou will see the error message.  Once you remove `[href]=\"'https://www.lendingtree.com?' + recommendedProduct[currentProduct].buttonUrl\"` from line 295 the code will render correctly.\r\n\r\n\r\n## What browsers are affected? \r\nNot a browser specific issue\r\n\r\n## Which AMP version is affected? \r\nAMP4EMAIL\r\n"},
{"text": "## What's the issue?\r\n\r\nThis is a documentation issue.\r\n\r\nThe [I2I issue template](https://github.com/ampproject/amphtml/edit/master/.github/ISSUE_TEMPLATE/intent-to-implement--i2i-.md) references [CONTRIBUTING.md](https://github.com/ampproject/amphtml/blob/master/CONTRIBUTING.md) for how to fill-up the I2I template, yet there's no reference to I2I in CONTRIBUTING.md.\r\n\r\n/cc @nainar"},
{"text": "The `see more` overflow element is impossible to see in the [overflow example](https://amp.dev/documentation/components/amp-list/?format=websites#specifying-an-overflow). We should give it a background. \r\n"},
{"text": "https://amp.dev/documentation/guides-and-tutorials/learn/amp-actions-and-events/ doesn't document what high and low level trust actions are. "},
{"text": "I have used 'animate' attribute on amp-accordion tag. It works fine but the amp validation is failing as it says:\r\nThe attribute 'animate' in tag 'amp-accordion' is set to the invalid value 'true'.\r\n\r\nI am not able to identify why amp-validation error is arising. Please provide a fix for this"},
{"text": "## What's the issue?\r\n\r\nAmp-carousel 2.0 not picking up SVG overrides for button classes\r\n\r\n## How do we reproduce the issue?\r\n\r\nFollow the documentation and override Slideshow buttons using the dedicated .amp-carousel-button classes. While SVG code is fully valid (Not complains from Chrome Editor on the format), it's not displayed and the box remains empty.\r\n\r\n## What browsers are affected?\r\n\r\nAll\r\n\r\n## Which AMP version is affected?\r\n\r\nAmp Carousel 2.0\r\n"},
{"text": "The current setup will break the amp.dev component doc importer as the readme.md will be associated with the latest version 1.0 resulting in the translations not being shown.\r\n\r\nPlease, copy the readme and mark the new version as experimental:\r\n\r\n```\r\n<td width=\"40%\"><strong>Availability</strong></td>\r\n    <td><a href=\"https://amp.dev/documentation/guides-and-tutorials/learn/experimental\">Experimental</a></td>\r\n```\r\n\r\n//cc @leafsy @sparhami "},
{"text": "\r\nWhile [Github](https://github.com/ampproject/amphtml/blob/master/spec/amp-html-format.md) states that script types can only be `application/ld+json` or `text/plain`, the [amp-access documentation](https://amp.dev/documentation/components/amp-access/) shows that application/json is permitted for the amp-access case.\r\n\r\nMy assumption is that the more pervasive view i.e. application/ld+json or text/plain for all cases and \"application/json\" for amp-access is the right model. \r\n\r\n\r\ncc @CrystalOnScript @jpettitt \r\n\r\n[edited by @jpettitt for markdown readability]"},
{"text": "# What's the issue?\r\n\r\nThe baseScore option is missing from the example markup.  The baseScore should have a reasonable default assigned to the example markup.\r\n\r\n## How do we reproduce the issue?\r\n\r\nhttps://amp.dev/documentation/components/amp-subscriptions-google/ \r\nhttps://github.com/ampproject/amphtml/blob/master/extensions/amp-subscriptions-google/amp-subscriptions-google.md\r\n\r\nThe \"Example with Markup\" on these pages is missing the baseScore.\r\n\r\n## What browsers are affected?\r\n\r\nN/A\r\n\r\n## Which AMP version is affected?\r\n\r\nDocumentation\r\n"},
{"text": "## What's the issue?\r\n\r\nIndentation for `</template>` is wrong in amp-list amp.dev documentation\r\n## How do we reproduce the issue?\r\n\r\nhttps://amp.dev/documentation/components/amp-list/\r\n\r\n1. Go to the above page\r\n2. Search for `</template>`\r\n3. Most of the occurrences have indentation wrong\r\n\r\n## What browsers are affected?\r\n\r\nAll browsers"},
{"text": "https://github.com/ampproject/amphtml/blob/master/examples/amp-action-macro.html\r\n\r\n![1](https://user-images.githubusercontent.com/9099754/69415928-16d3b100-0d48-11ea-8fe1-8a08d8752ae1.PNG)\r\n![2](https://user-images.githubusercontent.com/9099754/69415954-20f5af80-0d48-11ea-9674-153c20cefa27.PNG)\r\nThe element did not specify a layout attribute. Check https://amp.dev/documentation/guides-and-tutorials/develop/style_and_layout/control_layout and the respective element documentation for details.\r\nvf @ error.js:195\r\n(anonymous) @ error.js:128\r\nf.assert @ log.js:428\r\nD @ log.js:988\r\nFi.b.mg @ custom-element.js:430\r\nFi.b.jd @ custom-element.js:415\r\nFi.b.$f @ custom-element.js:910\r\nFi.b.upgrade @ custom-element.js:389\r\nIi @ custom-element-registry.js:89\r\ndm @ custom-element-registry.js:73\r\ncm @ extensions-impl.js:347\r\n(anonymous) @ extensions-impl.js:323\r\nf.addDocFactory @ extensions-impl.js:408\r\nf.addElement @ extensions-impl.js:322\r\n(anonymous) @ amp-action-macro.js:122\r\nf @ amp-action-macro.js:121\r\nf.registerExtension @ extensions-impl.js:154\r\n(anonymous) @ runtime.js:205\r\nPromise.then (async)\r\namp-action-macro @ runtime.js:201\r\nwi @ chunk.js:192\r\nsi.Ze @ chunk.js:410\r\n(anonymous) @ chunk.js:461\r\nPromise.then (async)\r\nzi @ chunk.js:460\r\nsi.ra @ chunk.js:479\r\nsi.runForStartup @ chunk.js:369\r\nti @ chunk.js:69\r\nim @ runtime.js:333\r\nb @ runtime.js:218\r\n(anonymous) @ runtime.js:260\r\n(anonymous) @ timer-impl.js:83\r\nsetTimeout (async)\r\nf.delay @ timer-impl.js:89\r\nkm @ runtime.js:998\r\ngm @ runtime.js:242\r\nlm @ runtime.js:344\r\n(anonymous) @ amp.js:118\r\nwi @ chunk.js:192\r\nsi.Ze @ chunk.js:410\r\n(anonymous) @ chunk.js:461\r\nPromise.then (async)\r\nzi @ chunk.js:460\r\nsi.ra @ chunk.js:479\r\n(anonymous) @ chunk.js:436\r\nPromise.then (async)\r\nsi.Ze @ chunk.js:416\r\n(anonymous) @ chunk.js:461\r\nPromise.then (async)\r\nzi @ chunk.js:460\r\nsi.ra @ chunk.js:479\r\n(anonymous) @ chunk.js:436\r\nPromise.then (async)\r\nsi.Ze @ chunk.js:416\r\n(anonymous) @ chunk.js:461\r\nPromise.then (async)\r\nzi @ chunk.js:460\r\nsi.ra @ chunk.js:479\r\nsi.runForStartup @ chunk.js:369\r\nti @ chunk.js:69\r\n(anonymous) @ amp.js:91\r\n(anonymous) @ amp.js:66.)\r\n"},
{"text": "## What's the issue?\r\n\r\nFollowing the websites advanced features tutorial-guide, I tried to add the sidebar menu in the demo News site. The hamburger button does not open the sidebar.\r\n\r\n## How do we reproduce the issue?\r\n\r\nI just followed the tutorial for advanced features about \"Navigating your site\". I wrote the code that I ran in browsers.\r\n\r\n## What browsers are affected?\r\n\r\nThe same code operates correctly on Desktop Safari, but not in Chrome or Firefox.\r\n\r\n## Which AMP version is affected?\r\n\r\nIs this a new issue? Or was it always broken? Paste the version of AMP where you saw this issue. (You can find the version printed in your browser's console.)\r\n\r\nI guess I'm using the current version of AMP."},
{"text": "I think we forgot to write the documentation for the stories expandable elements (tooltip) analytics: https://github.com/ampproject/amphtml/pull/25547(and maybe also https://github.com/ampproject/amphtml/pull/25649?)"},
{"text": "## What's the issue?\r\nThe amp-subscriptions documentation mentions the ability to use `amp-subscriptions` `data` from the entitlements response ([document section](https://amp.dev/documentation/components/amp-subscriptions/#paywall-dialogs). Turns out based on [this comment](https://github.com/ampproject/amphtml/issues/23181#issuecomment-510088010) this is not currently possible (more specifically `amp-list` should be used instead). The documentation should be updated to make this clear.\r\n\r\n## How do we reproduce the issue?\r\n\r\n1. Navigate to the [amp-subscriptions documentation](https://github.com/ampproject/amphtml/blob/master/extensions/amp-subscriptions/amp-subscriptions.md#paywall-dialogs)\r\n\r\n## What browsers are affected?\r\n\r\nN/A\r\n\r\n## Which AMP version is affected?\r\n\r\nMaster branch and public documentation (amp.dev)\r\n"},
{"text": "## What's the issue?\r\n\r\nThe `amp-form` component documentation currently has a blurb about redirecting after a submission, but it should be hidden when the currently-selected format is email. The \"Redirecting after a submission\" section is already hidden.\r\n\r\n## How do we reproduce the issue?\r\n\r\nhttps://amp.dev/documentation/components/amp-form/?format=email#action-xhr has the following:\r\n\r\n> To learn about redirecting the user after successfully submitting the form, see the Redirecting after a submission section below.\r\n\r\nThat should be hidden.\r\n\r\n/to @caroqliu "},
{"text": "## What's the issue?\r\n\r\nIn the documentation its clearly stated that AMP.navigateTo supports but I can't get it to work with select event on amp-autocomplete\r\n\r\nhttps://amp.dev/documentation/guides-and-tutorials/learn/amp-actions-and-events/\r\n\r\n## How do we reproduce the issue?\r\n\r\n1. go to https://site.ampify.io/test/navigate.html\r\n2. type \"n\" in the input field\r\n3. select the first result from the autocomplete\r\n4. navigates to https://example.com/?cityId=518&r=RANDOM  (RANDOM should to be substituted)\r\n\r\n## What browsers are affected?\r\n\r\nALL\r\n\r\n## Which AMP version is affected?\r\n\r\nLatest"},
{"text": "## What's the issue?\r\n\r\nDocumentation and examples refer to \"amp-bind-recaptcha\" as if it were an AMP component. It is not.\r\n\r\n## How do we reproduce the issue?\r\n\r\nSee <https://amp.dev/documentation/examples/components/amp-bind-recaptcha/>\r\n\r\n![image](https://user-images.githubusercontent.com/335669/80825532-9a72b300-8bae-11ea-8ff6-ea401cbb6497.png)\r\n\r\nAlso see: <https://amp.dev/documentation/components/amp-bind>\r\n\r\n![image](https://user-images.githubusercontent.com/335669/80825693-e58cc600-8bae-11ea-996a-d352022db23e.png)\r\n\r\n## What browsers are affected?\r\n\r\nNA\r\n\r\n## Which AMP version is affected?\r\n\r\nNA\r\n"},
{"text": "## What's the issue?\r\n\r\nI want to use overflow-x in code snippets on a site. The amp documentation does this: [example, scroll the code snipped](https://amp.dev/documentation/guides-and-tutorials/learn/spec/amp-boilerplate/?format=websites).\r\nBut the documentation also states that I'm not allowed to use [overflow](https://amp.dev/documentation/guides-and-tutorials/learn/spec/amphtml/?format=websites#properties) to add scrollbars anywhere.\r\n\r\n## How do we reproduce the issue?\r\n\r\nWell, you can use the example from above and add the development=1 argument which should tell you that this is invalid. But it doesn't.\r\n\r\n## Which AMP version is affected?\r\n\r\nVersion 2005151844001\r\n\r\n## What now?\r\n\r\nThe amp documentation makes a good case for allowing `overflow-x: auto` and I don't see a layout problem there. For other cases... I don't know, having amp components within a scolling div might cause issues. Other than that I don't really see a problem in having scrolling elements."},
{"text": "[Documentation describes that it defaults to `true`](https://amp.dev/documentation/components/amp-base-carousel/#miscellaneous), but basic samples omitting the `loop` attribute do not appear to have looping enabled.\r\n\r\nIs the documentation or the code wrong?\r\n\r\nEDIT: Updating the docs such that the default should be that absence => `false`, consistent with `amp-carousel` 0.1 and 0.2."},
{"text": "Original Launchpad bug 502789: https://bugs.launchpad.net/ipython/+bug/502789\nReported by: ellisonbg (Brian Granger).\n\nThe copyright statements in some source files are inaccurate and need to be updated.\n"},
{"text": "two documentation issues I stumbled upon:\n\ndocs/source/parallel/parallel_multiengine.txt\nthe important part what dv.track tracks is missing:\n\n```\ndv.track : bool\n    whether to instruct pyzmq to track when \n    This is [...]\n```\n\ndocs/source/config/plugins.txt\nparallemagic.py -> parallelmagic.py\nIPython/extensions/pretty does not exist in source.\n"},
{"text": "The documentation (http://ipython.org/ipython-doc/stable/interactive/shell.html) says to use 'ipython -p sh' to invoke ipython for use as a system shell; however, doing so gives the following error message:\n\n[TerminalIPythonApp] Bad config encountered during initialization:\n[TerminalIPythonApp] Unrecognized flag: '-p'\n"},
{"text": "Once you enter one of the documentation pages (e.g. http://ipython.org/ipython-doc/rel-0.12/index.html#), there is no link that would lead you back to front page (http://ipython.org). Clicking on `home` link or on the header with IPython logo at the top of the page both lead you to the main documentation page instead of on the http://ipython.org.\n"},
{"text": "The IPython testing documentation  http://ipython.org/ipython-doc/dev/development/testing.html \nstill mentions twisted tests and the trial test runner. \nI guess this should be removed since IPython no longer uses twisted?\n"},
{"text": "According to the docstring of `ipython.py`, \"the actual ipython script to be installed with 'python setup.py install' is\u00a0in './scripts' directory.\"  But the only thing in `scripts` is `ipython_win_post_install.py`, which isn't it.  \n"},
{"text": "Hello, \n\nWhile reading the doc page on IPython.core.display I noticed a broken link for an image. The page in questions is: \n\nhttp://ipython.org/ipython-doc/rel-0.13.1/api/generated/IPython.core.display.html?highlight=ipython.core.display#IPython.core.display\n\nThe html tag is: \n<img src=\"../../_images/inheritance93b715422d.png\" usemap=\"#inheritance93b715422d\" class=\"inheritance\">\n\nFrom the javascript console we see:\nFailed to load resource: the server responded with a status of 404 (Not Found) \nhttp://ipython.org/ipython-doc/rel-0.13.1/_images/inheritance93b715422d.png\n\nI quick look at the source for IPython didn't show a png with that name. \n\nRegards,\n-jk\n"},
{"labels":["documentation"],"text":"It seems that @radar started working on an overhaul of the Getting Started guide a while back but only the first (#38268) of the PRs batch (#38328 #38329) got merged. This resulted on the [guide](https://edgeguides.rubyonrails.org/getting_started.html) being inconsistent. More specifically, at the [hello rails section](https://edgeguides.rubyonrails.org/getting_started.html#say-hello-rails) we moved away from introducing a welcome controller, but [later in the guide](https://edgeguides.rubyonrails.org/getting_started.html#setting-the-application-home-page) it is still referenced.\r\n\r\nWhat do you think is the best course of action here? The rest of the work is still there (see stale PRs above), should this be reverted or re-open the rest of the PRs? I can help with proof-reading and squashing/rebasing."},{"labels":["documentation"],"text":"![image](https://user-images.githubusercontent.com/48516031/82900356-fb639000-9f8e-11ea-909f-cbdd1ff1155d.png)\r\nIn the Rails Installer page\r\n![image](https://user-images.githubusercontent.com/48516031/82900496-3665c380-9f8f-11ea-8e3b-59e09f97d8f7.png)\r\nShouldn't this be updated? Or specify the Rails Installer is for quickly installing lower versions of rails and if user wants the newest version of rails they should follow the following installation instructions?\r\n![image](https://user-images.githubusercontent.com/48516031/82901086-04a12c80-9f90-11ea-88f3-8446f89ce4f0.png)\r\n\r\n\r\n"},{"labels":["documentation"],"text":"The prod and edge versions of \"Getting Started with Rails\" both point to RailsInstaller as a great way to get started on Windows. However, RailsInstaller is offering only Ruby 2.2 and 2.3. When you get to the `bundle install` step, multiple of the standard gems now require at least Ruby 2.4. If we have a relationship with Engine Yard, perhaps a request from the right member of the team would move them to release more current versions of the RailsInstaller downloadables."},{"labels":[null,null,"documentation"],"text":"Rails Guides [state](https://edgeguides.rubyonrails.org/caching_with_rails.html#caching-in-development) that \"Rails provides the rails command dev:cache to easily toggle caching on/off\".\r\n\r\nThis is not entirely true since `NullStore` still caches via `LocalCache`, so there is no way to disable low-level cache within the scope of a single web request, regardless of the toggle.\r\n\r\nThe [low-level caching docs](https://edgeguides.rubyonrails.org/caching_with_rails.html#low-level-caching) don't really help since they provide a model-level example, giving the impression of a generic mechanism independent of web requests or Rack middleware.\r\n\r\nMy questions are:\r\n\r\n1. Why does `NullStore` implement the local cache strategy? Shouldn't it be a veritable bit bucket?\r\n2. Why isn't `ActiveSupport::Cache::Strategy::LocalCache` also toggled via `rails dev:cache`?\r\n3. At the very least, shouldn't this caveat be mentioned in Rails Guides, stating clearly that low-level cache still kicks in within the scope of a web request?\r\n\r\n\r\n### Steps to reproduce\r\n\r\nCreate a new Rails app and make sure to disable local caching:\r\n\r\n```\r\n$ bin/rails dev:cache\r\nDevelopment mode is no longer being cached.\r\n```\r\n\r\nAdd the following to the application layout or any view:\r\n\r\n```\r\n<pre>\r\n<%= Rails.cache.fetch('foo') { rand } %>\r\n<%= Rails.cache.fetch('foo') { rand } %>\r\n</pre>\r\n```\r\n\r\nRun `bin/rails s` and visit the modified view.\r\n\r\n\r\n### Expected behavior\r\n\r\nI'd expect `Rails.cache.fetch` to return different values given that \"development mode is no longer being cached\".\r\n\r\n\r\n### Actual behavior\r\n\r\n`Rails.cache.fetch` still caches values.\r\n\r\n\r\n### System configuration\r\n\r\n**Rails version**: 6.0.2.1\r\n\r\n**Ruby version**: 2.7.0p0\r\n"},{"labels":[null,"documentation"],"text":"There is a wrong guide in [RequestForgeryProtection comment](https://github.com/rails/rails/blob/08e4a71d02ee8556e5a7ebc4b0f8c364e0b8abd6/actionpack/lib/action_controller/metal/request_forgery_protection.rb) and [documentation](https://api.rubyonrails.org/classes/ActionController/RequestForgeryProtection.html)\r\n\r\n```ruby\r\nclass ApplicationController < ActionController::Base\r\n  protect_from_forgery unless: -> { request.format.json? }\r\nend\r\n```\r\n\r\n> It is generally safe to exclude XHR requests from CSRF protection (like the code snippet above does), because XHR requests can only be made from the same origin.\r\n\r\nHowever, as I know same origin policy cannot protect CSRF attack because [it does not prevent sending requests.](https://stackoverflow.com/a/33324803/2566679)\r\n\r\n> ... one origin is permitted to send information to another origin, but one origin is not permitted to receive information from another origin   [Same Origin Policy\r\n](https://www.w3.org/Security/wiki/Same_Origin_Policy)\r\n\r\nIf this is true, I think this guide must be modified, soon.\r\n"},{"labels":["documentation"],"text":"### Steps to reproduce\r\n<!-- (Guidelines for creating a bug report are [available\r\nhere](https://edgeguides.rubyonrails.org/contributing_to_ruby_on_rails.html#creating-a-bug-report)) -->\r\n\r\nFollow the Docs: https://guides.rubyonrails.org/testing.html#using-separate-files\r\n\r\n```ruby\r\n# lib/test/authenticated_test_helper.rb\r\nmodule AuthenticatedTestHelper\r\n  def sign_in(email, password)\r\n    post account_login_url(email: email, password: password)\r\n  end\r\n\r\n  def admin_sign_in(email, password)\r\n    post admin_session_url(email: email, password: password)\r\n  end\r\nend\r\n```\r\n\r\n```ruby\r\n# test/controllers/admin/samples_controller_test.rb\r\nrequire 'test_helper'\r\nrequire 'test/authenticated_test_helper' # << throws: `cannot load such file -- lib/test/authenticated_test_helper\r\n\r\nmodule Admin\r\n  module SamplesControllerTest < ActionDispatch::IntegrationTest\r\n    ...\r\n    ...\r\n  end\r\nend\r\n```\r\n\r\n### Expected behavior\r\n<!-- Tell us what should happen -->\r\nIt should find the file since it's defined in EXACTLY the location that's recommended\r\n\r\n### Actual behavior\r\n<!-- Tell us what happens instead -->\r\n```bash\r\n30: from -e:1:in `<main>'\r\n        29: from -e:1:in `require'\r\n        28: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:54:in `load'\r\n        27: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:54:in `load'\r\n        26: from /home/samps/sites/cannabislims/bin/rails:9:in `<main>'\r\n        25: from /home/samps/.gem/ruby/2.6.5/gems/zeitwerk-2.2.2/lib/zeitwerk/kernel.rb:23:in `require'\r\n        24: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:30:in `require'\r\n        23: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require_with_bootsnap_lfi'\r\n        22: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/loaded_features_index.rb:92:in `register'\r\n        21: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `block in require_with_bootsnap_lfi'\r\n        20: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `require'\r\n        19: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/commands.rb:18:in `<main>'\r\n        18: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/command.rb:46:in `invoke'\r\n        17: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/command/base.rb:69:in `perform'\r\n        16: from /home/samps/.gem/ruby/2.6.5/gems/thor-1.0.1/lib/thor.rb:392:in `dispatch'\r\n        15: from /home/samps/.gem/ruby/2.6.5/gems/thor-1.0.1/lib/thor/invocation.rb:127:in `invoke_command'\r\n        14: from /home/samps/.gem/ruby/2.6.5/gems/thor-1.0.1/lib/thor/command.rb:27:in `run'\r\n        13: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/commands/test/test_command.rb:33:in `perform'\r\n        12: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:39:in `run'\r\n        11: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:50:in `load_tests'\r\n        10: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:50:in `each'\r\n         9: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:50:in `block in load_tests'\r\n         8: from /home/samps/.gem/ruby/2.6.5/gems/zeitwerk-2.2.2/lib/zeitwerk/kernel.rb:23:in `require'\r\n         7: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:30:in `require'\r\n         6: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require_with_bootsnap_lfi'\r\n         5: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/loaded_features_index.rb:92:in `register'\r\n         4: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `block in require_with_bootsnap_lfi'\r\n         3: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `require'\r\n         2: from /home/samps/sites/cannabislims/test/controllers/admin/samples_controller_tests/authenticated_test.rb:2:in `<main>'\r\n         1: from /home/samps/.gem/ruby/2.6.5/gems/zeitwerk-2.2.2/lib/zeitwerk/kernel.rb:23:in `require'\r\n/home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:33:in `require': cannot load such file -- lib/test/authenticated_test_helper (LoadError)\r\n```\r\n### System configuration\r\n**Rails version**: 6.0.2.1\r\n\r\n**Ruby version**: 2.6.5p114 (2019-10-01 revision 67812) [x86_64-linux]\r\n"},{"labels":[null,null,"documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nDocs and code are out of sync. See \r\n\r\nhttps://nextjs.org/docs/basic-features/data-fetching#getstaticprops-static-generation\r\n\r\nand\r\n\r\nhttps://github.com/Timer/next.js/blob/canary/packages/next/next-server/server/render.tsx#L611\r\n\r\n## To Reproduce\r\n\r\n```\r\nexport async function getStaticProps(ctx: any) {\r\n  console.log(ctx) // no `preview` at all in ctx\r\n```\r\n\r\n## Expected behavior\r\n\r\nSee https://nextjs.org/docs/basic-features/data-fetching#getstaticprops-static-generation . \r\n\r\n\"preview is true if the page is in the preview mode and false otherwise. See the Preview Mode documentation\"\r\n"},{"labels":[null,null,"documentation"],"text":"In our current `canary` release `Link` no longer requires to know the path of a page for client side transitions. We should update our docs to reflect that the usage of the `as` parameter is now optional."},{"labels":[null,"documentation",null],"text":"Updates to documentation that were requested by feedback by using the feedback inputs in the Next.js site and in our other community channels (GitHub Discussions, issues):\r\n\r\n- Add missing title to: https://nextjs.org/docs/advanced-features/custom-error-page - The current page doesn't have an `h1` in the beginning, which affects the search results to Algolia looking for content there.\r\n- Remove data-fetch example from the `getInitialProps` docs as it's now using `getStaticProps` and update the example to no longer have `node-fetch` in dependencies.\r\n- Mention PWA on docs. We currently have search results for \"Progressive Web App\" in our search input but not for PWA as it's never mentioned.\r\n- Add caveats to API Routes and mention that next export can't be used with them.\r\n- Reduce the redundancy of next export docs that make it a bit more annoying to navigate (caveats and deployment docs)\r\n"},{"labels":[null,"documentation"],"text":"Lots of users miss that they can write server side code in `getStaticProps` and `getServerSideProps`.\r\n\r\nWe also see numerous issues about people trying to fetch API routes. We need to fix this in the documentation by making this one of the **first things** we have in the docs."},{"labels":[null,null,"documentation"],"text":"Our Fast Refresh docs currently say that `useEffect` will re-run after every code change, even if it has no dependencies. This is not only to ensure you always write resilient code, because it's also enforced by React Strict Mode. We should mention this and link to the strict mode docs.\r\n\r\nMore on this Twitter thread: https://twitter.com/tvernon_tech/status/1289192339819249666"},{"labels":[null,null,"documentation"],"text":"From our docs:\r\n\r\n> Next.js supports ES2020 dynamic import() for JavaScript. With it you can import JavaScript modules (inc. React Components) dynamically and work with them. They also work with SSR.\r\n>\r\n> You can think of dynamic imports as another way to split your code into manageable chunks.\r\n\r\nThe above is right, but the entire page only shows examples with `next/dynamic`, which can **only** be used for React Components, and you may end up thinking it's possible to use it with modules, in which case you'll need `import()` instead. We have to be more clear about the differences here."},{"labels":[null,"documentation"],"text":"Raised in feedback, the following page is a 404 but that shouldn't be the case: https://nextjs.org/docs/tag/v9.2.2/basic-features/pages.\r\n\r\nIt's used in https://nextjs.org/docs/basic-features/pages for the top notification.\r\n\r\nI'm currently working on the fix as I write the issue  "},{"labels":[null,"documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\n[Step 4](https://github.com/vercel/next.js/tree/canary/examples/cms-sanity#step-4-set-up-environment-variables) of the [readme](https://github.com/vercel/next.js/blob/canary/examples/cms-sanity/README.md) says, i have to create a env variable named `SANITY_PROJECT_ID` but to work locally with `yarn dev` it has to be `NEXT_PUBLIC_SANITY_PROJECT_ID` (as well?) \r\n\r\nSince i was not aware of this part of the next.js config, i was confused why it wasn’t working. Since i could access all env variables within my code, but not on runtime.\r\n\r\n## To Reproduce\r\n\r\nFollow the [steps](https://github.com/vercel/next.js/tree/canary/examples/cms-sanity#step-4-set-up-environment-variables) of the [Readme](https://github.com/vercel/next.js/blob/canary/examples/cms-sanity/README.md) to install & run the example.\r\n\r\n## Expected behavior\r\n\r\nFollow the readme and have a functioning project running with `yarn dev`\r\n\r\n## Screenshots\r\n\r\nWithout the `NEXT_PUBLIC_SANITY_PROJECT_ID` variable in the `.env.local` file:\r\n![image](https://user-images.githubusercontent.com/1867543/86346498-4abb7100-bc5d-11ea-83fa-23406fe6dec6.png)\r\n\r\n\r\n## System information\r\n\r\n- OS: macOS\r\n- Browser: Chrome\r\n- Version of Next.js: v9.4.4\r\n- Version of Node.js: v.13.8.0\r\n"},{"labels":[null,null,"documentation"],"text":"Currently, we only explain to use CSS Modules in the err.sh link. The error message does not provide guidance on what to do next, only the problem.\r\n\r\nSee the messages in this file:\r\nhttps://github.com/vercel/next.js/blob/canary/packages/next/build/webpack/config/blocks/css/messages.ts\r\n\r\nWe should update the error messages displayed to the user very clear that there's an alternative so they're not stuck thinking Next.js cannot import CSS."},{"labels":["documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nI upgraded to next@9.4.5-canary.12 from next@9.4.4 because of https://github.com/vercel/next.js/issues/14186\r\nEverything works with `next dev`. However, `next build` fails with:\r\n```js\r\nUnhandled error during request: TypeError: Cannot read property 'add' of undefined\r\n    at new _default ([HIDDEN_PATH]/node_modules/next/dist/next-server/lib/side-effect.js:1:407)\r\n    at d ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:36:320)\r\n    at $a ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:39:16)\r\n    at a.b.render ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:44:476)\r\n    at a.b.read ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:44:18)\r\n    at renderToStaticMarkup ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:54:462)\r\n    at renderDocument ([HIDDEN_PATH]/node_modules/next/dist/next-server/server/render.js:3:594)\r\n    at renderToHTML ([HIDDEN_PATH]/node_modules/next/dist/next-server/server/render.js:48:72)\r\n    at async renderReqToHTML ([HIDDEN_PATH]/.next/serverless/pages/[...slug].js:24441:22)\r\n    at async Object.exportPage [as default] ([HIDDEN_PATH]/node_modules/next/dist/export/worker.js:12:92)\r\nError occurred prerendering page \"/[...slug]\". Read more: https://err.sh/next.js/prerender-error\r\nTypeError: Cannot read property 'add' of undefined\r\n    at new _default ([HIDDEN_PATH]/node_modules/next/dist/next-server/lib/side-effect.js:1:407)\r\n    at d ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:36:320)\r\n    at $a ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:39:16)\r\n    at a.b.render ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:44:476)\r\n    at a.b.read ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:44:18)\r\n    at renderToStaticMarkup ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:54:462)\r\n    at renderDocument ([HIDDEN_PATH]/node_modules/next/dist/next-server/server/render.js:3:594)\r\n    at renderToHTML ([HIDDEN_PATH]/node_modules/next/dist/next-server/server/render.js:48:72)\r\n    at async renderReqToHTML ([HIDDEN_PATH]/.next/serverless/pages/[...slug].js:24441:22)\r\n    at async Object.exportPage [as default] ([HIDDEN_PATH]/node_modules/next/dist/export/worker.js:12:92)\r\nUnhandled error during request: TypeError: Cannot read property 'add' of undefined\r\n    at new _default ([HIDDEN_PATH]/node_modules/next/dist/next-server/lib/side-effect.js:1:407)\r\n    at d ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:36:320)\r\n    at $a ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:39:16)\r\n    at a.b.render ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:44:476)\r\n    at a.b.read ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:44:18)\r\n    at renderToStaticMarkup ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:54:462)\r\n    at renderDocument ([HIDDEN_PATH]/node_modules/next/dist/next-server/server/render.js:3:594)\r\n    at renderToHTML ([HIDDEN_PATH]/node_modules/next/dist/next-server/server/render.js:48:72)\r\n    at async renderReqToHTML ([HIDDEN_PATH]/.next/serverless/pages/_error.js:10604:22)\r\n    at async Object.exportPage [as default] ([HIDDEN_PATH]/node_modules/next/dist/export/worker.js:12:92)\r\nError occurred prerendering page \"/404\". Read more: https://err.sh/next.js/prerender-error\r\nTypeError: Cannot read property 'add' of undefined\r\n    at new _default ([HIDDEN_PATH]/node_modules/next/dist/next-server/lib/side-effect.js:1:407)\r\n    at d ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:36:320)\r\n    at $a ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:39:16)\r\n    at a.b.render ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:44:476)\r\n    at a.b.read ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:44:18)\r\n    at renderToStaticMarkup ([HIDDEN_PATH]/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:54:462)\r\n    at renderDocument ([HIDDEN_PATH]/node_modules/next/dist/next-server/server/render.js:3:594)\r\n    at renderToHTML ([HIDDEN_PATH]/node_modules/next/dist/next-server/server/render.js:48:72)\r\n    at async renderReqToHTML ([HIDDEN_PATH]/.next/serverless/pages/_error.js:10604:22)\r\n    at async Object.exportPage [as default] ([HIDDEN_PATH]/node_modules/next/dist/export/worker.js:12:92)\r\n> Build error occurred\r\nError: Export encountered errors on following paths:\r\n\t/404\r\n\t/[...slug]\r\n    at exportApp ([HIDDEN_PATH]/node_modules/next/dist/export/index.js:22:1166)\r\n    at runMicrotasks (<anonymous>)\r\n    at processTicksAndRejections (internal/process/task_queues.js:97:5)\r\n    at async build ([HIDDEN_PATH]/node_modules/next/dist/build/index.js:38:218)\r\n22:36:52.954  \r\nerror Command failed with exit code 1.\r\n```\r\nOur projects builds fine up until 9.4.4 (current stable) but canary fails.\r\n\r\n## To Reproduce\r\n\r\n~I don't understand enough the error to be able to reproduce it.\r\nWe don't have a custom 404 not _error pages which makes it even more confusing.\r\nAny hint where to look at would be appreciated so I can try to repro this (project is private, so I can't share it :/)~\r\n*Edit:* Alright, I started moving files around and commenting code and this is what it came down to:\r\n1. Create an empty NextJS app (next@9.4.5-canary.12)\r\n2. Create empty page (this is what I have):\r\n```javascript\r\n// pages/[...slug].js\r\nconst CatchAllPages = () => {\r\n  return <div>Test</div>\r\n}\r\nexport const getStaticPaths = async () => {\r\n  return {\r\n    paths: [],\r\n    fallback: true,\r\n  }\r\n}\r\nexport const getStaticProps = async (ctx) => {\r\n  return {\r\n    props: {},\r\n  }\r\n}\r\nexport default CatchAllPages\r\n```\r\n3. Create a custom `_document.js`:\r\n```javascript\r\nimport * as React from 'react'\r\nimport Document, {\r\n  Html,\r\n  Head as HeadContainer,\r\n  Main,\r\n  NextScript,\r\n} from 'next/document'\r\nimport Head from 'next/head'\r\n\r\nconst SomeComponent = React.memo(() => {\r\n  return (\r\n    <>\r\n      <Head>\r\n        <script\r\n          dangerouslySetInnerHTML={{ __html: '// this does not matter' }}\r\n        ></script>\r\n      </Head>\r\n    </>\r\n  )\r\n})\r\n\r\nexport default class CustomDocument extends Document {\r\n  render() {\r\n    return (\r\n      <Html>\r\n        <HeadContainer />\r\n        <body>\r\n          <Main />\r\n          <NextScript />\r\n          <SomeComponent />\r\n        </body>\r\n      </Html>\r\n    )\r\n  }\r\n}\r\n```\r\n4. Run `next build`\r\n\r\nApparently this has something to do with using `next/head` when rendering a component (e.g: in above `SomeComponent` which could live outside in `src/components`) in the `Html` tag in `body`.\r\n\r\n## Expected behavior\r\n\r\nIt should build or provide a better error message. If using `next/head` is no longer allowed in `Html` or in `_document` then it should display a better error with a link to some NextJS doc describing the issue (as it does with other error messages)\r\n\r\n## System information\r\n\r\n- OS: MacOS\r\n- Version of Next.js: next@9.4.5-canary.12\r\n- Version of Node.js: 12.18.0\r\n\r\n## Additional context\r\n\r\nWorks with 9.3.x & 9.4.4\r\n"},{"labels":[null,"documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nFast Refresh does not support unnamed function components ([closed issue](https://github.com/zeit/next.js/issues/12891)), yet the warning provided does not mention this.\r\n\r\n## To Reproduce\r\n\r\n`pages/index.js`:\r\n\r\n```\r\nimport '../src/somefile'\r\n\r\nexport default () => null;\r\n```\r\n\r\nThen edit and save `somefile`.\r\n\r\n### Works\r\n\r\n```\r\nimport '../src/somefile'\r\n\r\nconst Page = () => null;\r\n\r\nexport default Page;\r\n\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe warning should mention that Fast Refresh does not support unnamed function components.\r\n\r\n## System information\r\n\r\n- Version of Next.js: 9.4.1\r\n\r\n## Additional context\r\n\r\nI know that the warning was only introduced in 9.4.1 (and is a huge improvement on the error in 9.4.0), so it will most likely be subject to near future iterations. Here's how I read it:\r\n\r\n> Fast Refresh will perform a full reload when you edit a file that's imported by modules outside of the React tree.\r\n\r\nProbably my ignorance, but I was a bit confused about what \"modules outside of the React tree\" mean here. Does this mean the React components tree? Is `somefile` outside of the react tree? I mean, there is a dependency graph, but it is not a tree.\r\n\r\n> You might have a file which renders a React component but also exports a value that is imported by a non-React component.\r\n\r\nFirst, files don't 'render' a React component - files 'export' react components.\r\n\r\nI don't really understand whether this is said within the context of the whole dependency graph or what? Does this mean that if I have a file that exports a react component, but also some function (or a type like Props) that is then imported in a test, that will break Fast Refresh\r\n\r\n> Consider migrating the non-React component export to a separate file and importing it into both files.\r\n\r\nWhat's a \"non-React component`? An angular one?\r\n\r\n> It is also possible you are using class components at the top-level of your application, which disables Fast Refresh.\r\n\r\nThis is also unclear. What constitutes the \"top-level of your application\"? Some options:\r\n\r\n- A page's default export\r\n- `pages/_app.js`\r\n- `pages/_document.js`\r\n\r\n> Fast Refresh requires at least one function component in your React tree.\r\n\r\nThat is clear and the reproduction case above comply with this.\r\n\r\nAnyhow, while there is a place for improvement in my view to other parts of the warning, I believe my issue would be solved if the last line would say:\r\n\r\n> Fast Refresh requires at least one _named_ function component in your React tree.\r\n\r\nYet I'm not sure this is correct. Is it any odd function component that needs to be named, or the one exported from a page?"},{"labels":["documentation"],"text":"# Bug report\r\n\r\nI updated nextjs from `9.3.6` to `9.4.0`.\r\n\r\n## Describe the bug\r\n\r\nAfter updating I do not get typescript errors anymore checked during buildtime. This was working in `9.3.6`. \r\n\r\n## To Reproduce\r\n\r\nAs I have quite a complex mono repository, I'm unable to provide the source of that.\r\nMajor differences to a stock configuration is a custom babel configuration:\r\n\r\n```js\r\nmodule.exports = {\r\n  presets: [],\r\n  plugins: [\r\n    'lodash',\r\n    '@babel/proposal-class-properties',\r\n    'react-intl-auto',\r\n    'babel-plugin-graphql-tag'\r\n  ],\r\n  env: {\r\n    test: {\r\n     presets: [\r\n       '@babel/typescript',\r\n         [ '@babel/preset-env',\r\n           {\r\n           targets: {\r\n             node: process.versions.node\r\n           }\r\n         }\r\n         ],\r\n       '@babel/preset-react'\r\n     ]\r\n    },\r\n    nodeTs: {\r\n      presets: [\r\n        '@babel/typescript',\r\n        '@babel/preset-react',\r\n        [\r\n          '@babel/preset-env',\r\n          {\r\n            targets: {\r\n              node: process.versions.node\r\n            }\r\n          }\r\n        ]\r\n      ]\r\n    },\r\n    nextjs: {\r\n      presets: ['next/babel']\r\n    }\r\n  }\r\n}\r\n```\r\nAnd a custom next.config.ts:\r\n```ts\r\n/* eslint-disable @typescript-eslint/no-var-requires */\r\nconst path = require('path')\r\n\r\nconst withDevTool = (nextConfig: any = {}) => {\r\n  return {\r\n    ...nextConfig,\r\n    webpack(config: any, options: any) {\r\n      const { dev } = options\r\n      if (dev) {\r\n        config.devtool = 'eval-source-map'\r\n      }\r\n      if (typeof nextConfig.webpack === 'function') {\r\n        return nextConfig.webpack(config, options)\r\n      }\r\n      return config\r\n    },\r\n  }\r\n}\r\n\r\nexport default () => {\r\n  const optimizedImages = require('next-optimized-images')\r\n\r\n  const withTM = require('next-transpile-modules')([\r\n    'query-string',\r\n    'imask/esm',\r\n   /** ..some more private modules */\r\n  ])\r\n\r\n  const withCustomBabelConfigFile = require('next-plugin-custom-babel-config')\r\n  const withCSS = require('@zeit/next-css')\r\n\r\n  const withBundleAnalyzer = require('@next/bundle-analyzer')({\r\n    enabled: process.env.ANALYZE === 'true',\r\n  })\r\n\r\n  return withDevTool(\r\n    withBundleAnalyzer(\r\n      optimizedImages(\r\n        withCustomBabelConfigFile(\r\n          withCSS(\r\n            withTM({\r\n              babelConfigFile: path.resolve(\r\n                path.join(__dirname, '../../../../babel.config.js')\r\n              ),\r\n              distDir: 'dist',\r\n            })\r\n          )\r\n        )\r\n      )\r\n    )\r\n  )\r\n}\r\n```\r\n\r\nI'm using a custom server with additional express middleware.\r\n\r\n```ts\r\n// This file doesn't go through babel or webpack transformation.\r\n// Make sure the syntax and sources this file requires are compatible with the current node version you are running\r\n// See https://github.com/zeit/next.js/issues/1245 for discussions on Universal Webpack or universal Babel\r\nimport 'dotenv/config'\r\nimport express from 'express'\r\nimport next from 'next'\r\nimport { defaultMiddleware } from '../index'\r\nimport { appConfig } from './appConfig'\r\n\r\nconst applicationFactory = (): void => {\r\n  const dev = process.env.NODE_ENV !== 'production'\r\n  const conf = dev\r\n    ? // eslint-disable-next-line @typescript-eslint/no-var-requires\r\n      require('./next.config').default()\r\n    : {\r\n        distDir: 'dist',\r\n      }\r\n  const app = next({\r\n    dev,\r\n    conf,\r\n  })\r\n  const handle = app.getRequestHandler()\r\n  app.prepare().then(() => {\r\n    const server = express()\r\n    const proxyInstance = defaultMiddleware(server, appConfig)\r\n    server.get('*', (req, res) => {\r\n      return handle(req, res)\r\n    })\r\n    const port = process.env.PORT || 3000\r\n    // @ts-ignore\r\n    const listener = server.listen(port, (err: Error) => {\r\n      if (err) {\r\n        throw err\r\n      }\r\n      // tslint:disable-next-line:no-console\r\n      console.log(`> Ready on http://localhost:${port}`)\r\n    })\r\n    listener.on('upgrade', (req: any, socket: any, ...rest: any) => {\r\n      socket.on('error', (error: any) => {\r\n        console.error(error)\r\n      })\r\n      proxyInstance.upgrade(req, socket, ...rest)\r\n    })\r\n  })\r\n}\r\n\r\nexport default applicationFactory\r\n\r\n```\r\n\r\n## Expected behavior\r\n\r\nTypescript errors should be reported.\r\n\r\n## System information\r\n\r\n- OS:  Windows\r\n- Version of Next.js: 9.4.0\r\n- Version of Node.js: 12.13.1\r\n"},{"labels":[null,"documentation"],"text":"# Bug report\r\n\r\nI was having routing issues in my application after implementing Multi Zones. I went back to the docs' example and noticed the bug was also there. \r\n\r\nI hosted it on Now using the exact same example as the docs:\r\nhttps://github.com/zeit/next.js/tree/canary/examples/with-zones\r\n\r\nI'm not savvy enough to understand the origin of the bug. Where is this coming from?\r\n\r\n## Describe the bug\r\n\r\nVisiting the link [https://test-paul-with-zones-app.now.sh/blog/post/1](https://test-paul-with-zones-app.now.sh/blog/post/1) fails when request from the browser. It works only when navigating to it client-side (Next Link)\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior, please provide code snippets or a repository:\r\n\r\n1. Go to '[https://test-paul-with-zones-app.now.sh/blog/post/1](https://test-paul-with-zones-app.now.sh/blog/post/1)'\r\n2. See a 404 page\r\n3. Go to [https://test-paul-with-zones-app.now.sh/blog](https://test-paul-with-zones-app.now.sh/blog)\r\n4. Click on Post 1\r\n5. The page shows correctly\r\n\r\n## Expected behavior\r\n\r\nThe page should load and not show a 404\r\n"},{"labels":[null,"documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nIt isn't possible to have an `_error.tsx` file in the `/pages` folder without also having a `404.tsx` file. When running `next dev`, it works fine, but as soon as you run `next build`, the following error appears:\r\n\r\n```\r\nAutomatically optimizing pages ...\r\nError occurred prerendering page \"/404\". Read more: https://err.sh/next.js/prerender-error:\r\nError: Error for page /_error: pages with `getServerSideProps` can not be exported. See more info here: https://err.sh/next.js/gss-export\r\n```\r\n\r\nAccording to the official Next.js Blog, it should not be a problem to have a `_error.tsx` without a `404.tsx`: https://nextjs.org/blog/next-9-3#automatic-static-optimization-for-404\r\n\r\nThe reason I do not want a static 404 page is so that I can catch routes with a trailing slash and redirect accordingly to the route without a trailing slash. This must be done on the server.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior, please provide code snippets or a repository:\r\n\r\n1. Create an `_error.tsx` file with a `getServerSideProps()` function.\r\n2. Run `next build`.\r\n\r\n## Expected behavior\r\n\r\nThe error message says that it is trying to export the `_error.tsx` file even though it obviously shouldn't be. The application should still build. As soon as I add a `404.tsx` file or remove the `_error.tsx` file, it builds without a problem.\r\n\r\n## System information\r\n\r\n- OS: macOS and Alpine Linux (Docker)\r\n- Version of Next.js: 9.3\r\n- Version of Node.js: 13.13.0 and 14.0.0\r\n\r\n## Additional context\r\n\r\nJust in case it might be of any use, here is the code from my `_error.tsx` file:\r\n\r\n```\r\nimport React, { useEffect } from 'react';\r\nimport { GetServerSideProps } from 'next';\r\nimport Head from 'next/head';\r\nimport Router from 'next/router';\r\n\r\nimport { makeStyles, createStyles } from '@material-ui/core/styles';\r\nimport Container from '@material-ui/core/Container';\r\n\r\nimport useStandardHeaderTags from '../lib/useStandardHeaderTags';\r\nimport TitleElement from '../components/TitleElement';\r\n\r\nconst useStyles = makeStyles(() =>\r\n  createStyles({\r\n    root: {\r\n      textAlign: 'center'\r\n    }\r\n  })\r\n);\r\n\r\ninterface Props {\r\n  statusCode: number;\r\n}\r\n\r\nconst Error: React.FC<Props> = ({ statusCode }) => {\r\n  const classes = useStyles();\r\n  const title = statusCode === 404 ? '404' : 'Error';\r\n\r\n  return (\r\n    <>\r\n      <Head>\r\n        {useStandardHeaderTags(title)}\r\n      </Head>\r\n      <Container className={classes.root}>\r\n        <TitleElement text={title} />\r\n\r\n        {statusCode === 404\r\n          ? 'The page you are looking for could not be found.'\r\n          : 'An error occurred.'}\r\n      </Container>\r\n    </>\r\n  );\r\n};\r\n\r\nexport const getServerSideProps: GetServerSideProps = async ({ res, req }) => {\r\n  const statusCode = res ? res.statusCode : 404;\r\n\r\n  if (statusCode === 404) {\r\n    if (req.url.match(/\\/$/)) {\r\n      const withoutTrailingSlash = req.url.substr(0, req.url.length - 1);\r\n      if (res) {\r\n        res.writeHead(303, {\r\n          Location: withoutTrailingSlash\r\n        });\r\n        res.end();\r\n      }\r\n      else {\r\n        Router.push(withoutTrailingSlash);\r\n      }\r\n    }\r\n  }\r\n\r\n  return {\r\n    props: {\r\n      statusCode\r\n    }\r\n  };\r\n};\r\n\r\nexport default Error;\r\n```\r\n"},{"labels":[null,"documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\n`next build` fails with the error in the title after updating to 9.3.4 with a custom PostCSS config following the current documentation\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior, please provide code snippets or a repository:\r\n\r\n1. Create a `postcss.config.js` with the contents specified [here](https://nextjs.org/docs/advanced-features/customizing-postcss-config)\r\n2. Update to 9.3.4\r\n3. No more builds!\r\n\r\n## Expected behavior\r\n\r\nDocumentation needs to be updated with the correct \"base\" PostCSS config for 9.3.4. I know you apparently removed `postcss-flexbugs-fixes`, but I have no idea what else might have changed.\r\n"},{"labels":["documentation"],"text":"**update: managed to activate the grid with autoprefixer comment in the scss.** \r\n\r\n# Bug report\r\nthe docs state \"**New CSS features are automatically compiled for Internet Explorer 11 compatibility: Grid Layout and Gap property** \"\r\n\r\n## Describe the bug\r\ni have installed next 9.3 and added various ie11 browserlist configs like so: \r\n```\r\n  \"browserslist\": \">0.3%, ie 11\"\r\n``` \r\n\r\ntested on a simple grid component - did not work. \r\n\r\n## To Reproduce\r\nuse any grid layout with ie11 in the supported browsers. \r\n\r\n## Expected behavior\r\nIE11 to show a grid layout\r\n\r\nis there something missing in my config ? the docs did not elaborate and there is no example folder for this. "},{"labels":["documentation"],"text":"# Bug report\r\n\r\nThe [documentation on data fetching](https://github.com/zeit/next.js/blob/canary/docs/basic-features/data-fetching.md) mentions that `getServerSideProps` will be executed client side when using next/link. Was next/router forgotten to be mentioned in the documentation? For `getStaticProps`, next/router is already mentioned.\r\n\r\nWhile at it: there is an incomplete sentence on the same doc page:\r\n\r\n> and the result cannot be cached by a CDN without extra ."},{"labels":["documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\nWhen using `next/head` for adding og tags to the page, \r\nI am using \r\n`<meta property=\"og:title\" content=\"...\" />`\r\n\r\nFor some other tags like `name=\"title\"` , I am using\r\n`<meta name=\"title\" content=\"...\" />`\r\n\r\nBoth of these work fine and are added to the `head`.\r\n\r\nThe problem arises when I need to override these from a child component.\r\nIn this case, the `<meta name=\"title\" ... />` gets overridden correctly, but a new entry is added for `<meta property=\"og:title\" ... />`\r\n\r\nTo circumvent this, I have figured out that i can pass a `key` to the `<meta property-=\"...\"  />,` in which case it gets correctly overridden.\r\n\r\nIs this understanding correct?\r\nmeta tags with name are deduped by default, otherwise we need to pass in a key\r\nIs this behaviour documented anywhere?\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior, please provide code snippets or a repository:\r\n\r\n```\r\nfunction Parent() {\r\n  return (\r\n    <>\r\n    <Head>\r\n      <meta name=\"n1\" content=\"content1\" />\r\n      <meta property=\"p1\" content=\"content2\" />\r\n    </Head>\r\n    <Child />\r\n    </>\r\n  );\r\n}\r\n\r\nfunction Child() {\r\n  return (\r\n    <Head>\r\n      <meta name=\"n1\" content=\"content1\" />\r\n      <meta property=\"p1\" content=\"content2\" />\r\n    </Head>\r\n  )\r\n}\r\n```\r\n\r\n## Expected behavior\r\nthe <head> tag should have these meta tags:\r\n```\r\n<meta name=\"n1\" content=\"content1\" />\r\n<meta property=\"p1\" content=\"content2\" />\r\n```\r\n\r\nbut instead this is what I see:\r\n```\r\n<meta name=\"n1\" content=\"content1\" /> \r\n<meta property=\"p1\" content=\"content2\" />\r\n<meta property=\"p1\" content=\"content2\" />\r\n```\r\n## Screenshots\r\n\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n## System information\r\n\r\n- OS: macOS\r\n- Browser: Chrome\r\n- Version of Next.js: 9.1.4\r\n\r\n## Additional context\r\n\r\n"},{"labels":["documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nRunning `npm init next-app` creates a new Next.js project that uses Yarn, rather than npm. Given, the fact that I'm using npm to create the project, and the docs offer two options for getting started, one with npm and one with Yarn, this is completely unexpected behavior.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior, please provide code snippets or a repository:\r\n\r\n1. Read getting started docs\r\n2. Choose the `npm` option\r\n3. Run `npm init next-app`\r\n4. Get a project that uses Yarn\r\n\r\n## Expected behavior\r\n\r\nIt should create a project that uses npm for package management.\r\n\r\nHere ya go, Tim!"},{"labels":["documentation"],"text":"# Examples bug report\r\n\r\n## Example name\r\n\r\nGetting Started/Lazy Loading Modules\r\nhttps://nextjs.org/learn/excel/lazy-loading-modules/analyze-result\r\n\r\n## Describe the bug\r\n\r\nDocumentation says \"As you have seen, the firebase modules are contained inside the pages/index.js and pages/p/[id].js bundles.\"\r\nThis does not appear to be the case with NextJS 9.2.\r\nPresumably the NextJS 9.2 improved code splitting strategy has invalidated some of the description in the example.\r\n\r\n## To Reproduce\r\n\r\nFollow the steps in the Getting Started guide for the \"Lazy Loading Modules\" example.\r\n\r\n## Expected behavior\r\n\r\nThe text in the example (Getting Started - Lazy Loading Modules) should reflect new optimized code-splitting behavior.\r\n\r\n## Screenshots\r\n![2020-01-26](https://user-images.githubusercontent.com/11898246/73170472-9750cc80-4152-11ea-8831-08f7a695b8a9.png)\r\n\r\n## System information\r\n\r\n- OS: Ubuntu 18.04 LTS\r\n- Browser: firefox 72.0.2 Windows\r\n- NextJS: 9.2.1\r\n\r\n## Additional context\r\n\r\nAdd any other context about the problem here.\r\n"},{"labels":["documentation"],"text":"# Examples bug report\r\nThere is a bug on the **Fetching Data for Pages** section of the NextJS Learn on the official website\r\n## Example name\r\n`TypeError: Cannot read property 'medium' of null`\r\n\r\n## Describe the bug\r\nIn the sample API, the keyword used was 'batman'. Apparently all batman shows have image object return values other than null.\r\nHowever, when I changed the keyword to 'power', some of the shows did not have image object and they returned null.\r\nFor such shows where there are no images, this line (in [id].js page):\r\n`<img src={props.show.image.medium} />` breaks and throws the TypeError shown above.\r\n\r\n## To Reproduce\r\n1. Change the API link in the 'pages/index.js' to:\r\n`'https://api.tvmaze.com/search/shows?q=power'`\r\n2. On the browser, when the when the power shows have rendered, click on the second item on the list (The Power).\r\n3. See error\r\n\r\n## Expected behavior\r\nYou will be faced with an error screen like this:\r\n![error](https://user-images.githubusercontent.com/49484425/71109272-24544d80-21c5-11ea-884e-e864dc3496dc.PNG)\r\n\r\n\r\n## To Fix\r\nTaking into consideration, that some shows may be returning null values under their image property, I had to use optional chaining (an experimental feature) to take care of that. (there are other simpler logic to take care of null)\r\n\r\n**Sample show without Image:**\r\n![showWithoutImage](https://user-images.githubusercontent.com/49484425/71109548-b65c5600-21c5-11ea-93ee-f9ea0e4fb27c.PNG)\r\n\r\n**Sample show with Image:**\r\n![showWithImage](https://user-images.githubusercontent.com/49484425/71109534-afcdde80-21c5-11ea-9b85-55c6412ffb05.PNG)\r\n\r\n\r\n## System information\r\n- OS: [ Windows]\r\n- Browser (if applies) [Chromium-based]\r\n- Version of Next.js: [9.1.5]\r\n\r\n## Additional context\r\n"},{"labels":["documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nAfter deploying the tutorial project at https://nextjs.org/learn/excel/static-html-export/deploying-the-app. The resulting deployment on Now only works when clicking links. Hitting refresh on any page apart from the index will result in a 404 (as the file does not exist on the server). This appears to be because when testing locally with `serve` the html extension is stripped, but in Now this does not happen. In any case the static site should not depend on the server stripping the html extension.\r\n\r\n## To Reproduce\r\n\r\nFollow the tutorial at https://nextjs.org/learn/excel/static-html-export.\r\n\r\n- After deploying to Now go to the index page\r\n- Click the about link\r\n- Hit the refresh button\r\n\r\nA 404 is encountered\r\n\r\n## Expected behavior\r\n\r\nThe about page should reload\r\n"},{"labels":[null,"documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nUsing Next 9's routing solution for a Universal App, \"router.query\" for post-SSR client-side navigation is never populated even if `asPath` contains a query string for any of the following\r\nrouter access methods:\r\n```\r\nimport { SingletonRouter, useRouter, withRouter } from 'next/router';\r\n```\r\n\r\n## To Reproduce\r\n\r\nPerform an initial navigation to `/search?term=hello`, the server rendered page's router information contains a populated \"query\":\r\n```\r\nconst router = useRouter();\r\nconsole.log({ router }); // asPath: \"/search?term=hello\", pathname: \"/search\", query: { term: \"hello\" }\r\n```\r\n\r\nThen navigate to the same pathname, with a different query string: `/search?term=bye`. At this point Router empties \"query\", but has the correct new `asPath`:\r\n```\r\nconst router = useRouter();\r\nconsole.log({ router }); // asPath: \"/search?term=hello\", pathname: \"/search\", query: {}\r\n```\r\n\r\n## Expected behavior\r\n\r\nFor the client side navigation to a route with specified query parameters such as `/search?term=bye`, `router.query` contains the parsed query string.\r\n\r\nIf this is not expected, it would be helpful to better document this behavior and the correct approach on the client. I would prefer to always access routing information off of the available hook / HOC rather than off window on the client to avoid a split-brain solution between the client and the server.\r\n"},{"labels":["documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nThe `pathname` returned from `useRouter` doesn't match the description of this data in the [readme](https://github.com/zeit/next.js#routing).\r\n\r\nI'm unsure if this is a bug, or simply incomplete information in the current docs. Regardless, this is a point of semi-regular confusion on our team.\r\n\r\n## To Reproduce\r\n\r\n1. Create the following page:\r\n```tsx\r\n[locale]/posts/[id].tsx\r\n```\r\n\r\n2. Within this page, use `useRouter`:\r\n\r\n```tsx\r\nimport { useRouter } from 'next/router'\r\n\r\nconst Page = () => {\r\n  const router = useRouter()\r\n  console.log(router)\r\n\r\n  return (<h1>Example</h1>)\r\n}\r\n\r\nexport default Page\r\n```\r\n\r\n3. Visit this page in the browser, with some query string (i.e. `/fr/posts/abc?q=123`)\r\n4. View the log (either server logs, or in the browser console):\r\n```ts\r\n{\r\n  route: \"/[locale]/posts/[id]\",\r\n  pathname: \"/[locale]/posts/[id]\",\r\n  query: {\r\n    locale: \"fr\",\r\n    id: \"abc\",\r\n    \"q\": \"123\"\r\n  },\r\n  asPath: \"/fr/posts/abc?q=123\"\r\n}\r\n```\r\n\r\n## Expected behavior\r\n\r\nBased on the following descriptions of these values:\r\n\r\n> - `route` - String of the current route\r\n> - `pathname` - String of the current path excluding the query string\r\n> - `asPath` - String of the actual path (including the query) shows in the browser\r\n> [Reference](https://github.com/zeit/next.js#routing)\r\n\r\nI would expect the following `useRouter` results:\r\n\r\n```diff\r\n{\r\n  route: \"/[locale]/posts/[id]\",\r\n- pathname: \"/[locale]/posts/[id]\",\r\n+ pathname: \"/fr/posts/abc\",\r\n  query: {\r\n    locale: \"fr\",\r\n    id: \"abc\",\r\n    \"q\": \"123\"\r\n  },\r\n  asPath: \"/fr/posts/abc?q=123\"\r\n}\r\n```\r\n\r\n## Screenshots\r\n\r\nN/A\r\n\r\n## System information\r\n\r\n- OS: macOS\r\n- Browser: N/A[e.g. chrome, safari]\r\n- Version of Next.js: `9.0.7`\r\n\r\n## Additional context\r\n\r\nNone\r\n"},{"labels":["documentation"],"text":"Despite 'opting out' of static optimization, production build still fails owing to (one assumes) validation for that functionality.\r\n\r\nError as follows:\r\n\r\n```\r\nWarning: You have opted-out of Automatic Static Optimization due to `getInitialProps` in `pages/_app`.\r\nRead more: https://err.sh/next.js/opt-out-auto-static-optimization\r\n\r\n> Build error occurred\r\nError: automatic static optimization failed: found pages without a React Component as default export in [...list of files]\r\n```\r\n## To Reproduce\r\n\r\nAdd getInitialProps method to pages/_app class.\r\n\r\n/pages/example/index.tsx:\r\n\r\n```\r\nimport { TestOne, TestTwo } from './components';\r\n...\r\nexport default class Example extends Page {\r\nrender() {\r\n  <React.Fragment>\r\n    <TestOne />\r\n    <TestTwo />\r\n  </React.Fragment>\r\n}\r\n```\r\n/pages/example/components.tsx:\r\n\r\n```\r\nexport class TestOne extends React.Component {\r\n...\r\n}\r\n...\r\nexport class TestTwo extends React.Component {\r\n...\r\n}\r\n```\r\n\r\nAttempt production build.\r\n\r\n## Expected behavior\r\n\r\nA warning, but for the build to succeed.\r\n\r\n## System information\r\n\r\nnext@9.0.8\r\n\r\n## Additional context\r\n\r\nWe upgraded from next 8 to next 9 owing to its superior typescript offering. However, we are also using some legacy code that we have not been in a position to change to date which makes Static Optimization problematic. We are clearly not following React best practice, and also not using next routing as intended, so I realise that this is happening because we are off-message and we are working on fixing this within our own context.\r\n\r\nHowever, I still consider this a bug, as there is the implication that the pages check is being run in the background even when not necessary which will needlessly increase build time and possibly cause problems for other users trying to suppress the optimization for similar legacy reasons.\r\n"},{"labels":["documentation"],"text":"> \"Your `<Link>`'s `as` value is incompatible with the `href` value. This is invalid.\"\r\n\r\nRef: https://spectrum.chat/next-js/general/linking-static-page-routes~a3f0b5f2-397e-4a98-bb95-f98bdf5ddd65\r\n\r\nThis needs a err.sh link explaining how to solve it."},{"labels":["documentation"],"text":"# Examples bug report\r\n\r\n## Example name\r\n\r\nE1-static-export\r\n\r\n## Describe the bug\r\n\r\nI was following the static export tutorial. On the step to generate the individual post pages, after modifying the next.config.js, when running `npm run export` the generated static .html files all have empty `h1` elements. When loading the index page and using client-based navigation, the titles appear, but only the post body is shown on reload of the post page.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior, please provide code snippets or a repository:\r\n\r\n1. `git clone https://github.com/zeit/next-learn-demo.git`\r\n2. `npm install`\r\n3. Follow the Export into a Static HTML App [completing the Exporting Other Pages step](https://nextjs.org/learn/excel/static-html-export/exporting-other-pages)\r\n4. `npm run build`\r\n5. `npm run export`\r\n6. `cd out`\r\n7. `serve -p 8080`\r\n\r\n## Expected behavior\r\n\r\nThe titles specified in the next.config.js should have been baked into the static .html files.\r\n\r\n## System information\r\n\r\n- OS: Windows 7\r\n- Version of Next.js: 9.0.2\r\n"},{"labels":["documentation",null],"text":"# Examples bug report\r\n\r\n## Example name\r\n\r\nTypeScript\r\n\r\n## Describe the bug\r\n\r\nDuring the TypeScript example, the instructions are suppose to produce a compile error in the server console because of inaccurate configuration. Then later in the exercise, correct the configuration for the userAgent to successfully display in the browser window. However, using Firefox and Chrome, my userAgent displays correctly the entire time.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior, please provide code snippets or a repository:\r\n\r\n1. Follow the example instructions here:\r\nhttps://nextjs.org/learn/excel/typescript\r\n\r\n## Expected behavior\r\n\r\nA clear and concise description of what you expected to happen.\r\n\r\n## System information\r\n\r\n- OS: Windows 10\r\n- Browser: FireFox Developer Edition 69.0b4 (64-bit)\r\n- Version of Next.js: 9.0.2"},{"labels":["documentation"],"text":"This is more of an issue with the documentation than the component itself, but the `<Link>` component is very easy to accidentally abuse to create inaccessible markup, as I’ve recently experienced while working with other developers on Next projects.\r\n\r\nIn the [Using a component that supports onClick](https://github.com/zeit/next.js#using-a-component-that-supports-onclick) section, the documentation describes how the `<Link>` component can be used on any element that supports `onClick`, without also pointing out that doing so without appropriate ARIA attributes, styling, and keyboard handlers makes the element inaccessible to assistive technology and keyboard users. There’s a note about `passHref` and SEO, but as currently written it implies that adding `href` to a `<div>` would be a sufficient solution, which isn’t true.\r\n\r\nThe best practice is to use the `<a>` element for **all** links, but sometimes it’s necessary for a specific project to build a custom link with `role=\"link\"` and custom JavaScript, so I understand the utility of ensuring `<Link>` works on non-`<a>`'s. But it should at least attempt to throw a warning when a developer misuses it. For example, if they wrap it around a `<button>` or a `<div>` with no `role`. Even if there are no changes to the Next codebase itself, the documentation should make this pitfall clear to developers."},{"labels":["documentation"],"text":"Reported here: https://twitter.com/sudhirmith/status/1149706325854576640\r\n\r\nhttps://nextjs.org/docs#amp-behavior-with-next-export\r\n\r\nThis section needs to be updated to mention it outputs without the subdirectory as per the new behavior: https://github.com/zeit/next.js/blob/canary/UPGRADING.md#next-export-no-longer-exports-pages-as-indexhtml"},{"labels":["documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\n\r\nThe README contains this line:\r\n\r\n> To serve static files from the root directory you can add a folder called public and reference those files from the root, e.g: /robots.txt.\r\n\r\nHowever #7771 made this feature experimental. It should either be removed from the readme or the experimental flag to enable it should be mentioned.\r\n\r\n## To Reproduce\r\n\r\nRead README (https://github.com/zeit/next.js)\r\n\r\n## Expected behavior\r\n\r\nShould not include experimental features or should mention that the feature is experimental.\r\n\r\n## Screenshots\r\n\r\nN/A\r\n\r\n## System information\r\n\r\nN/A\r\n\r\n## Additional context\r\n\r\nN/A\r\n"},{"labels":["documentation"],"text":"# Bug report\r\n\r\n## Describe the bug\r\nIn the docs it says that \r\n![docs](https://user-images.githubusercontent.com/14829154/60832448-8fd0d280-a1c4-11e9-9303-8a9882b4f1c7.png)\r\n\r\nbut when I tried to use that types for req and res I got this:\r\n![2019-07-08_21-05](https://user-images.githubusercontent.com/14829154/60832474-a0814880-a1c4-11e9-8ee7-b44b6787dd77.png)\r\n\r\n## System information\r\n\r\n- OS: [Linux]\r\n- Browser (if applies) [chrome]\r\n- Version of Next.js: [e.g. 9.0.0]\r\n\r\nOr do I do something incorrectly?\r\n"},{"labels":["documentation"],"text":"Hi! I've just started reading the docs. This link is broken (missing):\r\n\r\n![Screen Shot 2019-03-13 at 08 03 01](https://user-images.githubusercontent.com/20521892/54263378-9b8af000-4568-11e9-9a8f-49fed316b740.png)\r\n\r\n\r\nI think it's important because it's the very first basic example provided in the documentation and this can drive people away. "},{"labels":["documentation",null,null,null],"text":"##  Bug\r\n\r\nPytorch documentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\r\n\"This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form\"\r\n![image](https://user-images.githubusercontent.com/4337024/93537302-022c5f00-f900-11ea-9a98-fa83a7b90654.png)\r\n\r\nSGD: http://proceedings.mlr.press/v28/sutskever13.pdf\r\n![image](https://user-images.githubusercontent.com/4337024/93537287-f9d42400-f8ff-11ea-9535-42d1eafb92f1.png)\r\n\r\nAs an implication both the adapted and the interpreted version of SGD have negative momentum\r\n\r\n\n\ncc @jlin27 @vincentqb"},{"labels":[null,"documentation",null,null],"text":"##  Bug\r\n\r\nFollowing an (adapted) version of the example provided in the docs for [`emit_nvtx`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx) produces the following error:\r\n\r\n```Python\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    with torch.autograd.profiler.emit_nvtx():\r\n  File \"/scratch/mattle/.conda/lib/python3.7/site-packages/torch/autograd/profiler.py\", line 553, in __enter__\r\n    False\r\nRuntimeError: Profiler is already enabled on this thread\r\n```\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\n```Python\r\nimport torch\r\n\r\nprint(f'Version = {torch.__version__}')\r\n\r\nx = torch.rand(100, 100, device='cuda')\r\n\r\nwith torch.autograd.profiler.profile():\r\n    temp = x * x\r\n    with torch.autograd.profiler.emit_nvtx():\r\n        temp = x * x\r\n```\r\n\r\nYields the following output:\r\n\r\n```Python\r\nVersion = 1.6.0\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    with torch.autograd.profiler.emit_nvtx():\r\n  File \"/scratch/mattle/.conda/lib/python3.7/site-packages/torch/autograd/profiler.py\", line 553, in __enter__\r\n    False\r\nRuntimeError: Profiler is already enabled on this thread\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo error\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.6.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 10.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7 (64-bit runtime)\r\nIs CUDA available: True\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: \r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 418.116.00\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] numpydoc==1.1.0\r\n[pip3] torch==1.6.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               10.1.243             h6bb024c_0  \r\n[conda] mkl                       2020.2                      256  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.1.0            py37h23d657b_0  \r\n[conda] mkl_random                1.1.1            py37h0573a6f_0  \r\n[conda] numpy                     1.19.1           py37hbc911f0_0  \r\n[conda] numpy-base                1.19.1           py37hfa32c7d_0  \r\n[conda] numpydoc                  1.1.0                     <pip>\r\n[conda] pytorch                   1.6.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.9\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\n\ncc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @jlin27"},{"labels":[null,"documentation",null,null],"text":"I am using this scheduler for my model and the \"verbose\" argument does not seem to exist within the source code of the documentation, even though the summarized documentation has it as an argument. Seems like a typo. \r\n\n\ncc @jlin27 @vincentqb"},{"labels":[null,"documentation",null],"text":"## 📚 Documentation\r\n\r\nThe code for torch.optim.lr_scheduler.CosineAnnealingWarmRestarts does not take argument \"verbose\" but documentation says otherwise.\r\n\n\ncc @ezyang @zou3519 @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n[From the docs:](https://pytorch.org/docs/stable/generated/torch.norm.html)\r\n\r\n`torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)`\r\n\r\n> **dim** (int, 2-tuple of python:ints, 2-list of python:ints, optional) – [...] If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.\r\n\r\nThe docs say that by default the norm will be applied to the last dimension, when the input tensor has more than two axes. I see different behaviour:\r\n```\r\na = torch.rand(3, 4, 5)\r\ntorch.norm(a)\r\n# tensor(4.7850)\r\n```\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"## 📚 Documentation\r\n\r\nFor some C++ functions (for example, torch::load), the [function documentation](https://pytorch.org/cppdocs/api/function_namespacetorch_1a4b369494adfb10b9a005aeb0bb6207cb.html?highlight=load) page shows a warning:  \r\n\r\n```\r\n\"doxygenfunction: Unable to resolve multiple matches for function torch::load with arguments (std::vector<torch::Tensor>&, LoadFromArgs&&…) in doxygen xml output for project PyTorch from directory: /var/lib/jenkins/workspace/docs/cpp/build/xml.\"\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/39305301/92850084-9ab57180-f41e-11ea-8e73-47015c71c87e.png)\r\n\n\ncc @yf225 @glaringlee @jlin27"},{"labels":["documentation",null,null],"text":"## 📚 Documentation\r\n'torch.per_channel_symmetric — per tensor, symmetric'  supposed to be\r\n'torch.per_channel_symmetric — per channel, symmetric' \r\nin docs/source/quantization-support.rst Line  320.\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @jlin27 @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @vkuzo"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\nDear @apaszke,\r\n\r\nI would like to kindly remind you of an matplotlib NameError exception due to matplotlib.pyplot.imshow(). To reproduce this error, you may run [reinforcement_q_learning.ipynb in google colab](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/2b3f06b04b5e96e4772746c20fcb4dcc/reinforcement_q_learning.ipynb).\r\n\r\nCheers\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## 📚 Documentation\r\n\r\nAt https://pytorch.org/docs/stable/optim.html, we see that the function `torch.optim.lr_scheduler.MultiStepLR` has a parameter called `verbose` that `If True, prints a message to stdout for each update. Default: False`.\r\n\r\nUsing `torch.__version__==1.6.0`, \r\nthis minimum example returns an error:\r\n\r\n```\r\nimport torch.nn as nn\r\nfrom torch.optim import Adam\r\nfrom torch.optim.lr_scheduler import MultiStepLR\r\n\r\nN, D_in, H, D_out = 64, 1000, 100, 10\r\n\r\nmodel = nn.Sequential(\r\n    nn.Linear(D_in, H),\r\n    nn.ReLU(),\r\n    nn.Linear(H, D_out),\r\n)\r\n\r\noptimizer = Adam(model.parameters(), lr=1e-04)\r\nscheduler = MultiStepLR(optimizer, milestones=[20,30], gamma=0.5, verbose=True)\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: __init__() got an unexpected keyword argument 'verbose'\r\n```\r\n\r\nFrom the [source code](https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#MultiStepLR) (linked in the documentation), this looks generalized to the other learning rate schedulers as well.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"##  Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. follow the  mnist tutorial (https://pytorch-lightning.readthedocs.io/en/stable/new-project.html)\r\n1. use Trainer(tpu_cores=1)\r\n1. run\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```\r\nTraceback (most recent call last):\r\n  File \"plmnist.py\", line 80, in <module>\r\n    trainer.fit(model, train_loader, val_loader)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\",line 1078, in fit\r\n    self.accelerator_backend.train(model)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/accelerators/tpu_backend.py\", line 87, in train\r\n    start_method=self.start_method\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 284, in spawn\r\n    return _run_direct(fn, args, nprocs, join, daemon, start_method)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 245, in _run_direct\r\n    fn(0, *args)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/accelerators/tpu_backend.py\", line 112, in tpu_train_in_process\r\n    results = trainer.run_pretrain_routine(model)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\",line 1239, in run_pretrain_routine\r\n    self.train()\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\r\n    self.run_training_epoch()\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 491, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 844, in run_training_batch\r\n    self.hiddens\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 1049, in optimizer_closure\r\n    training_step_output_for_epoch_end = copy(training_step_output)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/copy.py\", line 88, in copy\r\n    return copier(x)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/pytorch_lightning/core/step_result.py\", line 302, in __copy__\r\n    newone[k] = copy(v)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/copy.py\", line 96, in copy\r\n    rv = reductor(4)\r\n  File \"/***/anaconda3/envs/turing/lib/python3.7/site-packages/torch/tensor.py\", line 87, in __reduce_ex__\r\n    args = (self.cpu().numpy(),\r\nRuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nEnvironment\r\nCUDA:\r\n- GPU:\r\n- available: False\r\n- version: None\r\nPackages:\r\n- numpy: 1.19.0\r\n- pyTorch_debug: False\r\n- pyTorch_version: 1.6.0.dev20200622\r\n- pytorch-lightning: 0.9.0\r\n- tensorboard: 2.2.0\r\n- tqdm: 4.48.2\r\nSystem:\r\n- OS: Linux\r\n- architecture:\r\n- 64bit\r\n-\r\n- processor:\r\n- python: 3.7.7\r\n- version: #1 SMP Debian 4.14.81.bm.15 Sun Sep 8 05:02:31 UTC 2019\r\n\r\nPyTorch Version (e.g., 1.0): 1.6\r\nOS (e.g., Linux): Linux\r\nHow you installed PyTorch (conda, pip, source): pip\r\nBuild command you used (if compiling from source):\r\nPython version: 3.7.8\r\nCUDA/cuDNN version: None\r\nGPU models and configuration: None\r\nAny other relevant information: torch_xla:1.6.0\r\n\r\n## Additional context\r\n\r\n```\r\n/pytorch_lightning/trainer/training_loop.py(1049)optimizer_closure()\r\n\r\n1044\r\n1045                # if the user decides to finally reduce things in epoch_end, save raw output without graphs\r\n1046                if isinstance(training_step_output_for_epoch_end, torch.Tensor):\r\n1047                    training_step_output_for_epoch_end = training_step_output_for_epoch_end.detach()\r\n1048                elif is_result_obj:\r\n1049B->              training_step_output_for_epoch_end = copy(training_step_output) ###<- there should be detach before copy\r\n1050                    training_step_output_for_epoch_end.detach()\r\n1051                else:\r\n1052                    training_step_output_for_epoch_end = recursive_detach(training_step_output_for_epoch_end)\r\n1053\r\n```\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\nI was reading [this](https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork) wiki page to solve multiprocess issue and I noticed that `Autograd engine relays on threads pool, `, which is the first sentence, probably has a spelling error. \r\n\r\nI think it's supposed to be 'relies' rather than 'relays' because the latter is something to do with sending signals or such and the former means it depends upon something like the threads pool. Plus, I think there could be a link to python documentation to point out that this type of context is actually problematic: https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\r\n\r\nAlso, I think it will be useful to have an explicit import of `multiprocessing` like this:\r\n```python\r\nimport multiprocessing as mp\r\n\r\n# The rest of code samples.\r\n```\r\n\r\nThanks\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\nHi\r\n\r\n`torch.finfo.tiny` has the description \r\n> \"The smallest positive representable number\". \r\n\r\nIt's ambiguous, and not the actual smallest positive number. For example, `torch.finfo(\"torch.float16\").tiny` get **6.10e-5**, but the smallest positive representable number in FP16 should be **2^-24** or **5.96e-8**. `numpy.finfo.tiny` also has this problem.\r\n\r\nI check the code in [`TypeInfo.cpp`](https://github.com/pytorch/pytorch/blob/0651887eb4fd85ebbf65ab29ff2b634226871fee/torch/csrc/TypeInfo.cpp#L169), it uses `std::numeric_limits<>::min()` to get this value, but it should be `std::numeric_limits<>::denorm_min()` according the [document](http://www.cplusplus.com/reference/limits/numeric_limits/).\r\n\r\nI am not sure whether the `tiny` function should be modified or add new function, such as `denorm_tiny`, or just fix the description to be\r\n> \"The minimum normalized positive number\".\r\n\r\nThanks\n\ncc @jlin27"},{"labels":["documentation",null],"text":"I find this when working on https://github.com/pytorch/pytorch/issues/43667\r\n\r\nIf you replace the docs of `torch.bmm`\r\n```\r\nArgs:\r\n    input (Tensor): the first batch of matrices to be multiplied\r\n    mat2 (Tensor): the second batch of matrices to be multiplied\r\n    deterministic (bool, optional): flag to choose between a faster non-deterministic\r\n                                    calculation, or a slower deterministic calculation.\r\n                                    This argument is only available for sparse-dense CUDA bmm.\r\n                                    Default: ``False``\r\n    {out}\r\n```\r\nwith\r\n```\r\nArgs:\r\n    input (Tensor): the first batch of matrices to be multiplied\r\n    mat2 (Tensor): the second batch of matrices to be multiplied\r\n\r\nKeyword args:\r\n    deterministic (bool, optional): flag to choose between a faster non-deterministic\r\n                                    calculation, or a slower deterministic calculation.\r\n                                    This argument is only available for sparse-dense CUDA bmm.\r\n                                    Default: ``False``\r\n    {out}\r\n```\r\nyou will get an error\r\n```\r\ndocstring of torch.bmm:: WARNING: more than one target found for cross-reference 'bool': torch.FloatStorage.bool, torch.Tensor.bool\r\n```\r\n\r\ncc @jlin27 @mruberry "},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty\r\n\r\n#### API\r\n\r\n`torch.empty`\r\n\r\n#### Issue\r\n\r\nThe signature in document of API `torch.empty` is incomplete. It didn't include `memory_format`, which is actually accepted by the function.\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91071928-c35bff80-e606-11ea-8015-ff333c30633a.png)\r\n\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic\r\n\r\n#### API\r\n\r\n`torch.quantization.quantize_dynamic`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `module` , but it is not in the signature, and it is not accepted by the function.\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91071527-4cbf0200-e606-11ea-8903-51d6d25365a4.png)\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce_multigpu\r\n\r\n#### API\r\n\r\n`torch.distributed.all_reduce_multigpu`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `list`, but it is not in the signature, and it is not accepted by the function. It should be `tensor_list`\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91068305-1b443780-e602-11ea-8e9f-c065e3ac5092.png)\r\n\r\n\r\nRunning code :\r\n\r\n~~~python\r\nimport torch\r\ntorch.distributed.all_reduce_multigpu(list=[])\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: all_reduce_multigpu() got an unexpected keyword argument 'list'\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version:  1.6.0\r\n- Python version: 3.8.2\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve\r\n\r\n#### API\r\n\r\n`torch.lu_solve`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `b` , but it is not in the signature.\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91067635-2fd40000-e601-11ea-844c-651148a46bd6.png)\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp\r\n\r\n#### API\r\n\r\n`torch.clamp`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section under the following two signature:\r\n\r\n- `torch.clamp(input, *, min, out=None) → Tensor`\r\n- `torch.clamp(input, *, max, out=None) → Tensor`\r\n\r\nthere is an input argument `value` , but it is not in the signature, and it is not accepted by the function. It should be `min ` and `max` respectively\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91067305-bdfbb680-e600-11ea-8124-9ca4613ae37c.png)\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91067329-c5bb5b00-e600-11ea-98ab-5cbb6af8d4a2.png)\r\n\r\nRunning code :\r\n\r\n~~~python\r\nimport torch\r\na = torch.randn(4)\r\ntorch.clamp(a, max=0.5, value =1)\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTypeError: clamp() got an unexpected keyword argument 'value'\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version: 1.7.0.dev20200819+cpu\r\n- Python version: 3.8.2\r\n\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson\r\n\r\n#### API\r\n\r\n`torch.poisson`\r\n\r\n#### Issue\r\n\r\nThere is an format issue in the function's signature. The input `input * ` should be `input`.\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91066984-59d8f280-e600-11ea-87a4-8955a0144dd4.png)\r\n\r\n\r\nCode example:\r\n\r\n~~~python\r\nimport torch\r\nrates = torch.rand(4, 4) * 5 \r\ntorch.poisson(input=rates)\r\n~~~\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version: 1.7.0.dev20200819+cpu\r\n- Python version: 3.8.2\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/generated/torch.take.html#torch.take\r\n\r\n#### API\r\n\r\n`torch.take`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input ` indices`, but it is not in the signature, and it is not accepted by the function. It should be `index`\r\n\r\n![image](https://user-images.githubusercontent.com/24580222/91066752-154d5700-e600-11ea-9371-a77f1a9ed16b.png)\r\n\r\n\r\nRunning code :\r\n\r\n~~~python\r\nimport torch\r\nsrc = torch.tensor([[4, 3, 5], [6, 7, 8]])\r\ntorch.take(src, indices=torch.tensor([0, 2, 5]))\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTypeError: take() missing 1 required positional arguments: \"index\"\r\n~~~\r\n\r\n\r\n\r\nBut it throws no exception when running:\r\n\r\n~~~python\r\ntorch.take(src, index=torch.tensor([0, 2, 5]))\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version:  1.7.0.dev20200819+cpu\r\n- Python version: 3.8.2\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIn [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding), it is stated: \r\n\r\nwith `mode=\"mean\"` is equivalent to `Embedding` followed by `torch.mean(dim=0)`\r\n\r\nHowever, I think the author intends `dim=1` instead. This is seen by comparing the following example ... \r\n```\r\n>>> # an Embedding module containing 10 tensors of size 3\r\n>>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')\r\n>>> # a batch of 2 samples of 4 indices each\r\n>>> input = torch.LongTensor([1,2,4,5,4,3,2,9])\r\n>>> offsets = torch.LongTensor([0,4])\r\n>>> embedding_sum(input, offsets)\r\ntensor([[-0.8861, -5.4350, -0.0523],\r\n        [ 1.1306, -2.5798, -1.0044]])\r\n```\r\n... with the corresponding example in [`Embedding`](https://pytorch.org/docs/stable/generated/torch.mean.html).\r\n\r\n```\r\n>>> # an Embedding module containing 10 tensors of size 3\r\n>>> embedding = nn.Embedding(10, 3)\r\n>>> # a batch of 2 samples of 4 indices each\r\n>>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\r\n>>> embedding(input)\r\ntensor([[[-0.0251, -1.6902,  0.7172],\r\n         [-0.6431,  0.0748,  0.6969],\r\n         [ 1.4970,  1.3448, -0.9685],\r\n         [-0.3677, -2.7265, -0.1685]],\r\n\r\n        [[ 1.4970,  1.3448, -0.9685],\r\n         [ 0.4362, -0.4004,  0.9400],\r\n         [-0.6431,  0.0748,  0.6969],\r\n         [ 0.9124, -2.3616,  1.1151]]])\r\n```\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"I want to install pytorch1.5.1 torchvision0.6.1 in centos7 cuda10.2 python3.7. \r\n1.I clicked \"Previous Versions of PyTorch\" on the Pytroch site. \r\n2.I choice ‘v1.5.0’，‘wheel’，‘Linux and Windows’，‘# CUDA 10.2 pip install torch==1.5.0 torchvision==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html’.\r\n`(py37) [himap@localhost py37]$ pip install torch==1.5.1 -f https://download.pytorch.org/whl/torch_stable.html\r\nLooking in links: https://download.pytorch.org/whl/torch_stable.html\r\nRequirement already satisfied: torch==1.5.1 in ./lib/python3.7/site-packages (1.5.1+cu92)\r\nRequirement already satisfied: future in ./lib/python3.7/site-packages (from torch==1.5.1) (0.18.2)\r\nRequirement already satisfied: numpy in ./lib/python3.7/site-packages (from torch==1.5.1) (1.18.1)`\r\nAfter using this command, I get Pytroch1.5.1 + cu92\r\n`>>> torch.__version__\r\n'1.5.1+cu92'\r\n`\r\n3.When I use \"pip install torch==1.5.1+cu102 -f https://download.pytorch.org/whl/torch_stable.html\", he prompts me\" `Looking in links: https://download.pytorch.org/whl/torch_stable.html\r\nERROR: Could not find a version that satisfies the requirement torch==1.5.1+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 0.4.1, 0.4.1.post2, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.2.0+cpu, 1.2.0+cu92, 1.3.0, 1.3.0+cpu, 1.3.0+cu92, 1.3.1, 1.3.1+cpu, 1.3.1+cu92, 1.4.0, 1.4.0+cpu, 1.4.0+cu92, 1.5.0, 1.5.0+cpu, 1.5.0+cu101, 1.5.0+cu92, 1.5.1, 1.5.1+cpu, 1.5.1+cu101, 1.5.1+cu92, 1.6.0, 1.6.0+cpu, 1.6.0+cu101, 1.6.0+cu92)\r\nERROR: No matching distribution found for torch==1.5.1+cu102\"`\r\nSo I think there is a problem with the official order.\n\ncc @ezyang @seemethere @malfet @jlin27"},{"labels":["documentation",null,null],"text":"## 📚 Documentation\r\n\r\nThis issue refers to #42864, but the original issue was closed by the author while I was writing my response to it.\r\n\r\nThese are the docs at the moment:\r\n![loss1](https://user-images.githubusercontent.com/47462742/89954344-2f1a9180-dc31-11ea-9dc1-5e73ee5f1d88.PNG)\r\n\r\nBased on the [source](https://github.com/pytorch/pytorch/blob/0ff0fea42bf9721b87e01fe15445dfd3ea5f2093/aten/src/ATen/native/Loss.cpp#L74) the correct input/target shape is:\r\n- Input 1: (N, ∗) where ∗ means, any number of additional dimensions\r\n- Input 2: (N, ∗) same shape as Input 1, or it has to be broadcastable to that shape\r\n- Target: (N, ∗), same shape as Input 1 or Input 2, or it has to be broadcastable to that shape\r\n\r\nSo I think  this part in the docs should be fixed and an example of how to use the loss would be helpful.\r\n\n\ncc @jlin27 @albanD @mruberry"},{"labels":["documentation",null,null],"text":"## 📚 Documentation\r\n\r\nThe documentation of Adam since v1.6.0 suggests that it uses the weight decay fix proposed by paper \"Decoupled Weight Decay Regularization\":\r\nhttps://github.com/pytorch/pytorch/blob/4b4273a04e566dcdf4cb96882f49c41127e8d3f1/torch/optim/adam.py#L10-L11\r\n\r\nHowever I found this to not be the case. The actual weight decay in Adam is still the old one:\r\nhttps://github.com/pytorch/pytorch/blob/4b4273a04e566dcdf4cb96882f49c41127e8d3f1/torch/optim/adam.py#L99-L100\r\n\r\nIn contrast, the new weight decay fix is implemented in AdamW, and explained by the AdamW doc:\r\nhttps://github.com/pytorch/pytorch/blob/4b4273a04e566dcdf4cb96882f49c41127e8d3f1/torch/optim/adamw.py#L10\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4b4273a04e566dcdf4cb96882f49c41127e8d3f1/torch/optim/adamw.py#L73\r\n\r\nIt is possible that I misunderstood the current documentation of Adam, but I found it confusing and suggest that currently there is no difference between the Adam and AdamW implementations, while the difference still exists.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null,null],"text":"## 📚 Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIn the docs of [`nn.functional.interpolate`](https://pytorch.org/docs/master/nn.functional.html#interpolate) it is stated that `align_corners` is a `bool` with default value `False`. Furthermore, it is stated that `align_corners` **does not affect** interpolation modes 'area' as well as 'nearest' .\r\n\r\nIf you look at the implementation however, default of `align_corners` is `None`. Furthermore, when passing a `bool` for `align_corners` together with interpolation modes 'nearest' or 'area' to `nn.functional.interpolate`, a [ValueError is raised](https://github.com/pytorch/pytorch/blob/73642d9425a358b51a683cf6f95852d06cba1096/torch/nn/functional.py#)\r\n\r\nThere is thus a clear mismatch between docstrings and implemented behavior of `align_corners` in combination with modes 'nearest' and 'area' for `nn.functional.interpolate`. Based on the intended behvaior, either the code or the docstrings should be changed.\r\n\r\nRelated PR that introduced `align_corners` and may clarify originally intended behavior: #5927\r\n\r\nI would vote for changing the code to adhere to the behavior outlined in the docstrings, and am up to fixing the issue regardless of the made decision.\n\ncc @jlin27 @albanD @mruberry"},{"labels":[null,"documentation",null],"text":"## 📚 Documentation\r\n\r\nSome equations in the document do not display. For example, [L1LOSS](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss).\n\ncc @ezyang @zou3519 @jlin27"},{"labels":["documentation",null,null],"text":"The documentation for [OneCycleLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.OneCycleLR) mentions `verbose` as one of the arguments but the [code](https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR) doesn't have that argument. Thus, it raises an error if someone passes a value for `verbose` just going by the documentation.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\nThe description on non_blocking should mention pinned memory, since `non_blocking` only affects copies between pinned memory (on the CPU) and GPU.\r\n\r\nThe current description for the non_blocking argument says \"if True and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.\" \r\n\r\nThe docs for `Tensor.cuda()` are better:\r\n\r\n\"If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. Default: False.\"\r\n\r\nAdditionally, the description of non_blocking in `Tenosr.to()` should be improved. \n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"## 📚 Documentation\r\nStrange behavior in torch.distributions.negative_binomial:\r\nhttps://pytorch.org/docs/stable/distributions.html#negativebinomial\r\n\r\nWhen trying out the negative binomial distribution I found that the second input parameter prob (success probability) is actually the failure probability --> not consistent with documentation. This is also not consistent with other software such as scipy.\r\n\r\nExample:\r\nIn [1]: import torch \r\n   ...: from scipy.stats import nbinom \r\n   ...: import numpy as np \r\n   ...: n=2.0 \r\n   ...: x=0.0 \r\n   ...: p=.75 \r\n   ...: dis=torch.distributions.NegativeBinomial(n, 1-p) \r\n   ...: prob_torch=dis.log_prob(torch.tensor(x)).exp() \r\n   ...: prob_np=nbinom.pmf(x,n,p) \r\n   ...: np.isclose(prob_np,prob_torch.numpy())                                  \r\nOut[1]: True\r\n\r\n\n\ncc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw @jlin27"},{"labels":["documentation",null,null],"text":"## 📚 Documentation\r\n\r\nThe documentation for the `OneCycleLR` learning rate schedule refers to the argument `pct_start` as being a percentage [0,100], whereas it actually used as a proportion [0,1] in the code.\r\n\r\n> pct_start (float) – The percentage of the cycle (in number of steps) spent increasing the learning rate. Default: 0.3\r\n\r\ncompared to the implementation in `torch.optim.lr_scheduler.OneCycleLR`\r\n```python\r\n        self.step_size_up = float(pct_start * self.total_steps) - 1\r\n```\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"It looks like bitwise_and/bitwise_and_ have no arguments (like bitwise_not_) which is not true\r\n\r\n![image](https://user-images.githubusercontent.com/1041752/88780198-f091cb00-d18a-11ea-9b12-dcba701d339b.png)\r\n\n\ncc @jlin27"},{"labels":[null,"documentation"],"text":"There is a tiny mistake in the documentation of torch.cuda.amp.GradScaler. Under the description of __init__ parameters:\r\n```Python\r\n\"\"\"\r\n        growth_factor (float, optional, default=2.0):  Factor by which the scale is multiplied during\r\n            :meth:`update` if no inf/NaN gradients occur for ``growth_factor`` consecutive iterations\r\n\"\"\"\r\n```\r\nShould be for ``growth_interval`` consecutive iterations?\n\ncc @mcarilli @jlin27"},{"labels":["documentation",null,null],"text":"Documentation pretends that weights and bias are initialized uniformly in +-1/sqrt(in), whereas the code uses kaiming for weights and a kaiming variant for biases.\n\ncc @jlin27 @albanD @mruberry"},{"labels":[null,"documentation",null],"text":"##  Bug\r\n\r\nBuilding the docs for me as an end user (offline documentation) does not work.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Check out the pytorch repo at tag `v1.5.1`.\r\n1. Create and activate virtual environment: `cd /path/to/pytorch; python3 -m venv .venv; source .venv/bin/activate`\r\n1. Install pytorch version 1.5.1: `pip3 install torch==v1.5.1`\r\n1. Install requirements needed for docs: `cd docs; pip3 install -U -r requirements.txt`\r\n1. Trying to build the docs: `make; make html`\r\n\r\nAll steps together:\r\n```bash\r\ngit clone https://github.com/pytorch/pytorch.git\r\ncd pytorch\r\ngit checkout v1.5.1\r\npython3 -m venv .venv\r\nsource .venv/bin/activate\r\npip3 install torch==v1.5.1\r\ncd docs\r\npip3 install -U -r requirements.txt\r\nmake\r\nmake html\r\n```\r\n\r\nWhen the `make html` step tries to write the output, it fails like this:\r\n```\r\npreparing documents... done\r\nwriting output... [ 23%] distributions                                                                                                                                                                             \r\nException occurred:\r\n  File \"/usr/lib/python3.6/subprocess.py\", line 1364, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'katex': 'katex'\r\nThe full traceback has been saved in /tmp/sphinx-err-tsyb3hy5.log, if you want to report the issue to the developers.\r\nPlease also report this if it was a user error, so that a better error message can be provided next time.\r\nA bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!\r\nMakefile:38: recipe for target 'html' failed\r\nmake: *** [html] Error 2\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nI expect the build process to work. Other users seemed to have problems with it before [here](https://discuss.pytorch.org/t/generate-offline-documentation/64643/6) and [here](https://discuss.pytorch.org/t/accessing-pytorch-documentation-offline/20453), but I already avoided the mistakes made there.\r\n\r\n## Environment\r\n\r\nGenerated output from your\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py).\r\n\r\n```\r\nPyTorch version: 1.5.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Quadro P2000\r\nNvidia driver version: 410.104\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.19.1\r\n[pip3] pytorch-sphinx-theme==0.0.24\r\n[pip3] torch==1.5.1\r\n[conda] Could not collect\r\n```\n\ncc @ezyang @zou3519 @jlin27"},{"labels":[null,"documentation",null],"text":"## 📚 Documentation\r\n\r\n\"Distributed RPC Framework\" documentation (master version) contains the following note:\r\n\r\n> Please refer to [PyTorch Distributed Overview](https://pytorch.org/docs/master/rpc.html) for a brief introduction to all features related to distributed training.\r\n\r\n<img width=\"976\" alt=\"Screen Shot 2020-07-24 at 12 44 58\" src=\"https://user-images.githubusercontent.com/2459423/88383996-dd0deb00-cdab-11ea-86ed-89def1b30e58.png\">\r\n\r\nThe link behind \"PyTorch Distributed Overview\" is missing.\r\n\r\nI could send a PR with a quick fix, but I can not figure out where is the correct ressource...\r\n\r\ncc @mrshenli "},{"labels":["documentation",null,null],"text":"In https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam, towards the end, we have\r\n\r\n```\r\n                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\r\n```\r\n\r\nHowever, the [paper](https://arxiv.org/pdf/1412.6980.pdf) uses epsilon by adding it to `exp_avg_sq.sqrt()`; not to a whole fraction.\r\nThis is always the case, in both the Algorithm 1 formulation, as well as the one before Section 2.1.\r\n\r\nWhat pytorch does is that it uses `epsilon * sqrt(1 - beta_2^t)` instead of just `epsilon`, for the paper's algoritm formulation.\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n![image](https://user-images.githubusercontent.com/33288114/87407914-60ce1780-c5f5-11ea-80eb-4fdb8c374463.png)\r\n\r\nThis formula confuses me because this formula does not look like a matrix formula. I can't determine whether W or x is a matrix or a vector.I think your formula does not consider the consistency of matrix dimensions.\r\n\r\nAccording to the relevant weights and input dimensions provided by the official documentation ， I think the correct formula should be:\r\n![image](https://user-images.githubusercontent.com/33288114/87409139-1e0d3f00-c5f7-11ea-953e-3fe0ffcb727f.png)\r\n\r\n\r\n\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## 📚 Documentation\r\nFor the parameter 'dim', the documentation says 'If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.'\r\n\r\nMy code is:\r\nA = torch.randn(4, 3, 3)\r\nB = torch.norm(A)\r\nprint(B)\r\nThe result is:\r\ntensor(5.7707)\r\n\r\nIt seems that torch.norm computes the sum of the square of every number in the tensor A and returns the square root of the sum. However, the documentation says ' the vector norm will be applied to the last dimension'. I think what the function does contradicts the documentation. By the way, I prefer what the function does, because computing the Frobenius norm of a three or more dimensional tensor is useful. So, I think to change the documentation is a good idea, instead of to fix the implementation of torch.norm.\n\ncc @jlin27"},{"labels":[null,"documentation",null,null,null],"text":"##  Bug\r\n\r\nPer [documentation of MaxPool](https://pytorch.org/docs/stable/nn.html#maxpool2d), input is \"implicitly zero-padded on both sides for padding number of points.\" The implementation differs by implicitly padding with identity elements, rather than zero.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a tensor of signed type, filled with negative values.\r\n1. Invoke a MaxPool with positive padding.\r\n1. Compare against ground truth, either computed by hand or created by explicit zero-padding.\r\n\r\nMinimal code sample / unit test:\r\n```\r\n# Maxpool of non-positive values, such that padding is included in the computation of each result element.\r\nimport torch\r\nr = torch.nn.functional.max_pool2d(-torch.rand(2,2,64), kernel_size=[3,3], padding=1)\r\nassert torch.equal(r, torch.zeros_like(r))\r\n```\r\n\r\n## Expected behavior\r\n\r\nAccording to documentation, the values near the edge will be computed using the implicit zero-padding. The maximum of a negative number and zero is zero, so such values _ought_ to be zero. In the provided example, the assert should not fire.\r\n\r\nWhen the code sample above is run, however, the assert fires.\r\n\r\n## Environment\r\n\r\nInformation from collect_env.py:\r\n\r\n- PyTorch version: 1.5.1\r\n- Is debug build: No\r\n- CUDA used to build PyTorch: 10.2\r\n\r\n- OS: Microsoft Windows 10 Home\r\n- GCC version: (x86_64-win32-seh-rev0, Built by MinGW-W64 project) 8.1.0\r\n- CMake version: version 3.14.1\r\n\r\n- Python version: 3.8\r\n- Is CUDA available: Yes\r\n- CUDA runtime version: Could not collect\r\n- GPU models and configuration: GPU 0: GeForce GTX 960M\r\n- Nvidia driver version: 442.50\r\n- cuDNN version: Could not collect\r\n\r\n- Versions of relevant libraries:\r\n- [pip] Could not collect\r\n- [conda] Could not collect\r\n\r\n## Additional context\r\n\r\nN/A\n\ncc @ezyang @gchanan @zou3519 @jlin27 @albanD @mruberry"},{"labels":[null,"documentation",null],"text":"##  Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\ncompiling incorrectly fails with the error `error: passing ‘at::Tensor’ as ‘this’ argument discards qualifiers [-fpermissive]` (aka const correctness violated)\r\n\r\nThis follows the example from the docs (https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4N2at6Tensor4gradEv)\r\n\r\n> `Tensor &grad()`\r\n> Return a mutable reference to the gradient. \r\n> This is conventionally used as `t.grad() = x` to set a gradient to a completely new tensor.\r\n\r\n> `void backward(...)`\r\n> ...\r\n> This function accumulates gradients in the leaves - you might need to zero them before calling it.\r\n\r\nbut fails to compile\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. call backward()\r\n2. set the grad to zero\r\n3. compile\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```\r\n...error: passing ‘at::Tensor’ as ‘this’ argument discards qualifiers [-fpermissive]\r\n     z_grad = 0;\r\n              ^\r\n```\r\n```\r\n#include <torch/torch.h>                                                                                                                                                                                           \r\n#include <iostream>                                                                                                                                                                                                \r\n#include <vector>                                                                                                                                                                                                  \r\n                                                                                                                                                                                                                   torch::Device get_device()                                                                                                                                                                                         \r\n{                                                                                                                                                                                                                  \r\n    torch::Device device = torch::kCPU;                                                                                                                                                                            \r\n    if (torch::cuda::is_available()) {                                                                                                                                                                                     std::cout << \"CUDA is available! Training on GPU.\" << std::endl;                                                                                                                                           \r\n        device = torch::kCUDA;                                                                                                                                                                                     \r\n    }                                                                                                                                                                                                              \r\n    return device;                                                                                                                                                                                                 \r\n}                                                                                                                                                                                                                  \r\n                                                                                                                                                                                                                   \r\nint main() {                                                                                                                                                                                                       \r\n    using torch::Tensor;                                                                                                                                                                                           \r\n    auto device = get_device();                                                                                                                                                                                    \r\n    auto options =                                                                                                                                                                                                 \r\n        torch::TensorOptions()                                                                                                                                                                                     \r\n        .dtype(torch::kFloat32)                                                                                                                                                                                    \r\n        .layout(torch::kStrided)                                                                                                                                                                                   \r\n        .device(device)                                                                                                                                                                                            \r\n        .requires_grad(true);                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                                                                                                                                              \r\n    std::vector<float> z_vec = {1,2,3};                                                                                                                                                                                                                                                                   \r\n    Tensor z = torch::from_blob(z_vec.data(), {z_vec.size()}).to(device);                                                                                                                                          \r\n                                                                                                                                                                                                                   \r\n    Tensor z_times_2 = 2 * z;                                                                                                                                                                                      \r\n    z_times_2.backward({}, true); // keep graph                                                                                                                                                                    \r\n    Tensor& z_grad = z.grad(); // mutable, non-const ref\r\n    std::cout << z << std::endl;                                                                                                                                                                                   \r\n    std::cout << z_grad << std::endl;                                                                                                                                                                              \r\n    z_grad = 0;                                                                                                                                                                                                    \r\n    std::cout << z_grad << std::endl;                                                                                                                                                                              \r\n}\r\n```\r\nfails to compile with the error above, identifying `z_grad = 0;` as the problem   \r\n\r\nExactly as used in the docs, `z.grad() = 0` does not work either.\r\n\r\nEven with const_cast, it still fails to compile.\r\n`const_cast<Tensor&>(z_grad) = 0; ` or alternatively `Tensor& z_grad = const_cast<Tensor&>(z.grad()); // mutable, non-const ref`\r\n\r\nPassing `-fpermissive` to the compiler does allow it to compile, but this should not be necessary\r\n\r\n## Expected behavior\r\n\r\nIt should compile and set the gradient to zero.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): libtorch 1.5.1(stable, linux, libtorch, C++/Java, CUDA 10.2) CXX11 ABI\r\n - OS (e.g., Linux): Linux, Ubuntu 18.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): the downloads page (https://pytorch.org/get-started/locally/) and linking against it (https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.5.1.zip)\r\n - Build command you used (if compiling from source): not compiled from source\r\n - Python version: irrelevant\r\n - CUDA/cuDNN version: (probably irrelevant) CUDA 10.2, cuDNN 7.6.5\r\n - GPU models and configuration: (probably irrelevant) Quadro P1000\r\n - Additional information: gcc 7.3 (`g++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0`), cmake is pretty much exactly the minimal example here (https://pytorch.org/cppdocs/installing.html) but renamed and without MSVC\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\nFrom the module code overview page: https://pytorch.org/docs/master/_modules/ there are some modules whose source code documentation page isn't properly displaying and instead displays the \"*Oops! You've reached a dead end*\" page. \r\n\r\ne.g. \r\n\r\n* https://pytorch.org/docs/master/_modules/torch/__config__.html\r\n* https://pytorch.org/docs/master/_modules/torch/_jit_internal.html\r\n* https://pytorch.org/docs/master/_modules/torch/_lobpcg.html\r\n* https://pytorch.org/docs/master/_modules/torch/_lowrank.html\r\n* https://pytorch.org/docs/master/_modules/torch/jit/_script.html\r\n* https://pytorch.org/docs/master/_modules/torch/jit/_trace.html\r\n\r\nWhen I build the docs using a local copy of PyTorch however, the HTML files properly build for these pages. \r\n\r\ne.g. for `torch/__config__`\r\n\r\n**Locally built:**\r\n\r\n<img width=\"960\" alt=\"locally-built-pytorch-docs\" src=\"https://user-images.githubusercontent.com/54918401/86973415-85804600-c142-11ea-9358-2ebf95152bf8.png\">\r\n\r\n**Public page on PyTorch website:**\r\n\r\n<img width=\"1044\" alt=\"public-pytorch-docs\" src=\"https://user-images.githubusercontent.com/54918401/86973452-9335cb80-c142-11ea-8c88-0d39b1ba636c.png\">\r\n\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\nIt looks as though the code is missing for the [Audio Classification Tutorial](https://pytorch.org/tutorials/beginner/audio_classifier_tutorial.html). I didn't see code in the html document or at the [linked GitHib url](https://github.com/pytorch/tutorials/blob/master/beginner_source/audio_classifier_tutorial.py).\r\n\r\nThis is what I see:\r\n\r\n<img width=\"1440\" alt=\"Screenshot 2020-07-08 at 7 57 09 AM\" src=\"https://user-images.githubusercontent.com/6405428/86857353-e7b05b00-c0f0-11ea-998a-845795efefb8.png\">\r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Bug\r\n\r\nAccording to the documentation, the signature of `torch.cat` is `torch.cat(tensors, dim=0, out=None)`. However, it is not possible to call `torch.cat` with three positional arguments.\r\n\r\n## To Reproduce\r\n\r\n```\r\n>>> torch.cat([torch.rand(1, 2, 3), torch.rand(1, 2, 3)], 0, torch.FloatTensor(12))\r\nTypeError: cat() received an invalid combination of arguments - got (list, int, Tensor), but expected one of:\r\n * (tuple of Tensors tensors, name dim, *, Tensor out)\r\n * (tuple of Tensors tensors, int dim, *, Tensor out)\r\n```\r\n\r\nThe following works though, notice that I only added `out=` to the third argument. However, this should not be required as per the signature definition in the documentation.\r\n\r\n```\r\n>>> torch.cat([torch.rand(1, 2, 3), torch.rand(1, 2, 3)], 0, out=torch.FloatTensor(12))\r\ntensor([[[0.6310, 0.3772, 0.7654],\r\n         [0.8126, 0.7031, 0.6288]],\r\n\r\n        [[0.3101, 0.7344, 0.5670],\r\n         [0.0366, 0.5181, 0.3637]]])\r\n```\r\n\r\n## Expected behavior\r\n\r\n`torch.cat` should be callable without having to use a named argument for `out`.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.5.0+cu101\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 20.04 LTS\r\nGCC version: (Ubuntu 9.3.0-10ubuntu2) 9.3.0\r\nCMake version: version 3.16.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1650\r\nNvidia driver version: 440.64\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.5.0+cu101\r\n[pip3] torchvision==0.6.0+cu101\r\n[conda] numpy                     1.18.1                   pypi_0    pypi\r\n[conda] torch                     1.5.0+cu101              pypi_0    pypi\r\n[conda] torchvision               0.6.0+cu101              pypi_0    pypi\r\n```\n\ncc @jlin27"},{"labels":["documentation",null,null],"text":"## 📚 Documentation\r\nTorch 1.5.0 CPU, Linux\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L448\r\n\r\nBug API: `torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)`\r\n\r\nIf set T_max =5, initial learning_rate of optimizer are 0.5, eta_min=0:\r\n\r\nThe value calculated by framework are as flow :\r\n![image](https://user-images.githubusercontent.com/52485244/86489498-16e46680-bd97-11ea-9a10-622d48d4cf3e.png)\r\n\r\nBut the value calculated by formula in the document are as flow：\r\nhttps://pytorch.org/docs/stable/optim.html?highlight=cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR\r\n![image](https://user-images.githubusercontent.com/52485244/86489333-a2112c80-bd96-11ea-8e01-19fa5ae1227a.png)\r\n\r\nThese Two doesn't match, in epoch 5. one is 0, but the other is 0.0954915028125. I want to know which one is right, thank you very much!\n\ncc @jlin27 @vincentqb"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nThe link to [\"PyTorch\" organization on GitHub](https://github.com/orgs/pytorch/teams/facebook) seems to be broken, when accessed from PyTorch Governance (both on the [master version](https://pytorch.org/docs/master/community/governance.html#core-developers) and the [stable version](https://pytorch.org/docs/stable/community/governance.html#core-developers)).\r\n\r\nIt does not work even on [Github source code of pages](https://github.com/pytorch/pytorch/blob/master/docs/source/community/governance.rst#core-developers).\r\n\r\nIs this an old link? What should we replace it with?\n\ncc @jlin27"},{"labels":[null,"documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/cuda.html#torch.cuda.set_rng_state_all\r\n\r\n#### API\r\n\r\n`torch.utils.checkpoint.checkpoint_sequential`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input ` new_state`, but it is not in the signature, and it is not accepted by the function. It should be `new_states`\r\n\r\nRunning code :\r\n\r\n~~~python\r\ntorch.cuda.set_rng_state_all(new_state=(torch.ByteTensor(1)))\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTypeError: set_rng_state_all() got an unexpected keyword argument 'new_state'\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version:  1.5.0\r\n- Python version: 3.8.2\r\n\r\ncc @ngimel @jlin27"},{"labels":["documentation",null,null],"text":"## 📚 Documentation\r\n\r\nThe current documentation says it's an \"ordered dictionary that respects the order of insertion\", but it can, in fact, destroy its order. For example: \r\n\r\n![image](https://user-images.githubusercontent.com/38511765/85050917-aeb54400-b14b-11ea-9e61-5aa455df6019.png)\r\n\r\nWe should update our documentation to clarify its behavior, especially as users might try to enumerate the dict sequentially. \r\n\n\ncc @jlin27 @albanD @mruberry"},{"labels":[null,null,"documentation",null,null],"text":"##  📚 Documentation\r\ntorch==1.5.0\r\n\r\nThe problem is the same for all three functions. For example, let's consider [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad).\r\n\r\nThe documentation says: \"This mode has no effect when using `enable_grad` context manager\". This is not true:\r\n```python\r\nimport torch\r\nx = torch.tensor([1.0], requires_grad=True)\r\nwith torch.enable_grad():\r\n    with torch.no_grad():\r\n        y = x * 2\r\nassert not y.requires_grad\r\n```\r\n\r\nAnd I mean that the documentation should be fixed, not the behavior of `torch` 😄 \r\nThese functions just set grad enabled/disabled in `__enter__` (or in the constructor in the case of `set_grad_enabled`) and set the _previous_ state in `__exit__`. This is very intuitive. IMHO, it will be confusing if, for example, the behavior of `no_grad` changes depending on some other context.\n\ncc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen @jlin27"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\nWhen it comes to parameter `dim` in `torch.norm`, the documentation says\r\n```\r\ndim (int, 2-tuple of python:ints, 2-list of python:ints, optional) – \r\nIf it is an int, vector norm will be calculated, if it is 2-tuple of ints,\r\nmatrix norm will be calculated.\r\n```\r\n\r\nWhen `p` is specified and `dim` is a 2-tuple, I would expect `torch.norm` to compute an induced matrix norm, but instead it computes an L_p norm (vector norm) over a flattened matrix.\r\n\r\nFor example:\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: x = torch.eye(3, 3)\r\n\r\nIn [3]: torch.norm(x, p=1, dim=(-2, -1)) # dim is a tuple, want a matrix norm\r\nOut[3]: tensor(3.) # should be 1 in case of 1 induced matrix norm\r\n\r\n```\r\n\r\n\r\ncc @jlin27"},{"labels":["documentation",null,null,null],"text":"## 📚 Documentation\r\n\r\nI am following the following documentation;\r\n\r\nhttps://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.rpc_async\r\n\r\nWhen I use the given export commands, the program breaks showing that \r\n\r\nFor Address ENV\r\n\r\n```bash\r\nraise _env_error(\"MASTER_ADDR\")\r\nValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set\r\n```\r\n\r\nFor PORT ENV\r\n```bash\r\n raise _env_error(\"MASTER_PORT\")\r\nValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_PORT expected, but not set\r\n```\r\n\r\nModified as follows, fixes the error. \r\n\r\n```bash\r\nexport MASTER_ADDR=localhost\r\nexport MASTER_PORT=5678\r\n```\r\n\r\nI tested this with the following way\r\n\r\n```bash\r\npython3 -m venv ENV\r\nsource activate ENV/bin/activate\r\npip3 install torch\r\n```\r\n\r\nScript `test_torch_dist.py'\r\n\r\n```python\r\nimport sys\r\nimport torch\r\nimport torch.distributed.rpc as rpc\r\n\r\n\r\ndef p1():\r\n    print(\r\n        \"Worker 1\"\r\n    )\r\n    rpc.init_rpc(\"worker0\", rank=0, world_size=2)\r\n    fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3))\r\n    fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2))\r\n    result = fut1.wait() + fut2.wait()\r\n    print(result)\r\n    rpc.shutdown()\r\n\r\ndef p2():\r\n    print(\r\n        \"Worker 2\"\r\n    )\r\n    rpc.init_rpc(\"worker1\", rank=1, world_size=2)\r\n    rpc.shutdown()\r\n\r\n\r\nargs = sys.argv\r\n\r\nprint(\"Args {}\".format(int(args[1])))\r\n\r\nif int(args[1]) == 1:\r\n    p1()\r\n\r\nif int(args[1]) == 2:\r\n    p2()\r\n\r\n```\r\n\r\nTerminal 1:\r\n\r\n```bash\r\npython3 test_torch_dist.py 1\r\n```\r\n\r\nTerminal 2:\r\n\r\n```bash\r\npython3 test_torch_dist.py 2\r\n```\r\n\r\nMachine configs: \r\n```bash\r\nlsb_release -a\r\n\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 20.04 LTS\r\nRelease:        20.04\r\nCodename:       focal\r\n```\r\n\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jlin27 @jjlilley"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\nIn the Tensor API documentation page.\r\n\r\nI saw \r\n\r\n> self, index and src should have same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim.\r\n\r\nBut when I look at size() API. \r\n\r\nhttps://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\r\nIt doesn't mention it can take an argument. Is something missing?\n\ncc @jlin27"},{"labels":["documentation",null,null,null],"text":"## 📚 Documentation\r\n\r\nThe information regarding the [_add_graph()_ function in the Tensorboard doc](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_graph) is empty. The missing info seems to be there in its [source](https://pytorch.org/docs/stable/_modules/torch/utils/tensorboard/writer.html#SummaryWriter.add_graph) tough. \r\n\n\ncc @jlin27"},{"labels":["documentation",null],"text":"##  Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nCurrently, it is a bit tricky to switch from CPU build to CUDA build. Users will sometimes forget to specify the cudatoolkit requirement and find out that they installed the CPU build. Then they will attempt to follow the steps on pytorch.org, that is `conda install -c pytorch cudatoolkit=x.x pytorch`. But actually that is not enough because `cpuonly` is still there.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. `conda install -c pytorch cpuonly pytorch`\r\n2. `conda install -c pytorch cudatoolkit=x.x pytorch`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nBetter user message. \r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\n\ncc @jlin27"},{"labels":[null,"documentation",null,null],"text":"## 📚 Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nIn the getting started section [this](https://pytorch.org/get-started/locally/#linux-from-source) section the following quote appears:\r\n\r\n> You will also need to build from source if you want CUDA support.\r\n\r\nI guess this is from previous versions where CUDA support was not built-in and this can be removed, correct?\n\ncc @ezyang @gchanan @zou3519 @jlin27"},{"labels":[null,"documentation",null],"text":"##  Bug\r\n\r\nAccording to the [documentation](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.verify_ninja_availability), the `verify_ninja_availability()` function should return True if Ninja is available on the user system.\r\n\r\nHowever, quickly looking at the implementation shows that it returns nothing and just raises a RuntimeError if ninja is not available.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\n# Run on system with and without Ninja\r\n\r\nfrom torch.utils.cpp_extension import verify_ninja_availability\r\nverify_ninja_availability()   # Does not return bool\r\n```\r\n\r\n## Expected behavior\r\n\r\nEither the documentation should be updated to correctly tell it will raise an error, or the function itself should be modified to return a boolean.\r\n\r\nPersonally, I am more in favor of returning a boolean, as it is annoying to need to wrap a function in a try/except.\r\n\r\n## Environment\r\nPyTorch version: 1.5.0+cu101\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro M2000\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] lightnet-torch==1.1.0\r\n[pip3] numpy==1.16.1\r\n[pip3] torch==1.5.0+cu101\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.6.0+cu101\r\n[conda] Could not collect\r\n\n\ncc @yf225 @glaringlee"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/quantization.html#torch.quantization.add_quant_dequant\r\n\r\n#### API\r\n\r\n`torch.quantization.add_quant_dequant` \r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input :\r\n\r\n**we want to quantize** (*that*) –\r\n\r\nwhich should be part of the description for input argument `module`\r\n\r\n"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n#### Link\r\n\r\nhttps://pytorch.org/docs/stable/torch.html#torch.add\r\n\r\n#### API\r\n\r\n`torch.add`\r\n\r\n#### Issue\r\n\r\nIn the \"Parameters\" section, there is an input `value` , but it is not in the signature, and it is not accepted by the function. It should be `other`\r\n\r\nRunning code :\r\n\r\n~~~python\r\na = torch.randn(4)\r\ntorch.add(a, other=20, value=1)\r\n~~~\r\n\r\ngives exception \r\n\r\n~~~python\r\nTypeError: add() got an unexpected keyword argument 'value'\r\n~~~\r\n\r\n\r\n\r\n#### **System information**\r\n\r\n- OS: MacOS Mojave 10.14\r\n- PyTorch version: 1.7.0.dev20200819+cpu\r\n- Python version: 3.8.2\r\n"},{"labels":[null,"documentation",null],"text":"## 📚 Documentation\r\n\r\nThe document enclosing the details of how one installs the C++ distribution of Pytorch, found [here](https://pytorch.org/cppdocs/installing.html), is missing one paragraph of code between the sentences _In that case CMake configuration step would look something like follows:_ and _If all goes well, it will look something like this:_\r\n\r\nApologies if this is not the place to report this, this was the best fit according to the template text.\r\n\n\ncc @yf225 @glaringlee"},{"labels":[null,null,"documentation",null],"text":"Originally in https://github.com/pytorch/pytorch/issues/18095#issuecomment-633149307:\r\n\r\n@vadimkantorov:\r\nWorks: https://pytorch.org/docs/stable/torch.html#torch.flip\r\nBreaks: https://pytorch.org/docs/master/torch.html#torch.flip\r\n\r\n@t-vi:\r\nThey're now separate pages: https://pytorch.org/docs/master/generated/torch.flip.html#torch.flip\r\n...but it would break many links, including on the forums.\r\n\r\n@ezyang:\r\nYes, this will be a release blocker for next release.\r\n\r\ncc @ezyang @gchanan @zou3519 @mattip"},{"labels":["documentation",null],"text":"This is a summary of the potential doc improvements that I came up with. I can work on a pull request for these changes after review of what changes need to be made.\r\n\r\n1. Make clear that `torch.set_default_dtype` and `torch.get_default_dtype` only affect `torch.Tesor` and not `torch.tensor`.\r\n   \r\n    Suggestion:-\r\n    In [torch.set_default_dtype](https://pytorch.org/docs/stable/torch.html#torch.set_default_dtype) change:\r\n    The default floating point dtype is initially `torch.float32`. -> The default floating-point dtype used by `torch.Tensor` is initially `torch.float32`.\r\n\r\n    In [torch.get_default_dtype](https://pytorch.org/docs/stable/torch.html#torch.get_default_dtype) change:\r\n    Get the current default floating-point `torch.dtype` -> Get the current default floating point `torch.dtype` used by `torch.Tensor`.\r\n\r\n2. Specify the default value of **sci_mode** in [torch.set_printoptions](https://pytorch.org/docs/stable/torch.html#torch.set_printoptions). \r\n\r\n    Currently, it says to look for the default value defined by _Formatter. But to get to the default value a lot of work has to be done like:\r\n    ```python\r\n    torch.set_printoptions??  # Get location `torch/_tensor_str.py`\r\n    torch._tensor_str._Formatter??  # Default value defined in the __init__ method of this class\r\n    ```\r\n\r\n    Suggestion:-\r\n    Specify the default value of `False` in the docs or specify the exact path of `_Formatter` as `torch._tensor_str._Formatter`.\r\n\r\n3. The header of [torch.full_like](https://pytorch.org/docs/stable/torch.html#torch.full_like) is placed incorrectly.\r\n\r\n4. Add example for [torch.split](https://pytorch.org/docs/stable/torch.html#torch.split).\r\n\r\n5. Add a note/warning in [torch.squeeze](https://pytorch.org/docs/stable/torch.html#torch.squeeze). When using batch_size=1, using torch.squeeze can result in an error. So add a warning telling the users to be careful when using torch.squeeze.\r\n\r\n6. The header of [torch.randint(low=0, high, size](https://pytorch.org/docs/stable/torch.html#torch.randint) is placed incorrectly.\r\n\r\n7. The header of [torch.randint_like](https://pytorch.org/docs/stable/torch.html#torch.randint_like) is placed incorrectly.\r\n   \r\n8. Add documentation for `_use_new_zipfile_serialization=False` argument of [torch.save](https://pytorch.org/docs/stable/torch.html#torch.save) + Add example using the above argument and pickle_protocol=4/5.\r\n\r\n9. "},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\nFunctions with multiline signature are not properly displayed in the online docs\r\n\r\nTake `randint` as an example, the function signature takes two lines in the docstring. \r\n\r\nhttps://github.com/pytorch/pytorch/blob/15da26f8aafe50fb34d343d872c7461546d7a317/torch/_torch_docs.py#L4667-L4670\r\n\r\nBut on the docs website https://pytorch.org/docs/master/generated/torch.randint.html, the second line is not recognized as part of the function signature. \r\n\r\n![image](https://user-images.githubusercontent.com/6421097/82273101-21ab7f00-9942-11ea-8e9b-717d3fa52684.png)\r\n\r\nA similar issue happens to the index page as well: the second column shows the first line of the signature, instead of the description. \r\n\r\n![image](https://user-images.githubusercontent.com/6421097/82273128-3d168a00-9942-11ea-9f95-64cb698f4cb4.png)\r\n\n\ncc @ezyang @zou3519"},{"labels":["documentation",null],"text":"The editors portion of the CITATION file located [here](https://github.com/pytorch/pytorch/blob/master/CITATION) is not formatted properly. Specifically this part:\r\n\r\n`editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\textquotesingle Alch\\'{e}-Buc and E. Fox and R. Garnett}`\r\n\r\nMy goal is to make the citation work with [citeas.org](https://citeas.org). But I had to ignore that part of the citation to make it work."},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\nThere are three minor problems in the `torch.norm` docs:\r\nThe default value of `torch.norm`'s `p` parameter is `fro`, but at the same time, the table of values for `ord` (Bug 1: mismatch to the parameter name) lists no vector norm for `ord=\"fro\"` (only for `ord=None`) (Bug 2). Finally, the quote signs are wrong (Bug 3).\r\n"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\nTo reproduce:\r\n- go to https://pytorch.org/docs/master/generated/torch.geqrf.html?highlight=geqrf#torch.geqrf\r\n- Click  \"Click here to view docs for latest stable release.\"\r\n- Watch as it leads to a 404 page.\r\n"},{"labels":["documentation",null],"text":"## 📚 Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\nI was going through the [https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html](new) documentation. I tried to run the google colab notebook and looks like there is this error. "},{"labels":[null,null,null,"documentation",null,null],"text":"##  Bug\r\n\r\nDear community,\r\n\r\nI am a dev from [KeOps project](https://github.com/getkeops/keops/) building optimized operations written in C++/Cuda that are compatible with pytorch (among others scientific languages). \r\n\r\nWe successfully build python modules compatible with pyTorch since the pyTorch v0.2... But the last v1.5 broke our modules with a runtime error complaining about [missing symbols](https://github.com/getkeops/keops/issues/59).\r\n\r\n\r\nTo simplify the analysis you may find below a minimal working example that builds a module through pybind11n the spirit of [pytorch doc](https://pytorch.org/tutorials/advanced/cpp_frontend.html#writing-a-basic-application). \r\n\r\nIt should output a file `test_module.cpython-38-x86_64-linux-gnu.so` that can be imported from python. This module works well when building with pytorch 1.4 but raised a runtime error when building with pytorch v1.5. \r\n\r\n\r\n## To Reproduce\r\n\r\nAssuming pyTorch and pybind11 installed (e.g. through conda).  There are 2 files\r\n\r\n`module_test.cpp` contains\r\n\r\n```cpp\r\n#include <torch/extension.h>\r\n#include <pybind11/pybind11.h>\r\n// Main function\r\nat::Tensor foo(int s) {\r\n     return torch::eye(s);\r\n}    \r\n\r\n// PyBind11 entry point \r\nPYBIND11_MODULE(test_module, m) {\r\nm.def(\"foo\", &foo, \"Entry point to test module\");\r\n}\r\n```   \r\nand `CMakeList.txt` contains\r\n```\r\nproject(test_module LANGUAGES CXX)\r\n\r\nfind_package(Torch REQUIRED)\r\nfind_package(pybind11  REQUIRED)\r\n\r\npybind11_add_module(test_module ${CMAKE_CURRENT_SOURCE_DIR}/test_module.cpp)\r\ntarget_link_libraries(test_module PUBLIC \"${TORCH_LIBRARIES}\")\r\n```\r\nit can be compile with\r\n\r\n ```bash\r\n$ mkdir build\r\n$ cd build\r\n$ cmake -DCMAKE_PREFIX_PATH=\"/home/bcharlier/.conda/envs/keops/lib/python3.8/site-packages/torch/\" .. && make\r\n```\r\nand run with (note that `torch` is imported first)\r\n```\r\n$ python -c \"import torch; print(torch.__version__); import test_module; print(test_module.foo(3))\"\r\n\r\n1.5.0\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: /home/bcharlier/src/test_module/build/test_module.cpython-38-x86_64-linux-gnu.so: undefined symbol: _Z16THPVariable_WrapN2at6TensorE\r\n```\r\nThe missing symbol is not exactly the same when [building a module with the keops library](https://github.com/getkeops/keops/issues/59)... But I guess this is irrelevant.\r\n\r\n## Expected behavior\r\n\r\nIt works fine when compiling with pytorch v1.4\r\n\r\n```bash\r\n$ cmake -DCMAKE_PREFIX_PATH=\"/home/bcharlier/.conda/envs/keops_torch14/lib/python3.8/site-packages/torch/\" .. && make\r\n$ python -c \"import torch; print(torch.__version__); import test_module; print(test_module.foo(3))\"\r\n1.4.0\r\ntensor([[1., 0., 0.],\r\n        [0., 1., 0.],\r\n        [0., 0., 1.]])\r\n```\r\n\r\n## Environment\r\n\r\nThe bug was reported by various users running on linux. Here is my config:\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.5.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Arch Linux\r\nGCC version: (Arch Linux 9.3.0-1) 9.3.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: Quadro T2000\r\nNvidia driver version: 440.82\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.4\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               10.2.89              hfd86e86_1  \r\n[conda] mkl                       2020.0                      166  \r\n[conda] mkl-service               2.3.0            py38he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py38ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py38h962f231_0  \r\n[conda] numpy                     1.18.1           py38h4f9e942_0  \r\n[conda] numpy-base                1.18.1           py38hde5b4d6_1  \r\n[conda] pytorch                   1.5.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch\r\n\r\n```\n\ncc @ezyang @gchanan @zou3519 @malfet @yf225 @glaringlee"},{"labels":[null,"documentation",null],"text":"Currently documentation builds emit many warnings. We should resolve them, then turn on `-WT --keep-going` to report warnings as errors so that new build problems are caught as they are created.\r\n\n\ncc @ezyang @zou3519"},{"labels":["documentation",null],"text":"**Description**\r\nI was visiting https://mobyproject.org/ and when I clicked github link it redirected to 404 Page Not found.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Go to https://mobyproject.org/\r\n2.Click on Github Link\r\n\r\n**Describe the results you received:**\r\nIt redirected to `https://github.com/moby/moby/blob/moby/README.md` \r\n\r\n**Describe the results you expected:**\r\nI think it should be `https://github.com/moby/moby/` or to README page `https://github.com/moby/moby/blob/master/README.md`\r\n"},{"labels":["documentation",null,null],"text":"**Description**\r\nThe Enqueue function of ringbuff used for RingLogger located in [ring.go](https://github.com/moby/moby/blob/master/daemon/logger/ring.go#L152-L174) dose not work as it documented that  oldest item will be dropped to make room for new item when buffer is full. Instead, the new item will be dropped. \r\n\r\ncode snippet as shown below\r\n```\r\n// Enqueue adds a message to the buffer queue\r\n// If the message is too big for the buffer it drops the oldest messages to make room\r\n// If there are no messages in the queue and the message is still too big, it adds the message anyway.\r\nfunc (r *messageRing) Enqueue(m *Message) error {\r\n\tmSize := int64(len(m.Line))\r\n\r\n\tr.mu.Lock()\r\n\tif r.closed {\r\n\t\tr.mu.Unlock()\r\n\t\treturn errClosed\r\n\t}\r\n\tif mSize+r.sizeBytes > r.maxBytes && len(r.queue) > 0 {\r\n\t\tr.wait.Signal()\r\n\t\tr.mu.Unlock()\r\n\t\treturn nil\r\n\t}\r\n\r\n\tr.queue = append(r.queue, m)\r\n\tr.sizeBytes += mSize\r\n\tr.wait.Signal()\r\n\tr.mu.Unlock()\r\n\treturn nil\r\n}\r\n```\r\n\r\nA similar misleading comment also appears in the [ring_test.go](https://github.com/moby/moby/blob/master/daemon/logger/ring_test.go#L51-L70) file. the comment said that the queue should have message for \"5\" to \"10\", however, it's \"0\" to \"4\" being tested.\r\n\r\n```\r\nfunc TestRingCap(t *testing.T) {\r\n\tr := newRing(5)\r\n\tfor i := 0; i < 10; i++ {\r\n\t\t// queue messages with \"0\" to \"10\"\r\n\t\t// the \"5\" to \"10\" messages should be dropped since we only allow 5 bytes in the buffer\r\n\t\tif err := r.Enqueue(&Message{Line: []byte(strconv.Itoa(i))}); err != nil {\r\n\t\t\tt.Fatal(err)\r\n\t\t}\r\n\t}\r\n\r\n\t// should have messages in the queue for \"5\" to \"10\"\r\n\tfor i := 0; i < 5; i++ {\r\n\t\tm, err := r.Dequeue()\r\n\t\tif err != nil {\r\n\t\t\tt.Fatal(err)\r\n\t\t}\r\n\t\tif string(m.Line) != strconv.Itoa(i) {\r\n\t\t\tt.Fatalf(\"got unexpected message for iter %d: %s\", i, string(m.Line))\r\n\t\t}\r\n\t}\r\n.......\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nEither fix the comment and document to indicated that when buffer is full, new message will be skipped. or fix the code logic to reflect the document"},{"labels":["documentation"],"text":"\r\n<img width=\"1104\" alt=\"screen shot 2018-02-08 at 3 01 14 pm\" src=\"https://user-images.githubusercontent.com/15134885/35995402-32453670-0ce1-11e8-848b-14d1cc3b1e38.png\">\r\n**Description**\r\n\r\nGodoc is down for client reference\r\n\r\n**Steps to reproduce the issue:**\r\n1. https://godoc.org/github.com/docker/docker/client\r\n2. view the empty chasm that is a 404 error\r\n\r\nOther packages are listed fine on Godoc currently and I was able to view the docs just the other day perfectly fine, so I am guessing that something changed in the commits merged into `master` today...\r\n\r\n"},{"labels":["documentation",null,null],"text":"As a knowledgeable-but-not-expert user, I recently got this message:\r\n\r\n> ERROR: for containername  Cannot create container for service containername: privileged mode is incompatible with user namespaces.  You must run the container in the host namespace when running privileged mode\r\n\r\nMy first reaction was [\"what the **** does this mean?\"](https://media.giphy.com/media/kF0ClnIcSBiDe/giphy.gif).\r\n\r\nThis ticket documents how an average user (senior-ish software engineer, uses docker at work and at home) solved this problem using this error message and Google.\r\n\r\nI take full responsibility for my incompetence, but there are many more of my kind, and the humane thing to do is to guide them towards a solution. **We need a better error message.** I hope that ticket makes a strong case for a better error message and better documentation to support it.\r\n\r\n## Context\r\n\r\nI updated docker recently. Then things something stopped working, and this error message was the only hint I got. I don't understand any of the concept it mentions, or which steps to take in order to fix my problem.\r\n\r\n## Fixing the issue\r\n\r\nWhen you are knee-deep in complex issues all day, you can easily forget how the average user sees things, so I wrote down my thought process down while I fixed the issue so you could understand my perspective as an end user.\r\n\r\n1. What the f- does this mean? This worked yesterday! (at this point, I'm already 3 levels deep on a completely different issue)\r\n2. What's privileged mode? What are user name spaces?\r\n3. What does \"running the container in the host namespace\" mean? *What am I supposed to do?*\r\n4. Maybe if I try to run it without sudo... nope.\r\n5. *Googles the error message*\r\n6. \"Introduction to User Namespaces in Docker Engine\" ([link](https://success.docker.com/article/Introduction_to_User_Namespaces_in_Docker_Engine))\r\n7. \"A namespace wraps a global system resource in an abstraction that makes it appear to the processes within...\". Nope, can't understand any of that.\r\n8. I'm halfway through that page, and I'm still no closer to a useful explanation. What is this? What does any of this even mean? Why doesn't it work anymore?\r\n9. *Googles the error message again*\r\n10. Okay, so according to [this article](https://luppeng.wordpress.com/2016/07/04/docker-user-namespaces/), it seems to be a way to prevent a docker root user to do nasty stuff on the host machine. Why didn't they tell me that in step 6?\r\n11. \"--userns-remap=default\" can allegedly fix this. Nope, can't just add it to `docker-compose up`.\r\n12. `userns_mode: \"host\"` apparently disables that. All I need to do is bump the docker-compose version to 2.1\r\n13. \"2.1\" is not a valid version, for some reason that is completely unexplained by this generic error message. I'm using the latest version of docker and docker-compose, which is why I'm getting the errors above in the first place\r\n14. (at this point, I should state that I am slowly losing my cool)\r\n14. Oh, it seems like this feature was added in a version that's not available on the docker PPA. Time to update docker-compose to an unreleased version to fix a problem in a released version. [This page](https://github.com/docker/compose/releases) helped me.\r\n15. \"docker-compose version 1.19.0-rc3\". Sweet, it's working!\r\n16. Aaaaand it works. I don't know why it works, but it works, and that's enough for one day.\r\n\r\nAll of this, including creating the ticket and finding the Office Space gif took me about 1.5 hours. I believe a more appropriate error message and documentation page could have reduced that to a few minutes. I apologize for the odd format of this ticket, but I hope it's still helpful."},{"labels":[null,"documentation",null],"text":"**Description**\r\n\r\nWhen using `ENV` in the form `ENV <key> <value>` both single and double quotes are being\r\nstripped from `<value>`.\r\n\r\nAccording to [the documentation](https://docs.docker.com/engine/reference/builder/#env):\r\n\r\n> The `ENV` instruction has two forms. The first form, `ENV <key> <value>`, will set a single variable to a value. The entire string after the first space will be treated as the `<value>` - including characters such as spaces and quotes.\r\n\r\nI'm seeing double and single quotes being stripped from entries such as these:\r\n\r\n```\r\nENV DQUOTE One \"two two\" three four\r\nENV SQUOTE One 'two two' three four\r\n```\r\n\r\nBackslashes are also being interpreted:\r\n\r\n```\r\nENV SBACKSLASH One two\\ two three four\r\nENV DBACKSLASH One two\\\\ two three four\r\n```\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1. `git clone git@github.com:gentlemanautomaton/docker-env-quote-test.git`\r\n2. `cd docker-env-quote-test`\r\n3. `docker build -t gentlemanautomaton/docker-env-quote-test:latest .`\r\n4. `docker inspect gentlemanautomaton/docker-env-quote-test:latest`\r\n\r\n**Describe the results you received:**\r\n\r\n```\r\n\"Env\": [\r\n\t\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\r\n\t\"DQUOTE=One two two three four\",\r\n\t\"SQUOTE=One two two three four\",\r\n\t\"SBACKSLASH=One two two three four\",\r\n\t\"DBACKSLASH=One two\\\\ two three four\",\r\n\t\"BACKTICK=One `two two` three four\"\r\n],\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\n```\r\n\"Env\": [\r\n\t\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\r\n\t\"DQUOTE=One \\\"two two\\\" three four\",\r\n\t\"SQUOTE=One 'two two' three four\",\r\n\t\"SBACKSLASH=One two\\\\ two three four\",\r\n\t\"DBACKSLASH=One two\\\\\\\\ two three four\",\r\n\t\"BACKTICK=One `two two` three four\"\r\n],\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nIn the hope that it's useful, I created a test `Dockerfile` for reproduction in this repo:\r\n\r\n* https://github.com/gentlemanautomaton/docker-env-quote-test\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:       18.01.0-ce\r\n API version:   1.35\r\n Go version:    go1.9.2\r\n Git commit:    03596f5\r\n Built: Wed Jan 10 20:13:21 2018\r\n OS/Arch:       linux/amd64\r\n Experimental:  false\r\n Orchestrator:  swarm\r\n\r\nServer:\r\n Engine:\r\n  Version:      18.01.0-ce\r\n  API version:  1.35 (minimum version 1.12)\r\n  Go version:   go1.9.2\r\n  Git commit:   03596f5\r\n  Built:        Wed Jan 10 20:11:47 2018\r\n  OS/Arch:      linux/amd64\r\n  Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 3\r\n Running: 0\r\n Paused: 0\r\n Stopped: 3\r\nImages: 361\r\nServer Version: 18.01.0-ce\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 89623f28b87a6004d4b785663257362d1658a729\r\nrunc version: b2567b37d7b75eb4cf325b77297b140ea686ce8f\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.13.0-25-generic\r\nOperating System: Ubuntu 17.10\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 16\r\nTotal Memory: 31.47GiB\r\nName: REDACTED\r\nID: SVWR:PGRT:4NZT:KVIT:DVBE:EKDI:GG2T:46U2:5PEW:X4UL:LGQZ:SEL4\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nThe test was performed with `docker-ce` running on physical hardware."},{"labels":["documentation",null],"text":"Just a heads up that the examples over here - https://docs.docker.com/engine/reference/commandline/inspect/#extended-description are incorrect if an overlay network is used.  The Go template extracts 2 IPs in this case (one from the ingress layer and the second the one used by the container) and concatenates them.  Here is an example:\r\n\r\n```\r\n$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container1\r\n10.255.0.2110.0.1.9\r\n```\r\n\r\nThe above is on Docker 17.12\r\n\r\n```\r\n$ docker version\r\nClient:\r\n Version:       17.12.0-ce-rc2\r\n API version:   1.35\r\n Go version:    go1.9.2\r\n Git commit:    f9cde63\r\n Built: Tue Dec 12 06:38:26 2017\r\n OS/Arch:       linux/amd64\r\n\r\nServer:\r\n Engine:\r\n  Version:      17.12.0-ce-rc2\r\n  API version:  1.35 (minimum version 1.12)\r\n  Go version:   go1.9.2\r\n  Git commit:   f9cde63\r\n  Built:        Tue Dec 12 06:45:30 2017\r\n  OS/Arch:      linux/amd64\r\n  Experimental: false\r\n```"},{"labels":[null,"documentation"],"text":"**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\nThe link for `docker stack config` listed under \"Related commands\" at the end of page under URL -\r\n https://github.com/docker/cli/blob/master/docs/reference/commandline/deploy.md returns 404 page error.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Go to https://github.com/docker/cli/blob/master/docs/reference/commandline/deploy.md\r\n2. Scroll to the end of the page section \"Related Commands\"\r\n3. Click on the first link - https://github.com/docker/cli/blob/master/docs/reference/commandline/stack_config.md\r\n4. It shows 404 page not found error\r\n\r\n**Describe the results you received:**\r\n\r\nThe page reports 404 error \r\n\r\n**Describe the results you expected:**\r\n\r\nThe page should display reference to `docker stack config` command.\r\n"},{"labels":["documentation",null,null],"text":"**Description**\r\n\r\nLooks like documentation for docker network is outdated.\r\nTrying to learn docker network using the examples provided [here](https://docs.docker.com/engine/userguide/networking/work-with-networks/#link-containers-without-using-user-defined-networks).\r\n\r\n**Steps to reproduce the issue:**\r\n1.create docker network \r\n    `$ docker network create -o \"com.docker.network.bridge.host_binding_ipv4\"=\"172.23.0.1\" my-network`\r\n    bf64e40499c8c1644c8ed3b743f25dc72866ae81294cec92d86e216fbb9a0ab5\r\n\r\n2. inspect the network.\r\n\r\n    `$ docker network inspect my-network`\r\n\r\n```\r\n[\r\n    {\r\n        \"Name\": \"my-network\",\r\n        \"Id\": \"bf64e40499c8c1644c8ed3b743f25dc72866ae81294cec92d86e216fbb9a0ab5\",\r\n        \"Created\": \"2017-09-16T23:48:31.063263608-04:00\",\r\n        \"Scope\": \"local\",\r\n        \"Driver\": \"bridge\",\r\n        \"EnableIPv6\": false,\r\n        \"IPAM\": {\r\n            \"Driver\": \"default\",\r\n            \"Options\": {},\r\n            \"Config\": [\r\n                {\r\n                    \"Subnet\": \"172.18.0.0/16\",\r\n                    \"Gateway\": \"172.18.0.1\"\r\n                }\r\n            ]\r\n        },\r\n        \"Internal\": false,\r\n        \"Attachable\": false,\r\n        \"Ingress\": false,\r\n        \"ConfigFrom\": {\r\n            \"Network\": \"\"\r\n        },\r\n        \"ConfigOnly\": false,\r\n        \"Containers\": {},\r\n        \"Options\": {\r\n            \"com.docker.network.bridge.host_binding_ipv4\": \"172.23.0.1\"\r\n        },\r\n        \"Labels\": {}\r\n    }\r\n]\r\n```\r\n\r\n3. run redis in this network\r\n\r\n    `$ docker run -d -P --name redis --network my-network redis`\r\n\r\n```\r\nUnable to find image 'redis:latest' locally\r\nlatest: Pulling from library/redis\r\n065132d9f705: Pull complete \r\nfc32c7d9b0f4: Pull complete \r\nad60cc6fa431: Pull complete \r\nb21c99d8cf03: Pull complete \r\n357908014789: Pull complete \r\ne27e1cb0ca43: Pull complete \r\nDigest: sha256:fe77356e6e8d5c5200b9800e50ae71147efdc446a3cc4f601c607fbfd218015e\r\nStatus: Downloaded newer image for redis:latest\r\nf103a6542e50ecd5bb257b9c68214537f1392069d94ddfbf19416af3e54cc640\r\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint redis (1339918305c70f9e6416a5d499b5f6b8691450c9a3d310c8f7c3495c34a931ee): Error starting userland proxy: listen tcp 172.23.0.1:32777: bind: cannot assign requested address.\r\n```\r\n\r\n\r\n\r\n**Describe the results you received:**\r\n\r\ndocker should be able to run redis successfully in provided network.\r\n\r\n**Describe the results you expected:**\r\n\r\n```\r\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint redis (1339918305c70f9e6416a5d499b5f6b8691450c9a3d310c8f7c3495c34a931ee): Error starting userland proxy: listen tcp 172.23.0.1:32777: bind: cannot assign requested address.\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.09.0-ce-rc1\r\n API version:  1.31\r\n Go version:   go1.8.3\r\n Git commit:   ae21824\r\n Built:        Wed Sep  6 22:31:42 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.09.0-ce-rc1\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   ae21824\r\n Built:        Wed Sep  6 22:33:07 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 4\r\n Running: 0\r\n Paused: 0\r\n Stopped: 4\r\nImages: 21\r\nServer Version: 17.09.0-ce-rc1\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0\r\nrunc version: 3f2f8b84a77f73d38244dd690525642a72156c64\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.12.9-300.fc26.x86_64\r\nOperating System: Fedora 26 (Twenty Six)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 7.794GiB\r\nName: localhost.localdomain\r\nID: R3ET:HF5C:ILYK:S46A:5OYO:E67Z:GACB:AFJE:5V7G:EY3V:GBMX:IPTB\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nPhysical - Fedora 26"},{"labels":["documentation",null],"text":"**Description**\r\n\r\nThe docker compose documentation shows an example of Volume long syntax as below\r\n\r\n```yaml\r\nversion: \"3.2\"\r\nservices:\r\n  web:\r\n    image: nginx:alpine\r\n    ports:\r\n      - \"80:80\"\r\n\r\nnetworks:\r\n  webnet:\r\n\r\nvolumes:\r\n  - type: volume\r\n    source: mydata\r\n    target: /data\r\n    volume:\r\n      nocopy: true\r\n  - type: bind\r\n    source: ./static\r\n    target: /opt/app/static\r\n```\r\n> Note: The long syntax is new in v3.2\r\n\r\nI believe this format is only valid for volumes property of the service. Please correct me that is not the case \r\n\r\n**Steps to reproduce the issue:**\r\n$ docker-compose config\r\nERROR: In file './docker-compose.yml', volume must be a mapping, not an array.\r\n\r\n**Describe the results you received:**\r\nERROR: In file './docker-compose.yml', volume must be a mapping, not an array.\r\n\r\n\r\n**Describe the results you expected:**\r\nDocumentation examples should work\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:23:31 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:19:04 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:23:31 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:19:04 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\nvagrant@vagrant:~/so/volumes$ docker info\r\nContainers: 44\r\n Running: 5\r\n Paused: 0\r\n Stopped: 39\r\nImages: 92\r\nServer Version: 17.06.0-ce\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 355\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: active\r\n NodeID: smh2288801wg3b15f0twxaew1\r\n Is Manager: true\r\n ClusterID: 3pqs1zxfxe76sb3r462engtfa\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n  Force Rotate: 0\r\n Root Rotation In Progress: false\r\n Node Address: 192.168.33.100\r\n Manager Addresses:\r\n  192.168.33.100:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: cfb82a876ecc11b5ca0977d1733adbe58599088a\r\nrunc version: 2d41c047c83e09a6d61d464906feb2a2f3c52aa4\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.4.0-66-generic\r\nOperating System: Ubuntu 16.04.2 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 992.3MiB\r\nName: vagrant\r\nID: H5PY:YSAS:TWDZ:HRHQ:7WPR:GAOZ:5PNY:YZTD:UB2K:QDDD:DVY5:OJF3\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: tarunlalwani\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\n```\r\n$ docker-compose --version\r\ndocker-compose version 1.15.0, build e12f3b9\r\n```"},{"labels":[null,"documentation",null,null],"text":"Hi !\r\n\r\nI'm writing a tool (in Python) to get the logs from any containers through the Docker Engine's HTTP API.\r\n\r\nAfter reading [the doc](https://docs.docker.com/engine/api/v1.30/), and trying to implement something, I founded that the protocol used seems to be different from the one for Attach, it seems to be :\r\n\r\n```{\\xFF}\\r\\n{\\xFF}{datas}```\r\n\r\nWhere the first hex number is the size of the message, and the second one seems to be (empirical deduction) the number of bytes that are supposed to be ignored before the start of the actual payload (so the log).\r\nAnd it seems to work, for some containers at least ...\r\n\r\n**EDIT** : The second seems to be the number of bytes that are supposed to be ignored IF the container has ```tty: false```, otherwise, it seems to be the start of the log **END of the edit**\r\n\r\nWith one strange deduction : if ```\\x07``` is found, then it overrides the second {\\xFF} size, and the message starts after this value...\r\n\r\nAm I missing a part of the documentation, or am I not using the HTTP API in a wrong way ?\r\n\r\nThanks for your time,\r\n\r\n**NB** : The protocol above seems to be specific for the logs endpoint, it's not working for the events' endpoint.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.05.0-ce\r\n API version:  1.29\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.05.0-ce\r\n API version:  1.29 (minimum version 1.12)\r\n Go version:   go1.7.5\r\n Git commit:   89658be\r\n Built:        Thu May  4 22:10:54 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n"},{"labels":[null,"documentation",null],"text":"I was looking at [`dispatchers.go`](https://github.com/moby/moby/blob/f577caff19d486d8d01443507d891cb1b0891cdc/builder/dockerfile/dispatchers.go#L689-L693) today when I realized that you can actually specify `tcp` or `udp` when you use the `EXPOSE` instruction.\r\n\r\nThis isn't mentioned at all in the [Dockerfile reference for `EXPOSE`](https://docs.docker.com/engine/reference/builder/#expose). Is this intentional?"},{"labels":["documentation",null,null],"text":"**Description**\r\n\r\nThe `docker-run` man page I'm looking at describes the `--publish-all` option as follows:\r\n\r\n> Publish all exposed ports to random ports on the host interfaces.\r\n> [...]\r\n> When using -P, Docker will bind any exposed port to a random port on the host within an ephemeral port range defined by `/proc/sys/net/ipv4/ip_local_port_range`.\r\n\r\nHowever, the observed behaviour is, instead, that ports are allocated sequentially from the bottom of `ip_local_port_range`, with the sequence being reset whenever `dockerd` is restarted.\r\n\r\nMost definitions of \"random\" I can find mention \"unpredictability\", \"governed by or depending on chance\", or \"in no particular order\" as being defining characteristics; \"start from X and work your way up one by one\" doesn't meet any definition of \"random\" I can find.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1. Restart `dockerd`, just to get a clean slate.\r\n2. Run `docker run --rm -it --expose 4242 -P alpine sh`\r\n3. Run `docker ps` in another shell, and note the host port number assigned to the container.  It will almost certainly be the first value in `/proc/sys/net/ipv4/ip_local_port_range`.\r\n4. Exit out of that container.\r\n5. Repeat steps 2-4 another dozen times or so.  Note each time that the host port number is almost certainly the next value above the previous one (with a small chance that another process on the machine might have temporarily snaffled a port in the middle).  Continue to repeat until the experiment has sufficient statistical power to convince you this isn't just the cruel hand of random chance toying with your sanity.\r\n6. Restart `dockerd` again.\r\n7. Run `docker run --rm -it --expose 4242 -P alpine sh`\r\n8. Run `docker ps` is another shell, note that the host port has probably gone back to the first value in `/proc/sys/net/ipv4/ip_local_port_range` (or is pretty close to it).\r\n\r\n**Describe the results you received:**\r\n\r\nIncluded above.\r\n\r\n**Describe the results you expected:**\r\n\r\nThe host ports allocated should not be allocated sequentially when multiple containers with `-P` are run.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThe chances of getting the first port in `ip_local_port_range` on my system are about 1 in 28,000.  The chances of getting the next sequential port each time over 10 runs, if my arithmetic is correct, is 1 in 15,992,496,204,377,989,289,565,234,114,532,246,053,888,000.  I don't think I'm that lucky.\r\n\r\nOn the up side, at least dockerd doesn't reuse recently released ports, like it does with container IP addresses, and it doesn't blindly re-use allocated ports after restart.  So the problem isn't as horrible as it *could* be.  But it's still not great.\r\n\r\nIdeally, the relevant code would be fixed to allocate ports randomly, in line with the documentation, but if not, the documentation should be updated to accurately describe the port allocation process.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      17.06.0-ce\r\n API version:  1.30\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:17:22 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.06.0-ce\r\n API version:  1.30 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   02c1d87\r\n Built:        Fri Jun 23 21:16:12 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 2\r\n Running: 2\r\n Paused: 0\r\n Stopped: 0\r\nImages: 392\r\nServer Version: 17.06.0-ce\r\nStorage Driver: overlay\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins: \r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: cfb82a876ecc11b5ca0977d1733adbe58599088a\r\nrunc version: 2d41c047c83e09a6d61d464906feb2a2f3c52aa4\r\ninit version: 949e6fa\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 4.9.0-3-amd64\r\nOperating System: Debian GNU/Linux 9 (stretch)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 62.36GiB\r\nName: lolzno\r\nID: XCA5:PLPV:KBPE:SY63:O254:RZ3S:4UQS:C73M:KYHC:ASCI:25HL:GOQP\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nUsername: lolzno\r\nRegistry: https://index.docker.io/v1/\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: true\r\n\r\nWARNING: No swap limit support\r\n```\r\n"},{"labels":["documentation"],"text":"There is a giant link at the top of the README:\r\n\r\n> Docker users, see [Moby and Docker](https://mobyproject.org/#moby-and-docker) to clarify the relationship between the projects\r\n\r\nThe link has a target, `#moby-and-docker`, but that seems to be broken. The landing page doesn't *really* clarify what the differences are, perhaps Moby is some base library to Docker? (But then what is `libcontainerd`? More abstraction?)"},{"labels":[null,"documentation"],"text":"Docker API documentation needs to be updated.\r\n\r\nThe `/plugins` endpoint returns the following response: https://github.com/moby/moby/blob/0ac25dfc751fa4304ab45afd5cd8705c2235d101/api/types/plugin_responses.go#L1\r\n\r\nBased on https://github.com/moby/moby/blob/0ac25dfc751fa4304ab45afd5cd8705c2235d101/api/types/plugin.go#L8-L31\r\n\r\nThe plugin object is quite different from the one documented in the 200 response sample at https://docs.docker.com/engine/api/v1.30/#operation/PluginList\r\n"},{"labels":["documentation",null,null],"text":"**Description**\r\n\r\nThe link of the \"docker run\" clean up command is not correct. The link is \"https://docs.docker.com/engine/reference/run/#clean-up---rm\" and should be \"https://docs.docker.com/engine/reference/run/#clean-up--rm\".\r\n\r\n**Steps to reproduce the issue:**\r\n1. Go to https://docs.docker.com/engine/reference/run/\r\n2. Click the link\r\n3. It doesn't work\r\n4. Check https://docs.docker.com/engine/reference/run/#clean-up--rm\r\n\r\n**Describe the results you received:**\r\nThe link doesn't work\r\n\r\n**Describe the results you expected:**\r\nThe link should work\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nI notice that if I change the markdown code to have only one \"-\", the link of the markdown is broken.\r\n\r\n**Output of `docker version`:**\r\nIt is the online documentation of 17.06\r\n\r\n**Output of `docker info`:**\r\nIt is the online documentation of 17.06\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nNone"},{"labels":[null,"documentation",null,null],"text":"At the moment, the Dockerfile reference states the following:\r\n\r\n> A Dockerfile must start with a `FROM` instruction.\r\n\r\nHowever, this has since been changed because:\r\n\r\n> FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM.\r\n\r\nThe `docs/reference/builder.md` file should be updated to reflect this change."},{"labels":["documentation"],"text":"From the release notes of v17.06.0-ce-rc2\r\n\r\n> Placement now also take platform in account moby/moby#33144\r\n\r\nWhile the PR that introduced the feature also updated the API docs, it didn't update the [user docs for service create](https://github.com/moby/moby/blob/master/docs/reference/commandline/service_create.md#specify-service-constraints---constraint), that also mention the placement constraints.\r\n\r\nI also noticed that the documentation here https://github.com/docker/swarmkit/#features talks about placement constrains, and mentions `node.ip` as another supported constraint. I'm not sure this made it to 17.06 yet, but if it did, we also need to update the user docs page with that.\r\n\r\n/ping @nishanttotla since he was the one who introduced #33144\r\nand @mstanleyjones for visibility\r\n\r\n---\r\n\r\nIf you're triaging this issue and are thinking if this issue should be filed against docker/docker.github.io instead, keep in mind that docker/docker.github.io will fetches https://github.com/moby/moby/blob/master/docs/reference/ to build the user-facing docs, so this issue needs to be addressed on this repo."},{"labels":[null,"documentation"],"text":"As seen there: https://github.com/moby/moby/blob/df4ca50805baa1d1488d811e82125f607c8daa09/integration-cli/docker_api_swarm_service_test.go#L63,\r\nthere is an `insertDefaults` query parameter not stated in the docs."},{"labels":[null,"documentation"],"text":"### Documentation Problem Description\r\n\r\nPer @gdevillele , the API reference topic [Delete unused volumes](https://docs.docker.com/engine/api/v1.28/#operation/VolumePrune) mentions a `filters` query parameter, but doesn’t mention any supported argument (because there isn’t any). This doesn’t make sense, we should not mention this filters parameter.\r\n\r\n### Where Found\r\n\r\n[Delete unused volumes](https://docs.docker.com/engine/api/v1.28/#operation/VolumePrune) in the API reference docs. \r\n\r\n### About the docs\r\n\r\nCurrently, the  API docs source lives here in the `moby/moby` codebase, not in the docs repository https://github.com/docker/docker.github.io. \r\n\r\n@thaJeztah need guidance on how we will deal with the relationship between `moby/moby` and the docs repository going forward. I don't have permissions to assign issues, but I suggest assigning to @mstanleyjones ?\r\n\r\n### Reviewers\r\n\r\n@gdevillele @johndmulhausen @thaJeztah \r\n\r\n"},{"labels":[null,"documentation",null,null],"text":"Hello,\r\n\r\ni have an error using this page (click into \"Labels\" menù):\r\n\r\nhttps://docs.docker.com/engine/api/v1.28/#operation/ServiceCreate\r\n\r\nJS Console stacktrace:\r\n\r\nError: Can't load component schema at /paths/~1services~1create/post/parameters/0/schema/properties/Labels\r\n    at d (redoc.1.11.0.min.js:43)\r\n    at e.init (redoc.1.11.0.min.js:17)\r\n    at e.t.preinit (redoc.1.11.0.min.js:9)\r\n    at e.preinit (redoc.1.11.0.min.js:9)\r\n    at e.ngOnInit (redoc.1.11.0.min.js:17)\r\n    at t.ngDoCheck (redoc.1.11.0.min.js:15)\r\n    at e.detectChangesInternal (redoc.1.11.0.min.js:15)\r\n    at e.t.detectChanges (redoc.1.11.0.min.js:7)\r\n    at t.detectChangesInNestedViews (redoc.1.11.0.min.js:7)\r\n    at e.detectChangesInternal (redoc.1.11.0.min.js:16)\r\n    at e.t.detectChanges (redoc.1.11.0.min.js:7)\r\n    at t.detectChangesInNestedViews (redoc.1.11.0.min.js:7)\r\n    at e.detectChangesInternal (redoc.1.11.0.min.js:16)\r\n    at e.t.detectChanges (redoc.1.11.0.min.js:7)\r\n    at t.detectChangesInNestedViews (redoc.1.11.0.min.js:7)"},{"labels":["documentation",null],"text":"https://github.com/docker/docker/blob/17.03.x/docs/reference/commandline/service_create.md\r\n![image](https://cloud.githubusercontent.com/assets/1900106/24601239/6c42dcac-1858-11e7-8cfe-79ca04a46b90.png)\r\nOK with no issues\r\n\r\nhttps://docs.docker.com/engine/reference/commandline/service_create/#specify-service-constraints---constraint\r\n![image](https://cloud.githubusercontent.com/assets/1900106/24601250/79eaf074-1858-11e7-929f-b6a20e74b064.png)\r\nTable is messed up\r\nAnd `<tdnode.role</td>` is above the table\r\n\r\nThis is the table in HTML:\r\n```html\r\n<table>\r\n  <tr>\r\n    <th>node attribute</th>\r\n    <th>matches</th>\r\n    <th>example</th>\r\n  </tr>\r\n  <tr>\r\n    <td><tt>node.id</tt></td>\r\n    <td>Node ID</td>\r\n    <td><tt>node.id == 2ivku8v2gvtg4</tt></td>\r\n  </tr>\r\n  <tr>\r\n    <td><tt>node.hostname</tt></td>\r\n    <td>Node hostname</td>\r\n    <td><tt>node.hostname != node-2</tt></td>\r\n  </tr>\r\n  <tr>\r\n    &lt;td<tt>node.role</tt>&lt;/td&gt;\r\n    <td><tt>node role: manager</tt></td>\r\n    <td><tt>node.role == manager</tt></td>\r\n  </tr>\r\n  <tr>\r\n    <td><tt>node.labels</tt></td>\r\n    <td>user defined node labels</td>\r\n    <td><tt>node.labels.security == high</tt></td>\r\n  </tr>\r\n  <tr>\r\n    <td><tt>engine.labels</tt></td>\r\n    <td>Docker Engine's labels</td>\r\n    <td><tt>engine.labels.operatingsystem == ubuntu 14.04</tt></td>\r\n  </tr>\r\n</table>\r\n```"},{"labels":["documentation",null,null],"text":"Wrong containerd version in release notes for release v17.03.1-ce:\r\n```\r\nUpdate containerd to 595e75c212d19a81d2b808a518fe1afc1391dad5 #31662\r\n```\r\n(https://github.com/docker/docker/pull/31662/commits/74c52a7bab45f63defdbe4c33ec5c77416f2ecf7)\r\n\r\nBut here version changed to 4ab9917febca54791c5f071a9d1f404867857fcc:\r\nhttps://github.com/docker/docker/commit/00132cc4426d7914822b98f952d32a45b015485e"},{"labels":["documentation",null,null],"text":"platform: docker toobox for windows 17.0.03\r\n\r\nWhen running a container with a mount from a host drive, the docker host folder contents overwrites the container's folder contents entirely. There is no overlay of data as detailed in the docs.\r\n\r\nmy ubuntu container command: \r\n\r\n> docker run -d -P --name MYCONTAINER -v /c/Users/drupal8_www:/var/www/html MY:IMAGE\r\n\r\nby stopping and starting this i can clearly see and repeat the effect describe above.\r\n\r\nthanks"},{"labels":["documentation"],"text":"The `CopyToContainer` method's documentation simply states the following:\r\n> CopyToContainer copies content into the container filesystem.\r\n\r\nHowever, the [API endpoint's documentation](https://docs.docker.com/engine/api/v1.24/index.html#extract-an-archive-of-files-or-folders-to-a-directory-in-a-container) states the endpoint only accepts tarballs. The code in the `cli` package using `CopyToContainer` clearly shows files being archived to a tarball as well.\r\n\r\nI think `CopyToContainer`'s documentation should reflect that it expects the `content` argument to be a tarball.\r\n"},{"labels":["documentation",null],"text":"The TOC on the right side of https://docs.docker.com/engine/deprecated/ has some entries at the wrong indentation. All the entries should have the same indentation.\r\n\r\n<img width=\"233\" alt=\"image\" src=\"https://cloud.githubusercontent.com/assets/11357370/22755255/64201d9a-ee10-11e6-9bfd-4780cff3a403.png\">\r\n"},{"labels":["documentation",null,null],"text":"Hi, All,\r\n\r\nI found in swagger.yml and api doc, there are some missing status codes about network:\r\n\r\n```\r\nGET /networks/(id or name) misses status code of 500;\r\nDELETE /networks/(id or name) missed status code of 403;\r\n```\r\n\r\nHere is my environment, we can see that there are two network has a prefix network id of `a`:\r\n```\r\nroot@ubuntu:~# docker network ls\r\nNETWORK ID          NAME                DRIVER              SCOPE\r\nb80b14598d1f        allen               bridge              local\r\nabe5fa290b6d        allen2              bridge              local\r\ndb8dfef4e6de        allen3              bridge              local\r\na89bfaea7f72        allen4              bridge              local\r\nfd6a89aad8ff        bridge              bridge              local\r\n85ded5476b05        docker_gwbridge     bridge              local\r\nd909a9f88b4c        host                host                local\r\nz1sybtvc99hu        ingress             overlay             swarm\r\n615a39ff6c47        none                null                local\r\n```\r\nThen we send an http request like the following pic, we found the status code is 500 which is not versioned in the swagger.yml nor api docs:\r\n<img width=\"782\" alt=\"2017-02-03 1 25 57\" src=\"https://cloud.githubusercontent.com/assets/9465626/22580645/06843094-ea15-11e6-845d-92a959624c63.png\">\r\n\r\n\r\nSecond, we send a DELETE request for network `host`, then daemon will response a status code of 403 which is also not versioned in swagger.yml or api docs. Here is test pic:\r\n![wechatimg4](https://cloud.githubusercontent.com/assets/9465626/22580703/a0a774d8-ea15-11e6-8ee1-32d82da5fbd7.jpeg)\r\n\r\n**Steps to reproduce the issue:**\r\n1.\r\n2.\r\n3.\r\n\r\n**Describe the results you received:**\r\nThere is no status code 500 for api endpoint `GET /networks/(id or name), nor 404 for `DELETE /networks/(id or name)`\r\n\r\n**Describe the results you expected:**\r\nrelated status codes are versioned in doc\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nroot@ubuntu:~# docker version\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:50:17 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Tue Jan 17 09:50:17 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nroot@ubuntu:~# docker info\r\nContainers: 8\r\n Running: 0\r\n Paused: 0\r\n Stopped: 8\r\nImages: 37\r\nServer Version: 1.13.0\r\nStorage Driver: aufs\r\n Root Dir: /var/lib/docker/aufs\r\n Backing Filesystem: extfs\r\n Dirs: 202\r\n Dirperm1 Supported: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\nSwarm: active\r\n NodeID: gkdoxio9hz1b325t544ftpry4\r\n Is Manager: true\r\n ClusterID: 3bidcebapq2115pts6cik50ah\r\n Managers: 1\r\n Nodes: 1\r\n Orchestration:\r\n  Task History Retention Limit: 5\r\n Raft:\r\n  Snapshot Interval: 10000\r\n  Number of Old Snapshots to Retain: 0\r\n  Heartbeat Tick: 1\r\n  Election Tick: 3\r\n Dispatcher:\r\n  Heartbeat Period: 5 seconds\r\n CA Configuration:\r\n  Expiry Duration: 3 months\r\n Node Address: 192.168.59.103\r\n Manager Addresses:\r\n  192.168.59.103:2377\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 03e5862ec0d8d3b3f750e19fca3ee367e13c090e\r\nrunc version: 2f7393a47307a16f8cee44a37b262e8b81021e3e\r\ninit version: 949e6fa\r\nSecurity Options:\r\n apparmor\r\nKernel Version: 3.19.0-25-generic\r\nOperating System: Ubuntu 14.04.3 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 1\r\nTotal Memory: 1.954 GiB\r\nName: ubuntu\r\nID: Q2ZC:GWDN:27OH:GRMH:G6QU:W7QP:4TIX:Q5F6:YEVK:45XP:EXHC:HOB5\r\nDocker Root Dir: /var/lib/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 32\r\n Goroutines: 132\r\n System Time: 2017-01-29T14:51:41.049720516+08:00\r\n EventsListeners: 0\r\nRegistry: https://index.docker.io/v1/\r\nWARNING: No swap limit support\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nRegistry Mirrors:\r\n https://a.b.c/\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n"},{"labels":["documentation"],"text":"The [configuration file documentation](https://docs.docker.com/engine/reference/commandline/cli/#/configuration-files) states\r\n\r\n> The property psFormat specifies the default format for docker ps output. When the --format flag is not provided with the docker ps command, Docker’s client uses this property. If this property is not set, the client falls back to the default table format. For a list of supported formatting directives, see the [Formatting section in the docker ps documentation](https://docs.docker.com/engine/reference/commandline/ps/)\r\n\r\nThere's no such section in the docker ps documentation."},{"labels":[null,"documentation",null],"text":"The docs at https://docs.docker.com/engine/reference/commandline/version/ say to get the template fields for `docker version` output, use `docker version --format '{{json .}}'` which gives me:\r\n\r\n```\r\n{\"Client\":{\"Version\":\"1.13.0\",\"ApiVersion\":\"1.25\",\"GitCommit\":\"49bf474\",\"GoVersion\":\"go1.7.3\",\"Os\":\"darwin\",\"Arch\":\"amd64\",\"BuildTime\":\"Wed Jan 18 16:20:26 2017\"},\"Server\":{\"Version\":\"1.13.0\",\"ApiVersion\":\"1.25\",\"MinAPIVersion\":\"1.12\",\"GitCommit\":\"49bf474\",\"GoVersion\":\"go1.7.3\",\"Os\":\"linux\",\"Arch\":\"amd64\",\"KernelVersion\":\"4.9.5-moby\",\"Experimental\":true,\"BuildTime\":\"Wed Jan 18 16:20:26 2017\"}}\r\n```\r\n\r\nSo I try\r\n```\r\ndocker version --format '{{.Server.ApiVersion}}'\r\n\r\ntemplate: :1:9: executing \"\" at <.Server.ApiVersion>: can't evaluate field ApiVersion in type *types.Version\r\n```\r\n\r\nEventually I guess and try\r\n```\r\ndocker version --format '{{.Server.APIVersion}}'\r\n1.25\r\n```\r\n\r\nSo the output on the JSON list has a different casing, making it hard to work out what the Go names are. I blame Go's weird JSON handling, but I wonder if it can be fixed? Or maybe the docs just could add a note?\r\n\r\nActually the JSON has the same name\r\n```\r\ndocker version --format '{{json .Server.ApiVersion}}'\r\n\r\ntemplate: :1:14: executing \"\" at <.Server.ApiVersion>: can't evaluate field ApiVersion in type *types.Version\r\ndocker version --format '{{json .Server.APIVersion}}'\r\n\"1.25\"\r\n```\r\n\r\n```\r\nClient:\r\n Version:      1.13.0\r\n API version:  1.25\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      darwin/amd64\r\n\r\nServer:\r\n Version:      1.13.0\r\n API version:  1.25 (minimum version 1.12)\r\n Go version:   go1.7.3\r\n Git commit:   49bf474\r\n Built:        Wed Jan 18 16:20:26 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: true\r\n```\r\n\r\ncc @thaJeztah "},{"labels":["documentation"],"text":"The page https://docs.docker.com/engine/admin/logging/log_tags/ provides link `logging context` to https://github.com/docker/docker/blob/master/daemon/logger/context.go that is not available any more."},{"labels":["documentation"],"text":"'docker/' is no longer automatically added as a prefix (after https://github.com/docker/docker/pull/22384), but https://docs.docker.com/engine/admin/logging/log_tags/ still prepends 'docker/'. That log-opt tag would no longer result in the example log line. It would be missing the 'docker/' prefix."},{"labels":["documentation"],"text":"Seems like the Dockerfile reference web-page does not exist anymore or if the url has been changed google does not find it for some reason.\r\n\r\nThe page is still on github https://github.com/docker/docker/blob/master/docs/reference/builder.md"},{"labels":["documentation",null],"text":"I'm trying to run the example in this documentation: https://godoc.org/github.com/docker/docker/client#hdr-Usage\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"fmt\"\r\n\r\n\t\"github.com/docker/docker/api/types\"\r\n\t\"github.com/docker/docker/client\"\r\n)\r\n\r\nfunc main() {\r\n\tcli, err := client.NewEnvClient()\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tcontainers, err := cli.ContainerList(context.Background(), types.ContainerListOptions{})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tfor _, container := range containers {\r\n\t\tfmt.Printf(\"%s %s\\n\", container.ID[:10], container.Image)\r\n\t}\r\n}\r\n```\r\n\r\nBut for obvious reasons, when running the example against the latest release of Docker, 1.12.5, that doesn't work.\r\nThe error I get is: \r\n```\r\npanic: Error response from daemon: client is newer than server (client API version: 1.26, server API version: 1.24)\r\n```\r\nI understand that semantic versioning has no significance for this project, because the API breaks in point releases, but it would be cool if at least examples would work."},{"labels":[null,"documentation"],"text":"The API documentation (and swagger.yaml) says `POST /images/create` can only return a 200 or a 500. Snippet from swagger.yaml:\r\n```yaml\r\n  /images/create:\r\n    post:\r\n      ...\r\n      responses:\r\n        200:\r\n          description: \"no error\"\r\n        500:\r\n          description: \"server error\"\r\n          schema:\r\n            $ref: \"#/definitions/ErrorResponse\"\r\n```\r\n\r\nHowever, it can also return a 404 if you try to pull an image and it isn't found in a registry. Here's an example:\r\n\r\n```\r\n$ curl -v --unix-socket /var/run/docker.sock -X POST 'http://localhost/images/create?fromImage=bogus'\r\n...\r\n< HTTP/1.1 404 Not Found\r\n< Api-Version: 1.25\r\n< Content-Length: 75\r\n< Content-Type: application/json\r\n< Date: Wed, 30 Nov 2016 16:54:26 GMT\r\n< Docker-Experimental: true\r\n< Server: Docker/1.13.0-rc2 (linux)\r\n<\r\n{\"message\":\"repository bogus not found: does not exist or no read access\"}\r\n...\r\n```\r\n\r\nThat 404 response should be documented."},{"labels":["documentation",null,null,null],"text":"**Description**\r\n\r\nWhen I read documentation about live-restore option I supposed that it is supported by Docker for Windows. There is no one mention that it is not supported: https://docs.docker.com/engine/admin/live-restore/\r\n\r\nFurthermore, this documentation https://docs.docker.com/engine/reference/commandline/dockerd/ lists this option in \"Windows configuration file\" section. But I was not able to configure it in both ways (as an argument of dockerd and variable of daemon.json)\r\n\r\n**Steps to reproduce the issue:**\r\n1. When I try to run dockerd with --live-restore option I got a error\r\n```\r\nPS C:\\Users\\Administrator> dockerd --live-restore --register-service\r\nStatus: unknown flag: --live-restore\r\nSee 'dockerd --help'., Code: 125\r\n```\r\n2. When I added ``` \"live-restore\": true ``` to the daemon.json and started dockerd I got another error\r\n```\r\nPS C:\\Users\\Administrator> dockerd\r\nunable to configure the Docker daemon with file C:\\ProgramData\\docker\\config\\daemon.json: the following directives don't match any configuration option: live-restore\r\n```\r\n\r\n**Expected behaviour:**\r\nI expect that dockerd should run successfully with this option. Or at least have mention in the documentation that it is not supported\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:      1.12.2-cs2-ws-beta\r\n API version:  1.25\r\n Go version:   go1.7.1\r\n Git commit:   050b611\r\n Built:        Tue Oct 11 02:35:40 2016\r\n OS/Arch:      windows/amd64\r\n\r\nServer:\r\n Version:      1.12.2-cs2-ws-beta\r\n API version:  1.25\r\n Go version:   go1.7.1\r\n Git commit:   050b611\r\n Built:        Tue Oct 11 02:35:40 2016\r\n OS/Arch:      windows/amd64\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nContainers: 1\r\n Running: 0\r\n Paused: 0\r\n Stopped: 1\r\nImages: 1\r\nServer Version: 1.12.2-cs2-ws-beta\r\nStorage Driver: windowsfilter\r\n Windows:\r\nLogging Driver: json-file\r\nPlugins:\r\n Volume: local\r\n Network: nat null overlay\r\nSwarm: inactive\r\nDefault Isolation: process\r\nKernel Version: 10.0 14393 (14393.321.amd64fre.rs1_release_inmarket.161004-2338)\r\nOperating System: Windows Server 2016 Datacenter\r\nOSType: windows\r\nArchitecture: x86_64\r\nCPUs: 2\r\nTotal Memory: 4 GiB\r\nName: EC2AMAZ-P7L1INT\r\nID: 6JSQ:WIFX:QJ25:TM5T:JBOG:CR7M:NSOW:X2AJ:QJXR:PRID:CLE2:6Q67\r\nDocker Root Dir: C:\\ProgramData\\docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https://index.docker.io/v1/\r\nInsecure Registries:\r\n 127.0.0.0/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nIt is a EC2 instance which was launched from \"Windows Server 2016 with containers image\""},{"labels":["documentation",null,null],"text":"https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/#/for-a-direct-lvm-mode-configuration\n\nThis doc informs users to push all images to a remote registry and then informs to rm -f /var/lib/docker, thus removing everything including certs that may be stored under this directory tree.  For good reasons, we instruct users to change from the default devicemapper loopback configuration that we recommended in early releases, of which they maybe running UCP on those nodes.   Deleting the certs in /var/lib/docker/discovery_certs creates a far more complex recovery scenario than just copying the certs back after the lvm changes have been completed.  This is the error received when you try to start up the docker daemon after following the doc instructions on a UCP node.\n\nInitializing discovery with TLS\nError starting daemon: discovery initialization failed (Could not read CA certificate \\ \"/var/lib/docker/discovery_certs/ca.pem\\\" open /var/lib/docker/discovery_certs/ca.pem: no such file or directory\n\nCan additional steps be written into the referenced doc perhaps such as copy the /var/lib/docker directory to /tmp to enable the recovery of items such as certs, etc in order to  keep from  placing users into extensive recovery scenarios.  thnx\n"},{"labels":["documentation",null],"text":"On the `docker service ps` documentation page (https://docs.docker.com/engine/reference/commandline/service_ps/) only three filters are shown as supported: id, name and desired-state. However, I discovered that there is also the \"node\" filter which matches tasks by either the hostname of the node they are deployed on, or the NodeID of that node (confirmed on 1.12.1).\n\nThe API reference for the tasks API correctly mentions that the NodeID can be used as a parameter to the node filter (https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/list-tasks), but omits the fact that the hostname of the node can be also used as the value to that filter\n"},{"labels":["documentation",null],"text":"It is probably the case that the recommended way of working with images and a swarm of nodes is to use a private image registry or docker cloud solution. However, I feel that the current design (and documentation) is a bit incomplete. The problem is that if you are a newcomer and start creating your swarm, when you build your image on one of the swarm leaders, and then starts a service using that image, if the service is scheduled to run on other nodes wheere the image is not available then it will just silently fail. The error claiming that an image is missing is often not complete displayed either when using `docker service ps` since the message is too long, which requires the user to login into the machine where the docker daemon is running and checking the logs there... not very friendly.\nI have not found in the documentation that an image that is custom built must be pushed to a image registry in order to make the swarm work, which for me at least, took some precious time to discover the hard way. \nSo finally, I think the best solution would be if every swarm node could act as a private image registry out of the box, but only for the nodes part of that swarm. By doing that images will just be created once and everything will work magically as so many other things in docker. The other alternative I can think of is that when building an image you could pass a --swarm flag to tell docker that you want to build that same image on all the nodes part of the swarm. The problem with this solution is of course that if the swarm grows, the new nodes will not have that image and fail again silently...\n"},{"labels":["documentation"],"text":"In docker volumes tutorial there is still wrote \n\n```\nBecause of limitations in the mount function, moving subdirectories within the host’s source directory can give access from the container to the host’s file system. This requires a malicious user with access to host and its mounted directory.\n```\n\nThis was fixed last year: http://www.spinics.net/lists/linux-containers/msg31063.html\n"},{"labels":["documentation",null,null],"text":"The actual documentation for the creation of a service using the Docker API is incorrect for the port publishing section (at https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/create-a-service)\n\nThe **Example request** specifies the following:\n\n``` json\nPOST /services/create HTTP/1.1\nContent-Type: application/json\n\n{\n  ...\n  \"EndpointSpec\": {\n    \"Ports\": [\n      {\n        \"Protocol\": \"tcp\",\n        \"PublishedPort\": 8080,\n        \"TargetPort\": 80\n      }\n    ]\n  },\n ...\n}\n```\n\nThis is totally correct, but the **JSON parameters** shows the following:\n\n```\n* Endpoint – Properties that can be configured to access and load balance a service.\n  * Spec –\n    * Mode – The mode of resolution to use for internal load balancing between tasks (vip or dnsrr).\n    * Ports – Exposed ports that this service is accessible on from the outside, in the form of: \"Ports\": { \"<port>/<tcp|udp>: {}\" }\n```\n\nThis is totally incorrect and can lead to confusion.\n"},{"labels":["documentation"],"text":"Rename / move existing docs where needed, and update completion scripts for #26025 and #26716\n"},{"labels":["documentation",null,null],"text":"In the (IMHO really good) [overlay 2 documentation](https://docs.docker.com/engine/userguide/storagedriver/overlayfs-driver) chapter \"Image layering and sharing with OverlayFS (overlay2)\"  is written:\n\n> While the overlay driver only works with a single lower OverlayFS layer and hence requires hard links for implementation of multi-layered images, the overlay2 driver natively supports multiple lower OverlayFS layers (**up to 128**).\n\nWe did the experiment and \"docker build\" failed after **122 layers**.\n\n```\nRemoving intermediate container 1019d67fb4c7\nStep 123 : COPY Dockerfile /tmp/Dockerfile\nmax depth exceeded\n```\n\nI assume that  restriction comes from the 4 KB mount option size. It is mentioned that the shortened IDs have been implemented because of that.\n\nAn overlay2 mount looks like the following:\n\n```\noverlay on /var/lib/docker/overlay2/164d646991e089b06b55a5f1f03ab557a45753204a63b5dcc22991611563017f/merged type overlay \n(rw,relatime,\nlowerdir=l/WUS2HSYXXBUH7KQ5DXYVKY47V7:l/7FR5NGKHKOTQYM5I3W2BBT7RFJ,\nupperdir=164d646991e089b06b55a5f1f03ab557a45753204a63b5dcc22991611563017f/diff,\nworkdir=164d646991e089b06b55a5f1f03ab557a45753204a63b5dcc22991611563017f/work)\n```\n\nFor each layer we need strings like \":l/7FR5NGKHKOTQYM5I3W2BBT7RFJ\" which are 30 characters. In addition to that we have the strings:\n- `rw,relatime,lowerdir=`   -> 22 characters\n- `upperdir=164d646991e089b06b55a5f1f03ab557a45753204a63b5dcc22991611563017f/diff,` -> 80 characters\n- `workdir=164d646991e089b06b55a5f1f03ab557a45753204a63b5dcc22991611563017f/work` -> 78 characters\n\nI did some calculations and could not reproduce exactly the 122 layers. Nevertheless the 128 documented layers seem to be wrong.\n\n**BTW: Two other minor things we noticed:**\n\n> The **lowerest** layer contains the link file which contains\n\nShouldn't this be \"lowest\" :-)\n\n> These shortened identifiers are used for avoid hitting the **page size** limitation on mount arguments.\n\nIMHO page size is a bit unclear. E.g. the Linux kernel has huge pages with different size, for example on s390x we have 1 MiB, 2 GiB. I assume you just mean 4 KB here?\n"},{"labels":["documentation"],"text":"This is an issue with the documentation for setting up TLS in a swarm which can be found [here](https://docs.docker.com/swarm/configure-tls/). In [Step 3](https://docs.docker.com/swarm/configure-tls/#/step-3-create-and-sign-keys) it says that a file called `node-priv.key` will be created which will contain the private key. However, no reference to a `.key` file is found anywhere else in the document. I'm not really sure what changes need to be made, as far as I can tell it crates the private key in a file called `node-priv-key.pem`, but I'm not sure if anything else needed to be done.\n\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\nI'm not able to find the actual markdown file in the `/docs` folder in the repo which contains the guide.\n\n**Steps to reproduce the issue:**\n1. Go through the [swarm/configure-tls](https://docs.docker.com/swarm/configure-tls) guide\n2. Expect a `.key` file to be made at some point\n3. It isn't\n\n**Describe the results you received:**\nThe resulting keys two `.pem` files are created, with one of them suggesting it's the private key.\n\n**Describe the results you expected:**\nDocs to either refer to the private key consistently or have steps for creating the `.key` file\n"},{"labels":["documentation"],"text":"Current man pages are not all consistent in term of content, placement and sections. There is also a lot of _duplication_ between man pages and cli reference documentation as they are maintained separately. This issues is there to discuss on those subjects and follow evolutions of our generation process.\n\nThe main goals are the following and take most of the inspiration from `git` man pages and reference documentation :\n- Have consistent man page across commands.\n- Have cli reference documentation be in sync (or the same) as man page (just a web page for it).\n- Have (or not) man pages generated from source (part of it or all of them).\n### Man page style guide\n\nWe currently don't have any style guide for man pages, i.e. what information to put where and in what order. As code style guide, we might want to define a _man page style guide_ document that would define what makes a good man page for `docker` commands. This is related to #16219.\n\nThat could look something like :\n\n```\n## NAME\n\ncommand - short description of the command\n\n## SYNOPSIS\n\ncommand --with --some --flags --and [OPTIONS] …\n\n##  DESCRIPTION\n\nA description is more detailed than what's in the NAME section, explaining more in depth what the command does (but that's all)\n\n## OPTIONS\n\n-a --all\n\nFlags descriptions …\n\n## ADDITIONAL SECTION 1\n\nAdditional section are possible. An example would be `FILTERING` and `FORMATTING` for the `docker-ps` command, or something about the reference format for command that allow us to set them (like `docker-commit`, `docker-tag`, …).\n\n## ADDITIONAL SECTION 2\n\nNo limit in the number of section though 👼\n\n## EXAMPLES\n\nHere, we just gives example on how to do this on that ; this is not where we explain how to filter container (it's done in its own section), just a working example of it.\n\n## ENVIRONMENT AND CONFIGURATION VARIABLES\n\nIf there is any environment variable or configuration (in the client/daemon config file) related to the command, we'll talk about it here ; like `ImagesFormat` for `docker-images` formatting.\n\n## SEE ALSO\n\n```\n### Man page and cli ref doc maintainability\n\nMaintaining separately man pages and cli reference documentation is error prone, it's easy to forget some flags or description somewhere. It's related to #19088. The basic idea would be to generate man pages and cli reference documentation from the same source.\n\nGiven that we are trying to generate man page from the code (at least partially), we should also study a way to generate cli reference documentation at the same time — shouldn't be difficult to convert markdown/man page to hmtl 👼.\n#23825 introduced a way to generate man page directly from code — from a small subset of command right now, but goal is to do that for all commands.\n\nCurrent limitations / questions are the following (making the assumption that cli reference documentation is generated from the same source) :\n- Cobra does not seems to allow add any custom section (it only supports `EXAMPLE`).  It's possible to add `## Some Section` in the description part, but this would make the `OPTIONS` section be really far down in the man pages (which can be fine :angel:).\n- It makes \"documentation\" PR's becomes more difficult; to make changes to this part of the documentation now requires contributors to touch the actual code.\n- Should we keep around some part of the man pages/documentation in markdown files (that would be merge with generated content from cobra) ?\n\n/cc @thaJeztah @dnephin @icecrime \n"},{"labels":["documentation",null],"text":"Release notes list macvlan as released with 1.12:\n\nhttps://github.com/docker/docker/releases/tag/v1.12.0\nhttps://github.com/docker/docker/pull/23524\n\nOnly docs are in experimental\n\nhttps://github.com/docker/docker/blob/master/experimental/vlan-networks.md#user-content-getting-started\n\nPer @programmerq :\nprogrammerq [9:58 AM]  \nIt could do with a one or two paragraph description of what the macvlan driver is, what technologies it uses, the fact that it is a local scope network driver, and why you might use it instead of another local scope driver (bridge)\n\ncc: @aboch \n"},{"labels":["documentation",null],"text":"In the section \"Adding a host without a driver\" of the document, \"Use Docker Machine to provision hosts on cloud providers\", https://docs.docker.com/machine/get-started-cloud/#adding-a-host-without-a-driver\n\nThere is an example:\n\n``` bash\n$ docker-machine create --url=tcp://50.134.234.20:2376 custombox\n$ docker-machine ls\nNAME        ACTIVE   DRIVER    STATE     URL\ncustombox   *        none      Running   tcp://50.134.234.20:2376\n```\n\nHowever, the command will fail under `docker-machine 0.8.0`, as there should be `-d none`, so the example should be:\n\n``` bash\n$ docker-machine create --driver none --url=tcp://50.134.234.20:2376 custombox\n$ docker-machine ls\nNAME        ACTIVE   DRIVER    STATE     URL\ncustombox   *        none      Running   tcp://50.134.234.20:2376\n```\n"},{"labels":["documentation",null],"text":"https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.25.md needs to be updated. @thaJeztah kindly agreed to take care of this.\n"},{"labels":["documentation",null],"text":"This is in reference to PIDs cgroup limit https://github.com/docker/docker/pull/18697 \n\nI am confused. Whats the default value for PIDs cgroup limit? below link says setting the value to -1 allows unlimited processes to be forked\nRef:  https://docs.docker.com/engine/reference/commandline/run/\n\nBut i was checking on my machine and the default value for it is \"0\" in which situation its allowing me to do unlimited forks. First two containers dont have any PIDs limit set and by default it was '0'.\n\nRef: docker ps --quiet | xargs docker inspect --format '{{ .Id }}: PidsLimit={{ .HostConfig.PidsLimit }}'\n\\60b19cf162aaa262765608f1533d0532c324f03a2f5e1701d5d98392da624802: PidsLimit=0\n6f8a8454a6f23d29163a22ee9533756d706382bbe3ba467f11f80f3e6489fd4b: PidsLimit=0\n8f2ecc51968d29010d6b6dc58bbfcf7fb4dc4ef52b465f04ff1a37e7ed7e734a: PidsLimit=512\n"},{"labels":["documentation",null],"text":"Tried to follow the Docker Remote API 1.24 docs, and found a broken example:\n\nhttps://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/create-a-container\n\nThe example says:\n\n``` json\nPOST /containers/create HTTP/1.1\nContent-Type: application/json\n\n{\n       \"Hostname\": \"\",\n       \"Domainname\": \"\",\n       \"User\": \"\",\n       \"AttachStdin\": false,\n       \"AttachStdout\": true,\n       \"AttachStderr\": true,\n       \"Tty\": false,\n       \"OpenStdin\": false,\n       \"StdinOnce\": false,\n       \"Env\": [\n               \"FOO=bar\",\n               \"BAZ=quux\"\n       ],\n       \"Cmd\": [\n               \"date\"\n       ],\n       \"Entrypoint\": \"\",\n       \"Image\": \"ubuntu\",\n       \"Labels\": {\n               \"com.example.vendor\": \"Acme\",\n               \"com.example.license\": \"GPL\",\n               \"com.example.version\": \"1.0\"\n       },\n       \"Volumes\": {\n         \"/volumes/data\": {}\n       },\n       \"WorkingDir\": \"\",\n       \"NetworkDisabled\": false,\n       \"MacAddress\": \"12:34:56:78:9a:bc\",\n       \"ExposedPorts\": {\n               \"22/tcp\": {}\n       },\n       \"StopSignal\": \"SIGTERM\",\n       \"HostConfig\": {\n         \"Binds\": [\"/tmp:/tmp\"],\n         \"Links\": [\"redis3:redis\"],\n         \"Memory\": 0,\n         \"MemorySwap\": 0,\n         \"MemoryReservation\": 0,\n         \"KernelMemory\": 0,\n         \"CpuPercent\": 80,\n         \"CpuShares\": 512,\n         \"CpuPeriod\": 100000,\n         \"CpuQuota\": 50000,\n         \"CpusetCpus\": \"0,1\",\n         \"CpusetMems\": \"0,1\",\n         \"MaximumIOps\": 0,\n         \"MaximumIOBps\": 0,\n         \"BlkioWeight\": 300,\n         \"BlkioWeightDevice\": [{}],\n         \"BlkioDeviceReadBps\": [{}],\n         \"BlkioDeviceReadIOps\": [{}],\n         \"BlkioDeviceWriteBps\": [{}],\n         \"BlkioDeviceWriteIOps\": [{}],\n         \"MemorySwappiness\": 60,\n         \"OomKillDisable\": false,\n         \"OomScoreAdj\": 500,\n         \"PidMode\": \"\",\n         \"PidsLimit\": -1,\n         \"PortBindings\": { \"22/tcp\": [{ \"HostPort\": \"11022\" }] },\n         \"PublishAllPorts\": false,\n         \"Privileged\": false,\n         \"ReadonlyRootfs\": false,\n         \"Dns\": [\"8.8.8.8\"],\n         \"DnsOptions\": [\"\"],\n         \"DnsSearch\": [\"\"],\n         \"ExtraHosts\": null,\n         \"VolumesFrom\": [\"parent\", \"other:ro\"],\n         \"CapAdd\": [\"NET_ADMIN\"],\n         \"CapDrop\": [\"MKNOD\"],\n         \"GroupAdd\": [\"newgroup\"],\n         \"RestartPolicy\": { \"Name\": \"\", \"MaximumRetryCount\": 0 },\n         \"NetworkMode\": \"bridge\",\n         \"Devices\": [],\n         \"Ulimits\": [{}],\n         \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} },\n         \"SecurityOpt\": [],\n         \"StorageOpt\": {},\n         \"CgroupParent\": \"\",\n         \"VolumeDriver\": \"\",\n         \"ShmSize\": 67108864\n      },\n      \"NetworkingConfig\": {\n      \"EndpointsConfig\": {\n          \"isolated_nw\" : {\n              \"IPAMConfig\": {\n                  \"IPv4Address\":\"172.20.30.33\",\n                  \"IPv6Address\":\"2001:db8:abcd::3033\",\n                  \"LinkLocalIPs:[\"169.254.34.68\", \"fe80::3468\"]\n              },\n              \"Links\":[\"container_1\", \"container_2\"],\n              \"Aliases\":[\"server_x\", \"server_y\"]\n          }\n      }\n  }\n```\n\nThe most obvious typo is that `LinkLocalIPs` is missing closing quote, and as `NetworkingConfig` and `EndpointsConfig` are misaligned at the same indent, so the closing curly bracket are missing as well.\n"},{"labels":["documentation",null],"text":"documentation — fail, can't find reason, for PR #25203, are there anyone meeting this:\n\nBuild Errors:\nFix the path under windows for dockerimages.md: https://github.com/docker/docker/pull/25203\n# Filtered (engine) Summary:\n\n```\nFound: 852 files\nFound: 0 errors\n```\n"},{"labels":["documentation"],"text":"Reference to `docker daemon` v/s `dockerd` is ambiguous in \"Configuring and running Docker on various distributions\" (docs/admin/index.md).  See @mlaventure 's comment on https://github.com/docker/docker/pull/24970/files/88ebfc701238dcfacfaf1d3f0a363848460df452#r71992335\n\nThis article looks like it needs to be reworked for clarity in general. \n"},{"labels":["documentation"],"text":"Current docs/tutorial refer to listen address only. When https://github.com/docker/docker/pull/24237 goes in, need to clarify the distinction between listen address and advertise address and the address auto-detection behavior.\n"},{"labels":["documentation"],"text":"need to review docs for consistent usage of \"swarm\" and \"swarm mode\"\n\n\"swarm\" is not a proper noun. It should not be capitalized except when it comes at the beginning of the sentence.\n\nThe same is true of many of the concepts for \"swarm mode:\"\nworker node, manager node, etc.\n"},{"labels":["documentation",null],"text":"The event documentation needs to be updated for the `health_status:` actions that are now being sent [here](https://github.com/docker/docker/blob/576c9fa2007cfb5f379f67b11cd852800f9175c6/daemon/health.go#L142). \n"},{"labels":["documentation",null,null],"text":"The API uses an \"open schema\", which means that new properties _may_ be added to older API versions if a newer daemon is used. However, it should still be compatible with older clients.\n\nThis is described in the documentation; https://github.com/docker/docker/blob/v1.11.2/docs/reference/api/docker_remote_api.md\n\n> Docker's Remote API uses an open schema model. In this model, unknown properties in incoming messages are ignored. Client applications need to take this behavior into account to ensure they do not break when talking to newer Docker daemons.\n\nThis wording needs some improvement, because it doesn't clearly describe that older API versions can introduce \"unknown\" properties\n"},{"labels":["documentation"],"text":"We temporarily changed those links, because for this release, the docs were published before GA was released; change the links back to their actual URL before GA; see https://github.com/docker/docker/pull/24471\n"},{"labels":["documentation"],"text":"On the [Getting started with swarm mode](https://docs.docker.com/engine/swarm/swarm-tutorial/#/the-ip-address-of-the-manager-machine) page, we just need to remove `Tip: Docker recommends that every node in the cluster be on the same layer 3 (IP) subnet with all traffic permitted between nodes.` towards the bottom of the page since there is no broadcast domain constraint. Ping @thaJeztah \n\nThanks!\n"},{"labels":[null,"documentation",null,null,null,null],"text":"Taken from the CLI reference page of `docker node tasks`:\n\n> The `name` filter matches on all or part of a task's name.\n\nThis is wrong. The filtering is applied on the **exact service name**, see example below.\n\nI think this is actually more useful than filtering on task names and therefore propose to **change the filter name from `name` to `service`**.\n\nFurthermore, filtering should be extended to also cover substring matches, not only exact matches.\n\n``` bash\n$ docker service create --name top busybox top\n7ieif28g1smw7ppav7wiausmz\n\n# a task with name top.1 was created on this node\n$ docker node tasks self\nID                         NAME   SERVICE  IMAGE    LAST STATE          DESIRED STATE  NODE\nbrscnd76m5eh8l3cvmpt5lkj4  top.1  top      busybox  Running 51 seconds  Running        bd4242800842\n\n# filtering by part of the task's name does not find the task\n$ docker node tasks --filter name=to self\nID  NAME  SERVICE  IMAGE  LAST STATE  DESIRED STATE  NODE\n\n# filtering by the task's exact name does not find the task\n$ docker node tasks --filter name=top.1 self\nID  NAME  SERVICE  IMAGE  LAST STATE  DESIRED STATE  NODE\n\n# ==> only filtering by the SERVICE's name finds the task <==\n$ docker node tasks --filter name=top self\nID                         NAME   SERVICE  IMAGE    LAST STATE              DESIRED STATE  NODE\nbrscnd76m5eh8l3cvmpt5lkj4  top.1  top      busybox  Running About a minute  Running        bd4242800842\n\n$ docker version\nClient:\n Version:      1.12.0-dev\n API version:  1.25\n Go version:   go1.6.2\n Git commit:   83e6197-unsupported\n Built:        Tue Jun 28 20:00:19 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.12.0-dev\n API version:  1.25\n Go version:   go1.6.2\n Git commit:   83e6197-unsupported\n Built:        Tue Jun 28 20:00:19 2016\n OS/Arch:      linux/amd64\n```\n"},{"labels":["documentation",null],"text":"According to the engine-api Docs, resource settings are \"NanoCPUs\" and \"MemoryBytes\" - https://godoc.org/github.com/docker/engine-api/types/swarm#Resources\n\nThe corresponding docs for the remote API show simply \"CPU\" and \"Memory\" https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.24.md#create-a-service\n"},{"labels":[null,"documentation"],"text":"On `Docker version 1.11.2, build b9f10c9`, both `POST /v1.23/containers/<container ID>/exec` and `POST /v1.23/exec/<exec ID>/start` have the following body:\n\n``` json\n{\n  \"AttachStderr\": true,\n  \"AttachStdin\": true,\n  \"AttachStdout\": true,\n  \"Cmd\": [\n    \"true\"\n  ],\n  \"Container\": \"<container ID>\",\n  \"Detach\": false,\n  \"DetachKeys\": \"\",\n  \"Privileged\": false,\n  \"Tty\": true,\n  \"User\": \"\"\n}\n```\n\nOn `Docker version 1.12.0-rc2, build 906eacd, experimental`, both `POST /v1.24/containers/<container ID>/exec` and `POST /v1.24/exec/<exec ID>/start` have the following body:\n\n``` json\n{\n  \"AttachStderr\": true,\n  \"AttachStdin\": true,\n  \"AttachStdout\": true,\n  \"Cmd\": [\n    \"true\"\n  ],\n  \"Detach\": false,\n  \"DetachKeys\": \"\",\n  \"Privileged\": false,\n  \"Tty\": true,\n  \"User\": \"\"\n}\n```\n\nNote that the bodies are different -- the v1.23 body includes the container ID. This change doesn't appear in the Remote API documentation.\n"},{"labels":["documentation",null],"text":"When rendering a left nav where selections in that area render pages in the main content area, it's important to have a clear indication in the left nav where the user is.  That is, where the content for the main content area came from.\n\nI imagine I could find many examples of this, but I'm currently looking at [Docker networks feature overview](https://docs.docker.com/engine/userguide/networking/).  I would provide a screenshot, but imagebin isn't working for me right now.\n\nWhat I see on the screen indicates that I might be somewhere in the \"User Guide\" section, and I imagine that I know this page is talking about networking, so I might be somewhere around the \"Network configuration\" section, but the rendering of the left nav isn't helping me here.\n"},{"labels":["documentation",null],"text":"On the documentation page [_Automatically start containers_](https://docs.docker.com/engine/admin/host_integration/) the example for a [systemd](https://freedesktop.org/wiki/Software/systemd/) [service unit](https://docs.docker.com/engine/admin/host_integration/#systemd) with `docker run` is given as below.\n\n```\n[Service]\n…\nExecStart=/usr/bin/docker run --env foo=bar --name redis_server redis\nExecStop=/usr/bin/docker stop -t 2 redis_server ; /usr/bin/docker rm -f redis_server\n…\n```\n\nThis should be changed to the example below, using `ExecStopPost`.\n\n```\n[Service]\n…\nExecStart=/usr/bin/docker run --env foo=bar --name redis_server redis\nExecStop=/usr/bin/docker stop -t 2 redis_server\nExecStopPost=/usr/bin/docker rm -f redis_server\n…\n```\n\nFrom the [manual](https://www.freedesktop.org/software/systemd/man/systemd.service.html):\n\n> `ExecStop=`\n> \n> It is recommended to use this setting for commands that communicate with the service requesting clean termination. When the commands specified with this option are executed it should be assumed that the service is still fully up and is able to react correctly to all commands. For post-mortem clean-up steps use ExecStopPost= instead.\n> \n> `ExecStopPost=`\n> \n> Additional commands that are executed after the service is stopped. This includes cases where the commands configured in ExecStop= were used, where the service does not have any ExecStop= defined, or where the service exited unexpectedly. This argument takes multiple command lines, following the same scheme as described for ExecStart=. Use of these settings is optional. Specifier and environment variable substitution is supported. Note that – unlike ExecStop= – commands specified with this setting are invoked when a service failed to start up correctly and is shut down again.\n> \n> It is recommended to use this setting for clean-up operations that shall be executed even when the service failed to start up correctly. Commands configured with this setting need to be able to operate even if the service failed starting up half-way and left incompletely initialized data around. As the service's processes have been terminated already when the commands specified with this setting are executed they should not attempt to communicate with them.\n\nSo to ensure, that the Docker container is really deleted, and the service will start up again, after an unclean shutdown, that means the Docker container is stopped, but is still around, the Docker container deletion command should be in the directive `ExecStopPost=`.\n"},{"labels":["documentation",null],"text":"Feedback on Debian install from John Clements clements@brinckerhoff.org\n\nOn the Debian install page,\n\nhttps://docs.docker.com/engine/installation/linux/debian/\n\nStep 3 of the update your apt section reads:\n\n $ apt-get purge lxc-docker*\n $ apt-get purge docker.io*\n\nUnfortunately, on some shells, the globber will get ahold of the asterisk and try to find files in the local directory starting with lxc-docker, which isn’t what you want at all. Instead, this should probably read:\n\n $ apt-get purge \"lxc-docker_\"\n $ apt-get purge docker.io_\n\nThen, later, you suggest obtaining the signing key for the project with\n\n $ apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118 E89F3A912897C070ADBF76221572C52609D\n\nThis timed out for me repeatedly, but I tried\n\n $ apt-key adv --keyserver hkp://pool.sks-keyservers.net:80 --recv-keys 58118 E89F3A912897C070ADBF76221572C52609D\n\n… and that worked fine. Dunno if this is a transient failure, but I’m guessing the more general address is probably more robust.\n\nFinally, and this is minor, you might want to replace the calls to `service` with calls to `systemctl`.\n\nThanks!\n\nJohn Clements\n"},{"labels":["documentation",null,null],"text":"The installation instructions currently use `$releasever` to configure the right YUM repository to use, however the instructions don't take into account the `Server` releases output the version in a different format (e.g. `7Server` instead of just `7`).\n\n```\n$ sudo tee /etc/yum.repos.d/docker.repo <<-'EOF'\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n```\n\nThe installation script correctly strips the `Server` before generating the URL, so we should update the documentation to do the same, or show how to;\nhttps://github.com/docker/docker/blob/v1.11.2/hack/install.sh#L283-L285\n\nAs reported by Aleksandr Vinokurov on Twitter https://twitter.com/aleksandrvin/status/740548856316858368 (thanks!)\n\nAlso;\nhttps://access.redhat.com/discussions/2134691\n"},{"labels":["documentation"],"text":"Given there are apparently duplicate properties at the options level and HostConfig level the lack of documentation for the containers/create endpoint in the [Remote API](https://docs.docker.com/engine/reference/api/docker_remote_api_v1.23/#create-a-container) is a blocker.\n\nIn particular, without deep knowledge of the system, how are adopters supposed to differentiate between...\n\n`options.Volumes` vs. `options.HostConfig.Binds`\n\n...or...\n\n`options.ExposedPorts` vs. `options.HostConfig.PortBindings`\n\n...as they appear to be doing the same thing. Probably they are not, but really as [noted before](https://github.com/docker/docker/issues/2949) in an issue not properly closed according to those in the thread, these things are NOT self-describing. If the conventions correspond to equivalent operations documented elsewhere, then this link should be explicit.\n\nHostConfig is not even labelled - what does it do? Isn't the whole JSON-ish structure a host configuration? This is particularly galling since the /run endpoint describes that it 'takes a HostConfig' for backwards compatibility, and links to the documentation for containers/create to clarify, where there is no information describing what the HostConfig object is for, (versus the whole options object, which contains a bunch of host-specific config).\n\nBACKGROUND\n\nI am trying to execute through the Remote API a self-removing (--rm) container configuration already verified interactively through a command line `docker run` invocation. I can't be the only person who works in this way, starting with docker run, and then trying to use an API to automate invocation. \n\nMy attempt to code this through dockerode (backed by the Remote API) very challenging simply because the documentation is so poor (dockerode points to the Remote API documentation as documentation for its own API). \n"},{"labels":["documentation",null,null],"text":"I've tried to wrap my head around the [Docker IPv6 cluster documentation](https://docs.docker.com/engine/userguide/networking/default_network/ipv6/), but something does not work out.\n\nIt seems that both graphics for [Switched network environment](https://docs.docker.com/engine/userguide/networking/default_network/images/ipv6_switched_network_example.svg)\n![image](https://cloud.githubusercontent.com/assets/636669/15391455/90ccb634-1dc1-11e6-917f-a1da758b3777.png)\nand [Routed network environment](https://docs.docker.com/engine/userguide/networking/default_network/images/ipv6_routed_network_example.svg)\n![image](https://cloud.githubusercontent.com/assets/636669/15391479/aa4a9cfc-1dc1-11e6-8a56-db728d0be83e.png)\n are identical (beside some typos) and are not helping with my issue.\n\nMy use case:\n\nI have a /48 network (`2001:XXXX:45::/48`) and routed two /64 subnetworks (`2001:XXXX:45:2::/64` and `2001:XXXX:45:3::/64`) to my host and assigned one of it to the host and the other to the containers. So Host1 has\n\n```\n$ # ip -6 addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qlen 1000\n    inet6 2001:XXXX:45:2::2/64 scope global\n       valid_lft forever preferred_lft forever\n    inet6 fe80::ec4:7aff:fe06:5830/64 scope link\n       valid_lft forever preferred_lft forever\n181: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500\n    inet6 2001:XXXX:45:3::1/64 scope global tentative\n       valid_lft forever preferred_lft forever\n    inet6 fe80::1/64 scope link tentative\n       valid_lft forever preferred_lft forever\n\n\n$ ip -6 route\n2001:XXXX:45:2::/64 dev eth0  proto kernel  metric 256\n2001:XXXX:45:3::/64 dev docker0  proto kernel  metric 256\n2001:XXXX:45:3::/64 dev docker0  metric 1024\nfe80::223:9cff:fefc:6ff0 dev eth0  metric 1024\nfe80::/64 dev eth0  proto kernel  metric 256\nfe80::/64 dev docker0  proto kernel  metric 256\ndefault via fe80::223:9cff:fefc:6ff0 dev eth0  metric 1024\n```\n\nwhile Container1-1, Container1-2, Container1-X get started with\n\n```\nDOCKER_OPTS=\"--dns 8.8.8.8 --dns 8.8.4.4 --fixed-cidr-v6=2001:XXXX:45:3::/64 --ipv6\"\n```\n\nand result in this\n\n```\n$ docker run -it ubuntu:14.04 bash -c 'ip -6 addr; echo '====' ; ip -6 route'\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n190: eth0: <NO-CARRIER,BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500\n    inet6 2001:XXXX:45:3:0:242:ac11:2/64 scope global nodad\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe11:2/64 scope link tentative\n       valid_lft forever preferred_lft forever\n====\n2001:XXXX:45:3::/64 dev eth0  proto kernel  metric 256\nfe80::/64 dev eth0  proto kernel  metric 256\ndefault via 2001:XXXX:45:3::1 dev eth0  metric 1024\n```\n\nMy issue is: I can't make this work without a NDP proxy on the host. The containers are not able to reach outside world, unless i manually add a NDP proxy on my host, e.g.:\n\n```\n$ sysctl net.ipv6.conf.eth0.proxy_ndp=1\n$ ip -6 neigh add proxy 2001:XXXX:45:3:0:242:ac11:2 dev eth0\n```\n\nWhat am i missing? The documentation gives the impression that NDP proxying is only necessary if using a single /64 - and switches/routed environments should work without NDP proxy.\n"},{"labels":["documentation",null,null],"text":"The list of allowed configuration options in [this document](https://docs.docker.com/engine/reference/commandline/daemon/#daemon-configuration-file) is not applicable to Windows. Several of the options do not work on Windows. I think this requires minor clarification, perhaps a sample that is applicable to Windows:\n\nWhen used as is, the following is returned:\n\n``` none\nPS C:\\> dockerd\ntime=\"2016-05-12T15:40:59-07:00\" level=fatal msg=\"unable to configure the Docker daemon with file C:\\\\ProgramData\\\\docker\\\\config\\\\daemon.json: the following directives don't match any configuration option: iptables, bip, icc, api-cors-headers, exec-root, default-gateway-v6, ipv6, cgroup-parent, ip-mask, ip, userland-proxy, default-gateway, ip-forward, selinux-enabled, userns-remap, fixed-cidr-v6\\n\"\n```\n\nHere is a working sample:\n\n``` json\n{\n    \"authorization-plugins\": [],\n    \"dns\": [],\n    \"dns-opts\": [],\n    \"dns-search\": [],\n    \"exec-opts\": [],\n    \"storage-driver\": \"\",\n    \"storage-opts\": [],\n    \"labels\": [],\n    \"log-driver\": \"\", \n    \"mtu\": 0,\n    \"pidfile\": \"\",\n    \"graph\": \"\",\n    \"cluster-store\": \"\",\n    \"cluster-advertise\": \"\",\n    \"debug\": true,\n    \"hosts\": [\"tcp://0.0.0.0:2376\", \"npipe://\"],\n    \"log-level\": \"\",\n    \"tlsverify\": true,\n    \"tlscacert\": \"C:\\\\ProgramData\\\\docker\\\\certs.d\\\\ca.pem\",\n    \"tlscert\": \"C:\\\\ProgramData\\\\docker\\\\certs.d\\\\server-cert.pem\",\n    \"tlskey\": \"C:\\\\ProgramData\\\\docker\\\\certs.d\\\\server-key.pem\",\n    \"group\": \"\",\n    \"default-ulimits\": {},\n    \"bridge\": \"\",\n    \"fixed-cidr\": \"\",\n    \"raw-logs\": false,\n    \"registry-mirrors\": [],\n    \"insecure-registries\": [],\n    \"disable-legacy-registry\": false\n}\n\n```\n\nIf this makes sense, and is in line with the docs scope / purpose, I am happy write up and propose the changes.\n\nneilp\n"},{"labels":["documentation"],"text":"I was just looking into CVE-2015-5157. It seems to be a problem because modify_ldt syscall is allowed by default (https://github.com/docker/docker/blob/master/profiles/seccomp/default.json).\n\nHowever, on the docs page it says it should be blocked by default (https://docs.docker.com/engine/security/seccomp/) because \"Old syscall only used in 16-bit code and a potential information leak.\" which sounds like a sensible reason.\n\nSo why is the call allowed in default.json? \n"},{"labels":["documentation"],"text":"Tutorial  [Work with a development container](https://docs.docker.com/opensource/project/set-up-dev-env/) talks about the **binary** directory. Current development source makes **binary-client** and **binary-daemon**. This modifies a few steps, like the one where we copy the binary file.\n\nThe documentation can benefit from updates also.\n"},{"labels":["documentation",null],"text":"The image spec at https://github.com/docker/docker/blob/3d13fddd2bc4d679f0eaa68b0be877e5a816ad53/image/spec/v1.md hasn't been updated for the content addressability changes that went into Docker 1.10. This spec also seems to contain some outdated fields like \"image checksum\". It shouldn't be very hard to bring this up to date. The changes in 1.10 are described in https://gist.github.com/aaronlehmann/b42a2eaf633fc949f93b#new-image-config.\n\ncc @stevvooe @tonistiigi @dmcgowan \n"},{"labels":["documentation",null],"text":"The JSON for the  [`containers/create`](https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.22.md#create-a-container) endpoint  is missing `NetworkConfig`. API versions 1.22, 1.23 and 1.24 need to be updated to contain this in the example JSON.\n\n/cc @aboch @mavenugo @dnephin  \n"},{"labels":["documentation",null],"text":"On the [official networking documentation page](https://docs.docker.com/engine/userguide/networking/default_network/container-communication/), there's a paragraph that addresses using iptables to restrict access to containers by IP:\n\n> Docker’s forward rules permit all external source IPs by default. To allow only a specific IP or network to access the containers, insert a negated rule at the top of the DOCKER filter chain. For example, to restrict external access such that only source IP 8.8.8.8 can access the containers, the following rule could be added:\n> \n> `$ iptables -I DOCKER -i ext_if ! -s 8.8.8.8 -j DROP`\n\nThis may have been true for older versions of Engine but I can't get this to work in 1.10.3. The rule is applied as advertised, it comes first in iptables -L DOCKER, but the containers are still reachable from the whole outside world. If the docs could be upgraded for the current version of Docker, that'd be very helpful. Thanks!\n"}
]